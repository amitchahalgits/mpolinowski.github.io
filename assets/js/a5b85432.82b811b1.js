"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[58365],{3905:(e,n,a)=>{a.d(n,{Zo:()=>c,kt:()=>g});var t=a(67294);function s(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function r(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function o(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?r(Object(a),!0).forEach((function(n){s(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function i(e,n){if(null==e)return{};var a,t,s=function(e,n){if(null==e)return{};var a,t,s={},r=Object.keys(e);for(t=0;t<r.length;t++)a=r[t],n.indexOf(a)>=0||(s[a]=e[a]);return s}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)a=r[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(s[a]=e[a])}return s}var l=t.createContext({}),p=function(e){var n=t.useContext(l),a=n;return e&&(a="function"==typeof e?e(n):o(o({},n),e)),a},c=function(e){var n=p(e.components);return t.createElement(l.Provider,{value:n},e.children)},m={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},u=t.forwardRef((function(e,n){var a=e.components,s=e.mdxType,r=e.originalType,l=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),u=p(a),g=s,d=u["".concat(l,".").concat(g)]||u[g]||m[g]||r;return a?t.createElement(d,o(o({ref:n},c),{},{components:a})):t.createElement(d,o({ref:n},c))}));function g(e,n){var a=arguments,s=n&&n.mdxType;if("string"==typeof e||s){var r=a.length,o=new Array(r);o[0]=u;var i={};for(var l in n)hasOwnProperty.call(n,l)&&(i[l]=n[l]);i.originalType=e,i.mdxType="string"==typeof e?e:s,o[1]=i;for(var p=2;p<r;p++)o[p]=a[p];return t.createElement.apply(null,o)}return t.createElement.apply(null,a)}u.displayName="MDXCreateElement"},93802:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>p});var t=a(87462),s=(a(67294),a(3905));const r={sidebar_position:4510,slug:"2023-03-26",title:"Tensorflow 2 - Unsupervised Learning",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"Use Autoencoders to Increase Feature Resolution"},o=void 0,i={unversionedId:"IoT-and-Machine-Learning/ML/2023-03-26-tensorflow-unsupervised-learning-autoencoders-super-resolution/index",id:"IoT-and-Machine-Learning/ML/2023-03-26-tensorflow-unsupervised-learning-autoencoders-super-resolution/index",title:"Tensorflow 2 - Unsupervised Learning",description:"Use Autoencoders to Increase Feature Resolution",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-03-26-tensorflow-unsupervised-learning-autoencoders-super-resolution/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-03-26-tensorflow-unsupervised-learning-autoencoders-super-resolution",slug:"/IoT-and-Machine-Learning/ML/2023-03-26-tensorflow-unsupervised-learning-autoencoders-super-resolution/2023-03-26",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-03-26-tensorflow-unsupervised-learning-autoencoders-super-resolution/2023-03-26",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-03-26-tensorflow-unsupervised-learning-autoencoders-super-resolution/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Tensorflow",permalink:"/docs/tags/tensorflow"}],version:"current",sidebarPosition:4510,frontMatter:{sidebar_position:4510,slug:"2023-03-26",title:"Tensorflow 2 - Unsupervised Learning",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"Use Autoencoders to Increase Feature Resolution"},sidebar:"tutorialSidebar",previous:{title:"Tensorflow 2 - Unsupervised Learning",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-03-26-tensorflow-unsupervised-learning-generative-adversial-networks/2023-03-26"},next:{title:"Tensorflow 2 - Unsupervised Learning",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-03-24-tensorflow-unsupervised-learning-autoencoders/2023-03-24"}},l={},p=[{value:"Autoencoder Super-Resolution",id:"autoencoder-super-resolution",level:2},{value:"Load and Preprocess the Dataset",id:"load-and-preprocess-the-dataset",level:2},{value:"Build the Autoencoder",id:"build-the-autoencoder",level:2}],c={toc:p};function m(e){let{components:n,...r}=e;return(0,s.kt)("wrapper",(0,t.Z)({},c,r,{components:n,mdxType:"MDXLayout"}),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"Victoria Harbour, Hongkong",src:a(3551).Z,width:"2385",height:"1054"})),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"#tensorflow-unsupervised-learning"},"Tensorflow Unsupervised Learning"),(0,s.kt)("ul",{parentName:"li"},(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"#autoencoder-super-resolution"},"Autoencoder Super-Resolution")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"#load-and-preprocess-the-dataset"},"Load and Preprocess the Dataset")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"#build-the-autoencoder"},"Build the Autoencoder")))),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"#re-run-with-a-lower-compression-factor"},"Re-run with a lower Compression Factor"))),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},"See also:")),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Fun, fun, tensors: ",(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-19-tensorflow-introduction/2023-02-19"},"Tensor Constants, Variables and Attributes"),", ",(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-21-tensorflow-tensors-2/2023-02-21"},"Tensor Indexing, Expanding and Manipulations"),", ",(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-22-tensorflow-tensors-3/2023-02-22"},"Matrix multiplications, Squeeze, One-hot and Numpy")),(0,s.kt)("li",{parentName:"ul"},"Tensorflow 2 - Neural Network Regression: ",(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-23-tensorflow-neural-network-regression/2023-02-23"},"Building a Regression Model"),", ",(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-24-tensorflow-neural-network-regression-evaluation/2023-02-24"},"Model Evaluation"),", ",(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-25-tensorflow-neural-network-regression-experiments/2023-02-25"},"Model Optimization"),", ",(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-26-tensorflow-neural-network-regression-real-dataset/2023-02-26"},'Working with a "Real" Dataset'),", ",(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-26-tensorflow-neural-network-regression-data-preprocessing/2023-02-26"},"Feature Scaling")),(0,s.kt)("li",{parentName:"ul"},"Tensorflow 2 - Neural Network Classification: ",(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-27-tensorflow-neural-network-classification/2023-02-27"},"Non-linear Data and Activation Functions"),", ",(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-28-tensorflow-neural-network-classification-model-evaluation/2023-02-28"},"Model Evaluation and Performance Improvement"),", ",(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-03-02-tensorflow-neural-network-multi-classification/2023-03-02"},"Multiclass Classification Problems")),(0,s.kt)("li",{parentName:"ul"},"Tensorflow 2 - Convolutional Neural Networks: ",(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-03-03-tensorflow-convolutional-neural-network-binary-classifications/2023-03-03"},"Binary Image Classification"),", ",(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-03-05-tensorflow-convolutional-neural-network-multiclass-classifications/2023-03-05"},"Multiclass Image Classification")),(0,s.kt)("li",{parentName:"ul"},"Tensorflow 2 - Transfer Learning: ",(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-03-06-tensorflow-transfer-learning-feature-extraction/2023-03-06"},"Feature Extraction"),", ",(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-03-11-tensorflow-transfer-learning-fine-tuning/2023-03-11"},"Fine-Tuning"),", ",(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-03-16-tensorflow-transfer-learning-scaling/2023-03-16"},"Scaling")),(0,s.kt)("li",{parentName:"ul"},"Tensorflow 2 - Unsupervised Learning: ",(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-03-24-tensorflow-unsupervised-learning-autoencoders/2023-03-24"},"Autoencoder Feature Detection"),", ",(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-03-26-tensorflow-unsupervised-learning-autoencoders-super-resolution/2023-03-26"},"Autoencoder Super-Resolution"),", ",(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-03-26-tensorflow-unsupervised-learning-generative-adversial-networks/2023-03-26"},"Generative Adverserial Networks"))),(0,s.kt)("h1",{id:"tensorflow-unsupervised-learning"},"Tensorflow Unsupervised Learning"),(0,s.kt)("h2",{id:"autoencoder-super-resolution"},"Autoencoder Super-Resolution"),(0,s.kt)("p",null,"Train an ",(0,s.kt)("a",{parentName:"p",href:"https://mpolinowski.github.io/docs/IoT-and-Machine-Learning/ML/2023-03-24-tensorflow-unsupervised-learning-autoencoders/2023-03-24"},"Autoencoder Convolutional Network")," to upscale noisy, compressed images."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"import cv2\nimport glob\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pathlib\nimport random\nimport shutil\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, UpSampling2D, Dropout\nfrom tensorflow.keras.models import Model\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'BATCH_SIZE = 32\nSEED = 42\nIMAGE_SIZE = 250\nDATA_DIR = "../datasets/Labeled_Faces_in_the_Wild_Home/lfw"\n')),(0,s.kt)("p",null,"Download the ",(0,s.kt)("a",{parentName:"p",href:"https://vis-www.cs.umass.edu/lfw/"},"Labeled Faces in the Wild")," Dataset"),(0,s.kt)("blockquote",null,(0,s.kt)("p",{parentName:"blockquote"},(0,s.kt)("inlineCode",{parentName:"p"},"wget http://vis-www.cs.umass.edu/lfw/lfw.tgz"))),(0,s.kt)("h2",{id:"load-and-preprocess-the-dataset"},"Load and Preprocess the Dataset"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'# inspect a few images\ndef view_random_image(target_dir):\n    target_folder = str(target_dir)\n    random_image = random.sample(os.listdir(target_folder), 1)\n    \n    img = mpimg.imread(target_folder + "/" + random_image[0])\n    plt.imshow(img)\n    plt.title(str(img.shape))\n    plt.axis("off")\n    \n    return tf.constant(img)\n\nfig = plt.figure(figsize=(12, 6))\nplot1 = fig.add_subplot(1, 3, 1)\nimage_0 = view_random_image(target_dir = DATA_DIR)\nplot2 = fig.add_subplot(1, 3, 2)\nimage_1 = view_random_image(target_dir = DATA_DIR)\nplot2 = fig.add_subplot(1, 3, 3)\nimage_2 = view_random_image(target_dir = DATA_DIR)\n')),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"Autoencoder Super-Resolution",src:a(31097).Z,width:"950",height:"315"})),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# since we are not going to classify this dataset\n# we can copy all files into the same folder\ndataset = glob.glob(DATA_DIR+'/**/*.jpg') #returns path of images\nprint(len(dataset), dataset[:1])\n# 13233 ['../datasets/Labeled_Faces_in_the_Wild_Home/lfw/Aaron_Eckhart/Aaron_Eckhart_0001.jpg']\n\nfor file in dataset:\n    shutil.move(file, DATA_DIR)\n\nimages = glob.glob(DATA_DIR+'/*.jpg')\nprint(len(images), images[:1])\n# 13233 ['../datasets/Labeled_Faces_in_the_Wild_Home/lfw/Aaron_Eckhart_0001.jpg']\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# now with the images in place we can split them\nimages = glob.glob(DATA_DIR+'/*.jpg')\n\ntrain_X, val_X = train_test_split(images, test_size=0.2, random_state=SEED)\nprint(len(train_X), len(val_X))\n# 10586 2647\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'# the dataset now needs to be split into\n# a set of the source images and a set of\n# low resolution versions of those images\ndef get_training_data_48(images_location):\n  y_source_image = []\n  X_lowres_image = []\n\n  for img in os.listdir(images_location):\n    try:\n      image = cv2.imread(f"{images_location}/{img}", cv2.IMREAD_UNCHANGED)\n      reshaped_image = cv2.resize(image, (144, 144))\n      image_rgb = cv2.cvtColor(reshaped_image, cv2.COLOR_BGR2RGB)\n      if reshaped_image.shape[-1] == 3:\n        y_source_image.append(image_rgb)\n\n      image = cv2.resize(image, (48, 48))\n      reshaped_image = cv2.resize(image, (144, 144))\n      image_rgb = cv2.cvtColor(reshaped_image, cv2.COLOR_BGR2RGB)\n      if reshaped_image.shape[-1] == 3:\n        X_lowres_image.append(image_rgb)\n\n    except Exception as e:\n      # print(str(e))\n      pass\n      \n\n  return (np.array(X_lowres_image), np.array(y_source_image))\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"lowres_images, source_images = get_training_data_48(DATA_DIR)\nprint(lowres_images.shape, source_images.shape)\n# (13233, 144, 144, 3) (13233, 144, 144, 3)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'plt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.title("Original Image")\nplt.axis(False)\nplt.imshow(source_images[13131])\n\nplt.subplot(1, 2, 2)\nplt.title("Low-res Image")\nplt.axis(False)\nplt.imshow(lowres_images[13131])\n')),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"Autoencoder Super-Resolution",src:a(79427).Z,width:"950",height:"465"})),(0,s.kt)("h2",{id:"build-the-autoencoder"},"Build the Autoencoder"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"input_img = tf.keras.layers.Input(shape=(144, 144, 3))\n\nl1 = Conv2D(64, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-10))(input_img)\nl2 = Conv2D(64, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-10))(l1)\nl3 = MaxPool2D(padding='same')(l2)\n\nl4 = Conv2D(128, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-10))(l3)\nl5 = Conv2D(128, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-10))(l4)\nl6 = MaxPool2D(padding='same')(l5)\n\nl7 = Conv2D(256, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-10))(l6)\n\nl8 = UpSampling2D()(l7)\nl9 = Conv2D(128, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-10))(l8)\nl10 = Conv2D(128, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-10))(l9)\n\nl11 = tf.keras.layers.add([l10, l5])\n\nl12 = UpSampling2D()(l11)\nl13 = Conv2D(64, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-10))(l12)\nl14 = Conv2D(64, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-10))(l13)\n\nl15 = tf.keras.layers.add([l14, l2])\n\ndecoded_image = Conv2D(3, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-10))(l15)\n\nauto_encoder = Model(inputs=(input_img), outputs=decoded_image)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"tf.random.set_seed=SEED\nauto_encoder.compile(optimizer='adadelta', loss='mean_squared_error')\nauto_encoder.summary()\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"auto_encoder.fit(lowres_images,\n                 source_images,\n                 epochs=25,\n                 batch_size=BATCH_SIZE,\n                 shuffle=True)\n\n# Epoch 1/5\n# 229s 525ms/step - loss: 6488.4028\n# Epoch 2/25\n# 213s 514ms/step - loss: 4692.7637\n# Epoch 3/25\n# 211s 511ms/step - loss: 1421.0702\n# Epoch 4/25\n# 211s 510ms/step - loss: 543.7433\n# Epoch 5/25\n# 211s 509ms/step - loss: 414.3141\n# Epoch 6/25\n# 194s 468ms/step - loss: 357.2447\n# Epoch 7/25\n# 194s 467ms/step - loss: 320.0330\n# Epoch 8/25\n# 193s 467ms/step - loss: 294.0082\n# Epoch 9/25\n# 193s 467ms/step - loss: 274.6338\n# Epoch 10/25\n# 193s 467ms/step - loss: 259.9402\n# Epoch 11/25\n# 193s 467ms/step - loss: 248.2195\n# Epoch 12/25\n# 193s 467ms/step - loss: 238.6665\n# Epoch 13/25\n# 193s 467ms/step - loss: 230.7052\n# Epoch 14/25\n# 193s 467ms/step - loss: 224.1879\n# Epoch 15/25\n# 193s 467ms/step - loss: 218.4927\n# Epoch 16/25\n# 193s 467ms/step - loss: 213.6586\n# Epoch 17/25\n# 193s 467ms/step - loss: 209.4044\n# Epoch 18/25\n# 193s 467ms/step - loss: 205.5302\n# Epoch 19/25\n# 193s 467ms/step - loss: 202.2275\n# Epoch 20/25\n# 193s 467ms/step - loss: 199.2723\n# Epoch 21/25\n# 193s 467ms/step - loss: 196.5294\n# Epoch 22/25\n# 193s 467ms/step - loss: 187.7074\n# Epoch 23/25\n# 193s 467ms/step - loss: 171.8884\n# Epoch 24/25\n# 193s 467ms/step - loss: 169.2720\n# Epoch 25/25\n# 193s 467ms/step - loss: 167.2259\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"upscaled_images = auto_encoder.predict(lowres_images[0:9])\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'# plot results\nplt.figure(figsize=(12, 12))\n# ROW 1\nplt.subplot(3, 3, 1)\nplt.title("Original Image")\nplt.axis(False)\nplt.imshow(source_images[3])\nplt.subplot(3, 3, 2)\nplt.title("Low-Res Image")\nplt.axis(False)\nplt.imshow(lowres_images[3])\nplt.subplot(3, 3, 3)\nplt.title("Upscaled Image")\nplt.axis(False)\nplt.imshow(upscaled_images[3]/255)\n# ROW 2\nplt.subplot(3, 3, 4)\nplt.title("Original Image")\nplt.axis(False)\nplt.imshow(source_images[4])\nplt.subplot(3, 3, 5)\nplt.title("Low-Res Image")\nplt.axis(False)\nplt.imshow(lowres_images[4])\nplt.subplot(3, 3, 6)\nplt.title("Upscaled Image")\nplt.axis(False)\nplt.imshow(upscaled_images[4]/255)\n# ROW 3\nplt.subplot(3, 3, 7)\nplt.title("Original Image")\nplt.axis(False)\nplt.imshow(source_images[5])\nplt.subplot(3, 3, 8)\nplt.title("Low-Res Image")\nplt.axis(False)\nplt.imshow(lowres_images[5])\nplt.subplot(3, 3, 9)\nplt.title("Upscaled Image")\nplt.axis(False)\nplt.imshow(upscaled_images[5]/255)\n')),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"Autoencoder Super-Resolution",src:a(3546).Z,width:"948",height:"966"})),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"Autoencoder Super-Resolution",src:a(10142).Z,width:"948",height:"966"})),(0,s.kt)("h1",{id:"re-run-with-a-lower-compression-factor"},"Re-run with a lower Compression Factor"),(0,s.kt)("p",null,"In this example the reconstructed image looks very close to the low-resolution input image - which is already impressive knowing how much this information has been condensed. The original paper this is based on used larger 224x224px input images with a lower compression factor than the example above:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"144x144px input images"),(0,s.kt)("li",{parentName:"ul"},"Compressed to 48x48px"),(0,s.kt)("li",{parentName:"ul"},"Inflated back to 144x144px")),(0,s.kt)("p",null,"I cannot go higher with the resolution of the input image since this makes my graphic card very unhappy :(. But I can decrease the compression factor to give my model more details to work with - ",(0,s.kt)("inlineCode",{parentName:"p"},"144")," -> ",(0,s.kt)("inlineCode",{parentName:"p"},"72")," -> ",(0,s.kt)("inlineCode",{parentName:"p"},"144"),":"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'def get_training_data_72(images_location):\n  y_source_image = []\n  X_lowres_image = []\n\n  for img in os.listdir(images_location):\n    try:\n      image = cv2.imread(f"{images_location}/{img}", cv2.IMREAD_UNCHANGED)\n      reshaped_image = cv2.resize(image, (144, 144))\n      image_rgb = cv2.cvtColor(reshaped_image, cv2.COLOR_BGR2RGB)\n      if reshaped_image.shape[-1] == 3:\n        y_source_image.append(image_rgb)\n\n      image = cv2.resize(image, (72, 72))\n      reshaped_image = cv2.resize(image, (144, 144))\n      image_rgb = cv2.cvtColor(reshaped_image, cv2.COLOR_BGR2RGB)\n      if reshaped_image.shape[-1] == 3:\n        X_lowres_image.append(image_rgb)\n\n    except Exception as e:\n      # print(str(e))\n      pass\n      \n\n  return (np.array(X_lowres_image), np.array(y_source_image))\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"lowres_images_72, source_images_72 = get_training_data_72(DATA_DIR)\nprint(lowres_images_72.shape, source_images_72.shape)\n# (13233, 144, 144, 3) (13233, 144, 144, 3)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'plt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.title("Original Image")\nplt.axis(False)\nplt.imshow(source_images_72[13131])\n\nplt.subplot(1, 2, 2)\nplt.title("Low-res Image")\nplt.axis(False)\nplt.imshow(lowres_images_72[13131])\n')),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"Autoencoder Super-Resolution",src:a(2475).Z,width:"950",height:"465"})),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"auto_encoder.fit(lowres_images_72,\n                 source_images_72,\n                 epochs=25,\n                 batch_size=BATCH_SIZE,\n                 shuffle=True)\n\n# Epoch 1/25\n# 262s 633ms/step - loss: 103.1866\n# Epoch 2/25\n# 229s 553ms/step - loss: 97.3206\n# Epoch 3/25\n# 229s 553ms/step - loss: 93.4157\n# Epoch 4/25\n# 229s 553ms/step - loss: 90.4040\n# Epoch 5/25\n# 232s 560ms/step - loss: 87.6966\n# Epoch 6/25\n# 261s 631ms/step - loss: 85.4864\n# Epoch 7/25\n# 244s 589ms/step - loss: 83.4865\n# Epoch 8/25\n# 230s 555ms/step - loss: 81.8133\n# Epoch 9/25\n# 230s 556ms/step - loss: 80.1447\n# Epoch 10/25\n# 228s 550ms/step - loss: 78.6784\n# Epoch 11/25\n# 232s 560ms/step - loss: 77.3202\n# Epoch 12/25\n# 223s 538ms/step - loss: 76.0533\n# Epoch 13/25\n# 222s 536ms/step - loss: 74.9465\n# Epoch 14/25\n# 212s 512ms/step - loss: 73.8465\n# Epoch 15/25\n# 228s 552ms/step - loss: 72.7948\n# Epoch 16/25\n# 256s 618ms/step - loss: 71.9504\n# Epoch 17/25\n# 240s 579ms/step - loss: 71.0116\n# Epoch 18/25\n# 252s 608ms/step - loss: 70.2642\n# Epoch 19/25\n# 270s 652ms/step - loss: 69.4523\n# Epoch 20/25\n# 266s 642ms/step - loss: 68.6822\n# Epoch 21/25\n# 262s 632ms/step - loss: 67.9245\n# Epoch 22/25\n# 321s 776ms/step - loss: 67.3510\n# Epoch 23/25\n# 325s 784ms/step - loss: 66.6649\n# Epoch 24/25\n# 260s 627ms/step - loss: 66.0176\n# Epoch 25/25\n# 273s 660ms/step - loss: 65.4639\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"upscaled_images_72 = auto_encoder.predict(lowres_images_72[0:9])\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'# plot results\nplt.figure(figsize=(12, 12))\n# ROW 1\nplt.subplot(3, 3, 1)\nplt.title("Original Image")\nplt.axis(False)\nplt.imshow(source_images_72[0])\nplt.subplot(3, 3, 2)\nplt.title("Low-Res Image")\nplt.axis(False)\nplt.imshow(lowres_images_72[0])\nplt.subplot(3, 3, 3)\nplt.title("Upscaled Image")\nplt.axis(False)\nplt.imshow(upscaled_images_72[0]/255)\n# ROW 2\nplt.subplot(3, 3, 4)\nplt.title("Original Image")\nplt.axis(False)\nplt.imshow(source_images_72[1])\nplt.subplot(3, 3, 5)\nplt.title("Low-Res Image")\nplt.axis(False)\nplt.imshow(lowres_images_72[1])\nplt.subplot(3, 3, 6)\nplt.title("Upscaled Image")\nplt.axis(False)\nplt.imshow(upscaled_images_72[1]/255)\n# ROW 3\nplt.subplot(3, 3, 7)\nplt.title("Original Image")\nplt.axis(False)\nplt.imshow(source_images_72[2])\nplt.subplot(3, 3, 8)\nplt.title("Low-Res Image")\nplt.axis(False)\nplt.imshow(lowres_images_72[2])\nplt.subplot(3, 3, 9)\nplt.title("Upscaled Image")\nplt.axis(False)\nplt.imshow(upscaled_images_72[2]/255)\n')),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"Autoencoder Super-Resolution",src:a(8829).Z,width:"948",height:"966"})),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"Autoencoder Super-Resolution",src:a(1570).Z,width:"948",height:"966"})),(0,s.kt)("p",null,"Now this does look a lot better. There is a clear improvement over the low-resolution image. But it is not yet directly compareable in details to the source image."))}m.isMDXComponent=!0},31097:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/05_Tensorflow_Unsupervised_Learning_12-703c6cbbb9e96d542e0b92abebb9366f.png"},79427:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/05_Tensorflow_Unsupervised_Learning_13-d6100fe4b7ebe256dde206c7ed78fe98.png"},3546:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/05_Tensorflow_Unsupervised_Learning_14-537f26f6d49163fdaf7d502c2b91a39f.png"},10142:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/05_Tensorflow_Unsupervised_Learning_15-0367b99b6d517e5f31e1ea20ef87dfc9.png"},2475:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/05_Tensorflow_Unsupervised_Learning_16-050375e1850251760aae918d9f8372c8.png"},8829:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/05_Tensorflow_Unsupervised_Learning_17-73d5a5ed69a32e574060560961587779.png"},1570:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/05_Tensorflow_Unsupervised_Learning_18-1d6db270b7d3491f12e4fe127ec512e1.png"},3551:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-da0f4433cfd061cf6ed148c34ca4eb14.jpg"}}]);