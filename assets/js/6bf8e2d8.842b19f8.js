"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[35937],{681009:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>t,metadata:()=>s,toc:()=>d});var r=o(785893),i=o(603905);const t={sidebar_position:4690,slug:"2023-02-18",title:"Keras for Tensorflow - VGG16 Network Architecture",authors:"mpolinowski",tags:["Python","Machine Learning","Keras","Tensorflow"],description:"An example convolutional neural network is the VGG16 Architecture."},a=void 0,s={id:"IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-vgg16/index",title:"Keras for Tensorflow - VGG16 Network Architecture",description:"An example convolutional neural network is the VGG16 Architecture.",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-vgg16/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-vgg16",slug:"/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-vgg16/2023-02-18",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-vgg16/2023-02-18",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-vgg16/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Keras",permalink:"/docs/tags/keras"},{label:"Tensorflow",permalink:"/docs/tags/tensorflow"}],version:"current",sidebarPosition:4690,frontMatter:{sidebar_position:4690,slug:"2023-02-18",title:"Keras for Tensorflow - VGG16 Network Architecture",authors:"mpolinowski",tags:["Python","Machine Learning","Keras","Tensorflow"],description:"An example convolutional neural network is the VGG16 Architecture."},sidebar:"tutorialSidebar",previous:{title:"Tensorflow 2 - An (Re)Introduction 2023",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-02-19-tensorflow-introduction/2023-02-19"},next:{title:"Keras for Tensorflow - Recurrent Neural Networks",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-rnn/2023-02-18"}},l={},d=[{value:"Very Deep Convolutional Networks",id:"very-deep-convolutional-networks",level:2},{value:"Building the VGG16 Model",id:"building-the-vgg16-model",level:3},{value:"Training the VGG16 Model",id:"training-the-vgg16-model",level:3}];function c(e){const n={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.ah)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Guangzhou, China",src:o(495403).Z+"",width:"2830",height:"1272"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"#very-deep-convolutional-networks",children:"Very Deep Convolutional Networks"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#building-the-vgg16-model",children:"Building the VGG16 Model"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#training-the-vgg16-model",children:"Training the VGG16 Model"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://github.com/mpolinowski/tf-keras-2023",children:"Github Repository"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://keras.io/getting_started/",children:"Keras"})," is built on top of TensorFlow 2 and provides an API designed for human beings. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear & actionable error messages."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.em,{children:"See also:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-02-14-keras-introduction/2023-02-14",children:"Keras for Tensorflow - An (Re)Introduction 2023"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-02-16-keras-introduction-ann/2023-02-16",children:"Keras for Tensorflow - Artificial Neural Networks"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-02-17-keras-introduction-cnn/2023-02-17",children:"Keras for Tensorflow - Convolutional Neural Networks"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-vgg16/2023-02-18",children:"Keras for Tensorflow - VGG16 Network Architecture"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-rnn/2023-02-18",children:"Keras for Tensorflow - Recurrent Neural Networks"})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"very-deep-convolutional-networks",children:"Very Deep Convolutional Networks"}),"\n",(0,r.jsx)(n.h3,{id:"building-the-vgg16-model",children:"Building the VGG16 Model"}),"\n",(0,r.jsxs)(n.p,{children:["An example convolutional neural network is the ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1409.1556",children:"VGG16 Architecture"}),". The number 16 in the name VGG refers to the fact that it is 16 layers deep neural network (VGGnet - ",(0,r.jsx)(n.a,{href:"https://www.pyimagesearch.com/wp-content/uploads/2017/03/imagenet_vgg16.png",children:"Image Source"}),")."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Keras for Tensorflow - Convolutional Neural Networks",src:o(694414).Z+"",width:"470",height:"276"})}),"\n",(0,r.jsxs)(n.p,{children:["The VGG16 Model starts with an colour (",(0,r.jsx)(n.code,{children:"3"})," colour channels) image input of ",(0,r.jsx)(n.code,{children:"224x224"})," pixels and keeps applying filters to increase its depth. While using pooling layers to reduce its dimensions. The output layer end with a shape of ",(0,r.jsx)(n.code,{children:"1x1x1000"})," and uses a ",(0,r.jsx)(n.code,{children:"softmax"})," activation - meaning, the model will assign probabilities for 1000 classes."]}),"\n",(0,r.jsx)(n.p,{children:"To rebuild this model in Keras:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:"import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\n# defining the input shape\n# starting with a 224x224 colour image\ninput_shape = (224, 224, 3)\n\n# building the model\nmodel = Sequential()\n## convolutional layers with 64 filters + pooling => 224 x 224 x 64\nmodel.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu', input_shape=input_shape))\nmodel.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n## randomly drop 25% of neurons to prevent overfitting\nmodel.add(Dropout(0.25))\n## convolutional layers with 128 filters + pooling => 112 x 112 x 128\nmodel.add(Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n## randomly drop 10% of neurons to prevent overfitting\nmodel.add(Dropout(0.10))\n## convolutional layers with 256 filters + pooling => 56 x 56 x 256\nmodel.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n## randomly drop 10% of neurons to prevent overfitting\nmodel.add(Dropout(0.10))\n## convolutional layers with 256 filters + pooling => 28 x 28 x 512\nmodel.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n## randomly drop 10% of neurons to prevent overfitting\nmodel.add(Dropout(0.10))\n## convolutional layers with 256 filters + pooling => 14 x 14 x 512\nmodel.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n## randomly drop 10% of neurons to prevent overfitting\nmodel.add(Dropout(0.10))\n## flatten before dense layer => 1 x 1 x 4096\nmodel.add(Flatten())\nmodel.add(Dense(4096, activation='relu'))\nmodel.add(Dropout(0.25))\n# output layer assigns probability of 1000 classes\nmodel.add(Dense(1000, activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'Model: "sequential"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 224, 224, 64)      1792      \n                                                                 \n conv2d_1 (Conv2D)           (None, 224, 224, 64)      36928     \n                                                                 \n max_pooling2d (MaxPooling2D)  (None, 112, 112, 64)     0\n                                                                 \n dropout (Dropout)           (None, 112, 112, 64)      0         \n                                                                 \n conv2d_2 (Conv2D)           (None, 112, 112, 128)     73856     \n                                                                 \n conv2d_3 (Conv2D)           (None, 112, 112, 128)     147584    \n                                                                 \n max_pooling2d_1 (MaxPooling2D)  (None, 56, 56, 128)      0        \n                                                                 \n dropout_1 (Dropout)         (None, 56, 56, 128)       0         \n                                                                 \n conv2d_4 (Conv2D)           (None, 56, 56, 256)       295168    \n                                                                 \n conv2d_5 (Conv2D)           (None, 56, 56, 256)       590080    \n                                                                 \n conv2d_6 (Conv2D)           (None, 56, 56, 256)       590080    \n                                                                 \n max_pooling2d_2 (MaxPooling2D)  (None, 28, 28, 256)      0         \n                                                                 \n dropout_2 (Dropout)         (None, 28, 28, 256)       0         \n                                                                 \n conv2d_7 (Conv2D)           (None, 28, 28, 512)       1180160   \n                                                                 \n conv2d_8 (Conv2D)           (None, 28, 28, 512)       2359808   \n                                                                 \n conv2d_9 (Conv2D)           (None, 28, 28, 512)       2359808   \n                                                                 \n max_pooling2d_3 (MaxPooling2D)  (None, 14, 14, 512)      0         \n                                                                 \n dropout_3 (Dropout)         (None, 14, 14, 512)       0         \n                                                                 \n conv2d_10 (Conv2D)          (None, 14, 14, 512)       2359808   \n                                                                 \n conv2d_11 (Conv2D)          (None, 14, 14, 512)       2359808   \n                                                                 \n conv2d_12 (Conv2D)          (None, 14, 14, 512)       2359808   \n                                                                 \n max_pooling2d_4 (MaxPooling2D)  (None, 7, 7, 512)        0\n                                                                 \n dropout_4 (Dropout)         (None, 7, 7, 512)         0         \n                                                                 \n flatten (Flatten)           (None, 25088)             0         \n                                                                 \n dense (Dense)               (None, 4096)              102764544 \n                                                                 \n dropout_5 (Dropout)         (None, 4096)              0         \n                                                                 \n dense_1 (Dense)             (None, 1000)              4097000   \n                                                                 \n=================================================================\nTotal params: 121,576,232\nTrainable params: 121,576,232\nNon-trainable params: 0\n_________________________________________________________________\n'})}),"\n",(0,r.jsx)(n.h3,{id:"training-the-vgg16-model",children:"Training the VGG16 Model"}),"\n",(0,r.jsxs)(n.p,{children:["Instead of training this fresh model we can use Keras to download a pre-trained version of it, giving us a head start. The following code will download the ",(0,r.jsx)(n.a,{href:"https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5",children:"pre-training weights"})," for the VGG16 model:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:"# using the pre-trained vgg16 instead of a fresh version\nfrom tensorflow.keras.applications.vgg16 import VGG16\nvgg16 = VGG16()\n"})}),"\n",(0,r.jsx)(n.p,{children:"This helped us skipping the training. Now we can go straight to doing predictions. Again, Keras helps us with providing helper functions to load and preprocess a test image and run a prediction on it:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:"from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# using the pre-trained vgg16 instead of a fresh version\nvgg16 = VGG16()\n\n# load image for prediction\nimg = load_img('HK-LR2020_76.jpg', target_size=(224, 224))\nimg = img_to_array(img)\nimg = img.reshape(1,224,224,3)\n\n# run prediction\nyhat = vgg16.predict(img)\nlabel = decode_predictions(yhat)\nprediction = label[0][0]\nprint(prediction[1])\n"})}),"\n",(0,r.jsx)(n.p,{children:"Well, the training went in the right direction. But needs to be optimized for your specific use-case:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Keras for Tensorflow - Convolutional Neural Networks",src:o(59378).Z+"",width:"1444",height:"411"})}),"\n",(0,r.jsxs)(n.p,{children:["The model has been trained on the ImageNet dataset. We can check the included classes by the reading ",(0,r.jsx)(n.a,{href:"https://gist.github.com/mpolinowski/6cf2c314e8e2ad53ca24357d064dcba6",children:"following file"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cat ~/.keras/models/imagenet_class_index.json\n"})}),"\n",(0,r.jsx)(n.p,{children:"One of the included classes is:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'"808": [\n    "n04259630",\n    "sombrero"\n  ]\n'})}),"\n",(0,r.jsx)(n.p,{children:"So let's put this to a test :)"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Keras for Tensorflow - Convolutional Neural Networks",src:o(410702).Z+"",width:"1347",height:"365"})})]})}function p(e={}){const{wrapper:n}={...(0,i.ah)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},603905:(e,n,o)=>{o.d(n,{ah:()=>d});var r=o(667294);function i(e,n,o){return n in e?Object.defineProperty(e,n,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[n]=o,e}function t(e,n){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),o.push.apply(o,r)}return o}function a(e){for(var n=1;n<arguments.length;n++){var o=null!=arguments[n]?arguments[n]:{};n%2?t(Object(o),!0).forEach((function(n){i(e,n,o[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):t(Object(o)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(o,n))}))}return e}function s(e,n){if(null==e)return{};var o,r,i=function(e,n){if(null==e)return{};var o,r,i={},t=Object.keys(e);for(r=0;r<t.length;r++)o=t[r],n.indexOf(o)>=0||(i[o]=e[o]);return i}(e,n);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);for(r=0;r<t.length;r++)o=t[r],n.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(i[o]=e[o])}return i}var l=r.createContext({}),d=function(e){var n=r.useContext(l),o=n;return e&&(o="function"==typeof e?e(n):a(a({},n),e)),o},c={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},p=r.forwardRef((function(e,n){var o=e.components,i=e.mdxType,t=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),g=d(o),h=i,u=g["".concat(l,".").concat(h)]||g[h]||c[h]||t;return o?r.createElement(u,a(a({ref:n},p),{},{components:o})):r.createElement(u,a({ref:n},p))}));p.displayName="MDXCreateElement"},694414:(e,n,o)=>{o.d(n,{Z:()=>r});const r=o.p+"assets/images/Keras_Introduction_VGG16_Model_01-98a64e48ae8390037f5b07727bce4c02.png"},59378:(e,n,o)=>{o.d(n,{Z:()=>r});const r=o.p+"assets/images/Keras_Introduction_VGG16_Model_02-3f98ca76c438482b3f8984ea156fc079.png"},410702:(e,n,o)=>{o.d(n,{Z:()=>r});const r=o.p+"assets/images/Keras_Introduction_VGG16_Model_03-64081e7fe811f3e5709b44013d3e1df3.png"},495403:(e,n,o)=>{o.d(n,{Z:()=>r});const r=o.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-f80e63ee872dae25129198058ac93b4e.jpg"}}]);