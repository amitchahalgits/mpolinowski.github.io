"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[59485],{3905:(e,a,n)=>{n.d(a,{Zo:()=>p,kt:()=>d});var t=n(67294);function r(e,a,n){return a in e?Object.defineProperty(e,a,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[a]=n,e}function l(e,a){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),n.push.apply(n,t)}return n}function i(e){for(var a=1;a<arguments.length;a++){var n=null!=arguments[a]?arguments[a]:{};a%2?l(Object(n),!0).forEach((function(a){r(e,a,n[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(n,a))}))}return e}function s(e,a){if(null==e)return{};var n,t,r=function(e,a){if(null==e)return{};var n,t,r={},l=Object.keys(e);for(t=0;t<l.length;t++)n=l[t],a.indexOf(n)>=0||(r[n]=e[n]);return r}(e,a);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(t=0;t<l.length;t++)n=l[t],a.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var o=t.createContext({}),c=function(e){var a=t.useContext(o),n=a;return e&&(n="function"==typeof e?e(a):i(i({},a),e)),n},p=function(e){var a=c(e.components);return t.createElement(o.Provider,{value:a},e.children)},u={inlineCode:"code",wrapper:function(e){var a=e.children;return t.createElement(t.Fragment,{},a)}},m=t.forwardRef((function(e,a){var n=e.components,r=e.mdxType,l=e.originalType,o=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),m=c(n),d=r,f=m["".concat(o,".").concat(d)]||m[d]||u[d]||l;return n?t.createElement(f,i(i({ref:a},p),{},{components:n})):t.createElement(f,i({ref:a},p))}));function d(e,a){var n=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var l=n.length,i=new Array(l);i[0]=m;var s={};for(var o in a)hasOwnProperty.call(a,o)&&(s[o]=a[o]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var c=2;c<l;c++)i[c]=n[c];return t.createElement.apply(null,i)}return t.createElement.apply(null,n)}m.displayName="MDXCreateElement"},25465:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>o,contentTitle:()=>i,default:()=>u,frontMatter:()=>l,metadata:()=>s,toc:()=>c});var t=n(87462),r=(n(67294),n(3905));const l={sidebar_position:4590,slug:"2023-02-28",title:"Tensorflow 2 - Neural Network Classification",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"Model Evaluation and Performance Improvement"},i=void 0,s={unversionedId:"IoT-and-Machine-Learning/ML/2023-03-02-tensorflow-neural-network-multi-classification/2023-02-28-tensorflow-neural-network-classification-model-evaluation/index",id:"IoT-and-Machine-Learning/ML/2023-03-02-tensorflow-neural-network-multi-classification/2023-02-28-tensorflow-neural-network-classification-model-evaluation/index",title:"Tensorflow 2 - Neural Network Classification",description:"Model Evaluation and Performance Improvement",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-03-02-tensorflow-neural-network-multi-classification/2023-02-28-tensorflow-neural-network-classification-model-evaluation/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-03-02-tensorflow-neural-network-multi-classification/2023-02-28-tensorflow-neural-network-classification-model-evaluation",slug:"/IoT-and-Machine-Learning/ML/2023-03-02-tensorflow-neural-network-multi-classification/2023-02-28-tensorflow-neural-network-classification-model-evaluation/2023-02-28",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-03-02-tensorflow-neural-network-multi-classification/2023-02-28-tensorflow-neural-network-classification-model-evaluation/2023-02-28",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-03-02-tensorflow-neural-network-multi-classification/2023-02-28-tensorflow-neural-network-classification-model-evaluation/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Tensorflow",permalink:"/docs/tags/tensorflow"}],version:"current",sidebarPosition:4590,frontMatter:{sidebar_position:4590,slug:"2023-02-28",title:"Tensorflow 2 - Neural Network Classification",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"Model Evaluation and Performance Improvement"},sidebar:"tutorialSidebar",previous:{title:"Tensorflow 2 - Neural Network Classifications",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-03-02-tensorflow-neural-network-multi-classification/2023-03-02"},next:{title:"Tensorflow 2 - Neural Network Classification",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-02-28-tensorflow-neural-network-classification-model-evaluation/2023-02-28"}},o={},c=[{value:"Training &amp; Testing Datasplit",id:"training--testing-datasplit",level:2},{value:"Learning Rate",id:"learning-rate",level:2},{value:"Finding the ideal learning rate",id:"finding-the-ideal-learning-rate",level:3},{value:"Dynamically adjust the Learning Rate",id:"dynamically-adjust-the-learning-rate",level:3},{value:"Confusion Matrix",id:"confusion-matrix",level:2}],p={toc:c};function u(e){let{components:a,...l}=e;return(0,r.kt)("wrapper",(0,t.Z)({},p,l,{components:a,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"TST, Hong Kong",src:n(87508).Z,width:"1500",height:"557"})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#tensorflow-neural-network-classification"},"Tensorflow Neural Network Classification"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#training--testing-datasplit"},"Training \\& Testing Datasplit")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#learning-rate"},"Learning Rate"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#finding-the-ideal-learning-rate"},"Finding the ideal learning rate")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#dynamically-adjust-the-learning-rate"},"Dynamically adjust the Learning Rate")))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#confusion-matrix"},"Confusion Matrix"))))),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/tf-2023"},"Github Repository")),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"See also:")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Fun, fun, tensors: ",(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-19-tensorflow-introduction/2023-02-19"},"Tensor Constants, Variables and Attributes"),", ",(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-21-tensorflow-tensors-2/2023-02-21"},"Tensor Indexing, Expanding and Manipulations"),", ",(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-22-tensorflow-tensors-3/2023-02-22"},"Matrix multiplications, Squeeze, One-hot and Numpy")),(0,r.kt)("li",{parentName:"ul"},"Tensorflow 2 - Neural Network Regression: ",(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-23-tensorflow-neural-network-regression/2023-02-23"},"Building a Regression Model"),", ",(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-24-tensorflow-neural-network-regression-evaluation/2023-02-24"},"Model Evaluation"),", ",(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-25-tensorflow-neural-network-regression-experiments/2023-02-25"},"Model Optimization"),", ",(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-26-tensorflow-neural-network-regression-real-dataset/2023-02-26"},'Working with a "Real" Dataset'),", ",(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-26-tensorflow-neural-network-regression-data-preprocessing/2023-02-26"},"Feature Scaling")),(0,r.kt)("li",{parentName:"ul"},"Tensorflow 2 - Neural Network Classification: ",(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-27-tensorflow-neural-network-classification/2023-02-27"},"Non-linear Data and Activation Functions"),", ",(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-28-tensorflow-neural-network-classification-model-evaluation/2023-02-28"},"Model Evaluation and Performance Improvement"),", ",(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-03-02-tensorflow-neural-network-multi-classification/2023-03-02"},"Multiclass Classification Problems")),(0,r.kt)("li",{parentName:"ul"},"Tensorflow 2 - Convolutional Neural Networks: ",(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-03-03-tensorflow-convolutional-neural-network-binary-classifications/2023-03-03"},"Binary Image Classification"),", ",(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-03-05-tensorflow-convolutional-neural-network-multiclass-classifications/2023-03-05"},"Multiclass Image Classification")),(0,r.kt)("li",{parentName:"ul"},"Tensorflow 2 - Transfer Learning: ",(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-03-06-tensorflow-transfer-learning-feature-extraction/2023-03-06"},"Feature Extraction"),", ",(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-03-11-tensorflow-transfer-learning-fine-tuning/2023-03-11"},"Fine-Tuning"),", ",(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-03-16-tensorflow-transfer-learning-scaling/2023-03-16"},"Scaling")),(0,r.kt)("li",{parentName:"ul"},"Tensorflow 2 - Unsupervised Learning: ",(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-03-24-tensorflow-unsupervised-learning-autoencoders/2023-03-24"},"Autoencoder Feature Detection"),", ",(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-03-26-tensorflow-unsupervised-learning-autoencoders-super-resolution/2023-03-26"},"Autoencoder Super-Resolution"),", ",(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-03-26-tensorflow-unsupervised-learning-generative-adversial-networks/2023-03-26"},"Generative Adverserial Networks"))),(0,r.kt)("h1",{id:"tensorflow-neural-network-classification"},"Tensorflow Neural Network Classification"),(0,r.kt)("p",null,"Evaluating a models performance and tweaking the training."),(0,r.kt)("h2",{id:"training--testing-datasplit"},"Training & Testing Datasplit"),(0,r.kt)("p",null,"In the example earlier I used a single dataset to both train and test the model. Let's split this dataset so that we have a fresh testing dataset for the model."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# training and testing data split using scikit-learn\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42)\n\n# check shape\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n# ((800, 2), (200, 2), (800,), (200,))\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# rebuild the best model from above\n# train on training set and eval on testing\ntf.random.set_seed(42)\n\nmodel_circles_lr10e_3 = tf.keras.Sequential([\n    tf.keras.layers.Dense(4, activation="relu", name="input_layer"),\n    tf.keras.layers.Dense(4, activation="relu", name="dense_layer"),\n    tf.keras.layers.Dense(1, activation="sigmoid", name="output_layer")\n])\n\nmodel_circles_lr10e_3.compile(loss="binary_crossentropy",\n                       optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n                       metrics=["accuracy"])\n\nhistory_lr10e_3 = model_circles_lr10e_3.fit(X_train, y_train,\n                    validation_data=(X_test, y_test),\n                    epochs=2000)\n\n# Epoch 2000/2000\n# 25/25 [==============================] - 0s 4ms/step - loss: 2.2199e-04 - accuracy: 1.0000 - val_loss: 0.0131 - val_accuracy: 0.9950\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"decision_boundray(model=model_circles_lr10e_3, X=X_test, y=y_test)\n")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Tensorflow - Classification Problems",src:n(69047).Z,width:"568",height:"413"})),(0,r.kt)("h2",{id:"learning-rate"},"Learning Rate"),(0,r.kt)("p",null,"In the example above it took ",(0,r.kt)("strong",{parentName:"p"},"256")," cycles to get to an ",(0,r.kt)("strong",{parentName:"p"},"val_accuracy: 0.9000"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"25/25 [==============================] - 0s 3ms/step - loss: 0.2355 - accuracy: 0.9100 - val_loss: 0.2647 - val_accuracy: 0.8950\nEpoch 255/2000\n25/25 [==============================] - 0s 3ms/step - loss: 0.2338 - accuracy: 0.9137 - val_loss: 0.2636 - val_accuracy: 0.8900\nEpoch 256/2000\n25/25 [==============================] - 0s 3ms/step - loss: 0.2312 - accuracy: 0.9162 - val_loss: 0.2595 - val_accuracy: 0.9000\nEpoch 257/2000\n25/25 [==============================] - 0s 3ms/step - loss: 0.2255 - accuracy: 0.9162 - val_loss: 0.2390 - val_accuracy: 0.9000\nEpoch 258/2000\n25/25 [==============================] - 0s 3ms/step - loss: 0.2030 - accuracy: 0.9438 - val_loss: 0.2118 - val_accuracy: 0.9600\n")),(0,r.kt)("p",null,"By increasing the learning rate we allow Tensorflow to make bigger changes to the model weights after each epoch. This should increase the initial speed with which the model is moving towards the optimum:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'tf.random.set_seed(42)\n\nmodel_circles_lr10e_2 = tf.keras.Sequential([\n    tf.keras.layers.Dense(4, activation="relu", name="input_layer"),\n    tf.keras.layers.Dense(4, activation="relu", name="dense_layer"),\n    tf.keras.layers.Dense(1, activation="sigmoid", name="output_layer")\n])\n\nmodel_circles_lr10e_2.compile(loss="binary_crossentropy",\n                       optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n                       metrics=["accuracy"])\n\nhistory_lr10e_2 = model_circles_lr10e_2.fit(X_train, y_train,\n                    validation_data=(X_test, y_test),\n                    epochs=2000)\n\n# Epoch 2000/2000\n# 25/25 [==============================] - 0s 3ms/step - loss: 4.2108e-04 - accuracy: 1.0000 - val_loss: 0.0158 - val_accuracy: 0.9900\n')),(0,r.kt)("p",null,"With the increases learning rate we alread reach ",(0,r.kt)("strong",{parentName:"p"},"90%")," after ",(0,r.kt)("strong",{parentName:"p"},"25")," epochs:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"25/25 [==============================] - 0s 3ms/step - loss: 0.4151 - accuracy: 0.8475 - val_loss: 0.4481 - val_accuracy: 0.8050\nEpoch 14/2000\n25/25 [==============================] - 0s 3ms/step - loss: 0.3880 - accuracy: 0.8625 - val_loss: 0.4032 - val_accuracy: 0.8700\nEpoch 15/2000\n25/25 [==============================] - 0s 4ms/step - loss: 0.3304 - accuracy: 0.9025 - val_loss: 0.2930 - val_accuracy: 0.9550\nEpoch 16/2000\n25/25 [==============================] - 0s 3ms/step - loss: 0.2191 - accuracy: 0.9937 - val_loss: 0.2185 - val_accuracy: 1.0000\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'plt.figure(figsize=(14, 7))\nplt.subplot(1, 2, 1)\nplt.title("Training Dataset")\ndecision_boundray(model=model_circles_lr10e_2, X=X_train, y=y_train)\nplt.subplot(1, 2, 2)\nplt.title("Testing Dataset")\ndecision_boundray(model=model_circles_lr10e_2, X=X_test, y=y_test)\nplt.show()\n')),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Tensorflow - Classification Problems",src:n(32411).Z,width:"1148",height:"605"})),(0,r.kt)("h3",{id:"finding-the-ideal-learning-rate"},"Finding the ideal learning rate"),(0,r.kt)("p",null,"Comparing the learning progress of the previous two identical experiments - with a difference in learning rate ",(0,r.kt)("inlineCode",{parentName:"p"},"1e-3")," vs ",(0,r.kt)("inlineCode",{parentName:"p"},"1e-2")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\npd.DataFrame(history_lr10e_3.history).plot(ax=axes[0], title="Learning Rate 1e-3")\npd.DataFrame(history_lr10e_2.history).plot(ax=axes[1], title="Learning Rate 1e-2")\n# with a larger learning rate the loss/accuracy improves much quicker\n# but a larger learning rate also means that we get some overshots / fluctuations in performance\n')),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Tensorflow - Classification Problems",src:n(30018).Z,width:"981",height:"528"})),(0,r.kt)("h3",{id:"dynamically-adjust-the-learning-rate"},"Dynamically adjust the Learning Rate"),(0,r.kt)("p",null,"We can use the Keras ",(0,r.kt)("inlineCode",{parentName:"p"},"LearningRateScheduler()")," in a ",(0,r.kt)("strong",{parentName:"p"},"Callback")," to update the learning rate of the used optimizer according to the schedule function with each new epoch."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# create a new model based on model_circles_lr10e_3\ntf.random.set_seed(7)\n\nmodel_circles_lr_callback = tf.keras.Sequential([\n    tf.keras.layers.Dense(4, activation="relu", name="input_layer"),\n    tf.keras.layers.Dense(4, activation="relu", name="dense_layer"),\n    tf.keras.layers.Dense(1, activation="sigmoid", name="output_layer")\n])\n\nmodel_circles_lr_callback.compile(loss="binary_crossentropy",\n                                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n                                 metrics=["accuracy"])\n\n# introduce learning scheduler callback to increase the learning rate\n# with every epoch by `0.0001 times 10e(epoch/100)`\nlearning_rate_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/100))\n\nhistory_lr_callback = model_circles_lr_callback.fit(X_train, y_train,\n                                      validation_data=(X_test, y_test), epochs=500,\n                                      callbacks=[learning_rate_callback], verbose=1)\n\n# the learning rate, initially, increases sensibly. But later explodes leading to a terrible performance:\n# Epoch 500/500\n# 25/25 [==============================] - 0s 3ms/step - loss: 0.7646 - accuracy: 0.5025 - val_loss: 0.6961 - val_accuracy: 0.5000 - lr: 9.7724\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# plot the learning rate progression\nlr = 1e-4 * (10 ** (tf.range(500)/100))\nplt.plot(tf.range(500), lr)\nplt.ylabel("Learning Rate")\nplt.xlabel("Epoch")\nplt.show()\n# the adaptive learning rate stays reasonable up until the 300th epoch\n')),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Tensorflow - Classification Problems",src:n(9824).Z,width:"562",height:"432"})),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'pd.DataFrame(history_lr_callback.history).plot(title="Learning Rate Scheduler")\n# and the learning rate abve the 200th epoch leads to more and more fluctuations in loss and accuracy.\n')),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Tensorflow - Classification Problems",src:n(98939).Z,width:"543",height:"435"})),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# the "ideal learning rate" is usally 10 times smaller than the value at the bottom of the loss curve (`loss = f(lr)`)\nplt.figure(figsize=(10, 7))\nplt.xlabel("Learning Rate")\nplt.ylabel("Loss")\nplt.semilogx(lr, history_lr_callback.history["loss"])\nplt.show()\n# for the plot below this would be around `3*10e-3` and `4*10e-3`\n')),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Tensorflow - Classification Problems",src:n(22189).Z,width:"846",height:"606"})),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# re-run the model with the ideal learning rate\ntf.random.set_seed(42)\n\nmodel_circles_ideal_lr = tf.keras.Sequential([\n    tf.keras.layers.Dense(4, activation="relu", name="input_layer"),\n    tf.keras.layers.Dense(4, activation="relu", name="dense_layer"),\n    tf.keras.layers.Dense(1, activation="sigmoid", name="output_layer")\n])\n\nmodel_circles_ideal_lr.compile(loss="binary_crossentropy",\n                                 optimizer=tf.keras.optimizers.Adam(learning_rate=4*10e-3),\n                                 metrics=["accuracy"])\n\nhistory_ideal_lr = model_circles_ideal_lr.fit(X_train, y_train,\n                                      validation_data=(X_test, y_test),\n                                      epochs=500, verbose=1)\n\n# Epoch 500/500\n# 25/25 [==============================] - 0s 3ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\npd.DataFrame(history_ideal_lr.history).plot(ax=axes[0], title="Ideal Learning Rate")\ndecision_boundray(model=model_circles_ideal_lr, X=X_test, y=y_test)\n')),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Tensorflow - Classification Problems",src:n(30175).Z,width:"981",height:"528"})),(0,r.kt)("h2",{id:"confusion-matrix"},"Confusion Matrix"),(0,r.kt)("p",null,"So far I used ",(0,r.kt)("strong",{parentName:"p"},"Accuracy")," as metric to evaluate the performance of a trained model. But accuracy can fall short of representing a dataset with imbalanced classes. If a class only makes up 1% of a dataset even if our model fails 100% of the time to predict the class we still end up with a high accuracy overall:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"Accuracy = (tp + tn) / (tp + tn + fp + fn)"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"tf.keras.metrics.Accuracy()"),", ",(0,r.kt)("inlineCode",{parentName:"li"},"sklearn.metrics.accuracy_score()"))))),(0,r.kt)("p",null,"A metric that allows us to indicate the amount of false positive predictions is ",(0,r.kt)("strong",{parentName:"p"},"Precision"),":"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"Precision = tp / (tp + fp)"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"tf.keras.metrics.Precision()"),", ",(0,r.kt)("inlineCode",{parentName:"li"},"sklearn.metrics.precision_score()"))))),(0,r.kt)("p",null,"A metric to evaluate the amount of false negative predictions is ",(0,r.kt)("strong",{parentName:"p"},"Recall"),":"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"Recall = tp / (tp + fn)"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"tf.keras.metrics.Recall()"),", ",(0,r.kt)("inlineCode",{parentName:"li"},"sklearn.metrics.recall_score()"))))),(0,r.kt)("p",null,"Depending on our problem we can use precision or recall instead of accuracy as the training performance metric. A metric that combines both recall and precision is the ",(0,r.kt)("strong",{parentName:"p"},"F1 Score")," in SciKit_Learn:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"F1-score = 2 ",(0,r.kt)("em",{parentName:"em"}," (precision ")," recall)/(precision + recall)"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"sklearn.metrics.f1_score()"))))),(0,r.kt)("p",null,"A good visual representation of a models performance is the ",(0,r.kt)("strong",{parentName:"p"},"Confusion Matrix")," that compares predictions to the true value ",(0,r.kt)("inlineCode",{parentName:"p"},"sklearn.metrics.confusion_matrix()"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# check loss and accuracy of the previous model\nloss, accuracy = model_circles_ideal_lr.evaluate(X_test, y_test)\nprint(f"INFO :: Model loss - {loss:.5f}")\nprint(f"INFO :: Model accuracy - {(accuracy*100):.2f}%")\n# 7/7 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 1.0000\n# INFO :: Model loss - 0.01005\n# INFO :: Model accuracy - 100.00%\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# confusion matrix\n## get label predictions using the trained model\ny_pred = model_circles_ideal_lr.predict(X_test)\n# y_pred, y_test\n# the predictions we get are floats while the true values are binary `1` or `0`\n# round the prediction to be able to compare them:\ny_pred_rnd = tf.round(y_pred)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"confusion_matrix(y_pred_rnd, y_test)\n# array([[100,   0],\n#        [  0, 100]])\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# plot confusion matrix\nimport itertools\n\ndef plot_confusion_matrix(y_pred=y_pred_rnd, y_test=y_test):\n        figsize = (10, 10)\n\n        # create the confusion matrix\n        cm = confusion_matrix(y_pred_rnd, y_test)\n\n        # normalize\n        cm_norm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis]\n        # cm_norm\n        # array([[1., 0.],\n        #        [0., 1.]])\n\n        number_of_classes = cm.shape[0]\n        # 2\n\n        # plot matrix\n        fig, ax = plt.subplots(figsize=figsize)\n        cax = ax.matshow(cm, cmap=plt.cm.Greens)\n        fig.colorbar(cax)\n\n        # create classes\n        classes = False\n\n        if classes:\n            labels = classes\n        else:\n            labels = np.arange(cm.shape[0])\n\n        # axes lables\n        ax.set(title="Confusion Matrix",\n              xlabel="Prediction",\n              ylabel="True",\n              xticks=np.arange(number_of_classes),\n              yticks=np.arange(number_of_classes),\n              xticklabels=labels,\n              yticklabels=labels)\n\n        ax.xaxis.set_label_position("bottom")\n        ax.title.set_size(20)\n        ax.xaxis.label.set_size(20)\n        ax.yaxis.label.set_size(20)\n        ax.xaxis.tick_bottom()\n\n\n        # colour threshold\n        threshold = (cm.max() + cm.min()) / 2.\n\n        # add text to cells\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(j, i , f"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)",\n            horizontalalignment="center",\n            color="white" if cm[i, j] > threshold else "black",\n            size=15)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"plot_confusion_matrix(y_pred, y_test)\n")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Tensorflow - Classification Problems",src:n(10369).Z,width:"805",height:"804"})))}u.isMDXComponent=!0},69047:(e,a,n)=>{n.d(a,{Z:()=>t});const t=n.p+"assets/images/02_Tensorflow_Classifications_11-c83c16f3ff56abe4ab664c1e4ed8ca4f.png"},32411:(e,a,n)=>{n.d(a,{Z:()=>t});const t=n.p+"assets/images/02_Tensorflow_Classifications_12-a1925fa5a21c8f80e3c93605244752e2.png"},30018:(e,a,n)=>{n.d(a,{Z:()=>t});const t=n.p+"assets/images/02_Tensorflow_Classifications_13-48316ce8cb9e7286f788c4e1af51abdd.png"},9824:(e,a,n)=>{n.d(a,{Z:()=>t});const t=n.p+"assets/images/02_Tensorflow_Classifications_14-4ca685ca896590d81151575763910417.png"},98939:(e,a,n)=>{n.d(a,{Z:()=>t});const t=n.p+"assets/images/02_Tensorflow_Classifications_15-e17e2195906e45614d1fc6acd4369fd3.png"},22189:(e,a,n)=>{n.d(a,{Z:()=>t});const t=n.p+"assets/images/02_Tensorflow_Classifications_16-640ad1a03942320c83f9b0c4ccbe420e.png"},30175:(e,a,n)=>{n.d(a,{Z:()=>t});const t=n.p+"assets/images/02_Tensorflow_Classifications_17-bab885a91fcf2d852ff8c7185b7a61c9.png"},10369:(e,a,n)=>{n.d(a,{Z:()=>t});const t=n.p+"assets/images/02_Tensorflow_Classifications_18-af7c5f0deb53e8ae88c51a260867276d.png"},87508:(e,a,n)=>{n.d(a,{Z:()=>t});const t=n.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-f22d43080f9f8a797542a918e9317e01.jpg"}}]);