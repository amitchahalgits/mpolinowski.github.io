"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[24992],{153172:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>i,metadata:()=>o,toc:()=>c});var t=r(785893),s=r(603905);const i={sidebar_position:8090,slug:"2021-03-23",title:"Logstash 7 Working with Unstructured Data",authors:"mpolinowski",tags:["LINUX","Elasticsearch"]},a=void 0,o={id:"DevOps/Elasticsearch/2021-03-23-logstash-working-with-unstructured-data/index",title:"Logstash 7 Working with Unstructured Data",description:"Guangzhou, China",source:"@site/docs/DevOps/Elasticsearch/2021-03-23-logstash-working-with-unstructured-data/index.md",sourceDirName:"DevOps/Elasticsearch/2021-03-23-logstash-working-with-unstructured-data",slug:"/DevOps/Elasticsearch/2021-03-23-logstash-working-with-unstructured-data/2021-03-23",permalink:"/docs/DevOps/Elasticsearch/2021-03-23-logstash-working-with-unstructured-data/2021-03-23",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/DevOps/Elasticsearch/2021-03-23-logstash-working-with-unstructured-data/index.md",tags:[{label:"LINUX",permalink:"/docs/tags/linux"},{label:"Elasticsearch",permalink:"/docs/tags/elasticsearch"}],version:"current",sidebarPosition:8090,frontMatter:{sidebar_position:8090,slug:"2021-03-23",title:"Logstash 7 Working with Unstructured Data",authors:"mpolinowski",tags:["LINUX","Elasticsearch"]},sidebar:"tutorialSidebar",previous:{title:"Logstash 7 and Common Log Pattern",permalink:"/docs/DevOps/Elasticsearch/2021-03-24-logstash-common-grok-pattern/2021-03-24"},next:{title:"Logstash 7 Working with Structured Data",permalink:"/docs/DevOps/Elasticsearch/2021-03-22-logstash-working-with-structured-data/2021-03-22"}},l={},c=[{value:"Grok Pattern Filter",id:"grok-pattern-filter",level:2},{value:"Introduction",id:"introduction",level:3},{value:"Debugging Grok Pattern",id:"debugging-grok-pattern",level:3},{value:"Analyzing a Logfile",id:"analyzing-a-logfile",level:3},{value:"Using Multiple Pattern",id:"using-multiple-pattern",level:3}];function h(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.ah)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Guangzhou, China",src:r(880794).Z+"",width:"1500",height:"595"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#grok-pattern-filter",children:"Grok Pattern Filter"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#introduction",children:"Introduction"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#debugging-grok-pattern",children:"Debugging Grok Pattern"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#analyzing-a-logfile",children:"Analyzing a Logfile"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#using-multiple-pattern",children:"Using Multiple Pattern"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"grok-pattern-filter",children:"Grok Pattern Filter"}),"\n",(0,t.jsx)(n.h3,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html",children:"Grok is a great way to parse unstructured log data into something structured and queryable"}),". This tool is perfect for syslog logs, apache and other webserver logs, mysql logs, and in general, any log format that is generally written for humans and not computer consumption. Logstash ships with about 120 patterns by default. ",(0,t.jsx)(n.a,{href:"https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns",children:"You can find them here"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["As an example we already used the Grok filter to help us ingest an ",(0,t.jsx)(n.a,{href:"/docs/DevOps/Elasticsearch/2021-03-21-logstash-digesting-data/2021-03-21#preparing-the-configuration-files",children:"Apache access log"})," earlier:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'filter {\r\n  grok {\r\n    match => {"body" => "%{COMBINEDAPACHELOG}"}\r\n  }\r\n  date {\r\n    match => ["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]\r\n  }\r\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:"If you have an unstructured log that contains client email addresses you can use the following Grok pattern to extract them:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:"%{EMAILADDRESS:client_email}\n"})}),"\n",(0,t.jsx)(n.p,{children:"We can also combine pattern to match our logfile - for example:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-log",children:"2021-08-02T04:54:17.043+02:00 DEBUG An event happened\n"})}),"\n",(0,t.jsx)(n.p,{children:"Can be analyzed with the following pattern:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:"%{TIMESTAMP_ISO8601:time} %{LOGLEVEL:level} %{GREEDYDATA:body}\n"})}),"\n",(0,t.jsx)(n.h3,{id:"debugging-grok-pattern",children:"Debugging Grok Pattern"}),"\n",(0,t.jsxs)(n.p,{children:["You can generate those patterns using the ",(0,t.jsx)(n.a,{href:"http://grokdebug.herokuapp.com",children:"Grok Debugger"})," which comes with a very helpful auto-completion function:"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Logstash Grok Filter",src:r(104058).Z+"",width:"966",height:"740"})}),"\n",(0,t.jsx)(n.p,{children:"The Grok filter fulfils it's duty and brings structure to our log date:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\r\n  "time": [\r\n    [\r\n      "2021-08-02T04:54:17.043+02:00"\r\n    ]\r\n  ],\r\n  "level": [\r\n    [\r\n      "DEBUG"\r\n    ]\r\n  ],\r\n  "body": [\r\n    [\r\n      "An event happened"\r\n    ]\r\n  ]\r\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"analyzing-a-logfile",children:"Analyzing a Logfile"}),"\n",(0,t.jsx)(n.p,{children:"We now have a pattern that fits our log. Let's apply it to:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"/opt/logstash/grok_test"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-log",children:"2021-08-01T09:39:44Z INFO variable server value is connector\r\n2021-06-21T22:23:12Z ERROR cannot find the requested resource\r\n2021-05-14T14:52:36Z INFO variable server value is connector\r\n2021-03-26T01:22:56Z DEBUG initializing checksum\r\n2021-02-13T13:17:11Z INFO initializing the bootup\n"})}),"\n",(0,t.jsx)(n.p,{children:"The Logstash configuration to use our Grok filter pattern looks like:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"/opt/logstash/pipeline/logstash.conf"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'input {\r\n  file {\r\n    path => "/usr/share/logstash/grok_test"\r\n    start_position => "beginning"\r\n    sincedb_path => "/dev/null"\r\n  }\r\n}\r\nfilter {\r\n  grok {\r\n    match => { "message" => [\'%{TIMESTAMP_ISO8601:time} %{level:level} %{GREEDYDATA:body}\'] }\r\n  }\r\n  mutate {\r\n    remove_field => ["message", "@timestamp", "path", "host", "@version"]\r\n  }\r\n}\r\noutput {\r\n   elasticsearch {\r\n     hosts => "http://localhost:9200"\r\n     index => "grok_test"\r\n  }\r\n\r\nstdout {}\r\n\r\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:"And run Logstash with the new configuration file:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'docker run \\\r\n   --name logstash \\\r\n   --net=host \\\r\n   --rm -it \\\r\n   -v /opt/logstash/pipeline/logstash.conf:/usr/share/logstash/pipeline/logstash.conf \\\r\n   -v /opt/logstash/grok_test:/usr/share/logstash/grok_test \\\r\n   -e "ELASTIC_HOST=localhost:9200" \\\r\n   -e "XPACK_SECURITY_ENABLED=false" \\\r\n   -e "XPACK_REPORTING_ENABLED=false" \\\r\n   -e "XPACK_MONITORING_ENABLED=false" \\\r\n   -e "XPACK_MONITORING_ELASTICSEARCH_USERNAME=elastic" \\\r\n   -e "XPACK_MONITORING_ELASTICSEARCH_PASSWORD=changeme" \\\r\n   logstash:7.13.4\n'})}),"\n",(0,t.jsx)(n.p,{children:"And we can see that the ingestion ran nicely:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'[2021-08-02T08:22:19,478][INFO ][logstash.agent] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}\r\n{\r\n      "level" => "INFO",\r\n    "body" => "variable server value is connector",\r\n       "time" => "2021-05-14T14:52:36Z"\r\n}\r\n{\r\n      "level" => "INFO",\r\n    "body" => "variable server value is connector",\r\n       "time" => "2021-08-01T09:39:44Z"\r\n}\r\n{\r\n      "level" => "INFO",\r\n    "body" => "initializing the bootup",\r\n       "time" => "2021-02-13T13:17:11Z"\r\n}\r\n{\r\n      "level" => "ERROR",\r\n    "body" => "cannot find the requested resource",\r\n       "time" => "2021-06-21T22:23:12Z"\r\n}\r\n{\r\n      "level" => "DEBUG",\r\n    "body" => "initializing checksum",\r\n       "time" => "2021-03-26T01:22:56Z"\r\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"using-multiple-pattern",children:"Using Multiple Pattern"}),"\n",(0,t.jsx)(n.p,{children:"To see how Grok handles issues with our filter pattern we can add some data to our log that does not fit the format:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"/opt/logstash/grok_test"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-log",children:"2021-08-01T09:39:44Z INFO variable server value is connector\r\n2021-06-21T22:23:12Z ERROR cannot find the requested resource\r\n2021-05-14T14:52:36Z INFO variable server value is connector\r\n2021-03-26T01:22:56Z DEBUG initializing checksum\r\n2021-02-13T13:17:11Z INFO initializing the bootup\r\n116.112.4.48 GET /wp-admin/index.php\n"})}),"\n",(0,t.jsx)(n.p,{children:"First delete the old indexed data:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"curl -XDELETE -u elastic:changeme http://localhost:9200/grok_test\n"})}),"\n",(0,t.jsx)(n.p,{children:"And re-run Logtstash with the dame configuration as before:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\r\n          "time" => "2021-05-14T14:52:36Z",\r\n      "level" => "INFO",\r\n    "body" => "variable server value is connector"\r\n}\r\n{\r\n          "time" => "2021-06-21T22:23:12Z",\r\n      "level" => "ERROR",\r\n    "body" => "cannot find the requested resource"\r\n}\r\n{\r\n    "tags" => [\r\n        [0] "_grokparsefailure"\r\n    ]\r\n}\r\n{\r\n          "time" => "2021-08-01T09:39:44Z",\r\n      "level" => "INFO",\r\n    "body" => "variable server value is connector"\r\n}\r\n{\r\n          "time" => "2021-02-13T13:17:11Z",\r\n      "level" => "INFO",\r\n    "body" => "initializing the bootup"\r\n}\r\n{\r\n          "time" => "2021-03-26T01:22:56Z",\r\n      "level" => "DEBUG",\r\n    "body" => "initializing checksum"\r\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:"We now have a 6th entry that only contains a parse error, as expected. To fix this issue we can add a secondary pattern to our Grok filter. This filter will be used if the first one fails to parse a line in our data:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"/opt/logstash/pipeline/logstash.conf"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'input {\r\n  file {\r\n    path => "/usr/share/logstash/grok_test"\r\n    start_position => "beginning"\r\n    sincedb_path => "/dev/null"\r\n  }\r\n}\r\nfilter {\r\n  grok {\r\n    match => { "message" => [\r\n          \'%{TIMESTAMP_ISO8601:time} %{LOGLEVEL:level} %{GREEDYDATA:body}\',\r\n          \'%{IP:clientIP} %{WORD:httpMethod} %{URIPATH:url}\'\r\n        ] }\r\n    }\r\n    mutate {\r\n      remove_field => ["message", "@timestamp", "path", "host", "@version"]\r\n    }\r\n  }\r\noutput {\r\n   elasticsearch {\r\n     hosts => "http://localhost:9200"\r\n     index => "grok_test"\r\n  }\r\n\r\nstdout {}\r\n\r\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:"First delete the old indexed data:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"curl -XDELETE -u elastic:changeme http://localhost:9200/grok_test\n"})}),"\n",(0,t.jsx)(n.p,{children:"And re-run Logtstash with the same configuration as before:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\r\n    "level" => "INFO",\r\n     "time" => "2021-05-14T14:52:36Z",\r\n     "body" => "variable server value is connector"\r\n}\r\n{\r\n    "level" => "ERROR",\r\n     "time" => "2021-06-21T22:23:12Z",\r\n     "body" => "cannot find the requested resource"\r\n}\r\n{\r\n           "url" => "/wp-admin/index.php",\r\n      "clientIP" => "116.112.4.48",\r\n    "httpMethod" => "GET"\r\n}\r\n{\r\n    "level" => "INFO",\r\n     "time" => "2021-08-01T09:39:44Z",\r\n     "body" => "variable server value is connector"\r\n}\r\n{\r\n    "level" => "INFO",\r\n     "time" => "2021-02-13T13:17:11Z",\r\n     "body" => "initializing the bootup"\r\n}\r\n{\r\n    "level" => "DEBUG",\r\n     "time" => "2021-03-26T01:22:56Z",\r\n     "body" => "initializing checksum"\r\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:"The last line failed the first pattern and was then ingested using the second option we gave Logstash. It works!"})]})}function d(e={}){const{wrapper:n}={...(0,s.ah)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},603905:(e,n,r)=>{r.d(n,{ah:()=>c});var t=r(667294);function s(e,n,r){return n in e?Object.defineProperty(e,n,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[n]=r,e}function i(e,n){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),r.push.apply(r,t)}return r}function a(e){for(var n=1;n<arguments.length;n++){var r=null!=arguments[n]?arguments[n]:{};n%2?i(Object(r),!0).forEach((function(n){s(e,n,r[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(r,n))}))}return e}function o(e,n){if(null==e)return{};var r,t,s=function(e,n){if(null==e)return{};var r,t,s={},i=Object.keys(e);for(t=0;t<i.length;t++)r=i[t],n.indexOf(r)>=0||(s[r]=e[r]);return s}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)r=i[t],n.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(s[r]=e[r])}return s}var l=t.createContext({}),c=function(e){var n=t.useContext(l),r=n;return e&&(r="function"==typeof e?e(n):a(a({},n),e)),r},h={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},d=t.forwardRef((function(e,n){var r=e.components,s=e.mdxType,i=e.originalType,l=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),g=c(r),u=s,p=g["".concat(l,".").concat(u)]||g[u]||h[u]||i;return r?t.createElement(p,a(a({ref:n},d),{},{components:r})):t.createElement(p,a({ref:n},d))}));d.displayName="MDXCreateElement"},104058:(e,n,r)=>{r.d(n,{Z:()=>t});const t=r.p+"assets/images/Logstash_Grok_Filter_01-e43da1ce8448e9925369a0016d259110.png"},880794:(e,n,r)=>{r.d(n,{Z:()=>t});const t=r.p+"assets/images/photo-456tdsfggd_67gfh6dgdf4_d-2a2fa147cd26f24fb2448fd8aa06f41c.jpg"}}]);