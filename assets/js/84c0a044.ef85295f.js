"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[85317],{3905:(e,n,t)=>{t.d(n,{Zo:()=>c,kt:()=>u});var a=t(67294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function l(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?l(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):l(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},l=Object.keys(e);for(a=0;a<l.length;a++)t=l[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(a=0;a<l.length;a++)t=l[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var o=a.createContext({}),p=function(e){var n=a.useContext(o),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},c=function(e){var n=p(e.components);return a.createElement(o.Provider,{value:n},e.children)},d={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef((function(e,n){var t=e.components,i=e.mdxType,l=e.originalType,o=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),m=p(t),u=i,f=m["".concat(o,".").concat(u)]||m[u]||d[u]||l;return t?a.createElement(f,r(r({ref:n},c),{},{components:t})):a.createElement(f,r({ref:n},c))}));function u(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var l=t.length,r=new Array(l);r[0]=m;var s={};for(var o in n)hasOwnProperty.call(n,o)&&(s[o]=n[o]);s.originalType=e,s.mdxType="string"==typeof e?e:i,r[1]=s;for(var p=2;p<l;p++)r[p]=t[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},73156:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>r,default:()=>d,frontMatter:()=>l,metadata:()=>s,toc:()=>p});var a=t(87462),i=(t(67294),t(3905));const l={sidebar_position:4085,slug:"2023-09-23",title:"Audio Classification with Computer Vision",authors:"mpolinowski",tags:["Python","Machine Learning","PyTorch"],description:"Training an YOLOv8 Classifier to Identify Audio Files"},r=void 0,s={unversionedId:"IoT-and-Machine-Learning/ML/2023-09-23--yolo8-listen/index",id:"IoT-and-Machine-Learning/ML/2023-09-23--yolo8-listen/index",title:"Audio Classification with Computer Vision",description:"Training an YOLOv8 Classifier to Identify Audio Files",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-09-23--yolo8-listen/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-09-23--yolo8-listen",slug:"/IoT-and-Machine-Learning/ML/2023-09-23--yolo8-listen/2023-09-23",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-09-23--yolo8-listen/2023-09-23",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-09-23--yolo8-listen/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"PyTorch",permalink:"/docs/tags/py-torch"}],version:"current",sidebarPosition:4085,frontMatter:{sidebar_position:4085,slug:"2023-09-23",title:"Audio Classification with Computer Vision",authors:"mpolinowski",tags:["Python","Machine Learning","PyTorch"],description:"Training an YOLOv8 Classifier to Identify Audio Files"},sidebar:"tutorialSidebar",previous:{title:"DLIB Face Recognition",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-10-01--delib-face-detection/2023-10-01"},next:{title:"CVAT Semi-automatic and Automatic Annotation",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-09-21--cvat-automatic-annotation/2023-09-21"}},o={},p=[{value:"Visualize the Dataset",id:"visualize-the-dataset",level:2},{value:"Data Preprocessing",id:"data-preprocessing",level:2},{value:"Train-Test-Split",id:"train-test-split",level:3},{value:"Prepare Validation Data",id:"prepare-validation-data",level:3},{value:"Model Training",id:"model-training",level:2},{value:"Model Predictions",id:"model-predictions",level:2}],c={toc:p};function d(e){let{components:n,...l}=e;return(0,i.kt)("wrapper",(0,a.Z)({},c,l,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"TST, Hongkong",src:t(72256).Z,width:"1500",height:"620"})),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#audio-classification-with-computer-vision"},"Audio Classification with Computer Vision"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#visualize-the-dataset"},"Visualize the Dataset")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#data-preprocessing"},"Data Preprocessing"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#train-test-split"},"Train-Test-Split")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#prepare-validation-data"},"Prepare Validation Data")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#model-training"},"Model Training")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#model-predictions"},"Model Predictions"))))),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/mpolinowski/yolo-listen"},"Github Repository"))),(0,i.kt)("h1",{id:"audio-classification-with-computer-vision"},"Audio Classification with Computer Vision"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"The ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/karolpiczak/ESC-50"},"ESC-50 dataset")," is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification."),(0,i.kt)("p",{parentName:"blockquote"},"The dataset consists of 5-second-long recordings organized into 50 semantical classes (with 40 examples per class) loosely arranged into 5 major categories:")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import numpy as np\nfrom matplotlib import pyplot as plt\nfrom numpy.lib import stride_tricks\nimport os\nimport pandas as pd\nimport scipy.io.wavfile as wav\n")),(0,i.kt)("h2",{id:"visualize-the-dataset"},"Visualize the Dataset"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"esc50_df = pd.read_csv('dataset/ESC-50/esc50.csv')\nesc50_df.head()\n")),(0,i.kt)("div",null,(0,i.kt)("table",null,(0,i.kt)("thead",null,(0,i.kt)("tr",null,(0,i.kt)("th",null),(0,i.kt)("th",null,"filename"),(0,i.kt)("th",null,"fold"),(0,i.kt)("th",null,"target"),(0,i.kt)("th",null,"category"),(0,i.kt)("th",null,"esc10"),(0,i.kt)("th",null,"src_file"),(0,i.kt)("th",null,"take"))),(0,i.kt)("tbody",null,(0,i.kt)("tr",null,(0,i.kt)("th",null,"0"),(0,i.kt)("td",null,"1-100032-A-0.wav"),(0,i.kt)("td",null,"1"),(0,i.kt)("td",null,"0"),(0,i.kt)("td",null,"dog"),(0,i.kt)("td",null,"True"),(0,i.kt)("td",null,"100032"),(0,i.kt)("td",null,"A")),(0,i.kt)("tr",null,(0,i.kt)("th",null,"1"),(0,i.kt)("td",null,"1-100038-A-14.wav"),(0,i.kt)("td",null,"1"),(0,i.kt)("td",null,"14"),(0,i.kt)("td",null,"chirping_birds"),(0,i.kt)("td",null,"False"),(0,i.kt)("td",null,"100038"),(0,i.kt)("td",null,"A")),(0,i.kt)("tr",null,(0,i.kt)("th",null,"2"),(0,i.kt)("td",null,"1-100210-A-36.wav"),(0,i.kt)("td",null,"1"),(0,i.kt)("td",null,"36"),(0,i.kt)("td",null,"vacuum_cleaner"),(0,i.kt)("td",null,"False"),(0,i.kt)("td",null,"100210"),(0,i.kt)("td",null,"A")),(0,i.kt)("tr",null,(0,i.kt)("th",null,"3"),(0,i.kt)("td",null,"1-100210-B-36.wav"),(0,i.kt)("td",null,"1"),(0,i.kt)("td",null,"36"),(0,i.kt)("td",null,"vacuum_cleaner"),(0,i.kt)("td",null,"False"),(0,i.kt)("td",null,"100210"),(0,i.kt)("td",null,"B")),(0,i.kt)("tr",null,(0,i.kt)("th",null,"4"),(0,i.kt)("td",null,"1-101296-A-19.wav"),(0,i.kt)("td",null,"1"),(0,i.kt)("td",null,"19"),(0,i.kt)("td",null,"thunderstorm"),(0,i.kt)("td",null,"False"),(0,i.kt)("td",null,"101296"),(0,i.kt)("td",null,"A"))))),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"esc50_df['category'].value_counts()\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"dog                 40\nglass_breaking      40\ndrinking_sipping    40\nrain                40\ninsects             40\nlaughing            40\nhen                 40\nengine              40\nbreathing           40\ncrying_baby         40\nhand_saw            40\ncoughing            40\nsnoring             40\nchirping_birds      40\ntoilet_flush        40\npig                 40\nwashing_machine     40\nclock_tick          40\nsneezing            40\nrooster             40\nsea_waves           40\nsiren               40\ncat                 40\ndoor_wood_creaks    40\nhelicopter          40\ncrackling_fire      40\ncar_horn            40\nbrushing_teeth      40\nvacuum_cleaner      40\nthunderstorm        40\ndoor_wood_knock     40\ncan_opening         40\ncrow                40\nclapping            40\nfireworks           40\nchainsaw            40\nairplane            40\nmouse_click         40\npouring_water       40\ntrain               40\nsheep               40\nwater_drops         40\nchurch_bells        40\nclock_alarm         40\nkeyboard_typing     40\nwind                40\nfootsteps           40\nfrog                40\ncow                 40\ncrickets            40\nName: category, dtype: int64\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def fourier_transformation(sig, frameSize, overlapFac=0.5, window=np.hanning):\n    win = window(frameSize)\n    hopSize = int(frameSize - np.floor(overlapFac * frameSize))\n\n    # zeros at beginning (thus center of 1st window should be for sample nr. 0)   \n    samples = np.append(np.zeros(int(np.floor(frameSize/2.0))), sig)    \n    # cols for windowing\n    cols = np.ceil( (len(samples) - frameSize) / float(hopSize)) + 1\n    # zeros at end (thus samples can be fully covered by frames)\n    samples = np.append(samples, np.zeros(frameSize))\n\n    frames = stride_tricks.as_strided(samples, shape=(int(cols), frameSize), strides=(samples.strides[0]*hopSize, samples.strides[0])).copy()\n    frames *= win\n\n    return np.fft.rfft(frames) \n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def make_logscale(spec, sr=44100, factor=20.):\n    timebins, freqbins = np.shape(spec)\n\n    scale = np.linspace(0, 1, freqbins) ** factor\n    scale *= (freqbins-1)/max(scale)\n    scale = np.unique(np.round(scale))\n\n    # create spectrogram with new freq bins\n    newspec = np.complex128(np.zeros([timebins, len(scale)]))\n    for i in range(0, len(scale)):        \n        if i == len(scale)-1:\n            newspec[:,i] = np.sum(spec[:,int(scale[i]):], axis=1)\n        else:        \n            newspec[:,i] = np.sum(spec[:,int(scale[i]):int(scale[i+1])], axis=1)\n\n    # list center freq of bins\n    allfreqs = np.abs(np.fft.fftfreq(freqbins*2, 1./sr)[:freqbins+1])\n    freqs = []\n    for i in range(0, len(scale)):\n        if i == len(scale)-1:\n            freqs += [np.mean(allfreqs[int(scale[i]):])]\n        else:\n            freqs += [np.mean(allfreqs[int(scale[i]):int(scale[i+1])])]\n\n    return newspec, freqs\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'def plot_spectrogram(location, categorie, plotpath=None, binsize=2**10, colormap="jet"):\n    samplerate, samples = wav.read(location)\n\n    s = fourier_transformation(samples, binsize)\n\n    sshow, freq = make_logscale(s, factor=1.0, sr=samplerate)\n\n    ims = 20.*np.log10(np.abs(sshow)/10e-6) # amplitude to decibel\n\n    timebins, freqbins = np.shape(ims)\n\n    print("timebins: ", timebins)\n    print("freqbins: ", freqbins)\n\n    plt.figure(figsize=(15, 7.5))\n    plt.title(\'Class Label: \' + categorie)\n    plt.imshow(np.transpose(ims), origin="lower", aspect="auto", cmap=colormap, interpolation="none")\n    plt.colorbar()\n\n    plt.xlabel("time (s)")\n    plt.ylabel("frequency (hz)")\n    plt.xlim([0, timebins-1])\n    plt.ylim([0, freqbins])\n\n    xlocs = np.float32(np.linspace(0, timebins-1, 5))\n    plt.xticks(xlocs, ["%.02f" % l for l in ((xlocs*len(samples)/timebins)+(0.5*binsize))/samplerate])\n    ylocs = np.int16(np.round(np.linspace(0, freqbins-1, 10)))\n    plt.yticks(ylocs, ["%.02f" % freq[i] for i in ylocs])\n\n    if plotpath:\n        plt.savefig(plotpath, bbox_inches="tight")\n    else:\n        plt.show()\n\n    plt.clf()\n\n    return ims\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"plot = plot_spectrogram('dataset/ESC-50/audio/' + esc50_df[esc50_df['category'] == 'crow']['filename'].iloc[0], categorie='Crow')\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Audio Classification with Computer Vision",src:t(31671).Z,width:"1168",height:"663"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"plot = plot_spectrogram('dataset/ESC-50/audio/' + esc50_df[esc50_df['category'] == 'toilet_flush']['filename'].iloc[0], categorie='Toilet Flush')\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Audio Classification with Computer Vision",src:t(67287).Z,width:"1168",height:"663"})),(0,i.kt)("h2",{id:"data-preprocessing"},"Data Preprocessing"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'def audio_vis(location, filepath, binsize=2**10, colormap="jet"):\n    samplerate, samples = wav.read(location)\n\n    s = fourier_transformation(samples, binsize)\n\n    sshow, freq = make_logscale(s, factor=1.0, sr=samplerate)\n\n    with np.errstate(divide=\'ignore\'):\n        ims = 20.*np.log10(np.abs(sshow)/10e-6) # amplitude to decibel\n\n    timebins, freqbins = np.shape(ims)\n\n    plt.figure(figsize=(15, 7.5))\n    plt.imshow(np.transpose(ims), origin="lower", aspect="auto", cmap=colormap, interpolation="none")\n\n    plt.axis(\'off\')\n    plt.xlim([0, timebins-1])\n    plt.ylim([0, freqbins])\n    \n    plt.savefig(filepath, bbox_inches="tight")\n    plt.close()\n\n    return\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"conversion = []\n\nfor i in range(len(esc50_df.index)):\n    \n    filename = esc50_df['filename'].iloc[i]\n    location = 'dataset/ESC-50/audio/' + filename\n    category = esc50_df['category'].iloc[i]\n    catpath = 'dataset/ESC-50/spectrogram/' + category\n    filepath = catpath + '/' + filename[:-4] + '.jpg'\n\n    conversion.append({location, filepath})\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"conversion[0]\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"{'dataset/ESC-50/audio/1-100032-A-0.wav',\n 'dataset/ESC-50/spectrogram/dog/1-100032-A-0.jpg'}\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"for i in range(len(esc50_df.index)):\n    \n    filename = esc50_df['filename'].iloc[i]\n    location = 'dataset/ESC-50/audio/' + filename\n    category = esc50_df['category'].iloc[i]\n    catpath = 'dataset/ESC-50/spectrogram/' + category\n    filepath = catpath + '/' + filename[:-4] + '.jpg'\n\n    os.makedirs(catpath, exist_ok=True)\n    \n    audio_vis(location, filepath)\n")),(0,i.kt)("h3",{id:"train-test-split"},"Train-Test-Split"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"!pip install split-folders\nimport splitfolders\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"input_folder = 'dataset/ESC-50/spectrogram'\noutput = 'data'\n\nsplitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .2))\n")),(0,i.kt)("h3",{id:"prepare-validation-data"},"Prepare Validation Data"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"testing = [\n    'data/test/helicopter.wav',\n    'data/test/cat.wav'\n]\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'def test_vis(location, filepath, binsize=2**10, colormap="jet"):\n    samplerate, samples = wav.read(location)\n\n    s = fourier_transformation(samples, binsize)\n\n    sshow, freq = make_logscale(s, factor=1.0, sr=samplerate)\n\n    with np.errstate(divide=\'ignore\'):\n        ims = 20.*np.log10(np.abs(sshow)/10e-6) # amplitude to decibel\n\n    timebins, freqbins = np.shape(ims)\n\n    plt.figure(figsize=(15, 7.5))\n    plt.imshow(np.transpose(ims), origin="lower", aspect="auto", cmap=colormap, interpolation="none")\n\n    plt.axis(\'off\')\n    plt.xlim([0, timebins-1])\n    plt.ylim([0, freqbins])\n    \n    plt.savefig(filepath, bbox_inches="tight")\n    plt.close()\n\n    return\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"test_vis(testing[0], filepath='data/test/helicopter.jpg')\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"test_vis(testing[1], filepath='data/test/cat.jpg')\n")),(0,i.kt)("h2",{id:"model-training"},"Model Training"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from ultralytics import YOLO\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"model = YOLO('yolov8n-cls.pt')\n#model = YOLO('yolov8s-cls.pt')\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"results = model.train(data='./data', epochs=20, imgsz=640)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"metrics = model.val()\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Audio Classification with Computer Vision",src:t(25440).Z,width:"3000",height:"2250"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"print(metrics.top1)\nprint(metrics.top5)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"0.7824999690055847\n0.9524999856948853\n")),(0,i.kt)("h2",{id:"model-predictions"},"Model Predictions"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# Predict with the model\npred = model('data/test/helicopter.jpg')\n\n# helicopter 0.56, crying_baby 0.09, crickets 0.08, sea_waves 0.07, snoring 0.07, 3.5ms\n# Speed: 4.9ms preprocess, 3.5ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# Predict with the model\npred = model('data/test/cat.jpg')\n\n# cat 0.55, rooster 0.23, crying_baby 0.22, laughing 0.00, siren 0.00, 3.5ms\n# Speed: 13.0ms preprocess, 3.5ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n")))}d.isMDXComponent=!0},31671:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/class_label_crow-77084ae23c651b1372e37cbd24cbe905.webp"},67287:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/class_label_toilet_flush-cdbf02892d3c3b46ab810c48abf0e617.webp"},25440:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/confusion_matrix_normalized-1b1f3c39efc3e63170f39079544b57f8.webp"},72256:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-252551beac0b36b4ba53ccd380897f8e.jpg"}}]);