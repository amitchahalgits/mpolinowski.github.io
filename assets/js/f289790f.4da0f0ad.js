"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[99877],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>u});var i=t(67294);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,i)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function c(e,n){if(null==e)return{};var t,i,o=function(e,n){if(null==e)return{};var t,i,o={},a=Object.keys(e);for(i=0;i<a.length;i++)t=a[i],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(i=0;i<a.length;i++)t=a[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var l=i.createContext({}),s=function(e){var n=i.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},p=function(e){var n=s(e.components);return i.createElement(l.Provider,{value:n},e.children)},d={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},m=i.forwardRef((function(e,n){var t=e.components,o=e.mdxType,a=e.originalType,l=e.parentName,p=c(e,["components","mdxType","originalType","parentName"]),m=s(t),u=o,g=m["".concat(l,".").concat(u)]||m[u]||d[u]||a;return t?i.createElement(g,r(r({ref:n},p),{},{components:t})):i.createElement(g,r({ref:n},p))}));function u(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var a=t.length,r=new Array(a);r[0]=m;var c={};for(var l in n)hasOwnProperty.call(n,l)&&(c[l]=n[l]);c.originalType=e,c.mdxType="string"==typeof e?e:o,r[1]=c;for(var s=2;s<a;s++)r[s]=t[s];return i.createElement.apply(null,r)}return i.createElement.apply(null,t)}m.displayName="MDXCreateElement"},80015:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>c,toc:()=>s});var i=t(87462),o=(t(67294),t(3905));const a={sidebar_position:6970,slug:"2023-02-07",title:"Camera Surveillance System with OpenCV",authors:"mpolinowski",tags:["IoT"],image:"https://mpolinowski.github.io/img/search/mqtt.png",description:"Using OpenCV to detect motion in an RTSP Stream and display detection in an PyQt Interface"},r=void 0,c={unversionedId:"IoT-and-Machine-Learning/Home_Automation/2023-02-07-python-home-security/index",id:"IoT-and-Machine-Learning/Home_Automation/2023-02-07-python-home-security/index",title:"Camera Surveillance System with OpenCV",description:"Using OpenCV to detect motion in an RTSP Stream and display detection in an PyQt Interface",source:"@site/docs/IoT-and-Machine-Learning/Home_Automation/2023-02-07-python-home-security/index.md",sourceDirName:"IoT-and-Machine-Learning/Home_Automation/2023-02-07-python-home-security",slug:"/IoT-and-Machine-Learning/Home_Automation/2023-02-07-python-home-security/2023-02-07",permalink:"/docs/IoT-and-Machine-Learning/Home_Automation/2023-02-07-python-home-security/2023-02-07",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/Home_Automation/2023-02-07-python-home-security/index.md",tags:[{label:"IoT",permalink:"/docs/tags/io-t"}],version:"current",sidebarPosition:6970,frontMatter:{sidebar_position:6970,slug:"2023-02-07",title:"Camera Surveillance System with OpenCV",authors:"mpolinowski",tags:["IoT"],image:"https://mpolinowski.github.io/img/search/mqtt.png",description:"Using OpenCV to detect motion in an RTSP Stream and display detection in an PyQt Interface"},sidebar:"tutorialSidebar",previous:{title:"Home Automation",permalink:"/docs/category/home-automation"},next:{title:"OpenThread Border Router with Docker with Docker",permalink:"/docs/IoT-and-Machine-Learning/Home_Automation/2023-01-23-thread-edge-router-docker/2023-01-23"}},l={},s=[{value:"Buidling the User Interface",id:"buidling-the-user-interface",level:2},{value:"Backend Code",id:"backend-code",level:2},{value:"Handling UI Interactions",id:"handling-ui-interactions",level:3},{value:"Getting the Livevideo",id:"getting-the-livevideo",level:3},{value:"Motion Detection",id:"motion-detection",level:3}],p={toc:s};function d(e){let{components:n,...a}=e;return(0,o.kt)("wrapper",(0,i.Z)({},p,a,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Guangzhou, China",src:t(59224).Z,width:"1500",height:"720"})),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#buidling-the-user-interface"},"Buidling the User Interface")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#backend-code"},"Backend Code"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#handling-ui-interactions"},"Handling UI Interactions")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#getting-the-livevideo"},"Getting the Livevideo")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#motion-detection"},"Motion Detection"))))),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/opencv-pyqt-dvr"},"Github Repository")),(0,o.kt)("p",null,"This Project is going to use ",(0,o.kt)("strong",{parentName:"p"},"PyQt6")," to build a Desktop app connected to an ",(0,o.kt)("strong",{parentName:"p"},"OpenCV")," backend to process an INSTAR IP camera RTSP livestream and perform an ",(0,o.kt)("strong",{parentName:"p"},"Object Detection"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install pyqt6 pyqt6-tools opencv-python\n")),(0,o.kt)("p",null,"You should now be able to start the Qt Designer by typing ",(0,o.kt)("inlineCode",{parentName:"p"},"designer"),":"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Camera Surveillance System with OpenCV",src:t(9082).Z,width:"1703",height:"814"})),(0,o.kt)("h2",{id:"buidling-the-user-interface"},"Buidling the User Interface"),(0,o.kt)("p",null,"You can load the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/opencv-pyqt-dvr/blob/master/opencv-pyqt-dvr.ui"},"UI file")," and edit the user interface according to your needs:"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Camera Surveillance System with OpenCV",src:t(92913).Z,width:"1620",height:"997"})),(0,o.kt)("h2",{id:"backend-code"},"Backend Code"),(0,o.kt)("p",null,"In the previous step we generated an XML file with the ending ",(0,o.kt)("inlineCode",{parentName:"p"},".ui"),". We can now write a Python script that will start a Qt window application using this definition file:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},"from PyQt6.QtCore import *\nfrom PyQt6.QtGui import *\nfrom PyQt6.QtWidgets import *\nfrom PyQt6.uic import loadUiType\n\nimport sys\n\n# load ui definition from Qt designer\nui, _ = loadUiType('opencv-pyqt-dvr.ui')\n\n# initialize application\nclass DvrDashboard(QMainWindow, ui):\n    def __init__(self):\n        QMainWindow.__init__(self)\n        self.setupUi(self)\n\n# execute the Qt window\ndef main():\n    app = QApplication(sys.argv)\n    window = DvrDashboard()\n    window.show()\n    app.exec()\n\n# start\nif __name__ == '__main__':\n    main()\n")),(0,o.kt)("h3",{id:"handling-ui-interactions"},"Handling UI Interactions"),(0,o.kt)("p",null,"Next we need to provide functions that catch button presses inside the Qt application window and execute some code in our backend:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},'# initialize application\nclass DvrDashboard(QMainWindow, ui):\n    def __init__(self):\n        QMainWindow.__init__(self)\n        self.setupUi(self)\n        # handle clicks on button "monitoring"\n        self.monitoring.clicked.connect(self.get_livestream)\n        # handle clicks on button "exit"\n        self.exit.clicked.connect(self.close_window)\n\n    def get_livestream(self):\n        print("INFO :: Connecting to IP Camera")\n\n    def close_window(self):\n        print("WARNING :: Application shutdown")\n')),(0,o.kt)("h3",{id:"getting-the-livevideo"},"Getting the Livevideo"),(0,o.kt)("p",null,"I will use an ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/opencv-rtsp"},"INSTAR IP camera's RTSP Stream to feed OpenCV a live video"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},"def get_livestream(self):\n    print(\"INFO :: Connecting to IP Camera\")\n    cap = cv2.VideoCapture(RTSP_URL, cv2.CAP_FFMPEG)\n\n    if not cap.isOpened():\n        print('ERROR :: Cannot open RTSP stream')\n        exit(-1)\n\n    while True:\n        success, img = cap.read()\n        cv2.imshow(RTSP_URL, img)\n\n        if cv2.waitKey(1) == 27:  # Keep running until you press `esc`\n            break\n        \n    cap.release()\n    cv2.destroyAllWindows()\n")),(0,o.kt)("h3",{id:"motion-detection"},"Motion Detection"),(0,o.kt)("p",null,"For motion detection we can take a second still frame from our camera and compare it to the first one. As long as they both match no motion happened between both captures:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},"while True:\n    # get frame from livestream\n    success, img = cap.read()\n    # get second frame for motion detection\n    _, val_img = cap.read()\n    # get absolute difference between both frames\n    delta = cv2.absdiff(img, val_img)\n    # find contours in delta for moving object location\n    grayscale  = cv2.cvtColor(delta, cv2.COLOR_BGR2GRAY)\n    blur = cv2.GaussianBlur(grayscale, (5,5), 0)\n    canny = cv2.Canny(blur, 35, 75)\n    dilated = cv2.dilate(canny, None, iterations=3)\n    contours, _ = cv2.findContours(dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    \n    for ctr in contours:\n        # filter small objects\n        if cv2.contourArea(ctr) < 5000:\n            continue\n        # get bounding box location\n        x,y,w,h = cv2.boundingRect(ctr)\n        # draw bounding box around object\n        cv2.rectangle(img, (x,y), (x+w, y+h), (204,119,0), 2)\n        # get canny image\n        cv2.imwrite('detection_object_contour.jpg', canny)\n        # get marked image\n        cv2.imwrite('detection_bounding_box.jpg', img)\n\n        # display detection contour in frontend\n        detection_contour = QImage('detection_object_contour.jpg')\n        detection_contour_map = QPixmap.fromImage(detection_contour)\n        self.detection_contour.setPixmap(detection_contour_map)\n        # display detection bounding box in frontend\n        detection_image = QImage('detection_bounding_box.jpg')\n        detection_image_map = QPixmap.fromImage(detection_image)\n        self.detection.setPixmap(detection_image_map)\n                \n    # also show live video for reference\n    cv2.imshow(RTSP_URL, img)\n\n    if cv2.waitKey(1) == 27:  # Keep running until you press `esc`\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"INSTAR IP Camera Motion Detector",src:t(54953).Z,width:"399",height:"513"})))}d.isMDXComponent=!0},9082:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/OpenCV_PyQT_DVR_01-8ac9ce6d31e49e00f9c0616d7fe62cba.png"},54953:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/OpenCV_PyQT_DVR_02-420f5ea3ccb39618a5817a5bf027ac8f.gif"},92913:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/OpenCV_PyQT_DVR_02-c09703130193b52f792c96fcfa747032.png"},59224:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-0753bda3286b7a74d2498aafe8332ad5.jpg"}}]);