"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[96937],{93991:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>r,default:()=>p,frontMatter:()=>t,metadata:()=>l,toc:()=>c});var s=i(474848),a=i(28453);const t={sidebar_position:4085,slug:"2023-09-23",title:"Audio Classification with Computer Vision",authors:"mpolinowski",tags:["Python","Machine Learning","PyTorch"],description:"Training an YOLOv8 Classifier to Identify Audio Files"},r="Audio Classification with Computer Vision",l={id:"IoT-and-Machine-Learning/ML/2023-09-23--yolo8-listen/index",title:"Audio Classification with Computer Vision",description:"Training an YOLOv8 Classifier to Identify Audio Files",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-09-23--yolo8-listen/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-09-23--yolo8-listen",slug:"/IoT-and-Machine-Learning/ML/2023-09-23--yolo8-listen/2023-09-23",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-09-23--yolo8-listen/2023-09-23",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-09-23--yolo8-listen/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"PyTorch",permalink:"/docs/tags/py-torch"}],version:"current",sidebarPosition:4085,frontMatter:{sidebar_position:4085,slug:"2023-09-23",title:"Audio Classification with Computer Vision",authors:"mpolinowski",tags:["Python","Machine Learning","PyTorch"],description:"Training an YOLOv8 Classifier to Identify Audio Files"},sidebar:"tutorialSidebar",previous:{title:"DLIB Face Recognition",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-10-01--delib-face-detection/2023-10-01"},next:{title:"CVAT Semi-automatic and Automatic Annotation",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-09-21--cvat-automatic-annotation/2023-09-21"}},o={},c=[{value:"Visualize the Dataset",id:"visualize-the-dataset",level:2},{value:"Data Preprocessing",id:"data-preprocessing",level:2},{value:"Train-Test-Split",id:"train-test-split",level:3},{value:"Prepare Validation Data",id:"prepare-validation-data",level:3},{value:"Model Training",id:"model-training",level:2},{value:"Model Predictions",id:"model-predictions",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"TST, Hongkong",src:i(588429).A+"",width:"1500",height:"620"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"#audio-classification-with-computer-vision",children:"Audio Classification with Computer Vision"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#visualize-the-dataset",children:"Visualize the Dataset"})}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"#data-preprocessing",children:"Data Preprocessing"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#train-test-split",children:"Train-Test-Split"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#prepare-validation-data",children:"Prepare Validation Data"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#model-training",children:"Model Training"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#model-predictions",children:"Model Predictions"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/mpolinowski/yolo-listen",children:"Github Repository"})}),"\n"]}),"\n",(0,s.jsx)(n.h1,{id:"audio-classification-with-computer-vision",children:"Audio Classification with Computer Vision"}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.a,{href:"https://github.com/karolpiczak/ESC-50",children:"ESC-50 dataset"})," is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification."]}),"\n",(0,s.jsx)(n.p,{children:"The dataset consists of 5-second-long recordings organized into 50 semantical classes (with 40 examples per class) loosely arranged into 5 major categories:"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import numpy as np\nfrom matplotlib import pyplot as plt\nfrom numpy.lib import stride_tricks\nimport os\nimport pandas as pd\nimport scipy.io.wavfile as wav\n"})}),"\n",(0,s.jsx)(n.h2,{id:"visualize-the-dataset",children:"Visualize the Dataset"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"esc50_df = pd.read_csv('dataset/ESC-50/esc50.csv')\nesc50_df.head()\n"})}),"\n",(0,s.jsx)("div",{children:(0,s.jsxs)("table",{children:[(0,s.jsx)("thead",{children:(0,s.jsxs)("tr",{children:[(0,s.jsx)("th",{}),(0,s.jsx)("th",{children:"filename"}),(0,s.jsx)("th",{children:"fold"}),(0,s.jsx)("th",{children:"target"}),(0,s.jsx)("th",{children:"category"}),(0,s.jsx)("th",{children:"esc10"}),(0,s.jsx)("th",{children:"src_file"}),(0,s.jsx)("th",{children:"take"})]})}),(0,s.jsxs)("tbody",{children:[(0,s.jsxs)("tr",{children:[(0,s.jsx)("th",{children:"0"}),(0,s.jsx)("td",{children:"1-100032-A-0.wav"}),(0,s.jsx)("td",{children:"1"}),(0,s.jsx)("td",{children:"0"}),(0,s.jsx)("td",{children:"dog"}),(0,s.jsx)("td",{children:"True"}),(0,s.jsx)("td",{children:"100032"}),(0,s.jsx)("td",{children:"A"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("th",{children:"1"}),(0,s.jsx)("td",{children:"1-100038-A-14.wav"}),(0,s.jsx)("td",{children:"1"}),(0,s.jsx)("td",{children:"14"}),(0,s.jsx)("td",{children:"chirping_birds"}),(0,s.jsx)("td",{children:"False"}),(0,s.jsx)("td",{children:"100038"}),(0,s.jsx)("td",{children:"A"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("th",{children:"2"}),(0,s.jsx)("td",{children:"1-100210-A-36.wav"}),(0,s.jsx)("td",{children:"1"}),(0,s.jsx)("td",{children:"36"}),(0,s.jsx)("td",{children:"vacuum_cleaner"}),(0,s.jsx)("td",{children:"False"}),(0,s.jsx)("td",{children:"100210"}),(0,s.jsx)("td",{children:"A"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("th",{children:"3"}),(0,s.jsx)("td",{children:"1-100210-B-36.wav"}),(0,s.jsx)("td",{children:"1"}),(0,s.jsx)("td",{children:"36"}),(0,s.jsx)("td",{children:"vacuum_cleaner"}),(0,s.jsx)("td",{children:"False"}),(0,s.jsx)("td",{children:"100210"}),(0,s.jsx)("td",{children:"B"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("th",{children:"4"}),(0,s.jsx)("td",{children:"1-101296-A-19.wav"}),(0,s.jsx)("td",{children:"1"}),(0,s.jsx)("td",{children:"19"}),(0,s.jsx)("td",{children:"thunderstorm"}),(0,s.jsx)("td",{children:"False"}),(0,s.jsx)("td",{children:"101296"}),(0,s.jsx)("td",{children:"A"})]})]})]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"esc50_df['category'].value_counts()\n"})}),"\n",(0,s.jsx)(n.p,{children:"dog                 40\nglass_breaking      40\ndrinking_sipping    40\nrain                40\ninsects             40\nlaughing            40\nhen                 40\nengine              40\nbreathing           40\ncrying_baby         40\nhand_saw            40\ncoughing            40\nsnoring             40\nchirping_birds      40\ntoilet_flush        40\npig                 40\nwashing_machine     40\nclock_tick          40\nsneezing            40\nrooster             40\nsea_waves           40\nsiren               40\ncat                 40\ndoor_wood_creaks    40\nhelicopter          40\ncrackling_fire      40\ncar_horn            40\nbrushing_teeth      40\nvacuum_cleaner      40\nthunderstorm        40\ndoor_wood_knock     40\ncan_opening         40\ncrow                40\nclapping            40\nfireworks           40\nchainsaw            40\nairplane            40\nmouse_click         40\npouring_water       40\ntrain               40\nsheep               40\nwater_drops         40\nchurch_bells        40\nclock_alarm         40\nkeyboard_typing     40\nwind                40\nfootsteps           40\nfrog                40\ncow                 40\ncrickets            40\nName: category, dtype: int64"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def fourier_transformation(sig, frameSize, overlapFac=0.5, window=np.hanning):\n    win = window(frameSize)\n    hopSize = int(frameSize - np.floor(overlapFac * frameSize))\n\n    # zeros at beginning (thus center of 1st window should be for sample nr. 0)   \n    samples = np.append(np.zeros(int(np.floor(frameSize/2.0))), sig)    \n    # cols for windowing\n    cols = np.ceil( (len(samples) - frameSize) / float(hopSize)) + 1\n    # zeros at end (thus samples can be fully covered by frames)\n    samples = np.append(samples, np.zeros(frameSize))\n\n    frames = stride_tricks.as_strided(samples, shape=(int(cols), frameSize), strides=(samples.strides[0]*hopSize, samples.strides[0])).copy()\n    frames *= win\n\n    return np.fft.rfft(frames) \n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def make_logscale(spec, sr=44100, factor=20.):\n    timebins, freqbins = np.shape(spec)\n\n    scale = np.linspace(0, 1, freqbins) ** factor\n    scale *= (freqbins-1)/max(scale)\n    scale = np.unique(np.round(scale))\n\n    # create spectrogram with new freq bins\n    newspec = np.complex128(np.zeros([timebins, len(scale)]))\n    for i in range(0, len(scale)):        \n        if i == len(scale)-1:\n            newspec[:,i] = np.sum(spec[:,int(scale[i]):], axis=1)\n        else:        \n            newspec[:,i] = np.sum(spec[:,int(scale[i]):int(scale[i+1])], axis=1)\n\n    # list center freq of bins\n    allfreqs = np.abs(np.fft.fftfreq(freqbins*2, 1./sr)[:freqbins+1])\n    freqs = []\n    for i in range(0, len(scale)):\n        if i == len(scale)-1:\n            freqs += [np.mean(allfreqs[int(scale[i]):])]\n        else:\n            freqs += [np.mean(allfreqs[int(scale[i]):int(scale[i+1])])]\n\n    return newspec, freqs\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def plot_spectrogram(location, categorie, plotpath=None, binsize=2**10, colormap="jet"):\n    samplerate, samples = wav.read(location)\n\n    s = fourier_transformation(samples, binsize)\n\n    sshow, freq = make_logscale(s, factor=1.0, sr=samplerate)\n\n    ims = 20.*np.log10(np.abs(sshow)/10e-6) # amplitude to decibel\n\n    timebins, freqbins = np.shape(ims)\n\n    print("timebins: ", timebins)\n    print("freqbins: ", freqbins)\n\n    plt.figure(figsize=(15, 7.5))\n    plt.title(\'Class Label: \' + categorie)\n    plt.imshow(np.transpose(ims), origin="lower", aspect="auto", cmap=colormap, interpolation="none")\n    plt.colorbar()\n\n    plt.xlabel("time (s)")\n    plt.ylabel("frequency (hz)")\n    plt.xlim([0, timebins-1])\n    plt.ylim([0, freqbins])\n\n    xlocs = np.float32(np.linspace(0, timebins-1, 5))\n    plt.xticks(xlocs, ["%.02f" % l for l in ((xlocs*len(samples)/timebins)+(0.5*binsize))/samplerate])\n    ylocs = np.int16(np.round(np.linspace(0, freqbins-1, 10)))\n    plt.yticks(ylocs, ["%.02f" % freq[i] for i in ylocs])\n\n    if plotpath:\n        plt.savefig(plotpath, bbox_inches="tight")\n    else:\n        plt.show()\n\n    plt.clf()\n\n    return ims\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"plot = plot_spectrogram('dataset/ESC-50/audio/' + esc50_df[esc50_df['category'] == 'crow']['filename'].iloc[0], categorie='Crow')\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Audio Classification with Computer Vision",src:i(4139).A+"",width:"1168",height:"663"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"plot = plot_spectrogram('dataset/ESC-50/audio/' + esc50_df[esc50_df['category'] == 'toilet_flush']['filename'].iloc[0], categorie='Toilet Flush')\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Audio Classification with Computer Vision",src:i(733564).A+"",width:"1168",height:"663"})}),"\n",(0,s.jsx)(n.h2,{id:"data-preprocessing",children:"Data Preprocessing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def audio_vis(location, filepath, binsize=2**10, colormap="jet"):\n    samplerate, samples = wav.read(location)\n\n    s = fourier_transformation(samples, binsize)\n\n    sshow, freq = make_logscale(s, factor=1.0, sr=samplerate)\n\n    with np.errstate(divide=\'ignore\'):\n        ims = 20.*np.log10(np.abs(sshow)/10e-6) # amplitude to decibel\n\n    timebins, freqbins = np.shape(ims)\n\n    plt.figure(figsize=(15, 7.5))\n    plt.imshow(np.transpose(ims), origin="lower", aspect="auto", cmap=colormap, interpolation="none")\n\n    plt.axis(\'off\')\n    plt.xlim([0, timebins-1])\n    plt.ylim([0, freqbins])\n    \n    plt.savefig(filepath, bbox_inches="tight")\n    plt.close()\n\n    return\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"conversion = []\n\nfor i in range(len(esc50_df.index)):\n    \n    filename = esc50_df['filename'].iloc[i]\n    location = 'dataset/ESC-50/audio/' + filename\n    category = esc50_df['category'].iloc[i]\n    catpath = 'dataset/ESC-50/spectrogram/' + category\n    filepath = catpath + '/' + filename[:-4] + '.jpg'\n\n    conversion.append({location, filepath})\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"conversion[0]\n"})}),"\n","dataset/ESC-50/spectrogram/dog/1-100032-A-0.jpg","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"for i in range(len(esc50_df.index)):\n    \n    filename = esc50_df['filename'].iloc[i]\n    location = 'dataset/ESC-50/audio/' + filename\n    category = esc50_df['category'].iloc[i]\n    catpath = 'dataset/ESC-50/spectrogram/' + category\n    filepath = catpath + '/' + filename[:-4] + '.jpg'\n\n    os.makedirs(catpath, exist_ok=True)\n    \n    audio_vis(location, filepath)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"train-test-split",children:"Train-Test-Split"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"!pip install split-folders\nimport splitfolders\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"input_folder = 'dataset/ESC-50/spectrogram'\noutput = 'data'\n\nsplitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .2))\n"})}),"\n",(0,s.jsx)(n.h3,{id:"prepare-validation-data",children:"Prepare Validation Data"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"testing = [\n    'data/test/helicopter.wav',\n    'data/test/cat.wav'\n]\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def test_vis(location, filepath, binsize=2**10, colormap="jet"):\n    samplerate, samples = wav.read(location)\n\n    s = fourier_transformation(samples, binsize)\n\n    sshow, freq = make_logscale(s, factor=1.0, sr=samplerate)\n\n    with np.errstate(divide=\'ignore\'):\n        ims = 20.*np.log10(np.abs(sshow)/10e-6) # amplitude to decibel\n\n    timebins, freqbins = np.shape(ims)\n\n    plt.figure(figsize=(15, 7.5))\n    plt.imshow(np.transpose(ims), origin="lower", aspect="auto", cmap=colormap, interpolation="none")\n\n    plt.axis(\'off\')\n    plt.xlim([0, timebins-1])\n    plt.ylim([0, freqbins])\n    \n    plt.savefig(filepath, bbox_inches="tight")\n    plt.close()\n\n    return\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"test_vis(testing[0], filepath='data/test/helicopter.jpg')\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"test_vis(testing[1], filepath='data/test/cat.jpg')\n"})}),"\n",(0,s.jsx)(n.h2,{id:"model-training",children:"Model Training"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from ultralytics import YOLO\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"model = YOLO('yolov8n-cls.pt')\n#model = YOLO('yolov8s-cls.pt')\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"results = model.train(data='./data', epochs=20, imgsz=640)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"metrics = model.val()\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Audio Classification with Computer Vision",src:i(58450).A+"",width:"3000",height:"2250"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"print(metrics.top1)\nprint(metrics.top5)\n"})}),"\n",(0,s.jsx)(n.p,{children:"0.7824999690055847\n0.9524999856948853"}),"\n",(0,s.jsx)(n.h2,{id:"model-predictions",children:"Model Predictions"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Predict with the model\npred = model('data/test/helicopter.jpg')\n\n# helicopter 0.56, crying_baby 0.09, crickets 0.08, sea_waves 0.07, snoring 0.07, 3.5ms\n# Speed: 4.9ms preprocess, 3.5ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Predict with the model\npred = model('data/test/cat.jpg')\n\n# cat 0.55, rooster 0.23, crying_baby 0.22, laughing 0.00, siren 0.00, 3.5ms\n# Speed: 13.0ms preprocess, 3.5ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"})})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},4139:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/class_label_crow-77084ae23c651b1372e37cbd24cbe905.webp"},733564:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/class_label_toilet_flush-cdbf02892d3c3b46ab810c48abf0e617.webp"},58450:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/confusion_matrix_normalized-1b1f3c39efc3e63170f39079544b57f8.webp"},588429:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-252551beac0b36b4ba53ccd380897f8e.jpg"},28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var s=i(296540);const a={},t=s.createContext(a);function r(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);