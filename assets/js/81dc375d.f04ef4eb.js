"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[75886],{3905:(t,e,a)=>{a.d(e,{Zo:()=>d,kt:()=>g});var n=a(67294);function l(t,e,a){return e in t?Object.defineProperty(t,e,{value:a,enumerable:!0,configurable:!0,writable:!0}):t[e]=a,t}function r(t,e){var a=Object.keys(t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(t);e&&(n=n.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),a.push.apply(a,n)}return a}function i(t){for(var e=1;e<arguments.length;e++){var a=null!=arguments[e]?arguments[e]:{};e%2?r(Object(a),!0).forEach((function(e){l(t,e,a[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(a,e))}))}return t}function o(t,e){if(null==t)return{};var a,n,l=function(t,e){if(null==t)return{};var a,n,l={},r=Object.keys(t);for(n=0;n<r.length;n++)a=r[n],e.indexOf(a)>=0||(l[a]=t[a]);return l}(t,e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(t);for(n=0;n<r.length;n++)a=r[n],e.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(t,a)&&(l[a]=t[a])}return l}var s=n.createContext({}),p=function(t){var e=n.useContext(s),a=e;return t&&(a="function"==typeof t?t(e):i(i({},e),t)),a},d=function(t){var e=p(t.components);return n.createElement(s.Provider,{value:e},t.children)},m={inlineCode:"code",wrapper:function(t){var e=t.children;return n.createElement(n.Fragment,{},e)}},u=n.forwardRef((function(t,e){var a=t.components,l=t.mdxType,r=t.originalType,s=t.parentName,d=o(t,["components","mdxType","originalType","parentName"]),u=p(a),g=l,k=u["".concat(s,".").concat(g)]||u[g]||m[g]||r;return a?n.createElement(k,i(i({ref:e},d),{},{components:a})):n.createElement(k,i({ref:e},d))}));function g(t,e){var a=arguments,l=e&&e.mdxType;if("string"==typeof t||l){var r=a.length,i=new Array(r);i[0]=u;var o={};for(var s in e)hasOwnProperty.call(e,s)&&(o[s]=e[s]);o.originalType=t,o.mdxType="string"==typeof t?t:l,i[1]=o;for(var p=2;p<r;p++)i[p]=a[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},37250:(t,e,a)=>{a.r(e),a.d(e,{assets:()=>s,contentTitle:()=>i,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>p});var n=a(87462),l=(a(67294),a(3905));const r={sidebar_position:4750,slug:"2023-01-30",title:"SciKit Wine Quality",authors:"mpolinowski",tags:["Python","Machine Learning","SciKit"],description:"Predicting Wine Quality with Several Classification Techniques using SciKit Learn."},i=void 0,o={unversionedId:"IoT-and-Machine-Learning/ML/2023-01-30-predicting-wine-quality/index",id:"IoT-and-Machine-Learning/ML/2023-01-30-predicting-wine-quality/index",title:"SciKit Wine Quality",description:"Predicting Wine Quality with Several Classification Techniques using SciKit Learn.",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-01-30-predicting-wine-quality/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-01-30-predicting-wine-quality",slug:"/IoT-and-Machine-Learning/ML/2023-01-30-predicting-wine-quality/2023-01-30",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-01-30-predicting-wine-quality/2023-01-30",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-01-30-predicting-wine-quality/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"SciKit",permalink:"/docs/tags/sci-kit"}],version:"current",sidebarPosition:4750,frontMatter:{sidebar_position:4750,slug:"2023-01-30",title:"SciKit Wine Quality",authors:"mpolinowski",tags:["Python","Machine Learning","SciKit"],description:"Predicting Wine Quality with Several Classification Techniques using SciKit Learn."},sidebar:"tutorialSidebar",previous:{title:"Keras for Tensorflow - An (Re)Introduction 2023",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-02-14-keras-introduction/2023-02-14"},next:{title:"OpenCV Count My Money",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-01-28-opencv-coin-counter/2023-01-28"}},s={},p=[{value:"Dataset Exploration",id:"dataset-exploration",level:2},{value:"Feature Importance",id:"feature-importance",level:3},{value:"Red Wines",id:"red-wines",level:4},{value:"White Wines",id:"white-wines",level:4},{value:"Data Pre-Processing",id:"data-pre-processing",level:2},{value:"Binary Classification",id:"binary-classification",level:3},{value:"Data Normalization",id:"data-normalization",level:3},{value:"Data Splitting",id:"data-splitting",level:3},{value:"Fitting a Model",id:"fitting-a-model",level:2},{value:"Decision Tree",id:"decision-tree",level:3},{value:"Random Forest",id:"random-forest",level:3},{value:"AdaBoost",id:"adaboost",level:3},{value:"Gradient Boosting",id:"gradient-boosting",level:3},{value:"XGBoost",id:"xgboost",level:3}],d={toc:p};function m(t){let{components:e,...r}=t;return(0,l.kt)("wrapper",(0,n.Z)({},d,r,{components:e,mdxType:"MDXLayout"}),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"Guangzhou, China",src:a(85287).Z,width:"2385",height:"883"})),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#dataset-exploration"},"Dataset Exploration"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#feature-importance"},"Feature Importance"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#red-wines"},"Red Wines")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#white-wines"},"White Wines")))))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#data-pre-processing"},"Data Pre-Processing"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#binary-classification"},"Binary Classification")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#data-normalization"},"Data Normalization")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#data-splitting"},"Data Splitting")))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#fitting-a-model"},"Fitting a Model"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#decision-tree"},"Decision Tree")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#random-forest"},"Random Forest")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#adaboost"},"AdaBoost")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#gradient-boosting"},"Gradient Boosting")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#xgboost"},"XGBoost"))))),(0,l.kt)("p",null,(0,l.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/scikit-wine-quality"},"Github Repository")),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"Based on ",(0,l.kt)("a",{parentName:"p",href:"https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009"},"Red Wine Quality"),", ",(0,l.kt)("em",{parentName:"p"},"Simple and clean practice dataset for regression or classification modelling"))),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},(0,l.kt)("strong",{parentName:"p"},"Source"),": The two datasets are related to red and white variants of the Portuguese ",(0,l.kt)("a",{parentName:"p",href:"http://www.vinhoverde.pt/en/"},"Vinho Verde")," wine. For more details, consult the reference ",(0,l.kt)("a",{parentName:"p",href:"https://archive.ics.uci.edu/ml/datasets/wine+quality"},"Cortez et al., 2009"),". Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.). ")),(0,l.kt)("h2",{id:"dataset-exploration"},"Dataset Exploration"),(0,l.kt)("p",null,"The quality of a wine is determined by 11 input variables:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Fixed acidity"),(0,l.kt)("li",{parentName:"ul"},"Volatile acidity"),(0,l.kt)("li",{parentName:"ul"},"Citric acid"),(0,l.kt)("li",{parentName:"ul"},"Residual sugar"),(0,l.kt)("li",{parentName:"ul"},"Chlorides"),(0,l.kt)("li",{parentName:"ul"},"Free sulfur dioxide"),(0,l.kt)("li",{parentName:"ul"},"Total sulfur dioxide"),(0,l.kt)("li",{parentName:"ul"},"Density"),(0,l.kt)("li",{parentName:"ul"},"pH"),(0,l.kt)("li",{parentName:"ul"},"Sulfates"),(0,l.kt)("li",{parentName:"ul"},"Alcohol")),(0,l.kt)("p",null,"Start by downloading the dataset:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"mkdir data\nwget --directory-prefix=data https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality.names\nwget --directory-prefix=data https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\nwget --directory-prefix=data https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n")),(0,l.kt)("p",null,"And take a look at it:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'df = pd.read_csv("data/winequality-red.csv")\n\n# See the number of rows and columns\nprint("Rows, columns: " + str(df.shape))\n## Rows, columns: (1599, 1)\n\n# Missing Values\nprint(df.isna().sum())\n# fixed acidity;"volatile acidity";"citric acid";"residual sugar";"chlorides";"free sulfur dioxide";"total sulfur dioxide";"density";"pH";"sulphates";"alcohol";"quality"    0 <- nothing missing\n# dtype: int64\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},"# See the first five rows of the dataset\nprint(df.head())\n")),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"fixed acidity"),(0,l.kt)("th",{parentName:"tr",align:null},"volatile acidity"),(0,l.kt)("th",{parentName:"tr",align:null},"citric acid"),(0,l.kt)("th",{parentName:"tr",align:null},"residual sugar"),(0,l.kt)("th",{parentName:"tr",align:null},"chlorides"),(0,l.kt)("th",{parentName:"tr",align:null},"free sulfur dioxide"),(0,l.kt)("th",{parentName:"tr",align:null},"total sulfur dioxide"),(0,l.kt)("th",{parentName:"tr",align:null},"density"),(0,l.kt)("th",{parentName:"tr",align:null},"pH"),(0,l.kt)("th",{parentName:"tr",align:null},"sulphates"),(0,l.kt)("th",{parentName:"tr",align:null},"alcohol"),(0,l.kt)("th",{parentName:"tr",align:null},"quality"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"7.4"),(0,l.kt)("td",{parentName:"tr",align:null},"0.7"),(0,l.kt)("td",{parentName:"tr",align:null},"0"),(0,l.kt)("td",{parentName:"tr",align:null},"1.9"),(0,l.kt)("td",{parentName:"tr",align:null},"0.076"),(0,l.kt)("td",{parentName:"tr",align:null},"11"),(0,l.kt)("td",{parentName:"tr",align:null},"34"),(0,l.kt)("td",{parentName:"tr",align:null},"0.9978"),(0,l.kt)("td",{parentName:"tr",align:null},"3.51"),(0,l.kt)("td",{parentName:"tr",align:null},"0.56"),(0,l.kt)("td",{parentName:"tr",align:null},"9.4"),(0,l.kt)("td",{parentName:"tr",align:null},"5")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"7.8"),(0,l.kt)("td",{parentName:"tr",align:null},"0.88"),(0,l.kt)("td",{parentName:"tr",align:null},"0"),(0,l.kt)("td",{parentName:"tr",align:null},"2.6"),(0,l.kt)("td",{parentName:"tr",align:null},"0.098"),(0,l.kt)("td",{parentName:"tr",align:null},"25"),(0,l.kt)("td",{parentName:"tr",align:null},"67"),(0,l.kt)("td",{parentName:"tr",align:null},"0.9968"),(0,l.kt)("td",{parentName:"tr",align:null},"3.2"),(0,l.kt)("td",{parentName:"tr",align:null},"0.68"),(0,l.kt)("td",{parentName:"tr",align:null},"9.8"),(0,l.kt)("td",{parentName:"tr",align:null},"5")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"7.8"),(0,l.kt)("td",{parentName:"tr",align:null},"0.76"),(0,l.kt)("td",{parentName:"tr",align:null},"0.04"),(0,l.kt)("td",{parentName:"tr",align:null},"2.3"),(0,l.kt)("td",{parentName:"tr",align:null},"0.092"),(0,l.kt)("td",{parentName:"tr",align:null},"15"),(0,l.kt)("td",{parentName:"tr",align:null},"54"),(0,l.kt)("td",{parentName:"tr",align:null},"0.997"),(0,l.kt)("td",{parentName:"tr",align:null},"3.26"),(0,l.kt)("td",{parentName:"tr",align:null},"0.65"),(0,l.kt)("td",{parentName:"tr",align:null},"9.8"),(0,l.kt)("td",{parentName:"tr",align:null},"5")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"11.2"),(0,l.kt)("td",{parentName:"tr",align:null},"0.28"),(0,l.kt)("td",{parentName:"tr",align:null},"0.56"),(0,l.kt)("td",{parentName:"tr",align:null},"1.9"),(0,l.kt)("td",{parentName:"tr",align:null},"0.075"),(0,l.kt)("td",{parentName:"tr",align:null},"17"),(0,l.kt)("td",{parentName:"tr",align:null},"60"),(0,l.kt)("td",{parentName:"tr",align:null},"0.998"),(0,l.kt)("td",{parentName:"tr",align:null},"3.16"),(0,l.kt)("td",{parentName:"tr",align:null},"0.58"),(0,l.kt)("td",{parentName:"tr",align:null},"9.8"),(0,l.kt)("td",{parentName:"tr",align:null},"6")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"7.4"),(0,l.kt)("td",{parentName:"tr",align:null},"0.7"),(0,l.kt)("td",{parentName:"tr",align:null},"0"),(0,l.kt)("td",{parentName:"tr",align:null},"1.9"),(0,l.kt)("td",{parentName:"tr",align:null},"0.076"),(0,l.kt)("td",{parentName:"tr",align:null},"11"),(0,l.kt)("td",{parentName:"tr",align:null},"34"),(0,l.kt)("td",{parentName:"tr",align:null},"0.9978"),(0,l.kt)("td",{parentName:"tr",align:null},"3.51"),(0,l.kt)("td",{parentName:"tr",align:null},"0.56"),(0,l.kt)("td",{parentName:"tr",align:null},"9.4"),(0,l.kt)("td",{parentName:"tr",align:null},"5")))),(0,l.kt)("p",null,"I ran into an issue trying to plot the quality distribution with ",(0,l.kt)("inlineCode",{parentName:"p"},"plotly")," - I noticed that the source CSV file used ",(0,l.kt)("inlineCode",{parentName:"p"},";")," instead of ",(0,l.kt)("inlineCode",{parentName:"p"},",")," to separate cells. Now it works:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},"# quality distribution\nfig = px.histogram(df,x='quality')\nfig.show()\n")),(0,l.kt)("p",null,"The classes not balanced (e.g. there are much more normal wines than excellent or poor ones):"),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"Wine Quality",src:a(83062).Z,width:"1507",height:"889"})),(0,l.kt)("p",null,"The correlation matrix can show us what labels might have a good correlation with the perceived quality of the wine:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},'## correlation matrix\ncorr = df.corr()\nplt.subplots(figsize=(15,10))\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(240, 170, center="light", as_cmap=True))\nplt.show()\n')),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"Wine Quality",src:a(71904).Z,width:"1500",height:"982"})),(0,l.kt)("h3",{id:"feature-importance"},"Feature Importance"),(0,l.kt)("p",null,"The ",(0,l.kt)("strong",{parentName:"p"},"Correlation Matrix")," already gives us an idea but only after fitting a well performing model can we decide what features are most important when it comes to classifying our wines. ",(0,l.kt)("strong",{parentName:"p"},"SPOILER ALERT"),": From the 5 classifier I will use below the ",(0,l.kt)("strong",{parentName:"p"},"Random Forrest")," and ",(0,l.kt)("strong",{parentName:"p"},"XGBoost")," will have the best performance."),(0,l.kt)("p",null,"We can use the ",(0,l.kt)("inlineCode",{parentName:"p"},"feature_importances_")," function that is provided by the classifiers and extract a ranking of how important is each feature for the resulting classification, sort the results in a Panda Series using ",(0,l.kt)("a",{parentName:"p",href:"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.nlargest.html"},"nlargest")," and ",(0,l.kt)("a",{parentName:"p",href:"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.plot.html?highlight=plot#pandas.Series.plot"},"plot")," the results."),(0,l.kt)("h4",{id:"red-wines"},"Red Wines"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},"## RandomForestClassifier\nfeat_importances_forrest = pd.Series(forrest_model.feature_importances_, index=X_features.columns)\nfeat_importances_forrest.nlargest(11).plot(kind='pie', figsize=(10,10), title=\"Feature Importance :: RandomForestClassifier\")\nplt.show()\n\n## XGBClassifier\nfeat_importances_xbg = pd.Series(xgboost_model.feature_importances_, index=X_features.columns)\nfeat_importances_xbg.nlargest(11).plot(kind='pie', figsize=(10,10), title=\"Feature Importance :: XGBClassifier\")\nplt.show()\n")),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"Wine Quality",src:a(30377).Z,width:"1527",height:"660"})),(0,l.kt)("p",null,"Both classifier models agree that the ",(0,l.kt)("strong",{parentName:"p"},"Alcohol")," content is the most important factor. Followed by the concentration of ",(0,l.kt)("strong",{parentName:"p"},"Sulphates"),". But after that their opinions seem to drift apart."),(0,l.kt)("p",null,"Taking a look at the ",(0,l.kt)("inlineCode",{parentName:"p"},"good")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"bad")," wines:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},'# get only good wines\ndf_good = df[df[\'good\']==1]\nprint(":: Wines with good Quality ::")\nprint("")\nprint(df_good.describe())\n# get only bad wines\ndf_bad = df[df[\'good\']==0]\nprint("")\nprint(":: Wines with bad Quality ::")\nprint("")\nprint(df_bad.describe())\n')),(0,l.kt)("p",null,"We can see that wines that are labelled as being good tend to have a higher alcohol and sulphate concentration:"),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"Wine Quality",src:a(76516).Z,width:"1575",height:"394"})),(0,l.kt)("h4",{id:"white-wines"},"White Wines"),(0,l.kt)("p",null,"The same analysis for the white wine data shows also emphasizes the importance of the ",(0,l.kt)("strong",{parentName:"p"},"Alcohol")," content and a balance of sweetness and acidity. The sulfur factor is largely underrepresented:"),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"Wine Quality",src:a(32843).Z,width:"1666",height:"660"})),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"Wine Quality",src:a(41343).Z,width:"1579",height:"395"})),(0,l.kt)("h2",{id:"data-pre-processing"},"Data Pre-Processing"),(0,l.kt)("p",null,"And now to the nitty, gritty of actually getting those results. To be able to work with our dataset we first have to do some housecleaning."),(0,l.kt)("h3",{id:"binary-classification"},"Binary Classification"),(0,l.kt)("p",null,"As recommended by the ",(0,l.kt)("a",{parentName:"p",href:"https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009"},"Author")," we can make quality scale binary - everything with a quality below ",(0,l.kt)("inlineCode",{parentName:"p"},"7")," is just not worth your attention. So let's add another column ",(0,l.kt)("inlineCode",{parentName:"p"},"good")," and set it's value to ",(0,l.kt)("inlineCode",{parentName:"p"},"1")," if the quality is >= 7 and otherwise to ",(0,l.kt)("inlineCode",{parentName:"p"},"0"),":"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},"# make binary quality classification\ndf['good'] = [1 if x >= 7 else 0 for x in df['quality']]\n# separate feature and target variables\nX = df.drop(['quality', 'good'], axis = 1)\ny = df['good']\n# check distribution\nprint(df['good'].value_counts())\n# print first 5 rows\nprint(df.head())\n")),(0,l.kt)("p",null,"The result is that we have ",(0,l.kt)("inlineCode",{parentName:"p"},"217")," out of ",(0,l.kt)("inlineCode",{parentName:"p"},"1599")," wines that are worth trying:"),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"Wine Quality",src:a(25362).Z,width:"1350",height:"151"})),(0,l.kt)("h3",{id:"data-normalization"},"Data Normalization"),(0,l.kt)("p",null,"Because all the features in ",(0,l.kt)("inlineCode",{parentName:"p"},"X")," have different units / scales they cannot be compared directly but need to be normalized. We can use the ",(0,l.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"},"StandardScaler from SciKit Learn")," to standardize those features by removing the mean and scaling to unit variance:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},"# Normalize feature variables\nX_features = X\nX = StandardScaler().fit_transform(X)\n")),(0,l.kt)("h3",{id:"data-splitting"},"Data Splitting"),(0,l.kt)("p",null,"To train our model we need a training and validation dataset to be able to establish performance metrics. And again it is ",(0,l.kt)("inlineCode",{parentName:"p"},"sklearn")," with ",(0,l.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"},"train_test_split")," that helps us to split the arrays or matrices into random train and test subsets. To get a random 25/75 split we can use:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},"# Splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=42)\n")),(0,l.kt)("h2",{id:"fitting-a-model"},"Fitting a Model"),(0,l.kt)("p",null,"Now we have to find a model that we can fit to our dataset. I found an article by ",(0,l.kt)("a",{parentName:"p",href:"https://towardsdatascience.com/predicting-wine-quality-with-several-classification-techniques-179038ea6434"},"Terence Shin")," that already explored several solutions to the classification problem."),(0,l.kt)("h3",{id:"decision-tree"},"Decision Tree"),(0,l.kt)("p",null,(0,l.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/tree.html#tree"},"Decision Trees")," are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation."),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"sklearn")," provides a ",(0,l.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"},"A decision tree classifier.")," that we can apply to our problem"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},"## decision tree classifier\ntree_model = DecisionTreeClassifier(random_state=42)\n## use training dataset for fitting\ntree_model.fit(X_train, y_train)\n## run prediction based of the validation dataset\ny_pred1 = tree_model.predict(X_test)\n## get performance metrics\nprint(classification_report(y_test, y_pred1))\n")),(0,l.kt)("p",null,"Running the classifier returns the following - the metrics I am looking out for here are ",(0,l.kt)("inlineCode",{parentName:"p"},"precision")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"recall")," that give us a sense for the relation of true and false positives and negatives that were predicted during the validation run:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"precision")," = TP / (TP + FP) | "),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"recall")," = TP / (TP + FN)")),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null}),(0,l.kt)("th",{parentName:"tr",align:null},"precision"),(0,l.kt)("th",{parentName:"tr",align:null},"recall"),(0,l.kt)("th",{parentName:"tr",align:null},"f1-score"),(0,l.kt)("th",{parentName:"tr",align:null},"support"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.94"),(0,l.kt)("td",{parentName:"tr",align:null},"0.92"),(0,l.kt)("td",{parentName:"tr",align:null},"0.93"),(0,l.kt)("td",{parentName:"tr",align:null},"347")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"1"),(0,l.kt)("td",{parentName:"tr",align:null},"0.55"),(0,l.kt)("td",{parentName:"tr",align:null},"0.62"),(0,l.kt)("td",{parentName:"tr",align:null},"0.58"),(0,l.kt)("td",{parentName:"tr",align:null},"53")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"accuracy"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"0.88"),(0,l.kt)("td",{parentName:"tr",align:null},"400")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"macro avg"),(0,l.kt)("td",{parentName:"tr",align:null},"0.75"),(0,l.kt)("td",{parentName:"tr",align:null},"0.77"),(0,l.kt)("td",{parentName:"tr",align:null},"0.76"),(0,l.kt)("td",{parentName:"tr",align:null},"400")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"weighted avg"),(0,l.kt)("td",{parentName:"tr",align:null},"0.89"),(0,l.kt)("td",{parentName:"tr",align:null},"0.88"),(0,l.kt)("td",{parentName:"tr",align:null},"0.89"),(0,l.kt)("td",{parentName:"tr",align:null},"400")))),(0,l.kt)("p",null,'We get a reasonably high precision for "bad wines" (',(0,l.kt)("inlineCode",{parentName:"p"},"0"),'). But it is basically hit or miss for "good wines" (',(0,l.kt)("inlineCode",{parentName:"p"},"1"),"). Since the dataset is not balanced and weighting in on the bad side we might see a model overfitting here:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Label 0"),": ",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("em",{parentName:"li"},"PRECISION"),': Of all wines that were predicted as "not good", 94% were actually labelled with ',(0,l.kt)("inlineCode",{parentName:"li"},"0"),". "),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("em",{parentName:"li"},"RECALL"),": Of all wines that were truly labelled ",(0,l.kt)("inlineCode",{parentName:"li"},"0")," we predicted 92% correctly."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Label 1"),":",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("em",{parentName:"li"},"PRECISION"),': Of all wines that were predicted as "good", 55% were actually labelled with ',(0,l.kt)("inlineCode",{parentName:"li"},"1"),". "),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("em",{parentName:"li"},"RECALL"),": Of all wines that were truly labelled ",(0,l.kt)("inlineCode",{parentName:"li"},"1")," we predicted 62% correctly.")))),(0,l.kt)("h3",{id:"random-forest"},"Random Forest"),(0,l.kt)("p",null,"Next, another ",(0,l.kt)("inlineCode",{parentName:"p"},"sklearn")," classifier - the ",(0,l.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"},"RandomForestClassifier"),". A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the ",(0,l.kt)("inlineCode",{parentName:"p"},"max_samples")," parameter if ",(0,l.kt)("inlineCode",{parentName:"p"},"bootstrap=True")," (default), otherwise the whole dataset is used to build each tree:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},"## random forrest classifier\nforrest_model = RandomForestClassifier(random_state=42)\n## use training dataset for fitting\nforrest_model.fit(X_train, y_train)\n## run prediction based of the validation dataset\ny_pred2 = forrest_model.predict(X_test)\n## get performance metrics\nprint(classification_report(y_test, y_pred2))\n")),(0,l.kt)("p",null,"Running the classifier returns the following:"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null}),(0,l.kt)("th",{parentName:"tr",align:null},"precision"),(0,l.kt)("th",{parentName:"tr",align:null},"recall"),(0,l.kt)("th",{parentName:"tr",align:null},"f1-score"),(0,l.kt)("th",{parentName:"tr",align:null},"support"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.93"),(0,l.kt)("td",{parentName:"tr",align:null},"0.97"),(0,l.kt)("td",{parentName:"tr",align:null},"0.95"),(0,l.kt)("td",{parentName:"tr",align:null},"347")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"1"),(0,l.kt)("td",{parentName:"tr",align:null},"0.75"),(0,l.kt)("td",{parentName:"tr",align:null},"0.51"),(0,l.kt)("td",{parentName:"tr",align:null},"0.61"),(0,l.kt)("td",{parentName:"tr",align:null},"53")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"accuracy"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"0.91"),(0,l.kt)("td",{parentName:"tr",align:null},"400")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"macro avg"),(0,l.kt)("td",{parentName:"tr",align:null},"0.84"),(0,l.kt)("td",{parentName:"tr",align:null},"0.74"),(0,l.kt)("td",{parentName:"tr",align:null},"0.78"),(0,l.kt)("td",{parentName:"tr",align:null},"400")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"weighted avg"),(0,l.kt)("td",{parentName:"tr",align:null},"0.90"),(0,l.kt)("td",{parentName:"tr",align:null},"0.91"),(0,l.kt)("td",{parentName:"tr",align:null},"0.91"),(0,l.kt)("td",{parentName:"tr",align:null},"400")))),(0,l.kt)("p",null,'Again, we get a reasonably high precision for "bad wines" (',(0,l.kt)("inlineCode",{parentName:"p"},"0"),"). And using several random decision trees and averaging the results helped us tackle the overfitting - at least a bit (",(0,l.kt)("inlineCode",{parentName:"p"},"recall")," actually got worse):"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Label 0"),": ",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("em",{parentName:"li"},"PRECISION"),': Of all wines that were predicted as "not good", 93% were actually labelled with ',(0,l.kt)("inlineCode",{parentName:"li"},"0"),". "),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("em",{parentName:"li"},"RECALL"),": Of all wines that were truly labelled ",(0,l.kt)("inlineCode",{parentName:"li"},"0")," we predicted 97% correctly."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Label 1"),":",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("em",{parentName:"li"},"PRECISION"),': Of all wines that were predicted as "good", 75% were actually labelled with ',(0,l.kt)("inlineCode",{parentName:"li"},"1"),". "),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("em",{parentName:"li"},"RECALL"),": Of all wines that were truly labelled ",(0,l.kt)("inlineCode",{parentName:"li"},"1")," we predicted 51% correctly.")))),(0,l.kt)("h3",{id:"adaboost"},"AdaBoost"),(0,l.kt)("p",null,"An ",(0,l.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html"},"AdaBoost classifier")," is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},"## adaboost classifier\nadaboost_model = AdaBoostClassifier(random_state=42)\n## use training dataset for fitting\nadaboost_model.fit(X_train, y_train)\n## run prediction based of the validation dataset\ny_pred3 = adaboost_model.predict(X_test)\n## get performance metrics\nprint(classification_report(y_test, y_pred3))\n")),(0,l.kt)("p",null,"Running the classifier returns the following:"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null}),(0,l.kt)("th",{parentName:"tr",align:null},"precision"),(0,l.kt)("th",{parentName:"tr",align:null},"recall"),(0,l.kt)("th",{parentName:"tr",align:null},"f1-score"),(0,l.kt)("th",{parentName:"tr",align:null},"support"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.90"),(0,l.kt)("td",{parentName:"tr",align:null},"0.95"),(0,l.kt)("td",{parentName:"tr",align:null},"0.93"),(0,l.kt)("td",{parentName:"tr",align:null},"347")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"1"),(0,l.kt)("td",{parentName:"tr",align:null},"0.50"),(0,l.kt)("td",{parentName:"tr",align:null},"0.32"),(0,l.kt)("td",{parentName:"tr",align:null},"0.39"),(0,l.kt)("td",{parentName:"tr",align:null},"53")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"accuracy"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"0.87"),(0,l.kt)("td",{parentName:"tr",align:null},"400")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"macro avg"),(0,l.kt)("td",{parentName:"tr",align:null},"0.70"),(0,l.kt)("td",{parentName:"tr",align:null},"0.64"),(0,l.kt)("td",{parentName:"tr",align:null},"0.66"),(0,l.kt)("td",{parentName:"tr",align:null},"400")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"weighted avg"),(0,l.kt)("td",{parentName:"tr",align:null},"0.85"),(0,l.kt)("td",{parentName:"tr",align:null},"0.87"),(0,l.kt)("td",{parentName:"tr",align:null},"0.85"),(0,l.kt)("td",{parentName:"tr",align:null},"400")))),(0,l.kt)("p",null,(0,l.kt)("em",{parentName:"p"},"Nope")," ..."),(0,l.kt)("h3",{id:"gradient-boosting"},"Gradient Boosting"),(0,l.kt)("p",null,(0,l.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"},"Gradient Boosting")," classification algorithm builds an additive model in a forward stage-wise fashion. It allows for the optimization of arbitrary differentiable loss functions. In each stage ",(0,l.kt)("inlineCode",{parentName:"p"},"n_classes_ regression")," trees are fit on the negative gradient of the loss function. Binary classification is a special case where only a single regression tree is induced:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},"## gradient boost classifier\ngradient_model = GradientBoostingClassifier(random_state=42)\n## use training dataset for fitting\ngradient_model.fit(X_train, y_train)\n## run prediction based of the validation dataset\ny_pred4 = gradient_model.predict(X_test)\n## get performance metrics\nprint(classification_report(y_test, y_pred4))\n")),(0,l.kt)("p",null,"Running the classifier returns the following:"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null}),(0,l.kt)("th",{parentName:"tr",align:null},"precision"),(0,l.kt)("th",{parentName:"tr",align:null},"recall"),(0,l.kt)("th",{parentName:"tr",align:null},"f1-score"),(0,l.kt)("th",{parentName:"tr",align:null},"support"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.92"),(0,l.kt)("td",{parentName:"tr",align:null},"0.94"),(0,l.kt)("td",{parentName:"tr",align:null},"0.93"),(0,l.kt)("td",{parentName:"tr",align:null},"347")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"1"),(0,l.kt)("td",{parentName:"tr",align:null},"0.53"),(0,l.kt)("td",{parentName:"tr",align:null},"0.43"),(0,l.kt)("td",{parentName:"tr",align:null},"0.48"),(0,l.kt)("td",{parentName:"tr",align:null},"53")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"accuracy"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"0.88"),(0,l.kt)("td",{parentName:"tr",align:null},"400")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"macro avg"),(0,l.kt)("td",{parentName:"tr",align:null},"0.73"),(0,l.kt)("td",{parentName:"tr",align:null},"0.69"),(0,l.kt)("td",{parentName:"tr",align:null},"0.70"),(0,l.kt)("td",{parentName:"tr",align:null},"400")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"weighted avg"),(0,l.kt)("td",{parentName:"tr",align:null},"0.87"),(0,l.kt)("td",{parentName:"tr",align:null},"0.88"),(0,l.kt)("td",{parentName:"tr",align:null},"0.87"),(0,l.kt)("td",{parentName:"tr",align:null},"400")))),(0,l.kt)("p",null,"Better but not good, yet ..."),(0,l.kt)("h3",{id:"xgboost"},"XGBoost"),(0,l.kt)("p",null,(0,l.kt)("a",{parentName:"p",href:"https://xgboost.readthedocs.io/en/stable/index.html"},"XGBoost")," is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM):"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},"import xgboost as xgb\nxgboost_model = xgb.XGBClassifier(random_state=1)\nxgboost_model.fit(X_train, y_train)\ny_pred5 = xgboost_model.predict(X_test)print(classification_report(y_test, y_pred5))\n")),(0,l.kt)("p",null,"Running the classifier returns the following:"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null}),(0,l.kt)("th",{parentName:"tr",align:null},"precision"),(0,l.kt)("th",{parentName:"tr",align:null},"recall"),(0,l.kt)("th",{parentName:"tr",align:null},"f1-score"),(0,l.kt)("th",{parentName:"tr",align:null},"support"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.94"),(0,l.kt)("td",{parentName:"tr",align:null},"0.96"),(0,l.kt)("td",{parentName:"tr",align:null},"0.95"),(0,l.kt)("td",{parentName:"tr",align:null},"347")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"1"),(0,l.kt)("td",{parentName:"tr",align:null},"0.70"),(0,l.kt)("td",{parentName:"tr",align:null},"0.62"),(0,l.kt)("td",{parentName:"tr",align:null},"0.66"),(0,l.kt)("td",{parentName:"tr",align:null},"53")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"accuracy"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"0.92"),(0,l.kt)("td",{parentName:"tr",align:null},"400")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"macro avg"),(0,l.kt)("td",{parentName:"tr",align:null},"0.82"),(0,l.kt)("td",{parentName:"tr",align:null},"0.79"),(0,l.kt)("td",{parentName:"tr",align:null},"0.81"),(0,l.kt)("td",{parentName:"tr",align:null},"400")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"weighted avg"),(0,l.kt)("td",{parentName:"tr",align:null},"0.91"),(0,l.kt)("td",{parentName:"tr",align:null},"0.92"),(0,l.kt)("td",{parentName:"tr",align:null},"0.91"),(0,l.kt)("td",{parentName:"tr",align:null},"400")))),(0,l.kt)("p",null,"This is the best of the boosting classifier so far. The ",(0,l.kt)("inlineCode",{parentName:"p"},"precision")," for positives is 5% lower but the ",(0,l.kt)("inlineCode",{parentName:"p"},"recall")," is ",(0,l.kt)("inlineCode",{parentName:"p"},"11%")," better than the ",(0,l.kt)("a",{parentName:"p",href:"#random-forest"},"Random Forest")," results. So this might be the best performing classifier of the lot."))}m.isMDXComponent=!0},83062:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/Win_Quality_Dataset_01-c32ea1909fe02b36c4a52e94f3700d4d.png"},71904:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/Win_Quality_Dataset_02-a7fbc3ac1567084cda2fcd7c6d9f05e0.png"},25362:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/Win_Quality_Dataset_03-d2dd77b4306939e2784fb17b4c8be1a1.png"},30377:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/Win_Quality_Dataset_04-a010dc8acd0ed48b0f57f7f19814cd81.png"},76516:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/Win_Quality_Dataset_05-376ae6f66103016eb05e2425fd7c151b.png"},32843:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/Win_Quality_Dataset_06-4fb3107e8f1628feed516f9e427d2fc1.png"},41343:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/Win_Quality_Dataset_07-1223b78884cc30c0c831a1618341beb0.png"},85287:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-1a51e7538c86faba8096715524fe16fa.jpg"}}]);