"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[1769],{475625:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>r,toc:()=>c});var s=t(785893),a=t(603905);const i={sidebar_position:4790,slug:"2023-01-13",title:"YOLOv7 Label Conversion",authors:"mpolinowski",tags:["Python","Machine Learning","YOLO","Torch"],description:"Transferring PASCAL VOC labels to the YOLO format"},o=void 0,r={id:"IoT-and-Machine-Learning/ML/2023-01-13-yolov7_data_conversion/index",title:"YOLOv7 Label Conversion",description:"Transferring PASCAL VOC labels to the YOLO format",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-01-13-yolov7_data_conversion/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-01-13-yolov7_data_conversion",slug:"/IoT-and-Machine-Learning/ML/2023-01-13-yolov7_data_conversion/2023-01-13",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-01-13-yolov7_data_conversion/2023-01-13",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-01-13-yolov7_data_conversion/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"YOLO",permalink:"/docs/tags/yolo"},{label:"Torch",permalink:"/docs/tags/torch"}],version:"current",sidebarPosition:4790,frontMatter:{sidebar_position:4790,slug:"2023-01-13",title:"YOLOv7 Label Conversion",authors:"mpolinowski",tags:["Python","Machine Learning","YOLO","Torch"],description:"Transferring PASCAL VOC labels to the YOLO format"},sidebar:"tutorialSidebar",previous:{title:"YOLOv7 to Tensorflow",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-01-14-yolov7_to_tensorflow/2023-01-14"},next:{title:"YOLOv7 Training with Custom Data",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-01-10-yolov7_custom_data/2023-01-10"}},l={},c=[{value:"Getting YOLOv7",id:"getting-yolov7",level:2},{value:"Training Weights",id:"training-weights",level:3},{value:"Configuration",id:"configuration",level:3},{value:"Preparing the Dataset",id:"preparing-the-dataset",level:2},{value:"Model Fitting",id:"model-fitting",level:2},{value:"Testing",id:"testing",level:3},{value:"Correction",id:"correction",level:2},{value:"Testing",id:"testing-1",level:3},{value:"Prediction",id:"prediction",level:3},{value:"Extended Run",id:"extended-run",level:2},{value:"Testing",id:"testing-2",level:3},{value:"Predictions",id:"predictions",level:3}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.ah)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Guangzhou, China",src:t(606972).Z+"",width:"1500",height:"652"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"#getting-yolov7",children:"Getting YOLOv7"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#training-weights",children:"Training Weights"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#configuration",children:"Configuration"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#preparing-the-dataset",children:"Preparing the Dataset"})}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"#model-fitting",children:"Model Fitting"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#testing",children:"Testing"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"#correction",children:"Correction"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#testing-1",children:"Testing"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#prediction",children:"Prediction"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"#extended-run",children:"Extended Run"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#testing-2",children:"Testing"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#predictions",children:"Predictions"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["In the ",(0,s.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-01-05-yolov7/2023-01-05",children:"previous step"})," we cloned the YOLOv7 repository and run predictions using testing weights. I then ",(0,s.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-01-10-yolov7_custom_data/2023-01-10",children:"trained YOLOv7 with a custom dataset"}),' that was pre-labeled the "YOLO Way".']}),"\n",(0,s.jsxs)(n.p,{children:["Another annotations format that can be generated by ",(0,s.jsx)(n.strong,{children:"LabelImg"})," is the  ",(0,s.jsx)(n.strong,{children:"PASCAL VOC"})," format. I now want to see how to transfer a dataset that was labeled that way into a YOLO training workflow. I am going to use the following Kaggle dataset:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.kaggle.com/datasets/andrewmvd/face-mask-detection",children:"Face Mask Detection"})}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["And for the transfer code had help from ",(0,s.jsx)(n.a,{href:"https://towardsdatascience.com/convert-pascal-voc-xml-to-yolo-for-object-detection-f969811ccba5",children:"Convert PASCAL VOC XML to YOLO for Object Detection"})," by Ng Wai Foong."]}),"\n",(0,s.jsx)(n.h2,{id:"getting-yolov7",children:"Getting YOLOv7"}),"\n",(0,s.jsx)(n.h3,{id:"training-weights",children:"Training Weights"}),"\n",(0,s.jsxs)(n.p,{children:["I already went through ",(0,s.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-01-05-yolov7/2023-01-05",children:"all the steps to download and test-run YOLOv7"}),". I ran into ",(0,s.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-01-10-yolov7_custom_data/2023-01-10",children:"difficulties with my graphic card"})," only having 6Gig of VRAM (Nvidia GTX 1060) which forced me to reduce the batch size to ",(0,s.jsx)(n.code,{children:"1"}),". Since this freed up a lot of memory - a size of ",(0,s.jsx)(n.code,{children:"2"})," was too much, while ",(0,s.jsx)(n.code,{children:"1"})," underutilized the card - I want to use the following weights:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6e_training.pt\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"YOLOv7 Data Conversion",src:t(243727).Z+"",width:"1100",height:"754"})}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"e6e"})," increases the amounts of parameter from ~ 40Mio to ~ 150Mio... let's see what happens..."]}),"\n",(0,s.jsx)(n.h3,{id:"configuration",children:"Configuration"}),"\n",(0,s.jsx)(n.p,{children:"Now we need to configure YOLO. First create a copy of the following file and call it something like:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cp yolov7/cfg/training/yolov7-e6e.yaml yolov7/cfg/training/yolov7-e6e-ppe.yaml\n"})}),"\n",(0,s.jsx)(n.p,{children:"Here we only need to adjust the amount of classes we expect in our dataset - 3:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yml",children:"# parameters\nnc: 3  # number of classes\n"})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Error"}),": I am getting an ",(0,s.jsx)(n.code,{children:"IndexError: list index out of range"})," when trying to use ",(0,s.jsx)(n.code,{children:"yolov7-e6e.yaml"})," file. When I switch to ",(0,s.jsx)(n.code,{children:"yolov7-custom.yaml"})," the training works. Even though I am using the ",(0,s.jsx)(n.code,{children:"e6e"})," weights:"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'Traceback (most recent call last):\n  File "yolov7/train.py", line 616, in <module>\n    train(hyp, opt, device, tb_writer)\n  File "yolov7/train.py", line 363, in train\n    loss, loss_items = compute_loss_ota(pred, targets.to(device), imgs)  # loss scaled by batch_size\n  File "yolov7/utils/loss.py", line 585, in __call__\n    bs, as_, gjs, gis, targets, anchors = self.build_targets(p, targets, imgs)\n  File "yolov7/utils/loss.py", line 677, in build_targets\n    b, a, gj, gi = indices[i]\nIndexError: list index out of range\n'})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"UPDATE"}),": Apparently you need to ",(0,s.jsx)(n.a,{href:"https://blog.csdn.net/wxd1233/article/details/126330931",children:"use train_aux.py instead of train.py"})," to work with ",(0,s.jsx)(n.code,{children:"e6e"})," weights. ",(0,s.jsx)(n.a,{href:"#correction",children:"I will test this next"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Next create the directories ",(0,s.jsx)(n.code,{children:"yolov7/custom_data/ppe"})," and add a data file ",(0,s.jsx)(n.code,{children:"ppe-data.yml"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yml",children:"train: ./custom_data/ppe/train\nval: ./custom_data/ppe/validation\ntest: ./custom_data/ppe/test\n \n# Classes\nnc: 3  # number of classes\nnames: ['with_mask', 'without_mask', 'mask_worn_incorrect'] \n"})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:["The original class was ",(0,s.jsx)(n.code,{children:"mask_weared_incorrect"})," - I searched and replaced it to ",(0,s.jsx)(n.code,{children:"mask_worn_incorrect"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"preparing-the-dataset",children:"Preparing the Dataset"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://www.kaggle.com/datasets/andrewmvd/face-mask-detection",children:"This dataset"})," contains 853 images belonging to the 3 classes, as well as their bounding boxes in the ",(0,s.jsx)(n.strong,{children:"PASCAL VOC"})," format. The classes are:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"With mask"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"Without mask"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"Mask worn incorrectly"})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The annotations are in an XML format:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:"<annotation>\n    <folder>images</folder>\n    <filename>maksssksksss852.png</filename>\n    <size>\n        <width>267</width>\n        <height>400</height>\n        <depth>3</depth>\n    </size>\n    <segmented>0</segmented>\n    <object>\n        <name>with_mask</name>\n        <pose>Unspecified</pose>\n        <truncated>0</truncated>\n        <occluded>0</occluded>\n        <difficult>0</difficult>\n        <bndbox>\n            <xmin>139</xmin>\n            <ymin>94</ymin>\n            <xmax>198</xmax>\n            <ymax>147</ymax>\n        </bndbox>\n    </object>\n</annotation>\n"})}),"\n",(0,s.jsx)(n.p,{children:"The downloaded dataset is only split into images and annotations:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"yolov7/custom_data\n\u2514\u2500\u2500 ppe\n    \u251c\u2500\u2500 ppe-data.yml\n    \u251c\u2500\u2500 images\n    \u2514\u2500\u2500 annotations\n"})}),"\n",(0,s.jsx)(n.p,{children:"We now need to convert the annotation-data format from PASCAL VOC to YOLO and split the data into train, val, test-folders:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"yolov7/convert_xml_to_yolo.py"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-py",children:"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport os\nimport shutil\nimport xml.etree.ElementTree as ET\nimport glob\n\nimport json\n# Function for conversion XML to YOLO\n# based on https://towardsdatascience.com/convert-pascal-voc-xml-to-yolo-for-object-detection-f969811ccba5\ndef xml_to_yolo_bbox(bbox, w, h):\n    # xmin, ymin, xmax, ymax\n    x_center = ((bbox[2] + bbox[0]) / 2) / w\n    y_center = ((bbox[3] + bbox[1]) / 2) / h\n    width = (bbox[2] - bbox[0]) / w\n    height = (bbox[3] - bbox[1]) / h\n    return [x_center, y_center, width, height]\n\n# create folders\ndef create_folder(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n        print(\"INFO :: Path '%s' created\" %path)\n\ncreate_folder('custom_data/ppe/train/images')\ncreate_folder('custom_data/ppe/train/labels')\ncreate_folder('custom_data/ppe/validation/images')\ncreate_folder('custom_data/ppe/validation/labels')\ncreate_folder('custom_data/ppe/test/images')\ncreate_folder('custom_data/ppe/test/labels')\n\n# get all source files\nimg_src_folder = 'custom_data/ppe/images'\nlabel_src_folder = 'custom_data/ppe/annotations'\n\n_, _, files = next(os.walk(img_src_folder))\npos = 0\nfor f in files:\n        source_img = os.path.join(img_src_folder, f)\n        if pos < 700:\n            dest_folder = 'custom_data/ppe/train'\n        elif (pos >= 700 and pos < 800):\n            dest_folder = 'custom_data/ppe/validation'\n        else:\n            dest_folder = 'custom_data/ppe/test'\n        destination_img = os.path.join(dest_folder,'images', f)\n        shutil.copy(source_img, destination_img)\n\n        # check for corresponding label\n        label_file_basename = os.path.splitext(f)[0]\n        label_source_file = f\"{label_file_basename}.xml\"\n        label_dest_file = f\"{label_file_basename}.txt\"\n        \n        label_source_path = os.path.join(label_src_folder, label_source_file)\n        label_dest_path = os.path.join(dest_folder, 'labels', label_dest_file)\n        # if file exists, copy it to target folder\n        if os.path.exists(label_source_path):\n             # parse the content of the xml file\n            tree = ET.parse(label_source_path)\n            root = tree.getroot()\n            width = int(root.find(\"size\").find(\"width\").text)\n            height = int(root.find(\"size\").find(\"height\").text)\n            classes = ['with_mask', 'without_mask', 'mask_worn_incorrect']\n            result = []\n            for obj in root.findall('object'):\n                label = obj.find(\"name\").text\n                # check for new classes and append to list\n                index = classes.index(label)\n                pil_bbox = [int(x.text) for x in obj.find(\"bndbox\")]\n                yolo_bbox = xml_to_yolo_bbox(pil_bbox, width, height)\n                # convert data to string\n                bbox_string = \" \".join([str(x) for x in yolo_bbox])\n                result.append(f\"{index} {bbox_string}\")\n                if result:\n                    # generate a YOLO format text file for each xml file\n                    with open(label_dest_path, \"w\", encoding=\"utf-8\") as f:\n                        f.write(\"\\n\".join(result))\n\n        pos += 1\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Make sure that all the paths in here match your dir structure - also replace the ",(0,s.jsx)(n.code,{children:"/"})," with ",(0,s.jsx)(n.code,{children:"\\\\"})," in path variables if your are on Windows. And run the script:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python convert_xml_to_yolo.py\nINFO :: Path 'custom_data/ppe/train/images' created\nINFO :: Path 'custom_data/ppe/train/labels' created\nINFO :: Path 'custom_data/ppe/validation/images' created\nINFO :: Path 'custom_data/ppe/validation/labels' created\nINFO :: Path 'custom_data/ppe/test/images' created\nINFO :: Path 'custom_data/ppe/test/labels' created\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Check the ",(0,s.jsx)(n.code,{children:"custom_data"})," folder - you should now have a split all the data into the three training, testing and validating directories:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"yolov7/custom_data\n\u2514\u2500\u2500 ppe\n    \u251c\u2500\u2500 ppe-data.yml\n    \u251c\u2500\u2500 test\n    \u2502\xa0\xa0 \u251c\u2500\u2500 images\n    \u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 53 items\n    \u2502\xa0\xa0 \u2514\u2500\u2500 labels\n    \u2502\xa0\xa0     \u2514\u2500\u2500 53 items\n    \u251c\u2500\u2500 train\n    \u2502\xa0\xa0 \u251c\u2500\u2500 images\n    \u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 700 items\n    \u2502\xa0\xa0 \u2514\u2500\u2500 labels\n    \u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 700 items\n    \u2514\u2500\u2500 validation\n        \u251c\u2500\u2500 images\n        \u2502\xa0\xa0 \u2514\u2500\u2500 100 items\n        \u2514\u2500\u2500 labels\n            \u2514\u2500\u2500 100 items\n"})}),"\n",(0,s.jsx)(n.p,{children:"And all the labels are now in the expected YOLO format:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"0 0.2425 0.36099585062240663 0.115 0.2074688796680498\n"})}),"\n",(0,s.jsx)(n.h2,{id:"model-fitting",children:"Model Fitting"}),"\n",(0,s.jsx)(n.p,{children:"The data is now compatible with YOLOv7 and can be used to train a model to recognize personal protection equipment for us:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python train.py --epochs 100 --weights weights/yolov7-e6e_training.pt --data custom_data/ppe/ppe-data.yml --workers 4 --batch-size 1 --img 416 --cfg cfg/training/yolov7_custom.yaml --name yolov7-ppe\n"})}),"\n",(0,s.jsx)(n.p,{children:"The run took around 4hrs with the following - disappointing - results:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Epoch   gpu_mem       box       obj       cls     total\n99/99     2.67G   0.02546   0.01013  0.003265   0.03886\n\nClass                  Images      Labels   P           R\n                all    100         442      0.399       0.372\n          with_mask    100         368      0.765       0.802\n       without_mask    100         60       0.431       0.315\nmask_worn_incorrect    100         14       0           0\n100 epochs completed in 3.783 hours.\n\nOptimizer stripped from runs/train/yolov7-ppe4/weights/last.pt, 74.8MB\nOptimizer stripped from runs/train/yolov7-ppe4/weights/best.pt, 74.8MB\n"})}),"\n",(0,s.jsx)(n.h3,{id:"testing",children:"Testing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python test.py --weights runs/train/yolov7-ppe4/weights/best.pt \\\n    --task test \\\n    --data custom_data/ppe/ppe-data.yml\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Class                     Images      Labels    P           R   \n                 all      53          223       0.357       0.476\n           with_mask      53          191       0.676       0.749\n        without_mask      53          25        0.395       0.679\n mask_worn_incorrect      53          7         0           0\nSpeed: 56.5/1.1/57.6 ms inference/NMS/total per 640x640 image at batch-size 32\n"})}),"\n",(0,s.jsx)(n.p,{children:"Just terrible..."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"YOLOv7 Data Conversion",src:t(835202).Z+"",width:"1538",height:"671"})}),"\n",(0,s.jsx)(n.h2,{id:"correction",children:"Correction"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python train_aux.py --device 0 --epochs 20 --weights weights/yolov7-e6e_training.pt --data custom_data/ppe/ppe-data.yml --workers 4 --batch-size 1 --img 416 --cfg cfg/training/yolov7-e6e-ppe.yaml --name yolov7-aux-ppe\n"})}),"\n",(0,s.jsx)(n.p,{children:"Here I am getting the following error message:"}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.code,{children:"RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu)"})}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Hmm there are several proposed solutions: ",(0,s.jsx)(n.a,{href:"https://github.com/WongKinYiu/yolov7/issues/1101",children:"Github Issue"}),", ",(0,s.jsx)(n.a,{href:"https://stackoverflow.com/questions/74713315/yolov7-runtimeerror-indices-should-be-either-on-cpu-or-on-the-same-device-as",children:"Stack Overflow"}),". But none of the appear to be working for me right now. So let's drop the ",(0,s.jsx)(n.code,{children:"e6e"})," and try the regular YOLOv4 model:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python train.py --epochs 20 --weights weights/yolov7_training.pt --data custom_data/ppe/ppe-data.yml --workers 4 --batch-size 1 --img 640 640 --cfg cfg/training/yolov7_custom.yaml --name yolov7-regular-ppe\n"})}),"\n",(0,s.jsx)(n.p,{children:"And this looks more promising - already after 20 epochs:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Class                      Images      Labels    P           R\n                all        100         442       0.942       0.127\n          with_mask        100         368       0.827       0.38\n      without_mask         100         60        1           0\nmask_worn_incorrect        100         14        1           0\n20 epochs completed in 1.116 hours.\n\nOptimizer stripped from runs/train/yolov7-regular-ppe/weights/last.pt, 74.8MB\nOptimizer stripped from runs/train/yolov7-regular-ppe/weights/best.pt, 74.8MB\n"})}),"\n",(0,s.jsx)(n.h3,{id:"testing-1",children:"Testing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python test.py --weights runs/train/yolov7-regular-ppe/weights/best.pt \\\n    --task test \\\n    --data custom_data/ppe/ppe-data.yml\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"YOLOv7 Data Conversion",src:t(339182).Z+"",width:"1523",height:"670"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Class                     Images     Labels    P        R\n                all       53         223       0.935    0.166\n          with_mask       53         191       0.805    0.497\n      without_mask        53         25        1        0\nmask_worn_incorrect       53         7         1        0\n"})}),"\n",(0,s.jsx)(n.h3,{id:"prediction",children:"Prediction"}),"\n",(0,s.jsx)(n.p,{children:'The prediction works "best" for frontal shots of large groups of people. But there are a lot of false positives. The model fails completely when dealing with close-ups:'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python detect.py --weights runs/train/yolov7-regular-ppe/weights/best.pt \\\n    --conf 0.1 \\\n    --img-size 640 \\\n    --source custom_data/ppe/test/images/maksssksksss836.png\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"YOLOv7 Data Conversion",src:t(633181).Z+"",width:"1724",height:"543"})}),"\n",(0,s.jsx)(n.p,{children:"But it is far from great yet - the model has about a 50:50 chance to detect a mask. And zero chance to detect no mask or a wrongly worn mask. The next step is to extend the the training and see if the R-value improves over time."}),"\n",(0,s.jsx)(n.h2,{id:"extended-run",children:"Extended Run"}),"\n",(0,s.jsxs)(n.p,{children:["Night-shift run... ",(0,s.jsx)(n.code,{children:"yolov7x_training.pt"})]}),"\n",(0,s.jsxs)(n.p,{children:["I decided to give YOLOv7x a try. It is much more complex compared to YOLOv7 (36.9M vs 71.3M parameter). And - to my knowledge - it does not require you to use ",(0,s.jsx)(n.code,{children:"train_aux.py"})," that was causing the issues earlier. So all I needed was to download the matching training weights and create a copy of ",(0,s.jsx)(n.code,{children:"cfg/training/yolov7x.yaml"})," with the correct number of classes:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python train.py --device 0 --epochs 200 --weights weights/yolov7x_training.pt --data custom_data/ppe/ppe-data.yml --workers 4 --batch-size 1 --img 640 640 --cfg cfg/training/yolov7x-ppe.yaml --name yolov7x-ppe\n"})}),"\n",(0,s.jsxs)(n.p,{children:["The run over ",(0,s.jsx)(n.strong,{children:"200 Epochs"})," took my machine 14.5hrs and ended with the following results:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"199/199     3.34G   0.01889  0.008947  0.001511   0.02935\n   \nClass                       Images      Labels    P           R\n                all         100         442       0.135       0.744\n          with_mask         100         368       0.104       0.902\n       without_mask         100          60       0.211       0.617\nmask_worn_incorrect         100          14       0.0908      0.714\n200 epochs completed in 14.672 hours.\n\nOptimizer stripped from runs/train/yolov7x-ppe/weights/last.pt, 142.1MB\nOptimizer stripped from runs/train/yolov7x-ppe/weights/best.pt, 142.1MB\n"})}),"\n",(0,s.jsx)(n.h3,{id:"testing-2",children:"Testing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python test.py --weights runs/train/yolov7x-ppe/weights/best.pt \\\n    --task test \\\n    --data custom_data/ppe/ppe-data.yml\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"YOLOv7 Data Conversion",src:t(271432).Z+"",width:"2217",height:"675"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Class                  Images     Labels    P       R    \n                all    53         223       0.978   0.712\n          with_mask    53         191       0.935   0.77 \n       without_mask    53         25        1       0.8  \nmask_worn_incorrect    53         7         1       0.566\nSpeed: 81.4/0.8/82.2 ms inference/NMS/total per 640x640 image at batch-size 32\n"})}),"\n",(0,s.jsxs)(n.p,{children:["The confusion matrix correlates nicely with the test predictions. ",(0,s.jsx)(n.code,{children:"87%"})," of masks were identified correctly. For faces without masks we get ",(0,s.jsx)(n.code,{children:"85%"}),". For incorrectly worn masks - a much more complicated case - we are already at ",(0,s.jsx)(n.code,{children:"67%"}),". But the model is still seeing ",(0,s.jsx)(n.strong,{children:"a lot"})," of masks in the background that do not exist. But overall - a successful training ","\ud83d\udc4d"]}),"\n",(0,s.jsx)(n.h3,{id:"predictions",children:"Predictions"}),"\n",(0,s.jsxs)(n.p,{children:["For a comparison I want to run the same predictions I used with the previous model - ",(0,s.jsx)(n.code,{children:"maksssksksss836.png"})," and ",(0,s.jsx)(n.code,{children:"maksssksksss99.png"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python detect.py --weights runs/train/yolov7x-ppe/weights/best.pt \\\n    --conf 0.5 \\\n    --img-size 640 \\\n    --source custom_data/ppe/test/images/maksssksksss836.png\n"})}),"\n",(0,s.jsxs)(n.p,{children:["For the close-up image I decreased the confidence barrier to ",(0,s.jsx)(n.code,{children:"0.1"})," to see if the person on the right would show up with the mask in a side-profile. And he does - it is really impressive:"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"YOLOv7 Data Conversion",src:t(489253).Z+"",width:"1453",height:"495"})})]})}function h(e={}){const{wrapper:n}={...(0,a.ah)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},603905:(e,n,t)=>{t.d(n,{ah:()=>c});var s=t(667294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);n&&(s=s.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,s)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function r(e,n){if(null==e)return{};var t,s,a=function(e,n){if(null==e)return{};var t,s,a={},i=Object.keys(e);for(s=0;s<i.length;s++)t=i[s],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(s=0;s<i.length;s++)t=i[s],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var l=s.createContext({}),c=function(e){var n=s.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},d={inlineCode:"code",wrapper:function(e){var n=e.children;return s.createElement(s.Fragment,{},n)}},h=s.forwardRef((function(e,n){var t=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,h=r(e,["components","mdxType","originalType","parentName"]),p=c(t),g=a,m=p["".concat(l,".").concat(g)]||p[g]||d[g]||i;return t?s.createElement(m,o(o({ref:n},h),{},{components:t})):s.createElement(m,o({ref:n},h))}));h.displayName="MDXCreateElement"},243727:(e,n,t)=>{t.d(n,{Z:()=>s});const s=t.p+"assets/images/YOLOv7_Custom_Data_Conversion_01-fa5e37bc899526d6e0e98e694821d563.png"},835202:(e,n,t)=>{t.d(n,{Z:()=>s});const s=t.p+"assets/images/YOLOv7_Custom_Data_Conversion_02-78b3cf6d8d3941b62bb570ff7fba8540.png"},339182:(e,n,t)=>{t.d(n,{Z:()=>s});const s=t.p+"assets/images/YOLOv7_Custom_Data_Conversion_03-aa9e29dd7f4f5576cf2eac0199bf4ab8.png"},633181:(e,n,t)=>{t.d(n,{Z:()=>s});const s=t.p+"assets/images/YOLOv7_Custom_Data_Conversion_04-5ca794ce0e2cf4d10676c985db15aa3e.png"},271432:(e,n,t)=>{t.d(n,{Z:()=>s});const s=t.p+"assets/images/YOLOv7_Custom_Data_Conversion_05-dea6a8f5f3cae002ddcf735352f2e157.png"},489253:(e,n,t)=>{t.d(n,{Z:()=>s});const s=t.p+"assets/images/YOLOv7_Custom_Data_Conversion_06-f00ef5210e9778c409c85b98f85c92a6.png"},606972:(e,n,t)=>{t.d(n,{Z:()=>s});const s=t.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-ba3b23aa3d5392c02b451d1b2b911721.jpg"}}]);