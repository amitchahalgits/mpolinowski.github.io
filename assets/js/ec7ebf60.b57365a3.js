"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[51256],{493742:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>o,contentTitle:()=>s,default:()=>p,frontMatter:()=>i,metadata:()=>c,toc:()=>d});var r=a(785893),t=a(603905);const i={sidebar_position:9070,slug:"2021-11-01",title:"spaCy NER on Arch Linux",authors:"mpolinowski",tags:["Machine Learning","Python"]},s=void 0,c={id:"IoT-and-Machine-Learning/ML/2021-11-01--spacy_natural_language_processing/index",title:"spaCy NER on Arch Linux",description:"Victoria Harbour, Hongkong",source:"@site/docs/IoT-and-Machine-Learning/ML/2021-11-01--spacy_natural_language_processing/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2021-11-01--spacy_natural_language_processing",slug:"/IoT-and-Machine-Learning/ML/2021-11-01--spacy_natural_language_processing/2021-11-01",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-11-01--spacy_natural_language_processing/2021-11-01",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2021-11-01--spacy_natural_language_processing/index.md",tags:[{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Python",permalink:"/docs/tags/python"}],version:"current",sidebarPosition:9070,frontMatter:{sidebar_position:9070,slug:"2021-11-01",title:"spaCy NER on Arch Linux",authors:"mpolinowski",tags:["Machine Learning","Python"]},sidebar:"tutorialSidebar",previous:{title:"spaCy NER Predictions",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-11-02--spacy_ner_predictions/2021-11-02"},next:{title:"Introduction to Keras",permalink:"/docs/IoT-and-Machine-Learning/ML/2019-04-01--introduction-to-keras/2019-04-01"}},o={},d=[{value:"Project Setup",id:"project-setup",level:2},{value:"Data Preprocessing",id:"data-preprocessing",level:2},{value:"Data Formating",id:"data-formating",level:3},{value:"Data Extraction",id:"data-extraction",level:3},{value:"Text Cleaning",id:"text-cleaning",level:3},{value:"spaCy",id:"spacy",level:2},{value:"The spaCy Data Format",id:"the-spacy-data-format",level:3},{value:"Group Entities",id:"group-entities",level:4},{value:"Remove all O Tags",id:"remove-all-o-tags",level:4},{value:"List of Entities",id:"list-of-entities",level:4},{value:"Data Splitting - Test &amp; Training",id:"data-splitting---test--training",level:4},{value:"Spacy Base Configuration",id:"spacy-base-configuration",level:3},{value:"Data Preprocessing",id:"data-preprocessing-1",level:4},{value:"NER Model Training",id:"ner-model-training",level:4}];function l(n){const e={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.ah)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{alt:"Victoria Harbour, Hongkong",src:a(720705).Z+"",width:"1500",height:"663"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"/docs/IoT-and-Machine-Learning/ML/2021-10-31--tesseract_ocr_arch_linux/2021-10-31",children:"Part I - Tesseract OCR on Arch Linux"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"/docs/IoT-and-Machine-Learning/ML/2021-11-01--spacy_natural_language_processing/2021-11-01",children:"Part II - spaCy NER on Arch Linux"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"/docs/IoT-and-Machine-Learning/ML/2021-11-02--spacy_ner_predictions/2021-11-02",children:"Part III - spaCy NER Predictions"})}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.a,{href:"https://github.com/explosion/spaCy",children:"spaCy"})," is an open-source software library for advanced natural language processing, written in the programming languages Python."]}),"\n",(0,r.jsxs)(e.p,{children:["spaCy also supports deep learning workflows that allow connecting statistical models trained by popular machine learning libraries like TensorFlow, PyTorch or MXNet through its own machine learning library Thinc. Using Thinc as its backend, spaCy features convolutional neural network models for part-of-speech tagging, dependency parsing, text categorization and ",(0,r.jsx)(e.strong,{children:"named entity recognition"})," (",(0,r.jsx)(e.code,{children:"NER"}),")."]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"#project-setup",children:"Project Setup"})}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.a,{href:"#data-preprocessing",children:"Data Preprocessing"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"#data-formating",children:"Data Formating"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"#data-extraction",children:"Data Extraction"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"#text-cleaning",children:"Text Cleaning"})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.a,{href:"#spacy",children:"spaCy"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.a,{href:"#the-spacy-data-format",children:"The spaCy Data Format"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"#group-entities",children:"Group Entities"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"#remove-all-o-tags",children:"Remove all O Tags"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"#list-of-entities",children:"List of Entities"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"#data-splitting---test--training",children:"Data Splitting - Test & Training"})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.a,{href:"#spacy-base-configuration",children:"Spacy Base Configuration"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"#data-preprocessing-1",children:"Data Preprocessing"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"#ner-model-training",children:"NER Model Training"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"project-setup",children:"Project Setup"}),"\n",(0,r.jsx)(e.p,{children:"I am going to re-use the virtual environment created for Tesseract earlier:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"mkdir -p /opt/Python/pyOCR\r\npython -m venv .env\n"})}),"\n",(0,r.jsx)(e.p,{children:"You can re-enter the environment with:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"source .env/bin/activate\n"})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{alt:"spaCy",src:a(788004).Z+"",width:"1101",height:"762"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"pip install -U spacy\r\npython -m spacy download en_core_web_sm\n"})}),"\n",(0,r.jsx)(e.h2,{id:"data-preprocessing",children:"Data Preprocessing"}),"\n",(0,r.jsx)(e.h3,{id:"data-formating",children:"Data Formating"}),"\n",(0,r.jsxs)(e.p,{children:["Tou use the data I extracted from my name card collection in spaCy I first need to re-format it. I first start by creating a TSV (Tab-separated) file from the CSV (Comma-separated) data. In LibreOffice you can simply open the CSV file, select that it is Comma separated and then save it with the ",(0,r.jsx)(e.code,{children:"{Tab}"}),' as delimiter option - in MS Office there is an option called "Text (tab delimited) that does the same. But there are also ',(0,r.jsx)(e.a,{href:"https://onlinecsvtools.com/convert-csv-to-tsv",children:"Online Converters"})," available. This will change the format from:"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-log",children:"card_46.jpg,Mike,B-NAME\r\ncard_46.jpg,Polinowski,I-NAME\r\ncard_46.jpg,Business,B-DES\r\ncard_46.jpg,Card,I-DES\r\ncard_46.jpg,Designer,I-DES\n"})}),"\n",(0,r.jsx)(e.p,{children:"Into this:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-log",children:"card_46.jpg\tMike\tB-NAME\r\ncard_46.jpg\tPolinowski\tI-NAME\r\ncard_46.jpg\tBusiness\tB-DES\r\ncard_46.jpg\tCard\tI-DES\r\ncard_46.jpg\tDesigner\tI-DES\n"})}),"\n",(0,r.jsx)(e.p,{children:'Create a new Jupyter notebook "03_Data_Preprocessing":'}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"jupyter notebook\n"})}),"\n",(0,r.jsx)(e.h3,{id:"data-extraction",children:"Data Extraction"}),"\n",(0,r.jsx)(e.p,{children:"I can now open and read the TSV file and start working with it in Pandas:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"import numpy as np\r\nimport pandas as pd\r\nimport string\r\nimport re\r\n\r\nwith open('businessCardsTab.csv', mode='r',encoding='utf8',errors='ignore') as f:\r\n    text = f.read()\n"})}),"\n",(0,r.jsxs)(e.p,{children:["Run a ",(0,r.jsx)(e.code,{children:"print(text)"})," to verify that everything is working. You can also read the RAW input by running just ",(0,r.jsx)(e.code,{children:"text"})," - here you can see that every tab is symbolized by a ",(0,r.jsx)(e.code,{children:"\\t"})," and a new line by ",(0,r.jsx)(e.code,{children:"\\n"})]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"Mike\\tB-NAME\\ncard_46.jpg\\tPolinowski\\tI-NAME\\ncard_46.jpg\\tBusiness\\tB-DES\\ncard_46.jpg\\tCard\\tI-DES\\ncard_46.jpg\\tDesigner\\tI-DES\\n\n"})}),"\n",(0,r.jsx)(e.p,{children:"Now I can clean up the text using those delimiters:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"text.split('\\n')\n"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"['\\ufeffid\\ttext\\ttag',\r\n...\r\n  'card_46.jpg\\tMike\\tB-NAME',\r\n  'card_46.jpg\\tPolinowski\\tI-NAME',\r\n  'card_46.jpg\\tBusiness\\tB-DES',\r\n  'card_46.jpg\\tCard\\tI-DES',\r\n  'card_46.jpg\\tDesigner\\tI-DES',\r\n ...]\n"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"data = list(map(lambda x:x.split('\\t'),text.split('\\n')))\n"})}),"\n",(0,r.jsxs)(e.p,{children:["Print ",(0,r.jsx)(e.code,{children:"data"})," to verify:"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"[['\\ufeffid', 'text', 'ttag'],\r\n...\r\n  ['card_46.jpg', 'Mike', 'B-NAME'],\r\n  ['card_46.jpg', 'Polinowski', 'I-NAME'],\r\n  ['card_46.jpg', 'Business', 'B-DES'],\r\n  ['card_46.jpg', 'Card', 'I-DES'],\r\n  ['card_46.jpg', 'Designer', 'I-DES'],\r\n ...]\n"})}),"\n",(0,r.jsx)(e.p,{children:"Write data into a Pandas dataframe:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"df = pd.DataFrame(data[1:],columns=data[0])\n"})}),"\n",(0,r.jsxs)(e.p,{children:["Take data from line ",(0,r.jsx)(e.code,{children:"1"})," to end of file and use the first element ",(0,r.jsx)(e.code,{children:"0"})," as column headers:"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{alt:"spaCy NER",src:a(727056).Z+"",width:"1137",height:"531"})}),"\n",(0,r.jsx)(e.h3,{id:"text-cleaning",children:"Text Cleaning"}),"\n",(0,r.jsxs)(e.p,{children:["Removing white spaces and punctuation characters using the ",(0,r.jsx)(e.strong,{children:"String library"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"string.whitespace\r\n' \\t\\n\\r\\x0b\\x0c'\n"})}),"\n",(0,r.jsxs)(e.p,{children:["I can remove all of the above as provided by ",(0,r.jsx)(e.code,{children:"whitespace"}),". But we I need to retain a few of the special characters provided by ",(0,r.jsx)(e.code,{children:"punctuation"})," - e.g. the forward-slash, underscore, the @ symbol, etc.:"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"string.punctuation\r\n'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n"})}),"\n",(0,r.jsx)(e.p,{children:"So I can define:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"whitespace = string.whitespace\r\npunctuation = '!#$%&\\'()*+:;<=>?[\\\\]^`{|}~'\n"})}),"\n",(0,r.jsxs)(e.p,{children:["Now we can search for these within our data and replace every occurrence with an ",(0,r.jsx)(e.em,{children:"nothing"})," ",(0,r.jsx)(e.code,{children:"''"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"tableWhitespace = str.maketrans('','',whitespace)\r\ntablePunctuation = str.maketrans('','',punctuation)\r\n\r\ndef cleanText(txt):\r\n    text = str(txt)\r\n    text = text.lower()\r\n    removeWhitespace = text.translate(tableWhitespace)\r\n    removePunctuation = removeWhitespace.translate(tablePunctuation)\r\n    \r\n    return str(removePunctuation)\n"})}),"\n",(0,r.jsxs)(e.p,{children:["We can apply the ",(0,r.jsx)(e.code,{children:"cleanText"})," function on our text dataframe:"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"df['text'] = df['text'].apply(cleanText)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Additionally we can drop all missing values:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"dataClean = df.query(\"text != '' \")\r\ndataClean.dropna(inplace=True)\n"})}),"\n",(0,r.jsxs)(e.p,{children:["Verify that the cleaning worked with ",(0,r.jsx)(e.code,{children:"dataClean.head(10)"}),":"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{alt:"spaCy NER",src:a(26827).Z+"",width:"1061",height:"340"})}),"\n",(0,r.jsx)(e.h2,{id:"spacy",children:"spaCy"}),"\n",(0,r.jsx)(e.h3,{id:"the-spacy-data-format",children:"The spaCy Data Format"}),"\n",(0,r.jsx)(e.h4,{id:"group-entities",children:"Group Entities"}),"\n",(0,r.jsxs)(e.p,{children:["We now need to prepare our data so that we can train our spaCy model with it. For this we first group every card into an entity by ",(0,r.jsx)(e.code,{children:"id"})," - where ",(0,r.jsx)(e.code,{children:"id"})," is the JPG name:"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"group = dataClean.groupby(by='id')\n"})}),"\n",(0,r.jsx)(e.p,{children:"Verify that it was successful by printing:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"group.groups.keys()\r\ndict_keys(['card_01.jpg', 'card_02.jpg', 'card_03.jpg', 'card_04.jpg', 'card_05.jpg', 'card_06.jpg', 'card_07.jpg', 'card_08.jpg', 'card_09.jpg', 'card_10.jpg', 'card_11.jpg', 'card_12.jpg', 'card_13.jpg', 'card_14.jpg', 'card_16.jpg', 'card_17.jpg', 'card_18.jpg', 'card_20.jpg', 'card_22.jpg', 'card_25.jpg', 'card_26.jpg', 'card_27.jpg', 'card_28.jpg', 'card_29.jpg', 'card_31.jpg', 'card_32.jpg', 'card_33.jpg', 'card_34.jpg', 'card_35.jpg', 'card_36.jpg', 'card_37.jpg', 'card_38.jpg', 'card_39.jpg', 'card_41.jpg', 'card_43.jpg', 'card_44.jpg', 'card_45.jpg', 'card_46.jpg', 'card_50.jpg', 'card_52.jpg', 'card_57.jpg', 'card_58.jpg', 'card_59.jpg', 'card_60.jpg', 'card_62.jpg', 'card_63.jpg', 'card_65.jpg', 'card_66.jpg', 'card_67.jpg', 'card_68.jpg', 'card_69.jpg', 'card_70.jpg', 'card_71.jpg', 'card_72.jpg', 'card_79.jpg'])\n"})}),"\n",(0,r.jsxs)(e.p,{children:["To get all the information from one name card, e.g. ",(0,r.jsx)(e.code,{children:"card_46.jpg"}),", run:"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"group.get_group('card_46.jpg')\n"})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{alt:"spaCy NER",src:a(923787).Z+"",width:"1070",height:"529"})}),"\n",(0,r.jsxs)(e.p,{children:["For the training we will need the values for ",(0,r.jsx)(e.code,{children:"text"})," and ",(0,r.jsx)(e.code,{children:"tag"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"group.get_group('card_46.jpg')[['text', 'tag']].values\n"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"array([['mike', 'B-NAME'],\r\n       ['polinowski', 'I-NAME'],\r\n       ['business', 'B-DES'],\r\n       ['card', 'I-DES'],\r\n       ['designer', 'I-DES'],\r\n       ['987-4575-9567', 'B-PHONE'],\r\n       ['8365-9686-6997', 'B-PHONE'],\r\n       ['building', 'O'],\r\n       ['4', 'O'],\r\n       ['room', 'O'],\r\n       ['748', 'O'],\r\n       ['business', 'O'],\r\n       ['plaza', 'O'],\r\n       ['mong', 'O'],\r\n       ['kok', 'O'],\r\n       ['hongkong', 'O'],\r\n       ['lookmum@iamdoingphotoshop.com', 'B-EMAIL']], dtype=object)\n"})}),"\n",(0,r.jsx)(e.p,{children:"We can write this array into a variable:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"grouparray = group.get_group('card_46.jpg')[['text', 'tag']].values\r\ncontent = ''\r\nannotations = {'entities': []}\r\nstart = 0\r\nend = 0\r\n\r\nfor text, tag in grouparray:\r\n  print(text,tag)\n"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"mike B-NAME\r\npolinowski I-NAME\r\nbusiness B-DES\r\ncard I-DES\r\ndesigner I-DES\r\n987-4575-9567 B-PHONE\r\n8365-9686-6997 B-PHONE\r\nbuilding O\r\n4 O\r\nroom O\r\n748 O\r\nbusiness O\r\nplaza O\r\nmong O\r\nkok O\r\nhongkong O\r\nlookmum@iamdoingphotoshop.com B-EMAIL\n"})}),"\n",(0,r.jsx)(e.h4,{id:"remove-all-o-tags",children:"Remove all O Tags"}),"\n",(0,r.jsxs)(e.p,{children:["We can write this result into a string and strip all lines that are tagged with ",(0,r.jsx)(e.code,{children:"O"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"grouparray = group.get_group('card_46.jpg')[['text', 'tag']].values\r\ncontent = ''\r\nannotations = {'entities': []}\r\nstart = 0\r\nend = 0\r\n\r\nfor text, tag in grouparray:\r\n  text = str(text)\r\n  stringLength = len(text) + 1 # 1 x space after each word\r\n\r\n  start = end\r\n  end = start + stringLength\r\n\r\n  if tag != 'O':\r\n    annot = (start,end-1,tag)\r\n    annotations['entities'].append(annot)\r\n\r\n  content = content + text + ' '\n"})}),"\n",(0,r.jsx)(e.p,{children:"This now took our content:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"content\r\n'mike polinowski business card designer 987-4575-9567 8365-9686-6997 building 4 room 748 business plaza mong kok hongkong lookmum@iamdoingphotoshop.com '\n"})}),"\n",(0,r.jsx)(e.p,{children:"And turned it into this annotation that allows us to target the information we need:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"annotations\r\n{'entities': [(0, 4, 'B-NAME'),\r\n  (5, 15, 'I-NAME'),\r\n  (16, 24, 'B-DES'),\r\n  (25, 29, 'I-DES'),\r\n  (30, 38, 'I-DES'),\r\n  (39, 52, 'B-PHONE'),\r\n  (53, 67, 'B-PHONE'),\r\n  (121, 150, 'B-EMAIL')]}\n"})}),"\n",(0,r.jsx)(e.h4,{id:"list-of-entities",children:"List of Entities"}),"\n",(0,r.jsxs)(e.p,{children:["To be able to create these for all our cards we need to create a variable that holds all our name cards by ",(0,r.jsx)(e.code,{children:"id"})," - this is what we already had above:"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"cards = group.groups.keys()\n"})}),"\n",(0,r.jsxs)(e.p,{children:["We can use this to wrap our code into a FOR loop over all cards and append all ",(0,r.jsx)(e.code,{children:"content"})," + ",(0,r.jsx)(e.code,{children:"annotations"})," for each card to an list ",(0,r.jsx)(e.code,{children:"cardData"})," which is then concatenated into the ",(0,r.jsx)(e.code,{children:"allCardsData"})," list:"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"allCardsData = []\r\nfor card in cards:\r\n  cardData = []\r\n  grouparray = group.get_group(card)[['text', 'tag']].values\r\n  content = ''\r\n  annotations = {'entities': []}\r\n  start = 0\r\n  end = 0\r\n\r\n  for text, tag in grouparray:\r\n    text = str(text)\r\n    stringLength = len(text) + 1 # 1 x space after each word\r\n\r\n    start = end\r\n    end = start + stringLength\r\n\r\n    if tag != 'O':\r\n      annot = (start,end-1,tag)\r\n      annotations['entities'].append(annot)\r\n\r\n    content = content + text + ' '\r\n\r\n  cardData = (content, annotations)\r\n  allCardsData.append(cardData)\n"})}),"\n",(0,r.jsxs)(e.p,{children:["Run ",(0,r.jsx)(e.code,{children:"allCardsData"})," to get a complete list for all entities (name card ",(0,r.jsx)(e.code,{children:"content"})," + ",(0,r.jsx)(e.code,{children:"annotations"}),"). This is now the so called ",(0,r.jsx)(e.strong,{children:"spaCy Format"})," - the way the spaCy library consumes it's trainings data in."]}),"\n",(0,r.jsx)(e.h4,{id:"data-splitting---test--training",children:"Data Splitting - Test & Training"}),"\n",(0,r.jsxs)(e.p,{children:["We can use the ",(0,r.jsx)(e.code,{children:"random"})," library to make a random selection of entities to split them into a stack for trainings and test data:"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"import random\r\nrandom.shuffle(allCardsData)\n"})}),"\n",(0,r.jsxs)(e.p,{children:["Currently I have a stack of 55 cards in total - you can check your stack with ",(0,r.jsx)(e.code,{children:"len(allCardsData)"}),". I will now split this stack into 90:10 ratio:"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"TrainData = allCardsData[:50]\r\nTestData = allCardsData[50:]\n"})}),"\n",(0,r.jsxs)(e.p,{children:["Create a directory ",(0,r.jsx)(e.code,{children:"data"})," inside the apps work directory and save this data inside your folder using the ",(0,r.jsx)(e.code,{children:"pickle"})," library:"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"import pickle\r\npickle.dump(TrainData,open('./data/TrainData.pickle',mode='wb'))\r\npickle.dump(TestData,open('./data/TestData.pickle',mode='wb'))\n"})}),"\n",(0,r.jsx)(e.h3,{id:"spacy-base-configuration",children:"Spacy Base Configuration"}),"\n",(0,r.jsxs)(e.p,{children:["Go to ",(0,r.jsx)(e.a,{href:"https://spacy.io/usage/training",children:"Training Pipelines & Models"})," and generate/download your spaCy configuration template:"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{alt:"spaCy NER",src:a(451082).Z+"",width:"909",height:"703"})}),"\n",(0,r.jsxs)(e.p,{children:["After saving the starter config to a file ",(0,r.jsx)(e.code,{children:"base_config.cfg"}),", use the ",(0,r.jsx)(e.code,{children:"init fill-config"})," command to fill in the remaining defaults. Training configs should always be complete and without hidden defaults, to keep your experiments reproducible:"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"python -m spacy init fill-config ./base_config.cfg ./config.cfg\r\n\u2714 Auto-filled config with all values\r\n\u2714 Saved config\r\nconfig.cfg\n"})}),"\n",(0,r.jsx)(e.h4,{id:"data-preprocessing-1",children:"Data Preprocessing"}),"\n",(0,r.jsxs)(e.p,{children:["Here\u2019s ",(0,r.jsx)(e.a,{href:"https://spacy.io/usage/training#training-data",children:"an example"})," of creating a ",(0,r.jsx)(e.code,{children:".spacy"})," file from our NER annotations - modified to use the pickle files created above:"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"import spacy\r\nfrom spacy.tokens import DocBin\r\nimport pickle\r\n\r\nnlp = spacy.blank(\"en\")\r\n\r\ntraining_data = pickle.load(open('./data/TrainData.pickle', 'rb'))\r\ntesting_data = pickle.load(open('./data/TestData.pickle', 'rb'))\r\n\r\n# Create training data file in spaCy format\r\ndb = DocBin()\r\nfor text, annotations in training_data:\r\n    doc = nlp(text)\r\n    ents = []\r\n    for start, end, label in annotations['entities']:\r\n        span = doc.char_span(start, end, label=label)\r\n        ents.append(span)\r\n    doc.ents = ents\r\n    db.add(doc)\r\ndb.to_disk(\"./data/train.spacy\")\r\n\r\n# Create testing data file in spaCy format\r\ndb_test = DocBin()\r\nfor text, annotations in testing_data:\r\n    doc = nlp(text)\r\n    ents = []\r\n    for start, end, label in annotations['entities']:\r\n        span = doc.char_span(start, end, label=label)\r\n        ents.append(span)\r\n    doc.ents = ents\r\n    db.add(doc)\r\ndb_test.to_disk(\"./data/test.spacy\")\n"})}),"\n",(0,r.jsx)(e.p,{children:"Run the Python script from the terminal:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"python ./preprocess.py\n"})}),"\n",(0,r.jsxs)(e.p,{children:["And the spaCy files will be created in your ",(0,r.jsx)(e.code,{children:"data"})," directory:"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"ll ./data                                                                                              \ue0b2 \u2714 \r\ntotal 40\r\n\r\n1394 Dec 12 21:06 TestData.pickle\r\n106 Dec 13 12:48 test.spacy\r\n12070 Dec 12 21:06 TrainData.pickle\r\n9545 Dec 13 12:48 train.spacy\n"})}),"\n",(0,r.jsx)(e.h4,{id:"ner-model-training",children:"NER Model Training"}),"\n",(0,r.jsx)(e.p,{children:"Start by creating an output directory for the model training:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"mkdir ./output\n"})}),"\n",(0,r.jsx)(e.p,{children:"Now I can add my data and run a training with the configuration file:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"python -m spacy train ./config.cfg  --output ./output --paths.train ./data/train.spacy --paths.dev ./data/test.spacy\r\n\r\n\u2139 Saving to output directory: output\r\n\u2139 Using CPU\r\n\r\n=========================== Initializing pipeline ===========================\r\n[2021-12-13 12:55:39,208] [INFO] Set up nlp object from config\r\n[2021-12-13 12:55:39,214] [INFO] Pipeline: ['tok2vec', 'ner']\r\n[2021-12-13 12:55:39,217] [INFO] Created vocabulary\r\n[2021-12-13 12:55:39,218] [INFO] Finished initializing nlp object\r\n[2021-12-13 12:55:39,321] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\r\n\u2714 Initialized pipeline\r\n\r\n============================= Training pipeline =============================\r\n\u2139 Pipeline: ['tok2vec', 'ner']\r\n\u2139 Initial learn rate: 0.001\r\nE    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \r\n---  ------  ------------  --------  ------  ------  ------  ------\r\n  0       0          0.00     69.86    0.00    0.00    0.00    0.00\r\n 22     200        199.17   4224.89    0.00    0.00    0.00    0.00\r\n 50     400         87.29    138.64    0.00    0.00    0.00    0.00\r\n 84     600         41.00     68.52    0.00    0.00    0.00    0.00\r\n125     800         53.87     82.38    0.00    0.00    0.00    0.00\r\n175    1000         76.95    104.00    0.00    0.00    0.00    0.00\r\n240    1200         95.89    121.85    0.00    0.00    0.00    0.00\r\n310    1400         65.92    113.03    0.00    0.00    0.00    0.00\r\n410    1600         76.39    146.28    0.00    0.00    0.00    0.00\r\n\u2714 Saved pipeline to output directory\r\noutput/model-last\n"})}),"\n",(0,r.jsxs)(e.p,{children:["The models were successfully created inside the ",(0,r.jsx)(e.code,{children:"output"})," directory!"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"ll ./output\r\ntotal 16\r\n\r\n4096 Dec 13 12:55 model-best\r\n4096 Dec 13 12:55 model-last\n"})})]})}function p(n={}){const{wrapper:e}={...(0,t.ah)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(l,{...n})}):l(n)}},603905:(n,e,a)=>{a.d(e,{ah:()=>d});var r=a(667294);function t(n,e,a){return e in n?Object.defineProperty(n,e,{value:a,enumerable:!0,configurable:!0,writable:!0}):n[e]=a,n}function i(n,e){var a=Object.keys(n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(n);e&&(r=r.filter((function(e){return Object.getOwnPropertyDescriptor(n,e).enumerable}))),a.push.apply(a,r)}return a}function s(n){for(var e=1;e<arguments.length;e++){var a=null!=arguments[e]?arguments[e]:{};e%2?i(Object(a),!0).forEach((function(e){t(n,e,a[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(n,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(e){Object.defineProperty(n,e,Object.getOwnPropertyDescriptor(a,e))}))}return n}function c(n,e){if(null==n)return{};var a,r,t=function(n,e){if(null==n)return{};var a,r,t={},i=Object.keys(n);for(r=0;r<i.length;r++)a=i[r],e.indexOf(a)>=0||(t[a]=n[a]);return t}(n,e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(n);for(r=0;r<i.length;r++)a=i[r],e.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(n,a)&&(t[a]=n[a])}return t}var o=r.createContext({}),d=function(n){var e=r.useContext(o),a=e;return n&&(a="function"==typeof n?n(e):s(s({},e),n)),a},l={inlineCode:"code",wrapper:function(n){var e=n.children;return r.createElement(r.Fragment,{},e)}},p=r.forwardRef((function(n,e){var a=n.components,t=n.mdxType,i=n.originalType,o=n.parentName,p=c(n,["components","mdxType","originalType","parentName"]),h=d(a),g=t,j=h["".concat(o,".").concat(g)]||h[g]||l[g]||i;return a?r.createElement(j,s(s({ref:e},p),{},{components:a})):r.createElement(j,s({ref:e},p))}));p.displayName="MDXCreateElement"},720705:(n,e,a)=>{a.d(e,{Z:()=>r});const r=a.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-4f747fa38245d3c618169ab90d8c3f77.jpg"},727056:(n,e,a)=>{a.d(e,{Z:()=>r});const r=a.p+"assets/images/spaCy_01-af4e2d04bd3c31edd41657da1d1a3e81.png"},26827:(n,e,a)=>{a.d(e,{Z:()=>r});const r=a.p+"assets/images/spaCy_02-30873604eb690ea005efa08989d49bb6.png"},923787:(n,e,a)=>{a.d(e,{Z:()=>r});const r=a.p+"assets/images/spaCy_03-e30c82dc91a12a25fe7dab5a37d9a337.png"},451082:(n,e,a)=>{a.d(e,{Z:()=>r});const r=a.p+"assets/images/spaCy_0x-8e5ab73f3e816dfd54e38b2f33181a4d.png"},788004:(n,e,a)=>{a.d(e,{Z:()=>r});const r=a.p+"assets/images/spaCy_installation-b060ae507962d3a27dd65dc56e94fec2.png"}}]);