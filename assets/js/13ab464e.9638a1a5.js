"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[59179],{487635:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>l});var t=a(785893),i=a(603905);const r={sidebar_position:4930,slug:"2022-12-12",title:"Breast Histopathology Image Segmentation Part 6",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"Running Predictions"},o=void 0,s={id:"IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6/index",title:"Breast Histopathology Image Segmentation Part 6",description:"Running Predictions",source:"@site/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6",slug:"/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6/2022-12-12",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6/2022-12-12",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Tensorflow",permalink:"/docs/tags/tensorflow"}],version:"current",sidebarPosition:4930,frontMatter:{sidebar_position:4930,slug:"2022-12-12",title:"Breast Histopathology Image Segmentation Part 6",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"Running Predictions"},sidebar:"tutorialSidebar",previous:{title:"Tensorflow Image Classification",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-16-tf-cifar/2022-12-16"},next:{title:"Breast Histopathology Image Segmentation Part 5",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part5/2022-12-12"}},c={},l=[{value:"Loading the Model",id:"loading-the-model",level:2},{value:"Preparing the Test Image File",id:"preparing-the-test-image-file",level:2},{value:"Make Predictions",id:"make-predictions",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h2:"h2",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.ah)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Guangzhou, China",src:a(585591).Z+"",width:"1500",height:"383"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-10-tf-breast-cancer-classification-part1/2022-12-10",children:"Part 1: Data Inspection and Pre-processing"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part2/2022-12-11",children:"Part 2: Weights, Data Augmentations and Generators"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part3/2022-12-11",children:"Part 3: Model creation based on a pre-trained and a custom model"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part4/2022-12-11",children:"Part 4: Train our model to fit the dataset"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part5/2022-12-12",children:"Part 5: Evaluate the performance of your trained model"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6/2022-12-12",children:"Part 6: Running Predictions"})}),"\n"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://github.com/mpolinowski/tf-bc-classification",children:"Github"})}),"\n"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#loading-the-model",children:"Loading the Model"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#preparing-the-test-image-file",children:"Preparing the Test Image File"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#make-predictions",children:"Make Predictions"})}),"\n"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:["Based on ",(0,t.jsx)(n.a,{href:"https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images",children:"Breast Histopathology Images"})," by ",(0,t.jsx)(n.a,{href:"https://www.kaggle.com/paultimothymooney",children:"Paul Mooney"}),".\n",(0,t.jsx)(n.code,{children:"Invasive Ductal Carcinoma (IDC) is the most common subtype of all breast cancers. To assign an aggressiveness grade to a whole mount sample, pathologists typically focus on the regions which contain the IDC. As a result, one of the common pre-processing steps for automatic aggressiveness grading is to delineate the exact regions of IDC inside of a whole mount slide."}),"\n",(0,t.jsx)(n.a,{href:"https://youtu.be/8XsiMQQ-4mM",children:"Can recurring breast cancer be spotted with AI tech? - BBC News"})]}),"\n"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Citation: ",(0,t.jsx)(n.a,{href:"https://pubmed.ncbi.nlm.nih.gov/27563488/",children:"Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases"})]}),"\n",(0,t.jsx)(n.li,{children:"Dataset: 198,738 IDC(negative) image patches; 78,786 IDC(positive) image patches"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"loading-the-model",children:"Loading the Model"}),"\n",(0,t.jsxs)(n.p,{children:["After fitting the model to classify our source images into ",(0,t.jsx)(n.code,{children:"malignant"})," and ",(0,t.jsx)(n.code,{children:"benign"})," we can now use this model to make predictions about un-classified images. So load the model we want to use:"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.em,{children:"./RunPrediction.py"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# Model you want to use\nmodelName = 'resnet50_weights.hdf5'\n# modelName = 'custom_weights.hdf5'\nmodelPath = config.OUTPUT_PATH + '/' + modelName\nprint(\"Loading Breast Cancer detector model...\")\nmodel = load_model(modelPath)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"preparing-the-test-image-file",children:"Preparing the Test Image File"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:'# Test image\n# imagePath ="./sample_pictures/malignant.png"\nimagePath =  "./sample_pictures/benign.png"\n\n# Loading the input image using openCV\nimage = cv2.imread(imagePath)\n\n# Convert it from BGR to RGB and then resize it to 48x48, \n# the same parameter we used for training\nimage1 = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nimage1 = cv2.resize(image1, (48, 48))\nimage1 = img_to_array(image1)\n\n# Only for ResNet50\nimage1 = preprocess_input(image1)\n\n# The image is now represented by a NumPy array of shape (48, 48, 3), however \n# we need the dimensions to be (1, 3, 48, 48) so we can pass it\n# through the network and then we\'ll also preprocess the image by subtracting the \n# mean RGB pixel intensity from the ImageNet dataset\nimage1 /=  255.0\n\nimage1 = np.expand_dims(image1, axis=0)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"make-predictions",children:"Make Predictions"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:'# Pass the image through the model to determine if the person has malignant\n(benign, malignant) = model.predict(image1)[0]\n\n\n# Adding the probability in the label\nlabel = "benign" if benign > malignant else "malignant"\nlabel = "{}: {:.2f}%".format(label, max(benign, malignant) * 100)\n        \n# Showing the output image\nprint("RESULT :" +label)\n\ncv2.imshow("IDC", image)\nif cv2.waitKey(5000) & 0xFF == ord(\'q\'):\n        cv2.destroyAllWindows()\n'})})]})}function h(e={}){const{wrapper:n}={...(0,i.ah)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},603905:(e,n,a)=>{a.d(n,{ah:()=>l});var t=a(667294);function i(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function r(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function o(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?r(Object(a),!0).forEach((function(n){i(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function s(e,n){if(null==e)return{};var a,t,i=function(e,n){if(null==e)return{};var a,t,i={},r=Object.keys(e);for(t=0;t<r.length;t++)a=r[t],n.indexOf(a)>=0||(i[a]=e[a]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)a=r[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var c=t.createContext({}),l=function(e){var n=t.useContext(c),a=n;return e&&(a="function"==typeof e?e(n):o(o({},n),e)),a},d={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},h=t.forwardRef((function(e,n){var a=e.components,i=e.mdxType,r=e.originalType,c=e.parentName,h=s(e,["components","mdxType","originalType","parentName"]),g=l(a),m=i,p=g["".concat(c,".").concat(m)]||g[m]||d[m]||r;return a?t.createElement(p,o(o({ref:n},h),{},{components:a})):t.createElement(p,o({ref:n},h))}));h.displayName="MDXCreateElement"},585591:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-918471126c0472aad97358a725e1a399.jpg"}}]);