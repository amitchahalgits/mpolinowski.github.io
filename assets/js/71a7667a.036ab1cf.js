"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[53043],{92808:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>d,contentTitle:()=>i,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>l});var t=a(474848),o=a(28453);const r={sidebar_position:4860,slug:"2022-12-21",title:"Tensorflow Downsampling",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"Using Representation Learning to Downsample Images"},i=void 0,s={id:"IoT-and-Machine-Learning/ML/2022-12-21-tf-downsampling/index",title:"Tensorflow Downsampling",description:"Using Representation Learning to Downsample Images",source:"@site/docs/IoT-and-Machine-Learning/ML/2022-12-21-tf-downsampling/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2022-12-21-tf-downsampling",slug:"/IoT-and-Machine-Learning/ML/2022-12-21-tf-downsampling/2022-12-21",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-21-tf-downsampling/2022-12-21",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2022-12-21-tf-downsampling/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Tensorflow",permalink:"/docs/tags/tensorflow"}],version:"current",sidebarPosition:4860,frontMatter:{sidebar_position:4860,slug:"2022-12-21",title:"Tensorflow Downsampling",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"Using Representation Learning to Downsample Images"},sidebar:"tutorialSidebar",previous:{title:"Deep Convolutional Generative Adversarial Network",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-28-tf-gan-image-generator/2022-12-28"},next:{title:"Tensorflow Deep Dream",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-21-tf-deepdream/2022-12-21"}},d={},l=[{value:"Build the Autoencoder",id:"build-the-autoencoder",level:2},{value:"Train the Autoencoder",id:"train-the-autoencoder",level:2},{value:"Evaluation",id:"evaluation",level:2}];function c(e){const n={a:"a",code:"code",h2:"h2",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Guangzhou, China",src:a(550790).A+"",width:"1500",height:"383"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#build-the-autoencoder",children:"Build the Autoencoder"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#train-the-autoencoder",children:"Train the Autoencoder"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#evaluation",children:"Evaluation"})}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Using ",(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-20-tf-representation/2022-12-19",children:"Representation Learning"})," to downsample images."]}),"\n",(0,t.jsx)(n.h2,{id:"build-the-autoencoder",children:"Build the Autoencoder"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:'# build autoencoder model\nautoencoder = tf.keras.models.Sequential()\n\n# build the encoder CNN\nautoencoder.add(tf.keras.layers.Conv2D(64, (3,3), strides=1, padding="same", input_shape=(32, 32, 3)))\nautoencoder.add(tf.keras.layers.BatchNormalization())\nautoencoder.add(tf.keras.layers.Activation(\'relu\'))\nautoencoder.add(tf.keras.layers.AveragePooling2D((2,2), padding="same"))\n\nautoencoder.add(tf.keras.layers.Conv2D(32, (3,3), strides=1, padding="same"))\nautoencoder.add(tf.keras.layers.BatchNormalization())\nautoencoder.add(tf.keras.layers.Activation(\'relu\'))\n\n# representation layer\nautoencoder.add(tf.keras.layers.AveragePooling2D((2,2), padding="same"))\n\n# build the decoder CNN \nautoencoder.add(tf.keras.layers.BatchNormalization())\nautoencoder.add(tf.keras.layers.Activation(\'relu\'))\nautoencoder.add(tf.keras.layers.UpSampling2D((2, 2)))\n\nautoencoder.add(tf.keras.layers.Conv2D(64, (3,3), strides=1, padding="same"))\nautoencoder.add(tf.keras.layers.BatchNormalization())\nautoencoder.add(tf.keras.layers.Activation(\'relu\'))\nautoencoder.add(tf.keras.layers.UpSampling2D((2, 2)))\n\nautoencoder.add(tf.keras.layers.Conv2D(3, (3,3), strides=1, activation=\'sigmoid\', padding="same"))\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# compile model\nautoencoder.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.01))\nautoencoder.summary()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"train-the-autoencoder",children:"Train the Autoencoder"}),"\n",(0,t.jsxs)(n.p,{children:["Before I used ",(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-20-tf-representation/2022-12-19",children:"Representation Learning to remove digital noise from image files"}),". There we needed to compare the generated image from our CNN layers with a noise-free version of the image as a performance metric. For downsampling we need to compare the de-compressed image to the original image - the model first compresses features (encoding) and then de-compresses them (decoding) to try to match the original input:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# fit model to dataset\nautoencoder.fit(X_train,          \n          X_train, \n          epochs=20, \n          batch_size=200, \n          validation_data=(X_test, X_test))\n"})}),"\n",(0,t.jsx)(n.h2,{id:"evaluation",children:"Evaluation"}),"\n",(0,t.jsx)(n.p,{children:"And we end up with a lightly compressed version of the input images:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# test training\n# take 15 images from test set and predict compressed image\npredicted = autoencoder.predict(X_test[:10].reshape(-1, 32, 32, 3))\n# plot input vs output\nfig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))\nfor images, row in zip([X_test[:10], predicted], axes):\n    for img, ax in zip(images, row):\n        ax.imshow(img.reshape((32, 32, 3)))\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n\nplt.show()\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Tensorflow Transfer Learning",src:a(602562).A+"",width:"1573",height:"319"})})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},602562:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/Tensorflow_Transfer_Learning_01-bcedf01b351e75791b7d551b984abefd.png"},550790:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-918471126c0472aad97358a725e1a399.jpg"},28453:(e,n,a)=>{a.d(n,{R:()=>i,x:()=>s});var t=a(296540);const o={},r=t.createContext(o);function i(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);