"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[80082],{3905:(e,n,t)=>{t.d(n,{Zo:()=>c,kt:()=>m});var a=t(67294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=a.createContext({}),p=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},c=function(e){var n=p(e.components);return a.createElement(l.Provider,{value:n},e.children)},d={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},u=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=p(t),m=r,h=u["".concat(l,".").concat(m)]||u[m]||d[m]||i;return t?a.createElement(h,o(o({ref:n},c),{},{components:t})):a.createElement(h,o({ref:n},c))}));function m(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var i=t.length,o=new Array(i);o[0]=u;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var p=2;p<i;p++)o[p]=t[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}u.displayName="MDXCreateElement"},11010:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var a=t(87462),r=(t(67294),t(3905));const i={sidebar_position:4720,slug:"2023-02-16",title:"Keras for Tensorflow - Artificial Neural Networks",authors:"mpolinowski",tags:["Python","Machine Learning","Keras"],description:"Building a deep neural network using the MNIST dataset."},o=void 0,s={unversionedId:"IoT-and-Machine-Learning/ML/2023-02-16-keras-introduction-ann/index",id:"IoT-and-Machine-Learning/ML/2023-02-16-keras-introduction-ann/index",title:"Keras for Tensorflow - Artificial Neural Networks",description:"Building a deep neural network using the MNIST dataset.",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-02-16-keras-introduction-ann/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-02-16-keras-introduction-ann",slug:"/IoT-and-Machine-Learning/ML/2023-02-16-keras-introduction-ann/2023-02-16",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-02-16-keras-introduction-ann/2023-02-16",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-02-16-keras-introduction-ann/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Keras",permalink:"/docs/tags/keras"}],version:"current",sidebarPosition:4720,frontMatter:{sidebar_position:4720,slug:"2023-02-16",title:"Keras for Tensorflow - Artificial Neural Networks",authors:"mpolinowski",tags:["Python","Machine Learning","Keras"],description:"Building a deep neural network using the MNIST dataset."},sidebar:"tutorialSidebar",previous:{title:"Keras for Tensorflow - Convolutional Neural Networks",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-02-17-keras-introduction-cnn/2023-02-17"},next:{title:"YOLOv8 with AS-One",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-02-15-as-one-yolo-object-tracking/2023-02-15"}},l={},p=[{value:"MNIST Datasets",id:"mnist-datasets",level:2},{value:"Data Pre-Processing",id:"data-pre-processing",level:2},{value:"Building the Model",id:"building-the-model",level:2},{value:"Training the Model",id:"training-the-model",level:2}],c={toc:p};function d(e){let{components:n,...i}=e;return(0,r.kt)("wrapper",(0,a.Z)({},c,i,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Guangzhou, China",src:t(60189).Z,width:"2830",height:"1272"})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#mnist-datasets"},"MNIST Datasets")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#data-pre-processing"},"Data Pre-Processing")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#building-the-model"},"Building the Model")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#training-the-model"},"Training the Model"))),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/tf-keras-2023"},"Github Repository")),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://keras.io/getting_started/"},"Keras")," is built on top of TensorFlow 2 and provides an API designed for human beings. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear & actionable error messages."),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"See also:")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-14-keras-introduction/2023-02-14"},"Keras for Tensorflow - An (Re)Introduction 2023")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-16-keras-introduction-ann/2023-02-16"},"Keras for Tensorflow - Artificial Neural Networks")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-17-keras-introduction-cnn/2023-02-17"},"Keras for Tensorflow - Convolutional Neural Networks")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-vgg16/2023-02-18"},"Keras for Tensorflow - VGG16 Network Architecture")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-rnn/2023-02-18"},"Keras for Tensorflow - Recurrent Neural Networks"))),(0,r.kt)("h2",{id:"mnist-datasets"},"MNIST Datasets"),(0,r.kt)("p",null,"Keras gives us access to several datasets that we can use to train our models - for example MNIST:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import to_categorical\n\n# using the mnist dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n")),(0,r.kt)("p",null,"We can print the shape of the dataset to see that we have 60k training images and 10k images for validation - all 28x28 pixel in size. We can also use Matplotlib to display a sample image:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"print(x_train.shape, x_test.shape)\n# (60000, 28, 28) (10000, 28, 28)\n\nplt.imshow(x_train[666])\nplt.show()\n")),(0,r.kt)("p",null,"To check the assigned label we can run:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"print(y_train[666])\n# 0\n")),(0,r.kt)("p",null,"The image shows the number ",(0,r.kt)("inlineCode",{parentName:"p"},"0")," and this is also the assigned label for it:"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Keras for Tensorflow - Artificial Neural Networks",src:t(34281).Z,width:"1132",height:"540"})),(0,r.kt)("h2",{id:"data-pre-processing"},"Data Pre-Processing"),(0,r.kt)("p",null,"The features inside our image are represented with a number range of ",(0,r.kt)("inlineCode",{parentName:"p"},"0")," - ",(0,r.kt)("inlineCode",{parentName:"p"},"255"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"# data pre-processing\n## normalize training images\n## start by creating a single column 28x28=>784\nx_train = x_train.reshape(60000, 784)\nx_test = x_test.reshape(10000, 784)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\n\nprint(x_train[666])\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"[  ...\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n  50. 237. 203.  75.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  37.\n 232. 254. 254. 244.  15.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   9. 156.\n 254. 209. 250. 254. 131.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  31. 233.\n 163.  11. 143. 254. 233.  25.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   9. 164. 192.\n  11.   0.  74. 253. 254.  89.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3. 122. 254. 195.\n   0.   0.   0.  95. 254.  89.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   9. 254. 231.  44.\n   0.   0.   0. 127. 254. 146.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.  11. 192. 254.  95.   0.\n   0.   0.   0.  65. 254. 207.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0. 127. 254. 220.  35.   0.\n   0.   0.   0.  12. 254. 237.  45.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.  21. 233. 253.  86.   0.   0.\n   0.   0.   0.  12. 255. 254.  71.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0. 181. 254. 249.   0.   0.   0.\n   0.   0.   0.  12. 254. 254.  71.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0. 208. 254. 148.   0.   0.   0.\n   0.   0.   0. 113. 254. 237.  45.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0. 119. 250. 254. 129.   0.   0.   0.\n   0.   0.   0. 183. 254. 207.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0. 190. 254. 254.  13.   0.   0.   0.\n   0.   0. 112. 254. 254.  91.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0. 190. 254. 192.   5.   0.   0.   0.\n   0.  12. 185. 254. 251.  78.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0. 190. 254. 147.   0.   0.   0.   0.\n   0. 173. 254. 252. 128.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0. 190. 254. 225.  40.  66.  25.  16.\n 102. 243. 254. 212.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.  96. 254. 254. 230. 254. 221. 214.\n 254. 254. 217.  28.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.  10. 167. 254. 254. 254. 254. 254.\n 254. 217.  86.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   6.  67. 194. 254. 254. 227.\n 100.  11.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   ...\n   ]\n")),(0,r.kt)("p",null,"We have to normalize them to range between ",(0,r.kt)("inlineCode",{parentName:"p"},"0")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"1")," by dividing their values by ",(0,r.kt)("inlineCode",{parentName:"p"},"255"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"x_train /=255\nx_test /=255\n")),(0,r.kt)("p",null,"Now we have to represent the labels as vectors - so turning our ",(0,r.kt)("inlineCode",{parentName:"p"},"0")," label into ",(0,r.kt)("inlineCode",{parentName:"p"},"[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]"),". And Keras provides us with the right utility to do that - ",(0,r.kt)("inlineCode",{parentName:"p"},"to_categorical()"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"# vectorize labels for the 10 categories from 0-9\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\nprint(y_train[666])\n# [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n")),(0,r.kt)("h2",{id:"building-the-model"},"Building the Model"),(0,r.kt)("p",null,"We need to build a sequential model with two dense layers. The first one receives our preprocessed images that have a shape of ",(0,r.kt)("inlineCode",{parentName:"p"},"28x28=784"),". In the second layer the input shape is given by the previous layer and does not need to be defined. The output uses a ",(0,r.kt)("inlineCode",{parentName:"p"},"softmax")," activation - so all label will receive a probability and in the end we will pick the one that has the highest to be our predicted label:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://keras.io/api/layers/activations/"},"Activation Functions"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"relu function: Rectified linear unit activation function"),(0,r.kt)("li",{parentName:"ul"},"sigmoid function: For ",(0,r.kt)("strong",{parentName:"li"},"binary classifications")),(0,r.kt)("li",{parentName:"ul"},"softmax function: For ",(0,r.kt)("strong",{parentName:"li"},"multi-label classifications"))))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"# building the model\nmodel = Sequential()\n\nmodel.add(Dense(128, activation='relu', input_shape=(784, )))\nmodel.add(Dense(128, activation='relu'))\n\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n")),(0,r.kt)("p",null,"The compiled model now looks like:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'Model: "sequential"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 128)               100480    \n                                                                 \n dense_1 (Dense)             (None, 128)               16512     \n                                                                 \n dense_2 (Dense)             (None, 10)                1290      \n                                                                 \n=================================================================\nTotal params: 118,282\nTrainable params: 118,282\nNon-trainable params: 0\n_________________________________________________________________\n')),(0,r.kt)("h2",{id:"training-the-model"},"Training the Model"),(0,r.kt)("p",null,"Now we have our data and a model. Next step is to fit the model to our training data and see if we can start making label predictions based off our validation data. Let's start by running a 10 epochs long training cycle using 128 images per batch:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"# fitting the model\nmodel.fit(x_train, y_train, batch_size=128, epochs=10)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"469/469 [==============================] - 4s 2ms/step - loss: 0.3179 - accuracy: 0.9097\nEpoch 2/10\n469/469 [==============================] - 1s 2ms/step - loss: 0.1278 - accuracy: 0.9620\nEpoch 3/10\n469/469 [==============================] - 1s 2ms/step - loss: 0.0882 - accuracy: 0.9731\nEpoch 4/10\n469/469 [==============================] - 1s 2ms/step - loss: 0.0646 - accuracy: 0.9803\nEpoch 5/10\n469/469 [==============================] - 1s 2ms/step - loss: 0.0497 - accuracy: 0.9851\nEpoch 6/10\n469/469 [==============================] - 1s 2ms/step - loss: 0.0403 - accuracy: 0.9875\nEpoch 7/10\n469/469 [==============================] - 1s 2ms/step - loss: 0.0323 - accuracy: 0.9902\nEpoch 8/10\n469/469 [==============================] - 1s 2ms/step - loss: 0.0263 - accuracy: 0.9915\nEpoch 9/10\n469/469 [==============================] - 1s 2ms/step - loss: 0.0220 - accuracy: 0.9929\nEpoch 10/10\n469/469 [==============================] - 1s 2ms/step - loss: 0.0197 - accuracy: 0.9936\n")),(0,r.kt)("p",null,"We can see the accuracy increasing from ",(0,r.kt)("inlineCode",{parentName:"p"},"0.9097")," to ",(0,r.kt)("inlineCode",{parentName:"p"},"0.9936")," and the loss decreasing from ",(0,r.kt)("inlineCode",{parentName:"p"},"0.3179")," to ",(0,r.kt)("inlineCode",{parentName:"p"},"0.0197"),". To evaluate the fitted model we can now use our validation dataset and see how accurate it can classify those images:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"# validation run\nval_loss, val_score = model.evaluate(x_test, y_test)\nprint(val_loss, val_score)\n# 0.07639817893505096 0.9789999723434448\n")),(0,r.kt)("p",null,"Now it is time to run a prediction with our model:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"# run prediction\npred = model.predict(x_test)\n\n## show prediction probabilities\nprint(pred[666])\n## print label with highest probability\npred_max = np.argmax(pred, axis=1)\nprint(pred_max[666])\n\n## show corresponding image\n## reshaping data 784 => 28x28\n## to be able to show the image\nx = x_test[666].reshape(28, 28)\nplt.imshow(x)\nplt.show()\n")),(0,r.kt)("p",null,"The image looks like a ",(0,r.kt)("inlineCode",{parentName:"p"},"7")," and the prediction also assigns ",(0,r.kt)("inlineCode",{parentName:"p"},"7")," by far the highest probability \ud83d\udc4d"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# prediction probabilities\n[8.6091750e-10 4.5242775e-08 6.5181263e-07 1.2005190e-06 1.4038225e-11\n 1.9814760e-10 3.8750800e-15 9.9999774e-01 4.4739803e-09 3.6081954e-07]\n# predicted label with highest probability\n7\n")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Keras for Tensorflow - Artificial Neural Networks",src:t(78262).Z,width:"1354",height:"445"})))}d.isMDXComponent=!0},34281:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/Keras_Introduction_MNIST_Model_Training_01-1685dd47eac76a7d0f4b5e4793abe6a5.png"},78262:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/Keras_Introduction_MNIST_Model_Training_02-d1e5e3375effe244b26c277508c63e6b.png"},60189:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-f80e63ee872dae25129198058ac93b4e.jpg"}}]);