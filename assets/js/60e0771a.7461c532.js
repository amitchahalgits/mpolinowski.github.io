"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[69886],{231542:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>g,frontMatter:()=>s,metadata:()=>o,toc:()=>r});var a=t(474848),i=t(28453);const s={sidebar_position:4080,slug:"2023-10-01",title:"DLIB Face Recognition",authors:"mpolinowski",tags:["Python","Machine Learning"],description:"Detect faces in images and compare their feature vector to known entities"},c=void 0,o={id:"IoT-and-Machine-Learning/ML/2023-10-01--delib-face-detection/index",title:"DLIB Face Recognition",description:"Detect faces in images and compare their feature vector to known entities",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-10-01--delib-face-detection/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-10-01--delib-face-detection",slug:"/IoT-and-Machine-Learning/ML/2023-10-01--delib-face-detection/2023-10-01",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-10-01--delib-face-detection/2023-10-01",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-10-01--delib-face-detection/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"}],version:"current",sidebarPosition:4080,frontMatter:{sidebar_position:4080,slug:"2023-10-01",title:"DLIB Face Recognition",authors:"mpolinowski",tags:["Python","Machine Learning"],description:"Detect faces in images and compare their feature vector to known entities"},sidebar:"tutorialSidebar",previous:{title:"Vector Databases for AI Applications",permalink:"/docs/IoT-and-Machine-Learning/ML/2024-03-22--introduction-to-vectordbs/2024-03-22"},next:{title:"Audio Classification with Computer Vision",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-09-23--yolo8-listen/2023-09-23"}},l={},r=[{value:"Detect Face Location (CPU)",id:"detect-face-location-cpu",level:2},{value:"Crop Location",id:"crop-location",level:3},{value:"Get all the Training Images",id:"get-all-the-training-images",level:3},{value:"Face Recognition",id:"face-recognition",level:2},{value:"Compare Faces",id:"compare-faces",level:3},{value:"Test Image 1",id:"test-image-1",level:4},{value:"Test Image 2",id:"test-image-2",level:4},{value:"Test Image 3",id:"test-image-3",level:4},{value:"Test Image 4",id:"test-image-4",level:4},{value:"Test Image 5",id:"test-image-5",level:4},{value:"Test Image 6",id:"test-image-6",level:4},{value:"Detect Face Location (GPU + Batch Processing)",id:"detect-face-location-gpu--batch-processing",level:2},{value:"Draw Bounding Boxes",id:"draw-bounding-boxes",level:2},{value:"Save Feature Vector",id:"save-feature-vector",level:2}];function d(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"TST, Hongkong",src:t(94908).A+"",width:"1500",height:"620"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"#detect-face-location-cpu",children:"Detect Face Location (CPU)"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#crop-location",children:"Crop Location"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#get-all-the-training-images",children:"Get all the Training Images"})}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"#face-recognition",children:"Face Recognition"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"#compare-faces",children:"Compare Faces"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#test-image-1",children:"Test Image 1"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#test-image-2",children:"Test Image 2"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#test-image-3",children:"Test Image 3"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#test-image-4",children:"Test Image 4"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#test-image-5",children:"Test Image 5"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#test-image-6",children:"Test Image 6"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#detect-face-location-gpu--batch-processing",children:"Detect Face Location (GPU + Batch Processing)"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#draw-bounding-boxes",children:"Draw Bounding Boxes"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#save-feature-vector",children:"Save Feature Vector"})}),"\n"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://github.com/mpolinowski/yolo-listen",children:"Github Repository"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://github.com/ageitgey/face_recognition",children:"Face Recognition"})}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Recognize and manipulate faces from Python or from the command line with the world's simplest face recognition library. Built using ",(0,a.jsx)(n.a,{href:"http://dlib.net/",children:"dlib"}),"'s state-of-the-art face recognition built with deep learning. The model has an accuracy of 99.38% on the ",(0,a.jsx)(n.a,{href:"http://vis-www.cs.umass.edu/lfw/",children:"Labeled Faces in the Wild"})," benchmark."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"!pip install face_recognition\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import face_recognition\nimport numpy as np\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'img_bobby ="faces/bobbie_w_draper.jpg"\nimg_jim ="faces/jim_holden.jpg"\nimg_amos ="faces/amos_burton.jpg"\nimg_camina ="faces/camina_drummer.jpg"\nimg_naomi ="faces/naomi_nagata.jpg"\nimg_chrisjen ="faces/chrisjen_avasarala.jpg"\n\nimage_path = img_bobby\n'})}),"\n",(0,a.jsx)(n.h2,{id:"detect-face-location-cpu",children:"Detect Face Location (CPU)"}),"\n",(0,a.jsx)(n.p,{children:"Re-run the following steps for all training images above:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"image = face_recognition.load_image_file(image_path)\nface_locations = face_recognition.face_locations(image)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"crop-location",children:"Crop Location"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import cv2 as cv\nimport matplotlib.pyplot as plt\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"img = cv.imread(image_path)\nimg = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"plt.imshow(img)\nplt.axis('off')\n\nfor face_location in face_locations:  \n    plt.plot(face_location[3], face_location[0], 'ro') \n    plt.plot(face_location[1], face_location[0], 'r+')     \n    plt.plot(face_location[3], face_location[2], 'bo')\n    plt.plot(face_location[1], face_location[2], 'b+')\n\nplt.show()\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(201338).A+"",width:"373",height:"389"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"for face_location in face_locations: \n    x1, y1 = face_location[3], face_location[2]\n    x2, y2 = face_location[1], face_location[2]\n    x3, y3 = face_location[1], face_location[0]\n    x4, y4 = face_location[3], face_location[0]\n\ntop_left_x = min([x1,x2,x3,x4])\ntop_left_y = min([y1,y2,y3,y4])\nbot_right_x = max([x1,x2,x3,x4])\nbot_right_y = max([y1,y2,y3,y4])\n\ncropped_image = img[top_left_y:bot_right_y, top_left_x:bot_right_x]\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"plt.imshow(cropped_image)\nplt.axis('off')\n"})}),"\n",(0,a.jsx)(n.p,{children:"(-0.5, 267.5, 266.5, -0.5)"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(519973).A+"",width:"390",height:"389"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"cv.imwrite('faces/cut/bobbie_w_draper.jpg', cv.cvtColor(cropped_image, cv.COLOR_RGB2BGR))\n"})}),"\n",(0,a.jsx)(n.p,{children:"True"}),"\n",(0,a.jsx)(n.h3,{id:"get-all-the-training-images",children:"Get all the Training Images"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"bobby_train = face_recognition.load_image_file(img_bobby)\njim_train = face_recognition.load_image_file(img_jim)\namos_train = face_recognition.load_image_file(img_amos)\ncamina_train = face_recognition.load_image_file(img_camina)\nnaomi_train = face_recognition.load_image_file(img_naomi)\nchrisjen_train = face_recognition.load_image_file(img_chrisjen)\n\nbobby_encoding = face_recognition.face_encodings(bobby_train)[0]\njim_encoding = face_recognition.face_encodings(jim_train)[0]\namos_encoding = face_recognition.face_encodings(amos_train)[0]\ncamina_encoding = face_recognition.face_encodings(camina_train)[0]\nnaomi_encoding = face_recognition.face_encodings(naomi_train)[0]\nchrisjen_encoding = face_recognition.face_encodings(chrisjen_train)[0]\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from glob import glob\n\ncropped_images = glob('./faces/cut/*.jpg')\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"plt.figure(figsize=(12, 8))\nplt.suptitle('Training Images')\n\nax = plt.subplot(2, 3, 1)\nimg_path = cropped_images[0]\nimg_title = 'face: ' + cropped_images[0][12:-4]\nplt.title(img_title, fontsize='medium')\nimage = plt.imread(img_path)\nplt.imshow(image, cmap=plt.cm.binary)\n\nax = plt.subplot(2, 3, 2)\nimg_path = cropped_images[1]\nimg_title = 'face: ' + cropped_images[1][12:-4]\nplt.title(img_title, fontsize='medium')\nimage = plt.imread(img_path)\nplt.imshow(image, cmap=plt.cm.binary)\n\nax = plt.subplot(2, 3, 3)\nimg_path = cropped_images[2]\nimg_title = 'face: ' + cropped_images[2][12:-4]\nplt.title(img_title, fontsize='medium')\nimage = plt.imread(img_path)\nplt.imshow(image, cmap=plt.cm.binary)\n\nax = plt.subplot(2, 3, 4)\nimg_path = cropped_images[3]\nimg_title = 'face: ' + cropped_images[3][12:-4]\nplt.title(img_title, fontsize='medium')\nimage = plt.imread(img_path)\nplt.imshow(image, cmap=plt.cm.binary)\n\nax = plt.subplot(2, 3, 5)\nimg_path = cropped_images[4]\nimg_title = 'face: ' + cropped_images[4][12:-4]\nplt.title(img_title, fontsize='medium')\nimage = plt.imread(img_path)\nplt.imshow(image, cmap=plt.cm.binary)\n\nax = plt.subplot(2, 3, 6)\nimg_path = cropped_images[5]\nimg_title = 'face: ' + cropped_images[5][12:-4]\nplt.title(img_title, fontsize='medium')\nimage = plt.imread(img_path)\nplt.imshow(image, cmap=plt.cm.binary)\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(315400).A+"",width:"986",height:"737"})}),"\n",(0,a.jsx)(n.h2,{id:"face-recognition",children:"Face Recognition"}),"\n",(0,a.jsx)(n.p,{children:'Loading a bunch of test images with "unknown" faces:'}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'test_image1 = face_recognition.load_image_file("faces/test/unknown_01.jpg")\ntest_image2 = face_recognition.load_image_file("faces/test/unknown_02.jpg")\ntest_image3 = face_recognition.load_image_file("faces/test/unknown_03.jpg")\ntest_image4 = face_recognition.load_image_file("faces/test/unknown_04.jpg")\ntest_image5 = face_recognition.load_image_file("faces/test/unknown_05.jpg")\ntest_image6 = face_recognition.load_image_file("faces/test/unknown_06.jpg")\n\ntest1_encoding = face_recognition.face_encodings(test_image1)\ntest2_encoding = face_recognition.face_encodings(test_image2)\ntest3_encoding = face_recognition.face_encodings(test_image3)\ntest4_encoding = face_recognition.face_encodings(test_image4)\ntest5_encoding = face_recognition.face_encodings(test_image5)\ntest6_encoding = face_recognition.face_encodings(test_image6)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"compare-faces",children:"Compare Faces"}),"\n",(0,a.jsx)(n.p,{children:"Compare all detected images in the test dataset to the training images:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'trained_images = [bobby_encoding, jim_encoding, amos_encoding, camina_encoding, naomi_encoding, chrisjen_encoding]\ntrained_faces = np.array(["bobbie_w_draper", "jim_holden", "amos_burton", "camina_drummer", "naomi_nagata", "chrisjen_avasarala"])\n'})}),"\n",(0,a.jsx)(n.h4,{id:"test-image-1",children:"Test Image 1"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"test1_results = []\n\nfor detection in test1_encoding:\n    result = face_recognition.compare_faces(trained_images, detection)\n    test1_results.append(trained_faces[result])\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"test_img1 = plt.imread('faces/test/unknown_01.jpg')\nplt.title('detected faces: \\n' + str(test1_results), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img1)\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(617854).A+"",width:"515",height:"321"})}),"\n",(0,a.jsx)(n.h4,{id:"test-image-2",children:"Test Image 2"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"test2_results = []\n\nfor detection in test2_encoding:\n    result = face_recognition.compare_faces(trained_images, detection)\n    test1_results.append(trained_faces[result])\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"test_img2 = plt.imread('faces/test/unknown_02.jpg')\nplt.title('detected faces: \\n' + str(test2_results), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img2)\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(6529).A+"",width:"515",height:"332"})}),"\n",(0,a.jsx)(n.h4,{id:"test-image-3",children:"Test Image 3"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"test3_results = []\n\nfor detection in test3_encoding:\n    result = face_recognition.compare_faces(trained_images, detection)\n    test3_results.append(trained_faces[result])\n\ntest3_results\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"test_img3 = plt.imread('faces/test/unknown_03.jpg')\nplt.title('detected faces: \\n' + str(test3_results), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img3)\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(341881).A+"",width:"512",height:"422"})}),"\n",(0,a.jsx)(n.h4,{id:"test-image-4",children:"Test Image 4"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"test4_results = []\n\nfor detection in test4_encoding:\n    result = face_recognition.compare_faces(trained_images, detection)\n    test4_results.append(trained_faces[result])\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"test_img4 = plt.imread('faces/test/unknown_04.jpg')\nplt.title('detected faces: \\n' + str(test4_results), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img4)\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(523158).A+"",width:"702",height:"306"})}),"\n",(0,a.jsx)(n.h4,{id:"test-image-5",children:"Test Image 5"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"test5_results = []\n\nfor detection in test5_encoding:\n    result = face_recognition.compare_faces(trained_images, detection)\n    test5_results.append(trained_faces[result])\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"test_img5 = plt.imread('faces/test/unknown_05.jpg')\nplt.title('detected faces: \\n' + str(test5_results), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img5)\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(563811).A+"",width:"664",height:"332"})}),"\n",(0,a.jsx)(n.h4,{id:"test-image-6",children:"Test Image 6"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"test6_results = []\n\nfor detection in test6_encoding:\n    result = face_recognition.compare_faces(trained_images, detection)\n    test6_results.append(trained_faces[result])\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"test_img6 = plt.imread('faces/test/unknown_06.jpg')\nplt.title('detected faces: \\n' + str(test6_results), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img6)\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(63424).A+"",width:"515",height:"384"})}),"\n",(0,a.jsx)(n.h2,{id:"detect-face-location-gpu--batch-processing",children:"Detect Face Location (GPU + Batch Processing)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"all_training_images = glob('./faces/*.jpg')\nlen(all_training_images)\n"})}),"\n",(0,a.jsx)(n.p,{children:"60"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"ran_gen = np.random.default_rng()\n\nplt.figure(figsize=(14, 12))\nplt.suptitle('Training Images')\n\nfor i in range(16):\n    ax = plt.subplot(4, 4, i+1)\n    random_index = ran_gen.integers(low=0, high=59, size=1)\n    i = random_index[0]\n    img_loc = all_training_images[i]\n    img_title = 'label: ' + all_training_images[i][8:-4]\n    image = plt.imread(img_loc)\n    plt.imshow(image)\n    plt.title(img_title, fontsize='small')\n    plt.axis(False)\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(67655).A+"",width:"1048",height:"1064"})}),"\n",(0,a.jsx)(n.p,{children:"For this experiment I collected 10 images from all faces that I used before. All training images only contain one face - so I expect only getting one location that I can map to the image label:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'image_labels = []\nface_locations = []\n\nfor image_path in all_training_images:\n    image_labels.append(image_path[8:-5])\n    image = face_recognition.load_image_file(image_path)\n    location = face_recognition.face_locations(image, model="cnn")\n    face_locations.append(location)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"print(len(image_labels), len(face_locations))\n"})}),"\n",(0,a.jsx)(n.p,{children:"60 60"}),"\n",(0,a.jsx)(n.p,{children:"Now I can get the feature vector for every detected face by it's bounding box:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"face_encodings = []\ni = 0\n\nfor location in face_locations:\n    image = face_recognition.load_image_file(all_training_images[i])\n    encoding = face_recognition.face_encodings(image, location)[0]\n\n    face_encodings.append(encoding)\n    i+=1\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"len(face_encodings)\n"})}),"\n",(0,a.jsx)(n.p,{children:"60"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"testcnn1_results = []\n\nfor detection in test1_encoding:\n    result = face_recognition.compare_faces(face_encodings, detection)\n    testcnn1_results.append(np.array(image_labels)[result])\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"classes = []\n\ntest_img1 = plt.imread('faces/test/unknown_01.jpg')\n\nfor result in testcnn1_results:\n    label, count = np.unique(result, return_counts=True)\n    classes.append(\n        (\n            # check for unlabeled faces\n            count[0] if 0 < len(count) else None, \n            label[0] if 0 < len(label) else None\n        )\n    )\n    \nplt.title(str(classes), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img1)\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(551999).A+"",width:"515",height:"317"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"testcnn2_results = []\n\nfor detection in test2_encoding:\n    result = face_recognition.compare_faces(face_encodings, detection)\n    testcnn2_results.append(np.array(image_labels)[result])\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"classes = []\n\ntest_img2 = plt.imread('faces/test/unknown_02.jpg')\n\nfor result in testcnn2_results:\n    label, count = np.unique(result, return_counts=True)\n    classes.append(\n        (\n            # check for unlabeled faces\n            count[0] if 0 < len(count) else None, \n            label[0] if 0 < len(label) else None\n        )\n    )\n    \nplt.title(str(classes), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img2)\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(702001).A+"",width:"515",height:"317"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"testcnn3_results = []\n\nfor detection in test3_encoding:\n    result = face_recognition.compare_faces(face_encodings, detection)\n    testcnn3_results.append(np.array(image_labels)[result])\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"classes = []\ntest_img3 = plt.imread('faces/test/unknown_03.jpg')\n\nfor result in testcnn3_results:\n    label, count = np.unique(result, return_counts=True)\n    classes.append(\n        (\n            # check for unlabeled faces\n            count[0] if 0 < len(count) else None, \n            label[0]\n if 0 < len(label) else None\n        )\n    )\n    \nplt.title(str(classes), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img3)\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(643931).A+"",width:"512",height:"407"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"testcnn4_results = []\n\nfor detection in test4_encoding:\n    result = face_recognition.compare_faces(face_encodings, detection)\n    testcnn4_results.append(np.array(image_labels)[result])\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"classes = []\ntest_img4 = plt.imread('faces/test/unknown_04.jpg')\n\nfor result in testcnn4_results:\n    label, count = np.unique(result, return_counts=True)\n    classes.append(\n        (\n            # check for unlabeled faces\n            count[0] if 0 < len(count) else None, \n            label[0] if 0 < len(label) else None\n        )\n    )\n    \nplt.title(str(classes), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img4)\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(734621).A+"",width:"515",height:"291"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"testcnn5_results = []\n\nfor detection in test5_encoding:\n    result = face_recognition.compare_faces(face_encodings, detection)\n    testcnn5_results.append(np.array(image_labels)[result])\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"classes = []\ntest_img5 = plt.imread('faces/test/unknown_05.jpg')\n\nfor result in testcnn5_results:\n    label, count = np.unique(result, return_counts=True)\n    classes.append(\n        (\n            # check for unlabeled faces\n            count[0] if 0 < len(count) else None, \n            label[0] if 0 < len(label) else None\n        )\n    )\n    \nplt.title(str(classes), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img5)\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(64807).A+"",width:"515",height:"317"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"testcnn6_results = []\n\nfor detection in test6_encoding:\n    result = face_recognition.compare_faces(face_encodings, detection)\n    testcnn6_results.append(np.array(image_labels)[result])\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"classes = []\ntest_img6 = plt.imread('faces/test/unknown_06.jpg')\n\nfor result in testcnn6_results:\n    label, count = np.unique(result, return_counts=True)\n    classes.append(\n        (\n            # check for unlabeled faces\n            count[0] if 0 < len(count) else None, \n            label[0] if 0 < len(label) else None\n        )\n    )\n    \nplt.title(str(classes), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img6)\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(577526).A+"",width:"515",height:"369"})}),"\n",(0,a.jsx)(n.h2,{id:"draw-bounding-boxes",children:"Draw Bounding Boxes"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from PIL import Image, ImageDraw\n"})}),"\n",(0,a.jsx)(n.p,{children:"I'm using the encodings/labels for the 60 training images generated above:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"print(len(face_encodings), len(image_labels))\n"})}),"\n",(0,a.jsx)(n.p,{children:"60 60"}),"\n",(0,a.jsx)(n.p,{children:"Load a test image that contains one of the faces above:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"unknown_image = face_recognition.load_image_file('faces/test/unknown_01.jpg')\n\n# Find all the faces and face encodings in the unknown image\nfound_faces = face_recognition.face_locations(unknown_image)\nfound_face_encodings = face_recognition.face_encodings(unknown_image, found_faces)\n\n# Convert the image to a PIL-format image so that we can draw on top of it with the Pillow library\npil_image = Image.fromarray(unknown_image)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Create a Pillow ImageDraw Draw instance to draw with\ndraw = ImageDraw.Draw(pil_image)\n\n# Loop through each face found in the unknown image\nfor (top, right, bottom, left), face_encoding in zip(found_faces, found_face_encodings):\n    # See if the face is a match for the known face(s)\n    matches = face_recognition.compare_faces(face_encodings, face_encoding)\n\n    name = "Unknown"\n\n    # If a match was found in face_encodings, just use the first one.\n    # if True in matches:\n    #     first_match_index = matches.index(True)\n    #     name = image_labels[first_match_index]\n\n    # Or instead, use the known face with the smallest distance to the new face\n    face_distances = face_recognition.face_distance(face_encodings, face_encoding)\n    best_match_index = np.argmin(face_distances)\n    if matches[best_match_index]:\n        name = image_labels[best_match_index]\n    # Draw a box around the face using the Pillow module\n    draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n    # Draw a label with a name below the face\n    text_width, text_height = draw.textsize(name)\n    draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n    draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n\n# Remove the drawing library from memory as per the Pillow docs\ndel draw\n\n# Display the resulting image\npil_image.show()\n'})}),"\n",(0,a.jsx)(n.p,{children:"/tmp/ipykernel_11211/4178765491.py:25: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\ntext_width, text_height = draw.textsize(name)\n/tmp/ipykernel_11211/4178765491.py:25: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\ntext_width, text_height = draw.textsize(name)"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(87909).A+"",width:"1280",height:"720"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"unknown_image2 = face_recognition.load_image_file('faces/test/unknown_02.jpg')\n\n# Find all the faces and face encodings in the unknown image\nfound_faces2 = face_recognition.face_locations(unknown_image2)\nfound_face_encodings2 = face_recognition.face_encodings(unknown_image2, found_faces2)\n\n# Convert the image to a PIL-format image so that we can draw on top of it with the Pillow library\npil_image2 = Image.fromarray(unknown_image2)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Create a Pillow ImageDraw Draw instance to draw with\ndraw = ImageDraw.Draw(pil_image2)\n\n# Loop through each face found in the unknown image\nfor (top, right, bottom, left), face_encoding in zip(found_faces2, found_face_encodings2):\n    # See if the face is a match for the known face(s)\n    matches = face_recognition.compare_faces(face_encodings, face_encoding)\n\n    name = "Unknown"\n\n    # If a match was found in face_encodings, just use the first one.\n    # if True in matches:\n    #     first_match_index = matches.index(True)\n    #     name = image_labels[first_match_index]\n\n    # Or instead, use the known face with the smallest distance to the new face\n    face_distances = face_recognition.face_distance(face_encodings, face_encoding)\n    best_match_index = np.argmin(face_distances)\n    if matches[best_match_index]:\n        name = image_labels[best_match_index]\n\n    # Draw a box around the face using the Pillow module\n    draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n    # Draw a label with a name below the face\n    text_width, text_height = draw.textsize(name)\n    draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n    draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n\n# Remove the drawing library from memory as per the Pillow docs\ndel draw\n\n# Display the resulting image\npil_image2.show()\n'})}),"\n",(0,a.jsx)(n.p,{children:"/tmp/ipykernel_11211/1125746434.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\ntext_width, text_height = draw.textsize(name)\n/tmp/ipykernel_11211/1125746434.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\ntext_width, text_height = draw.textsize(name)"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(431551).A+"",width:"780",height:"439"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"unknown_image3 = face_recognition.load_image_file('faces/test/unknown_03.jpg')\n\n# Find all the faces and face encodings in the unknown image\nfound_faces3 = face_recognition.face_locations(unknown_image3)\nfound_face_encodings3 = face_recognition.face_encodings(unknown_image3, found_faces3)\n\n# Convert the image to a PIL-format image so that we can draw on top of it with the Pillow library\npil_image3 = Image.fromarray(unknown_image3)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Create a Pillow ImageDraw Draw instance to draw with\ndraw = ImageDraw.Draw(pil_image3)\n\n# Loop through each face found in the unknown image\nfor (top, right, bottom, left), face_encoding in zip(found_faces3, found_face_encodings3):\n    # See if the face is a match for the known face(s)\n    matches = face_recognition.compare_faces(face_encodings, face_encoding)\n\n    name = "Unknown"\n\n    # If a match was found in face_encodings, just use the first one.\n    # if True in matches:\n    #     first_match_index = matches.index(True)\n    #     name = image_labels[first_match_index]\n\n    # Or instead, use the known face with the smallest distance to the new face\n    face_distances = face_recognition.face_distance(face_encodings, face_encoding)\n    best_match_index = np.argmin(face_distances)\n    if matches[best_match_index]:\n        name = image_labels[best_match_index]\n\n    # Draw a box around the face using the Pillow module\n    draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n    # Draw a label with a name below the face\n    text_width, text_height = draw.textsize(name)\n    draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n    draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n\n# Remove the drawing library from memory as per the Pillow docs\ndel draw\n\n# Display the resulting image\npil_image3.show()\n'})}),"\n",(0,a.jsx)(n.p,{children:"/tmp/ipykernel_11211/3035399979.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\ntext_width, text_height = draw.textsize(name)\n/tmp/ipykernel_11211/3035399979.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\ntext_width, text_height = draw.textsize(name)"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(316804).A+"",width:"2400",height:"1800"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"unknown_image4 = face_recognition.load_image_file('faces/test/unknown_04.jpg')\n\n# Find all the faces and face encodings in the unknown image\nfound_faces4 = face_recognition.face_locations(unknown_image4)\nfound_face_encodings4 = face_recognition.face_encodings(unknown_image4, found_faces4)\n\n# Convert the image to a PIL-format image so that we can draw on top of it with the Pillow library\npil_image4 = Image.fromarray(unknown_image4)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Create a Pillow ImageDraw Draw instance to draw with\ndraw = ImageDraw.Draw(pil_image4)\n\n# Loop through each face found in the unknown image\nfor (top, right, bottom, left), face_encoding in zip(found_faces4, found_face_encodings4):\n    # See if the face is a match for the known face(s)\n    matches = face_recognition.compare_faces(face_encodings, face_encoding)\n\n    name = "Unknown"\n\n    # If a match was found in face_encodings, just use the first one.\n    # if True in matches:\n    #     first_match_index = matches.index(True)\n    #     name = image_labels[first_match_index]\n\n    # Or instead, use the known face with the smallest distance to the new face\n    face_distances = face_recognition.face_distance(face_encodings, face_encoding)\n    best_match_index = np.argmin(face_distances)\n    if matches[best_match_index]:\n        name = image_labels[best_match_index]\n\n    # Draw a box around the face using the Pillow module\n    draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n    # Draw a label with a name below the face\n    text_width, text_height = draw.textsize(name)\n    draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n    draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n\n# Remove the drawing library from memory as per the Pillow docs\ndel draw\n\n# Display the resulting image\npil_image4.show()\n'})}),"\n",(0,a.jsx)(n.p,{children:"/tmp/ipykernel_11211/329699440.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\ntext_width, text_height = draw.textsize(name)\n/tmp/ipykernel_11211/329699440.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\ntext_width, text_height = draw.textsize(name)\n/tmp/ipykernel_11211/329699440.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\ntext_width, text_height = draw.textsize(name)"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(657666).A+"",width:"3328",height:"1698"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"unknown_image5 = face_recognition.load_image_file('faces/test/unknown_05.jpg')\n\n# Find all the faces and face encodings in the unknown image\nfound_faces5 = face_recognition.face_locations(unknown_image5)\nfound_face_encodings5 = face_recognition.face_encodings(unknown_image5, found_faces5)\n\n# Convert the image to a PIL-format image so that we can draw on top of it with the Pillow library\npil_image5 = Image.fromarray(unknown_image5)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Create a Pillow ImageDraw Draw instance to draw with\ndraw = ImageDraw.Draw(pil_image5)\n\n# Loop through each face found in the unknown image\nfor (top, right, bottom, left), face_encoding in zip(found_faces5, found_face_encodings5):\n    # See if the face is a match for the known face(s)\n    matches = face_recognition.compare_faces(face_encodings, face_encoding)\n\n    name = "Unknown"\n\n    # If a match was found in face_encodings, just use the first one.\n    # if True in matches:\n    #     first_match_index = matches.index(True)\n    #     name = image_labels[first_match_index]\n\n    # Or instead, use the known face with the smallest distance to the new face\n    face_distances = face_recognition.face_distance(face_encodings, face_encoding)\n    best_match_index = np.argmin(face_distances)\n    if matches[best_match_index]:\n        name = image_labels[best_match_index]\n\n    # Draw a box around the face using the Pillow module\n    draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n    # Draw a label with a name below the face\n    text_width, text_height = draw.textsize(name)\n    draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n    draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n\n# Remove the drawing library from memory as per the Pillow docs\ndel draw\n\n# Display the resulting image\npil_image5.show()\n'})}),"\n",(0,a.jsx)(n.p,{children:"/tmp/ipykernel_11211/864981556.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\ntext_width, text_height = draw.textsize(name)\n/tmp/ipykernel_11211/864981556.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\ntext_width, text_height = draw.textsize(name)\n/tmp/ipykernel_11211/864981556.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\ntext_width, text_height = draw.textsize(name)"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(315800).A+"",width:"1920",height:"1080"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"unknown_image6 = face_recognition.load_image_file('faces/test/unknown_06.jpg')\n\n# Find all the faces and face encodings in the unknown image\nfound_faces6 = face_recognition.face_locations(unknown_image6)\nfound_face_encodings6 = face_recognition.face_encodings(unknown_image6, found_faces6)\n\n# Convert the image to a PIL-format image so that we can draw on top of it with the Pillow library\npil_image6 = Image.fromarray(unknown_image6)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Create a Pillow ImageDraw Draw instance to draw with\ndraw = ImageDraw.Draw(pil_image6)\n\n# Loop through each face found in the unknown image\nfor (top, right, bottom, left), face_encoding in zip(found_faces6, found_face_encodings6):\n    # See if the face is a match for the known face(s)\n    matches = face_recognition.compare_faces(face_encodings, face_encoding)\n\n    name = "Unknown"\n\n    # If a match was found in face_encodings, just use the first one.\n    # if True in matches:\n    #     first_match_index = matches.index(True)\n    #     name = image_labels[first_match_index]\n\n    # Or instead, use the known face with the smallest distance to the new face\n    face_distances = face_recognition.face_distance(face_encodings, face_encoding)\n    best_match_index = np.argmin(face_distances)\n    if matches[best_match_index]:\n        name = image_labels[best_match_index]\n\n    # Draw a box around the face using the Pillow module\n    draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n    # Draw a label with a name below the face\n    text_width, text_height = draw.textsize(name)\n    draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n    draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n\n# Remove the drawing library from memory as per the Pillow docs\ndel draw\n\n# Display the resulting image\npil_image6.show()\n'})}),"\n",(0,a.jsx)(n.p,{children:"/tmp/ipykernel_11211/1615432601.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\ntext_width, text_height = draw.textsize(name)\n/tmp/ipykernel_11211/1615432601.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\ntext_width, text_height = draw.textsize(name)"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"DLIB Face Recognition",src:t(813126).A+"",width:"1500",height:"1002"})}),"\n",(0,a.jsx)(n.h2,{id:"save-feature-vector",children:"Save Feature Vector"}),"\n",(0,a.jsxs)(n.p,{children:["Export encodings and labels to use them in a Flask App -> see ",(0,a.jsx)(n.code,{children:"./main.py"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"with open('features/face_encodings.npy', 'wb') as f:\n    np.save(f, face_encodings)\n\nwith open('features/image_labels.npy', 'wb') as f:\n    np.save(f, image_labels)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"with open('features/face_encodings.npy', 'rb') as f:\n    feature_vectors = np.load(f)\n\nwith open('features/image_labels.npy', 'rb') as f:\n    feature_labels = np.load(f)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"np.unique(feature_labels, return_counts=True)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"    (array(amos_burton, bobbie_w_draper, camina_drummer,\n            chrisjen_avasarala, jim_holden, naomi_nagata, dtype=<U18),\n     array(10, 10, 10, 10, 10, 10))\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"feature_vectors.shape\n"})}),"\n",(0,a.jsx)(n.p,{children:"(60, 128)"})]})}function g(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},201338:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_10_0-3fc99340460a5394b27cc4f3f5c9d7fd.png"},519973:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_12_1-34ededf10548acc3e3dac3d0586e6145.png"},315400:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_17_1-4418223b167876be1fa8babce9252dc7.png"},617854:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_24_1-f63226da303d670adfec0d4b4026bacc.png"},6529:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_27_1-a0008dd355ec2171e6e929f44c33f48e.png"},341881:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_30_1-351af2c686b85232dc0209d055fe7c2b.png"},523158:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_33_1-d8ba9c2f02a6b87fab02cf050cda3194.png"},563811:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_36_1-8138e3bdc663ad0b2245d5a557597a43.png"},63424:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_39_1-194c400485fe4b708793d0be5d119a6a.png"},67655:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_42_0-0f8929362920e806619fe16c01b757ed.png"},551999:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_50_1-73608f4bd670f7c287a2dbbe52e8e996.png"},702001:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_52_1-e3f4ebb76c4e94450785d19ebc6cb6f1.png"},643931:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_54_1-5574a5145b291dbfda722e0e69be9a1b.png"},734621:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_56_1-0820cc189b3250cc40e7b6a8b70733ab.png"},64807:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_58_1-e6bd78a118a0ea887a49ee266a3ebd7a.png"},577526:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_60_1-e1999a249a86692234578015252719de.png"},87909:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_67_1-4c30cad6378124c03a4f69e1b4cba729.png"},431551:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_69_1-ddebae2f57dfecd2dd614c48db18fae5.png"},316804:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_71_1-4074a578690c43746bfa44c511d13b1c.png"},657666:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_73_1-6e49aa6a5523c16699e27bb1ffab0384.png"},315800:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_75_1-2de8685de457822eb43a1158aa3a8e82.png"},813126:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/output_77_1-3809e9b204faa60015065cced483eac6.png"},94908:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-252551beac0b36b4ba53ccd380897f8e.jpg"},28453:(e,n,t)=>{t.d(n,{R:()=>c,x:()=>o});var a=t(296540);const i={},s=a.createContext(i);function c(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:c(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);