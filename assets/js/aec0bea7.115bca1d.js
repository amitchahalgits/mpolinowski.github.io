"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[63773],{796049:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var r=i(785893),t=i(603905);const o={sidebar_position:4820,slug:"2023-01-08",title:"MiDaS Depth Vision",authors:"mpolinowski",tags:["Python","Machine Learning","Torch"],description:"MiDaS computes relative inverse depth from a single image."},a=void 0,s={id:"IoT-and-Machine-Learning/ML/2023-01-08-depth-vision-midas/index",title:"MiDaS Depth Vision",description:"MiDaS computes relative inverse depth from a single image.",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-01-08-depth-vision-midas/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-01-08-depth-vision-midas",slug:"/IoT-and-Machine-Learning/ML/2023-01-08-depth-vision-midas/2023-01-08",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-01-08-depth-vision-midas/2023-01-08",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-01-08-depth-vision-midas/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Torch",permalink:"/docs/tags/torch"}],version:"current",sidebarPosition:4820,frontMatter:{sidebar_position:4820,slug:"2023-01-08",title:"MiDaS Depth Vision",authors:"mpolinowski",tags:["Python","Machine Learning","Torch"],description:"MiDaS computes relative inverse depth from a single image."},sidebar:"tutorialSidebar",previous:{title:"YOLOv7 Training with Custom Data",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-01-10-yolov7_custom_data/2023-01-10"},next:{title:"YOLOv7 Introduction",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-01-05-yolov7/2023-01-05"}},c={},d=[{value:"Working with Image Files",id:"working-with-image-files",level:2},{value:"Choosing the Right Model",id:"choosing-the-right-model",level:3},{value:"GPU or CPU",id:"gpu-or-cpu",level:3},{value:"Transformations",id:"transformations",level:3},{value:"Prediction",id:"prediction",level:3},{value:"Working with Video Streams",id:"working-with-video-streams",level:2}];function l(e){const n={a:"a",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.ah)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Guangzhou, China",src:i(625139).Z+"",width:"1500",height:"652"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"#working-with-image-files",children:"Working with Image Files"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#choosing-the-right-model",children:"Choosing the Right Model"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#gpu-or-cpu",children:"GPU or CPU"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#transformations",children:"Transformations"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#prediction",children:"Prediction"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#working-with-video-streams",children:"Working with Video Streams"})}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["MiDaS - see ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1907.01341",children:"Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer"})," by Ren\xe9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, Vladlen Koltun - computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://github.com/mpolinowski/torch-depth",children:"Github Repository"})}),"\n",(0,r.jsxs)(n.p,{children:["MiDaS depends on ",(0,r.jsx)(n.a,{href:"https://huggingface.co/docs/timm/index",children:"timm"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install timm\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:"import cv2\nimport os\nimport torch\nimport urllib.request\n\nimport matplotlib.pyplot as plt\n"})}),"\n",(0,r.jsx)(n.h2,{id:"working-with-image-files",children:"Working with Image Files"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:'url, filename = ("https://cdn.wallpapersafari.com/45/74/Ye9R0H.jpg", "bridge.jpg")\nurllib.request.urlretrieve(url, filename)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"choosing-the-right-model",children:"Choosing the Right Model"}),"\n",(0,r.jsxs)(n.p,{children:["Depending on your Hardware you can choose one of three models with different ",(0,r.jsx)(n.a,{href:"https://github.com/isl-org/MiDaS#Accuracy",children:"accuracy and speed"}),":"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"MiDaS Depth Vision",src:i(160739).Z+"",width:"832",height:"698"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:'# Load a model\n#model_type = "DPT_Large"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)\n#model_type = "DPT_Hybrid"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)\nmodel_type = "MiDaS_small"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)\n\nmidas = torch.hub.load("intel-isl/MiDaS", model_type)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"gpu-or-cpu",children:"GPU or CPU"}),"\n",(0,r.jsx)(n.p,{children:"You can run the prediction either on CUDA/Nvidia or CPU:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:'# Use GPU if available\ndevice = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")\nmidas.to(device)\nmidas.eval()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"transformations",children:"Transformations"}),"\n",(0,r.jsxs)(n.p,{children:["Inputs need to be transformed to match the dataset the model was trained with. These are available on ",(0,r.jsx)(n.strong,{children:"Torch Hub"})," and can be downloaded matching the model you choose earlier:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:'# Use transforms to resize and normalize the image\nmidas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")\n\nif model_type == "DPT_Large" or model_type == "DPT_Hybrid":\n    transform = midas_transforms.dpt_transform\nelse:\n    transform = midas_transforms.small_transform\n'})}),"\n",(0,r.jsx)(n.p,{children:"Once downloaded you can apply them to your input image:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:"# Apply transforms\nimg = cv2.imread(filename)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\ninput_batch = transform(img).to(device)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"prediction",children:"Prediction"}),"\n",(0,r.jsx)(n.p,{children:"Now we can run the prediction:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:'# Predict and resize to original resolution\nwith torch.no_grad():\n    prediction = midas(input_batch)\n\n    prediction = torch.nn.functional.interpolate(\n        prediction.unsqueeze(1),\n        size=img.shape[:2],\n        mode="bicubic",\n        align_corners=False,\n    ).squeeze()\n\noutput = prediction.cpu().numpy()\n'})}),"\n",(0,r.jsx)(n.p,{children:"And show the result depth map:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:"# Show depth map\nplt.imshow(output)\nplt.show()\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"MiDaS Depth Vision",src:i(188389).Z+"",width:"1275",height:"473"})}),"\n",(0,r.jsx)(n.h2,{id:"working-with-video-streams",children:"Working with Video Streams"}),"\n",(0,r.jsxs)(n.p,{children:["Now that we know that the model is working I now want to see if I can feed it the ",(0,r.jsx)(n.a,{href:"https://github.com/mpolinowski/opencv-rtsp",children:"RTSP Stream of my INSTAR IP Camera"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:"# Get the video stream\nRTSP_URL = 'rtsp://admin:instar@192.168.2.120/livestream/12'\n\nos.environ['OPENCV_FFMPEG_CAPTURE_OPTIONS'] = 'rtsp_transport;udp'\n\ncap = cv2.VideoCapture(RTSP_URL, cv2.CAP_FFMPEG)\n\nif not cap.isOpened():\n    print('Cannot open RTSP stream')\n    exit(-1)\n\nwhile True:\n    success, img = cap.read()\n    cv2.imshow('RTSP stream', img)\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):  # Keep running until you press `q`\n        break\n"})}),"\n",(0,r.jsx)(n.p,{children:"This code will output the original video from our camera. So we now need to add the prediction code into the while loop like so:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:"while True:\n    success, frame = cap.read()\n\n    # Apply transforms\n    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    input_batch = transform(img).to(device)\n\n    # Predict and resize to original resolution\n    with torch.no_grad():\n        prediction = midas(input_batch)\n\n        prediction = torch.nn.functional.interpolate(\n            prediction.unsqueeze(1),\n            size=img.shape[:2],\n            mode=\"bicubic\",\n            align_corners=False,\n        ).squeeze()\n\n    output = prediction.cpu().numpy()\n    \n    plt.imshow(output)\n    plt.pause(0.00001)\n    \n    cv2.imshow('RTSP stream', img)\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):  # Keep running until you press `q`\n        break\n\nplt.show()\n"})}),"\n",(0,r.jsx)(n.p,{children:"This now outputs the RTSP source through OpenCV and the corresponding prediction with Matplotlib:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"MiDaS Depth Vision",src:i(173317).Z+"",width:"1271",height:"432"})})]})}function h(e={}){const{wrapper:n}={...(0,t.ah)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},603905:(e,n,i)=>{i.d(n,{ah:()=>d});var r=i(667294);function t(e,n,i){return n in e?Object.defineProperty(e,n,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[n]=i,e}function o(e,n){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),i.push.apply(i,r)}return i}function a(e){for(var n=1;n<arguments.length;n++){var i=null!=arguments[n]?arguments[n]:{};n%2?o(Object(i),!0).forEach((function(n){t(e,n,i[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):o(Object(i)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(i,n))}))}return e}function s(e,n){if(null==e)return{};var i,r,t=function(e,n){if(null==e)return{};var i,r,t={},o=Object.keys(e);for(r=0;r<o.length;r++)i=o[r],n.indexOf(i)>=0||(t[i]=e[i]);return t}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)i=o[r],n.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(t[i]=e[i])}return t}var c=r.createContext({}),d=function(e){var n=r.useContext(c),i=n;return e&&(i="function"==typeof e?e(n):a(a({},n),e)),i},l={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},h=r.forwardRef((function(e,n){var i=e.components,t=e.mdxType,o=e.originalType,c=e.parentName,h=s(e,["components","mdxType","originalType","parentName"]),p=d(i),m=t,u=p["".concat(c,".").concat(m)]||p[m]||l[m]||o;return i?r.createElement(u,a(a({ref:n},h),{},{components:i})):r.createElement(u,a({ref:n},h))}));h.displayName="MDXCreateElement"},160739:(e,n,i)=>{i.d(n,{Z:()=>r});const r=i.p+"assets/images/Torch_Depth_Vision_01-a5276b5d2a1b2a212599ceb94882c296.png"},188389:(e,n,i)=>{i.d(n,{Z:()=>r});const r=i.p+"assets/images/Torch_Depth_Vision_02-f9d6b125f9fd2d03ddf56c64c6fd6714.png"},173317:(e,n,i)=>{i.d(n,{Z:()=>r});const r=i.p+"assets/images/Torch_Depth_Vision_03-ee193cea9d2c45ca0c7dcaffeeb8c90d.png"},625139:(e,n,i)=>{i.d(n,{Z:()=>r});const r=i.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-ba3b23aa3d5392c02b451d1b2b911721.jpg"}}]);