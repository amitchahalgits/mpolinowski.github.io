"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[75331],{441992:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>c});var a=t(474848),i=t(28453);const s={sidebar_position:4127,slug:"2023-08-31",title:"Detectron Object Detection with OpenImages Dataset (WIP)",authors:"mpolinowski",tags:["Python","Machine Learning","PyTorch"],description:"Detectron2 is a platform for object detection, segmentation and other visual recognition tasks."},o="Detectron Object Detection with OpenImages Dataset (WIP)",r={id:"IoT-and-Machine-Learning/ML/2023-08-31--semantic_segmentation_detectron2_openimages_dataset/index",title:"Detectron Object Detection with OpenImages Dataset (WIP)",description:"Detectron2 is a platform for object detection, segmentation and other visual recognition tasks.",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-08-31--semantic_segmentation_detectron2_openimages_dataset/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-08-31--semantic_segmentation_detectron2_openimages_dataset",slug:"/IoT-and-Machine-Learning/ML/2023-08-31--semantic_segmentation_detectron2_openimages_dataset/2023-08-31",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-08-31--semantic_segmentation_detectron2_openimages_dataset/2023-08-31",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-08-31--semantic_segmentation_detectron2_openimages_dataset/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"PyTorch",permalink:"/docs/tags/py-torch"}],version:"current",sidebarPosition:4127,frontMatter:{sidebar_position:4127,slug:"2023-08-31",title:"Detectron Object Detection with OpenImages Dataset (WIP)",authors:"mpolinowski",tags:["Python","Machine Learning","PyTorch"],description:"Detectron2 is a platform for object detection, segmentation and other visual recognition tasks."},sidebar:"tutorialSidebar",previous:{title:"YOLOv8 Image Classifier",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-09-01--yolo-i-know-flowers/2023-09-01"},next:{title:"Instance Segmentation with PyTorch (Mask RCNN)",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-08-30--instance_segmentation_detectron2_model_zoo_mask_rcnn/2023-08-30"}},l={},c=[{value:"Dataset",id:"dataset",level:2},{value:"Curate Dataset by Class",id:"curate-dataset-by-class",level:3},{value:"Model Training",id:"model-training",level:2},{value:"Model Evaluation",id:"model-evaluation",level:2},{value:"Model Predictions",id:"model-predictions",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"TST, Hong Kong",src:t(814350).A+"",width:"1500",height:"811"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"#detectron-object-detection-with-openimages-dataset-wip",children:"Detectron Object Detection with OpenImages Dataset (WIP)"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"#dataset",children:"Dataset"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#curate-dataset-by-class",children:"Curate Dataset by Class"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#model-training",children:"Model Training"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#model-evaluation",children:"Model Evaluation"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#model-predictions",children:"Model Predictions"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://github.com/mpolinowski/pt-seg-i-see-you",children:"Github Repository"})}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Related"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-08-27--image-segmentation-with-pytorch/2023-08-27",children:"Image Segmentation with PyTorch"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-08-28--semantic-segmentation-detectron2-model-zoo/2023-08-28",children:"Semantic Segmentation Detectron2 Model Zoo"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-08-29--semantic-segmentation-detectron2-model-zoo-faster-rcnn/2023-08-29",children:"Semantic Segmentation Detectron2 Model Zoo: Faster RCNN"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-08-30--instance_segmentation_detectron2_model_zoo_mask_rcnn/2023-08-30",children:"Semantic Segmentation Detectron2 Model Zoo: Mask RCNN"})}),"\n",(0,a.jsx)(n.li,{children:"Detectron Object Detection with OpenImages Dataset (WIP)"}),"\n"]}),"\n",(0,a.jsx)(n.h1,{id:"detectron-object-detection-with-openimages-dataset-wip",children:"Detectron Object Detection with OpenImages Dataset (WIP)"}),"\n",(0,a.jsx)(n.h2,{id:"dataset",children:"Dataset"}),"\n",(0,a.jsxs)(n.p,{children:["Download the annotations for the detection boxes from ",(0,a.jsx)(n.a,{href:"https://storage.googleapis.com/openimages/web/download_v7.html",children:"OpenImages"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"!pip install opencv-python\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import ast\nimport json\nimport cv2 as cv\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport random\nimport shutil\nimport torch\n\nfrom detectron2 import model_zoo\nfrom detectron2.config import get_cfg as _get_cfg\nfrom detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_train_loader\nfrom detectron2.engine import DefaultTrainer, HookBase, DefaultPredictor\nfrom detectron2.structures import BoxMode\nimport detectron2.utils.comm as comm\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"!wget https://storage.googleapis.com/openimages/v6/oidv6-train-annotations-bbox.csv -P '../datasets/OpenImages/annotations'\n!wget https://storage.googleapis.com/openimages/v5/validation-annotations-bbox.csv -P '../datasets/OpenImages/annotations'\n!wget https://storage.googleapis.com/openimages/v5/test-annotations-bbox.csv -P '../datasets/OpenImages/annotations'\n\n!wget https://raw.githubusercontent.com/openimages/dataset/master/downloader.py -P './helper'\n"})}),"\n",(0,a.jsxs)(n.p,{children:["--2023-09-02 15:26:06--  ",(0,a.jsx)(n.a,{href:"https://storage.googleapis.com/openimages/v5/test-annotations-bbox.csv",children:"https://storage.googleapis.com/openimages/v5/test-annotations-bbox.csv"}),"\nLoaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\nResolving storage.googleapis.com (storage.googleapis.com)... 142.251.214.144, 172.217.164.112, 142.250.72.208, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|142.251.214.144|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 77484237 (74M) [text/csv]\nSaving to: \u2018../datasets/OpenImages/annotations/test-annotations-bbox.csv\u2019"]}),"\n",(0,a.jsx)(n.p,{children:"test-annotations-bb 100%[===================>]  73.89M  8.05MB/s    in 12s"}),"\n",(0,a.jsx)(n.p,{children:"2023-09-02 15:26:19 (6.31 MB/s) - \u2018../datasets/OpenImages/annotations/test-annotations-bbox.csv\u2019 saved [77484237/77484237]"}),"\n",(0,a.jsx)(n.h3,{id:"curate-dataset-by-class",children:"Curate Dataset by Class"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Scripts by ",(0,a.jsx)(n.a,{href:"https://github.com/computervisioneng/train-yolov8-custom-dataset-step-by-step-guide/tree/master/prepare_data",children:"computervisioneng"})]}),"\n",(0,a.jsxs)(n.li,{children:["Get class IDs from ",(0,a.jsx)(n.a,{href:"https://gist.github.com/mpolinowski/7b297ad73545e0c670a248a2c350b011",children:"here"})]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Create a text file containing all the image IDs that you're interested in downloading.\n\nclass_id = '/m/01g317' # 'Person'\n# class_id = '/m/0k4j' # 'Car'\n# class_id = '/m/0h2r6' # 'Van'\n# class_id = '/m/01bjv' # 'Bus'\n# class_id = '/m/07r04' # 'Truck'\n# class_id = '/m/01jfm_' # 'Vehicle registration plate'\n# class_id = '/m/01lrl' # 'Carnivore'\n\ntrain_bboxes_filename = '../datasets/OpenImages/annotations/oidv6-train-annotations-bbox.csv'\nvalidation_bboxes_filename = '../datasets/OpenImages/annotations/validation-annotations-bbox.csv'\ntest_bboxes_filename = '../datasets/OpenImages/annotations/test-annotations-bbox.csv'\n\nimage_list_file_path = '../datasets/OpenImages/image_class_person.txt'\n# image_list_file_path = '../datasets/OpenImages/image_class_car.txt'\n# image_list_file_path = '../datasets/OpenImages/image_class_van.txt'\n# image_list_file_path = '../datasets/OpenImages/image_class_bus.txt'\n# image_list_file_path = '../datasets/OpenImages/image_class_truck.txt'\n# image_list_file_path = '../datasets/OpenImages/image_class_plates.txt'\n# image_list_file_path = '../datasets/OpenImages/image_class_carnivore.txt'\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"image_list_file_list = []\nfor j, filename in enumerate([train_bboxes_filename, validation_bboxes_filename, test_bboxes_filename]):\n    print(filename)\n    with open(filename, 'r') as f:\n        line = f.readline()\n        while len(line) != 0:\n            id, _, class_name, _, x1, x2, y1, y2, _, _, _, _, _ = line.split(',')[:13]\n            if class_name in [class_id] and id not in image_list_file_list:\n                image_list_file_list.append(id)\n                with open(image_list_file_path, 'a') as fw:\n                    fw.write('{}/{}\\n'.format(['train', 'validation', 'test'][j], id))\n            line = f.readline()\n\n        f.close()\n\n# the download returned 395931 images with class 'Person' -> I reduced them to 1% over all classes (3961) for this test run\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Run the following script to download all files, making sure you have the dependencies installed:\n\n# !python helper/downloader.py --image_list='../datasets/OpenImages/image_class_person.txt' --download_folder='../datasets/OpenImages/complete' --num_processes=5\n!python helper/downloader.py '../datasets/OpenImages/image_class_person_1%.txt' --download_folder='../datasets/OpenImages/complete' --num_processes=5\n"})}),"\n",(0,a.jsx)(n.p,{children:"Downloading images: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3961/3961 [17:32,  3.76it/s]"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# get ids for all files from list that were used\nfile_ids = []\n\ntext_file = '../datasets/OpenImages/image_class_person_1%.txt'\nfiles_list = open(text_file, 'r')\n\nlines = files_list.readlines()\n\nfor line in lines:\n    file_ids.append(line.strip()[-16:])\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"print(len(file_ids))\n"})}),"\n",(0,a.jsx)(n.p,{children:"3961"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"for j, filename in enumerate([train_bboxes_filename, validation_bboxes_filename, test_bboxes_filename]):\n    print(j, filename)\n"})}),"\n",(0,a.jsx)(n.p,{children:"0 ../datasets/OpenImages/annotations/oidv6-train-annotations-bbox.csv\n1 ../datasets/OpenImages/annotations/validation-annotations-bbox.csv\n2 ../datasets/OpenImages/annotations/test-annotations-bbox.csv"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Train/Test/Split + generate YOLO compatible annotations\nDATA_ALL_DIR = '../datasets/OpenImages/complete'\nDATA_OUT_DIR = '../datasets/OpenImages/split'\n\n# create directories\nfor set_ in ['train', 'validation', 'test']:\n    for dir_ in [os.path.join(DATA_OUT_DIR, set_),\n                 os.path.join(DATA_OUT_DIR, set_, 'imgs'),\n                 os.path.join(DATA_OUT_DIR, set_, 'anns')]:\n        if os.path.exists(dir_):\n            shutil.rmtree(dir_)\n        os.mkdir(dir_)\n\n# save images and annotations\nfor j, filename in enumerate([train_bboxes_filename, validation_bboxes_filename, test_bboxes_filename]):\n    set_ = ['train', 'validation', 'test'][j]\n    print(filename)\n    with open(filename, 'r') as f:\n        line = f.readline()\n        while len(line) != 0:\n            # get bbox\n            id, _, class_name, _, x1, x2, y1, y2, _, _, _, _, _ = line.split(',')[:13]\n            # take all bboxes with the correct class\n            if class_name in [class_id]:\n                # but remove all that are not used (the example only uses 1% of all available images)\n                if id in file_ids:\n                    if not os.path.exists(os.path.join(DATA_OUT_DIR, set_, 'imgs', '{}.jpg'.format(id))):\n                        shutil.copy(os.path.join(DATA_ALL_DIR, '{}.jpg'.format(id)),\n                                    os.path.join(DATA_OUT_DIR, set_, 'imgs', '{}.jpg'.format(id)))\n                    # yolo conform annotations\n                    with open(os.path.join(DATA_OUT_DIR, set_, 'anns', '{}.txt'.format(id)), 'a') as f_ann:\n                        # class_id, xc, yx, w, h\n                        x1, x2, y1, y2 = [float(j) for j in [x1, x2, y1, y2]]\n                        xc = (x1 + x2) / 2\n                        yc = (y1 + y2) / 2\n                        w = x2 - x1\n                        h = y2 - y1\n    \n                        f_ann.write('0 {} {} {} {}\\n'.format(xc, yc, w, h))\n                        f_ann.close()\n\n            line = f.readline()\n"})}),"\n",(0,a.jsx)(n.p,{children:"../datasets/OpenImages/annotations/oidv6-train-annotations-bbox.csv\n../datasets/OpenImages/annotations/validation-annotations-bbox.csv\n../datasets/OpenImages/annotations/test-annotations-bbox.csv"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"IMGS_TRAIN_DIR = DATA_OUT_DIR + '/train/imgs'\nANNS_TRAIN_DIR = DATA_OUT_DIR + '/train/anns'\n\nif __name__ == \"__main__\":\n    files = os.listdir(IMGS_TRAIN_DIR)\n    while True:\n        fig = plt.figure()\n        k = random.randint(0, len(files) - 1)\n        img = cv.imread(os.path.join(IMGS_TRAIN_DIR, files[k]))\n        ann_file = os.path.join(ANNS_TRAIN_DIR, files[k][:-4] + '.txt')\n\n        h_img, w_img, _ = img.shape\n        with open(ann_file, 'r') as f:\n            lines = [l[:-1] for l in f.readlines() if len(l) > 2]\n            for line in lines:\n                line = line.split(' ')\n                class_, x0, y0, w, h = line\n                x1 = int((float(x0) - (float(w) / 2)) * w_img)\n                y1 = int((float(y0) - (float(h) / 2)) * h_img)\n                x2 = x1 + int(float(w) * w_img)\n                y2 = y1 + int(float(h) * h_img)\n                img = cv.rectangle(img,\n                                    (x1, y1),\n                                    (x2, y2),\n                                    (0, 255, 0),\n                                    4)\n        mng = plt.get_current_fig_manager()\n        plt.imshow(cv.cvtColor(img, cv.COLOR_BGR2RGB))\n        plt.show()\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Model Training",src:t(371975).A+"",width:"1260",height:"935"})}),"\n",(0,a.jsx)(n.h2,{id:"model-training",children:"Model Training"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# create loss function https://github.com/computervisioneng/train-object-detector-detectron2\nclass ValidationLoss(HookBase):\n    """\n    A hook that computes validation loss during training.\n\n    Attributes:\n        cfg (CfgNode): The detectron2 config node.\n        _loader (iterator): An iterator over the validation dataset.\n    """\n\n    def __init__(self, cfg):\n        """\n        Args:\n            cfg (CfgNode): The detectron2 config node.\n        """\n        super().__init__()\n        self.cfg = cfg.clone()\n        # Switch to the validation dataset\n        self.cfg.DATASETS.TRAIN = cfg.DATASETS.VAL\n        # Build the validation data loader iterator\n        self._loader = iter(build_detection_train_loader(self.cfg))\n\n    def after_step(self):\n        """\n        Computes the validation loss after each training step.\n        """\n        # Get the next batch of data from the validation data loader\n        data = next(self._loader)\n        with torch.no_grad():\n            # Compute the validation loss on the current batch of data\n            loss_dict = self.trainer.model(data)\n\n            # Check for invalid losses\n            losses = sum(loss_dict.values())\n            assert torch.isfinite(losses).all(), loss_dict\n\n            # Reduce the loss across all workers\n            loss_dict_reduced = {"val_" + k: v.item() for k, v in\n                                 comm.reduce_dict(loss_dict).items()}\n            losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n\n            # Save the validation loss in the trainer storage\n            if comm.is_main_process():\n                self.trainer.storage.put_scalars(total_val_loss=losses_reduced,\n                                                 **loss_dict_reduced)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# create detectron2 configuration https://github.com/computervisioneng/train-object-detector-detectron2\ndef get_cfg(output_dir, learning_rate, batch_size, iterations, checkpoint_period, model, device, nmr_classes):\n    \"\"\"\n    Create a Detectron2 configuration object and set its attributes.\n\n    Args:\n        output_dir (str): The path to the output directory where the trained model and logs will be saved.\n        learning_rate (float): The learning rate for the optimizer.\n        batch_size (int): The batch size used during training.\n        iterations (int): The maximum number of training iterations.\n        checkpoint_period (int): The number of iterations between consecutive checkpoints.\n        model (str): The name of the model to use, which should be one of the models available in Detectron2's model zoo.\n        device (str): The device to use for training, which should be 'cpu' or 'cuda'.\n        nmr_classes (int): The number of classes in the dataset.\n\n    Returns:\n        The Detectron2 configuration object.\n    \"\"\"\n    cfg = _get_cfg()\n\n    # Merge the model's default configuration file with the default Detectron2 configuration file.\n    cfg.merge_from_file(model_zoo.get_config_file(model))\n    # Set the training and validation datasets and exclude the test dataset.\n    cfg.DATASETS.TRAIN = ('train')\n    cfg.DATASETS.VAL = ('validation')\n    cfg.DATASETS.TEST = ()\n    # Set the device to use for training.\n    if device in ['cpu']:\n        cfg.MODEL.DEVICE = 'cpu'\n    # Set the number of data loader workers.\n    cfg.DATALOADER.NUM_WORKERS = 2\n    # Set the model weights to the ones pre-trained on the COCO dataset.\n    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model)\n    # Set the batch size used by the solver.\n    cfg.SOLVER.IMS_PER_BATCH = batch_size\n    # Set the checkpoint period.\n    cfg.SOLVER.CHECKPOINT_PERIOD = checkpoint_period\n    # Set the base learning rate.\n    cfg.SOLVER.BASE_LR = learning_rate\n    # Set the maximum number of training iterations.\n    cfg.SOLVER.MAX_ITER = iterations\n    # Set the learning rate scheduler steps to an empty list, which means the learning rate will not be decayed.\n    cfg.SOLVER.STEPS = []\n    # Set the batch size used by the ROI heads during training.\n    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n    # Set the number of classes.\n    cfg.MODEL.ROI_HEADS.NUM_CLASSES = nmr_classes\n    # Set the output directory.\n    cfg.OUTPUT_DIR = output_dir\n    \n    return cfg\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# get dataset https://github.com/computervisioneng/train-object-detector-detectron2\ndef get_dicts(img_dir, ann_dir):\n    """\n    Read the annotations for the dataset in YOLO format and create a list of dictionaries containing information for each\n    image.\n\n    Args:\n        img_dir (str): Directory containing the images.\n        ann_dir (str): Directory containing the annotations.\n\n    Returns:\n        list[dict]: A list of dictionaries containing information for each image. Each dictionary has the following keys:\n            - file_name: The path to the image file.\n            - image_id: The unique identifier for the image.\n            - height: The height of the image in pixels.\n            - width: The width of the image in pixels.\n            - annotations: A list of dictionaries, one for each object in the image, containing the following keys:\n                - bbox: A list of four integers [x0, y0, w, h] representing the bounding box of the object in the image,\n                        where (x0, y0) is the top-left corner and (w, h) are the width and height of the bounding box,\n                        respectively.\n                - bbox_mode: A constant from the `BoxMode` class indicating the format of the bounding box coordinates\n                             (e.g., `BoxMode.XYWH_ABS` for absolute coordinates in the format [x0, y0, w, h]).\n                - category_id: The integer ID of the object\'s class.\n    """\n    dataset_dicts = []\n    for idx, file in enumerate(os.listdir(ann_dir)):\n        # annotations should be provided in yolo format\n\n        record = {}\n\n        filename = os.path.join(img_dir, file[:-4] + \'.jpg\')\n        height, width = cv.imread(filename).shape[:2]\n\n        record["file_name"] = filename\n        record["image_id"] = idx\n        record["height"] = height\n        record["width"] = width\n\n        objs = []\n        with open(os.path.join(ann_dir, file)) as r:\n            lines = [l[:-1] for l in r.readlines()]\n\n        for _, line in enumerate(lines):\n            if len(line) > 2:\n                label, cx, cy, w_, h_ = line.split(\' \')\n\n                obj = {\n                    "bbox": [int((float(cx) - (float(w_) / 2)) * width),\n                             int((float(cy) - (float(h_) / 2)) * height),\n                             int(float(w_) * width),\n                             int(float(h_) * height)],\n                    "bbox_mode": BoxMode.XYWH_ABS,\n                    "category_id": int(label),\n                }\n\n                objs.append(obj)\n        record["annotations"] = objs\n        dataset_dicts.append(record)\n    return dataset_dicts\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# register dataset https://github.com/computervisioneng/train-object-detector-detectron2\ndef register_datasets(root_dir, class_list_file):\n    \"\"\"\n    Registers the train and validation datasets and returns the number of classes.\n\n    Args:\n        root_dir (str): Path to the root directory of the dataset.\n        class_list_file (str): Path to the file containing the list of class names.\n\n    Returns:\n        int: The number of classes in the dataset.\n    \"\"\"\n    # Read the list of class names from the class list file.\n    with open(class_list_file, 'r') as reader:\n        classes_ = [l[:-1] for l in reader.readlines()]\n\n    # Register the train and validation datasets.\n    for d in ['train', 'validation']:\n        DatasetCatalog.register(d, lambda d=d: get_dicts(os.path.join(root_dir, d, 'imgs'),\n                                                         os.path.join(root_dir, d, 'anns')))\n        # Set the metadata for the dataset.\n        MetadataCatalog.get(d).set(thing_classes=classes_)\n\n    return len(classes_)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# train the dataset\ndef train(output_dir, data_dir, class_list_file, learning_rate, batch_size, iterations, checkpoint_period, device,\n          model):\n    """\n    Train a Detectron2 model on a custom dataset.\n\n    Args:\n        output_dir (str): Path to the directory to save the trained model and output files.\n        data_dir (str): Path to the directory containing the dataset.\n        class_list_file (str): Path to the file containing the list of class names in the dataset.\n        learning_rate (float): Learning rate for the optimizer.\n        batch_size (int): Batch size for training.\n        iterations (int): Maximum number of training iterations.\n        checkpoint_period (int): Number of iterations after which to save a checkpoint of the model.\n        device (str): Device to use for training (e.g., \'cpu\' or \'cuda\').\n        model (str): Name of the model configuration to use. Must be a key in the Detectron2 model zoo.\n\n    Returns:\n        None\n    """\n\n    # Register the dataset and get the number of classes\n    nmr_classes = register_datasets(data_dir, class_list_file)\n\n    # Get the configuration for the model\n    cfg = get_cfg(output_dir, learning_rate, batch_size, iterations, checkpoint_period, model, device, nmr_classes)\n\n    # Create the output directory\n    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n\n    # Create the trainer object\n    trainer = DefaultTrainer(cfg)\n\n    # Create a custom validation loss object\n    val_loss = ValidationLoss(cfg)\n\n    # Register the custom validation loss object as a hook to the trainer\n    trainer.register_hooks([val_loss])\n\n    # Swap the positions of the evaluation and checkpointing hooks so that the validation loss is logged correctly\n    trainer._hooks = trainer._hooks[:-2] + trainer._hooks[-2:][::-1]\n\n    # Resume training from a checkpoint or load the initial model weights\n    trainer.resume_or_load(resume=False)\n\n    # Train the model\n    trainer.train()\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"CLASSES = '../datasets/OpenImages/class_names.txt' # add classes to this file - 1 per line\nDATA_DIR = '../datasets/OpenImages/split' # point to dir that contains your train/validation/test folders\nOUTPUT_DIR = '../saved_models/OpenImages_Model' # weight will be saved here at interval set below\nDEVICE = 'gpu' # 'cpu'\nLR = 0.00001\nBATCH_SIZE = 4\nITERATIONS = 12000 # how many epochs do you want to train?\nCHECKPOINT_PERIOD = 3000 # save weights at this epoch interval\n# MODEL= 'COCO-Detection/fast_rcnn_R_50_FPN_1x.yaml' # 2.6 GB Train Mem\nMODEL= 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml' # 3.0 GB Train Mem\n# MODEL= 'COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml' # 4.1 GB Train Mem\n# MODEL = 'COCO-Detection/retinanet_R_101_FPN_3x.yaml' # 5.2 GB Train Mem\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"train(\n    OUTPUT_DIR,\n    DATA_DIR,\n    CLASSES,\n    device=DEVICE,\n    learning_rate=float(LR),\n    batch_size=int(BATCH_SIZE),\n    iterations=int(ITERATIONS),\n    checkpoint_period=int(CHECKPOINT_PERIOD),\n    model=MODEL)\n"})}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Problem"}),": ",(0,a.jsx)(n.code,{children:"Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected."})]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"WIP"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"    \x1b[32m[09/03 16:20:53 d2.data.build]: \x1b[0mRemoved 0 images with no usable annotations. 3671 images left.\n    \x1b[32m[09/03 16:20:53 d2.data.build]: \x1b[0mDistribution of instances among all 1 categories:\n    \x1b[36m|  category  | #instances   |\n    |:----------:|:-------------|\n    |   Person   | 15218        |\n    |            |              |\x1b[0m\n    \x1b[32m[09/03 16:20:53 d2.data.dataset_mapper]: \x1b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n    \x1b[32m[09/03 16:20:53 d2.data.build]: \x1b[0mUsing training sampler TrainingSampler\n    \x1b[32m[09/03 16:20:53 d2.data.common]: \x1b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n    \x1b[32m[09/03 16:20:53 d2.data.common]: \x1b[0mSerializing 3671 elements to byte tensors and concatenating them all ...\n    \x1b[32m[09/03 16:20:53 d2.data.common]: \x1b[0mSerialized dataset takes 1.17 MiB\n    \x1b[32m[09/03 16:20:54 d2.data.build]: \x1b[0mRemoved 0 images with no usable annotations. 72 images left.\n    \x1b[32m[09/03 16:20:54 d2.data.build]: \x1b[0mDistribution of instances among all 1 categories:\n    \x1b[36m|  category  | #instances   |\n    |:----------:|:-------------|\n    |   Person   | 197          |\n    |            |              |\x1b[0m\n    \x1b[32m[09/03 16:20:54 d2.data.dataset_mapper]: \x1b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n    \x1b[32m[09/03 16:20:54 d2.data.build]: \x1b[0mUsing training sampler TrainingSampler\n    \x1b[32m[09/03 16:20:54 d2.data.common]: \x1b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n    \x1b[32m[09/03 16:20:54 d2.data.common]: \x1b[0mSerializing 72 elements to byte tensors and concatenating them all ...\n    \x1b[32m[09/03 16:20:54 d2.data.common]: \x1b[0mSerialized dataset takes 0.02 MiB\n    \x1b[32m[09/03 16:20:54 d2.checkpoint.detection_checkpoint]: \x1b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n\n\n    model_final_280758.pkl: 167MB [00:21, 7.86MB/s]                                                                                                        \n    Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n    Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n    Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n    Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n    Some model parameters or buffers are not found in the checkpoint:\n    \x1b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\x1b[0m\n    \x1b[34mroi_heads.box_predictor.cls_score.{bias, weight}\x1b[0m\n\n\n    \x1b[32m[09/03 16:21:15 d2.engine.train_loop]: \x1b[0mStarting training from iteration 0\n    \n...\n\n    \x1b[32m[09/03 22:11:31 d2.utils.events]: \x1b[0m eta: 0:00:00  iter: 11999  total_loss: 0.6341  loss_cls: 0.1783  loss_box_reg: 0.3643  loss_rpn_cls: 0.01865  loss_rpn_loc: 0.02812  total_val_loss: 0.7417  val_loss_cls: 0.2803  val_loss_box_reg: 0.4251  val_loss_rpn_cls: 0.02334  val_loss_rpn_loc: 0.02181    time: 1.2499  last_time: 1.3406  data_time: 0.0571  last_data_time: 0.0050   lr: 1e-05  max_mem: 4713M\n    \x1b[32m[09/03 22:11:37 d2.engine.hooks]: \x1b[0mOverall training speed: 11998 iterations in 4:09:55 (1.2499 s / it)\n    \x1b[32m[09/03 22:11:37 d2.engine.hooks]: \x1b[0mTotal training time: 5:50:16 (1:40:20 on hooks)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"model-evaluation",children:"Model Evaluation"}),"\n",(0,a.jsxs)(n.p,{children:["Visualizing the ",(0,a.jsx)(n.code,{children:"metrics.json"})," file generated in your above defined output dir:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"def moving_average(a, n=3):\n    ret = np.cumsum(a, dtype=float)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n\n\n\nmetrics_file = OUTPUT_DIR + '/metrics.json'\n\nwith open(metrics_file, 'r') as f:\n    metrics = [ast.literal_eval(l[:-1]) for l in f.readlines()]\n    f.close()\n\ntrain_loss = [float(v['loss_box_reg']) for v in metrics if 'loss_box_reg' in v.keys()]\nval_loss = [float(v['val_loss_box_reg']) for v in metrics if 'val_loss_box_reg' in v.keys()]\n\nN = 40\n\ntrain_loss_avg = moving_average(train_loss, n=N)\nval_loss_avg = moving_average(val_loss, n=N)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"plt.plot(range(20 * N - 1, 20 * len(train_loss), 20), train_loss_avg, label='train loss')\nplt.plot(range(20 * N - 1, 20 * len(train_loss), 20), val_loss_avg, label='val loss')\nplt.title('Faster RCNN R50-FPN-3x Training Loss')\nplt.legend()\nplt.grid()\nplt.show()\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Model Training",src:t(813286).A+"",width:"556",height:"435"})}),"\n",(0,a.jsx)(n.h2,{id:"model-predictions",children:"Model Predictions"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Load config from a config file\ncfg = _get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(MODEL))\ncfg.MODEL.WEIGHTS = OUTPUT_DIR + '/model_0011999.pth'\ncfg.MODEL.DEVICE = 'cuda'\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Create predictor instance\npredictor = DefaultPredictor(cfg)\n\n# Load image\nimage = cv.imread(OUTPUT_DIR + "/sz.jpg")\n\n# Perform prediction\noutputs = predictor(image)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"outputs\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Display predictions\nthreshold = 0.5\n\npreds = outputs["instances"].pred_classes.tolist()\nscores = outputs["instances"].scores.tolist()\nbboxes = outputs["instances"].pred_boxes\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"for j, bbox in enumerate(bboxes):\n    bbox = bbox.tolist()\n\n    score = scores[j]\n    pred = preds[j]\n\n    if score > threshold:\n        x1, y1, x2, y2 = [int(i) for i in bbox]\n\n        cv.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 5)\n\ncv.imshow('image', image)\ncv.waitKey(0)\n"})})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},371975:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/Model_Eval_01-33892eb88f17b07e8eac443d4cfb8b1e.webp"},813286:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/Model_Eval_02-9284f234ccf3b7a7326b5f307f4a14d3.webp"},814350:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-f940fa4541ff8a00764cf3f41cd6b985.jpg"},28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var a=t(296540);const i={},s=a.createContext(i);function o(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);