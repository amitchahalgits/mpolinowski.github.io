"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[35937],{3905:(e,n,o)=>{o.d(n,{Zo:()=>p,kt:()=>g});var t=o(67294);function a(e,n,o){return n in e?Object.defineProperty(e,n,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[n]=o,e}function r(e,n){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),o.push.apply(o,t)}return o}function i(e){for(var n=1;n<arguments.length;n++){var o=null!=arguments[n]?arguments[n]:{};n%2?r(Object(o),!0).forEach((function(n){a(e,n,o[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):r(Object(o)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(o,n))}))}return e}function l(e,n){if(null==e)return{};var o,t,a=function(e,n){if(null==e)return{};var o,t,a={},r=Object.keys(e);for(t=0;t<r.length;t++)o=r[t],n.indexOf(o)>=0||(a[o]=e[o]);return a}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)o=r[t],n.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(a[o]=e[o])}return a}var s=t.createContext({}),d=function(e){var n=t.useContext(s),o=n;return e&&(o="function"==typeof e?e(n):i(i({},n),e)),o},p=function(e){var n=d(e.components);return t.createElement(s.Provider,{value:n},e.children)},c={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},u=t.forwardRef((function(e,n){var o=e.components,a=e.mdxType,r=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=d(o),g=a,m=u["".concat(s,".").concat(g)]||u[g]||c[g]||r;return o?t.createElement(m,i(i({ref:n},p),{},{components:o})):t.createElement(m,i({ref:n},p))}));function g(e,n){var o=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var r=o.length,i=new Array(r);i[0]=u;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l.mdxType="string"==typeof e?e:a,i[1]=l;for(var d=2;d<r;d++)i[d]=o[d];return t.createElement.apply(null,i)}return t.createElement.apply(null,o)}u.displayName="MDXCreateElement"},49195:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>s,contentTitle:()=>i,default:()=>c,frontMatter:()=>r,metadata:()=>l,toc:()=>d});var t=o(87462),a=(o(67294),o(3905));const r={sidebar_position:4690,slug:"2023-02-18",title:"Keras for Tensorflow - VGG16 Network Architecture",authors:"mpolinowski",tags:["Python","Machine Learning","Keras","Tensorflow"],description:"An example convolutional neural network is the VGG16 Architecture."},i=void 0,l={unversionedId:"IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-vgg16/index",id:"IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-vgg16/index",title:"Keras for Tensorflow - VGG16 Network Architecture",description:"An example convolutional neural network is the VGG16 Architecture.",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-vgg16/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-vgg16",slug:"/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-vgg16/2023-02-18",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-vgg16/2023-02-18",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-vgg16/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Keras",permalink:"/docs/tags/keras"},{label:"Tensorflow",permalink:"/docs/tags/tensorflow"}],version:"current",sidebarPosition:4690,frontMatter:{sidebar_position:4690,slug:"2023-02-18",title:"Keras for Tensorflow - VGG16 Network Architecture",authors:"mpolinowski",tags:["Python","Machine Learning","Keras","Tensorflow"],description:"An example convolutional neural network is the VGG16 Architecture."},sidebar:"tutorialSidebar",previous:{title:"Tensorflow 2 - An (Re)Introduction 2023",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-02-19-tensorflow-introduction/2023-02-19"},next:{title:"Keras for Tensorflow - Recurrent Neural Networks",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-rnn/2023-02-18"}},s={},d=[{value:"Very Deep Convolutional Networks",id:"very-deep-convolutional-networks",level:2},{value:"Building the VGG16 Model",id:"building-the-vgg16-model",level:3},{value:"Training the VGG16 Model",id:"training-the-vgg16-model",level:3}],p={toc:d};function c(e){let{components:n,...r}=e;return(0,a.kt)("wrapper",(0,t.Z)({},p,r,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Guangzhou, China",src:o(95403).Z,width:"2830",height:"1272"})),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"#very-deep-convolutional-networks"},"Very Deep Convolutional Networks"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"#building-the-vgg16-model"},"Building the VGG16 Model")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"#training-the-vgg16-model"},"Training the VGG16 Model"))))),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/tf-keras-2023"},"Github Repository")),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"https://keras.io/getting_started/"},"Keras")," is built on top of TensorFlow 2 and provides an API designed for human beings. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear & actionable error messages."),(0,a.kt)("p",null,(0,a.kt)("em",{parentName:"p"},"See also:")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-14-keras-introduction/2023-02-14"},"Keras for Tensorflow - An (Re)Introduction 2023")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-16-keras-introduction-ann/2023-02-16"},"Keras for Tensorflow - Artificial Neural Networks")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-17-keras-introduction-cnn/2023-02-17"},"Keras for Tensorflow - Convolutional Neural Networks")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-vgg16/2023-02-18"},"Keras for Tensorflow - VGG16 Network Architecture")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-rnn/2023-02-18"},"Keras for Tensorflow - Recurrent Neural Networks"))),(0,a.kt)("h2",{id:"very-deep-convolutional-networks"},"Very Deep Convolutional Networks"),(0,a.kt)("h3",{id:"building-the-vgg16-model"},"Building the VGG16 Model"),(0,a.kt)("p",null,"An example convolutional neural network is the ",(0,a.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1409.1556"},"VGG16 Architecture"),". The number 16 in the name VGG refers to the fact that it is 16 layers deep neural network (VGGnet - ",(0,a.kt)("a",{parentName:"p",href:"https://www.pyimagesearch.com/wp-content/uploads/2017/03/imagenet_vgg16.png"},"Image Source"),")."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Keras for Tensorflow - Convolutional Neural Networks",src:o(94414).Z,width:"470",height:"276"})),(0,a.kt)("p",null,"The VGG16 Model starts with an colour (",(0,a.kt)("inlineCode",{parentName:"p"},"3")," colour channels) image input of ",(0,a.kt)("inlineCode",{parentName:"p"},"224x224")," pixels and keeps applying filters to increase its depth. While using pooling layers to reduce its dimensions. The output layer end with a shape of ",(0,a.kt)("inlineCode",{parentName:"p"},"1x1x1000")," and uses a ",(0,a.kt)("inlineCode",{parentName:"p"},"softmax")," activation - meaning, the model will assign probabilities for 1000 classes."),(0,a.kt)("p",null,"To rebuild this model in Keras:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},"import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\n# defining the input shape\n# starting with a 224x224 colour image\ninput_shape = (224, 224, 3)\n\n# building the model\nmodel = Sequential()\n## convolutional layers with 64 filters + pooling => 224 x 224 x 64\nmodel.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu', input_shape=input_shape))\nmodel.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n## randomly drop 25% of neurons to prevent overfitting\nmodel.add(Dropout(0.25))\n## convolutional layers with 128 filters + pooling => 112 x 112 x 128\nmodel.add(Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n## randomly drop 10% of neurons to prevent overfitting\nmodel.add(Dropout(0.10))\n## convolutional layers with 256 filters + pooling => 56 x 56 x 256\nmodel.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n## randomly drop 10% of neurons to prevent overfitting\nmodel.add(Dropout(0.10))\n## convolutional layers with 256 filters + pooling => 28 x 28 x 512\nmodel.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n## randomly drop 10% of neurons to prevent overfitting\nmodel.add(Dropout(0.10))\n## convolutional layers with 256 filters + pooling => 14 x 14 x 512\nmodel.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n## randomly drop 10% of neurons to prevent overfitting\nmodel.add(Dropout(0.10))\n## flatten before dense layer => 1 x 1 x 4096\nmodel.add(Flatten())\nmodel.add(Dense(4096, activation='relu'))\nmodel.add(Dropout(0.25))\n# output layer assigns probability of 1000 classes\nmodel.add(Dense(1000, activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},'Model: "sequential"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 224, 224, 64)      1792      \n                                                                 \n conv2d_1 (Conv2D)           (None, 224, 224, 64)      36928     \n                                                                 \n max_pooling2d (MaxPooling2D)  (None, 112, 112, 64)     0\n                                                                 \n dropout (Dropout)           (None, 112, 112, 64)      0         \n                                                                 \n conv2d_2 (Conv2D)           (None, 112, 112, 128)     73856     \n                                                                 \n conv2d_3 (Conv2D)           (None, 112, 112, 128)     147584    \n                                                                 \n max_pooling2d_1 (MaxPooling2D)  (None, 56, 56, 128)      0        \n                                                                 \n dropout_1 (Dropout)         (None, 56, 56, 128)       0         \n                                                                 \n conv2d_4 (Conv2D)           (None, 56, 56, 256)       295168    \n                                                                 \n conv2d_5 (Conv2D)           (None, 56, 56, 256)       590080    \n                                                                 \n conv2d_6 (Conv2D)           (None, 56, 56, 256)       590080    \n                                                                 \n max_pooling2d_2 (MaxPooling2D)  (None, 28, 28, 256)      0         \n                                                                 \n dropout_2 (Dropout)         (None, 28, 28, 256)       0         \n                                                                 \n conv2d_7 (Conv2D)           (None, 28, 28, 512)       1180160   \n                                                                 \n conv2d_8 (Conv2D)           (None, 28, 28, 512)       2359808   \n                                                                 \n conv2d_9 (Conv2D)           (None, 28, 28, 512)       2359808   \n                                                                 \n max_pooling2d_3 (MaxPooling2D)  (None, 14, 14, 512)      0         \n                                                                 \n dropout_3 (Dropout)         (None, 14, 14, 512)       0         \n                                                                 \n conv2d_10 (Conv2D)          (None, 14, 14, 512)       2359808   \n                                                                 \n conv2d_11 (Conv2D)          (None, 14, 14, 512)       2359808   \n                                                                 \n conv2d_12 (Conv2D)          (None, 14, 14, 512)       2359808   \n                                                                 \n max_pooling2d_4 (MaxPooling2D)  (None, 7, 7, 512)        0\n                                                                 \n dropout_4 (Dropout)         (None, 7, 7, 512)         0         \n                                                                 \n flatten (Flatten)           (None, 25088)             0         \n                                                                 \n dense (Dense)               (None, 4096)              102764544 \n                                                                 \n dropout_5 (Dropout)         (None, 4096)              0         \n                                                                 \n dense_1 (Dense)             (None, 1000)              4097000   \n                                                                 \n=================================================================\nTotal params: 121,576,232\nTrainable params: 121,576,232\nNon-trainable params: 0\n_________________________________________________________________\n')),(0,a.kt)("h3",{id:"training-the-vgg16-model"},"Training the VGG16 Model"),(0,a.kt)("p",null,"Instead of training this fresh model we can use Keras to download a pre-trained version of it, giving us a head start. The following code will download the ",(0,a.kt)("a",{parentName:"p",href:"https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5"},"pre-training weights")," for the VGG16 model:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},"# using the pre-trained vgg16 instead of a fresh version\nfrom tensorflow.keras.applications.vgg16 import VGG16\nvgg16 = VGG16()\n")),(0,a.kt)("p",null,"This helped us skipping the training. Now we can go straight to doing predictions. Again, Keras helps us with providing helper functions to load and preprocess a test image and run a prediction on it:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},"from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"# using the pre-trained vgg16 instead of a fresh version\nvgg16 = VGG16()\n\n# load image for prediction\nimg = load_img('HK-LR2020_76.jpg', target_size=(224, 224))\nimg = img_to_array(img)\nimg = img.reshape(1,224,224,3)\n\n# run prediction\nyhat = vgg16.predict(img)\nlabel = decode_predictions(yhat)\nprediction = label[0][0]\nprint(prediction[1])\n")),(0,a.kt)("p",null,"Well, the training went in the right direction. But needs to be optimized for your specific use-case:"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Keras for Tensorflow - Convolutional Neural Networks",src:o(59378).Z,width:"1444",height:"411"})),(0,a.kt)("p",null,"The model has been trained on the ImageNet dataset. We can check the included classes by the reading ",(0,a.kt)("a",{parentName:"p",href:"https://gist.github.com/mpolinowski/6cf2c314e8e2ad53ca24357d064dcba6"},"following file"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"cat ~/.keras/models/imagenet_class_index.json\n")),(0,a.kt)("p",null,"One of the included classes is:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-json"},'"808": [\n    "n04259630",\n    "sombrero"\n  ]\n')),(0,a.kt)("p",null,"So let's put this to a test :)"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Keras for Tensorflow - Convolutional Neural Networks",src:o(10702).Z,width:"1347",height:"365"})))}c.isMDXComponent=!0},94414:(e,n,o)=>{o.d(n,{Z:()=>t});const t=o.p+"assets/images/Keras_Introduction_VGG16_Model_01-98a64e48ae8390037f5b07727bce4c02.png"},59378:(e,n,o)=>{o.d(n,{Z:()=>t});const t=o.p+"assets/images/Keras_Introduction_VGG16_Model_02-3f98ca76c438482b3f8984ea156fc079.png"},10702:(e,n,o)=>{o.d(n,{Z:()=>t});const t=o.p+"assets/images/Keras_Introduction_VGG16_Model_03-64081e7fe811f3e5709b44013d3e1df3.png"},95403:(e,n,o)=>{o.d(n,{Z:()=>t});const t=o.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-f80e63ee872dae25129198058ac93b4e.jpg"}}]);