"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[89015],{69232:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>l});var i=t(785893),r=t(603905);const o={sidebar_position:4830,slug:"2023-01-05",title:"YOLOv7 Introduction",authors:"mpolinowski",tags:["Python","Machine Learning","YOLO","Torch"],description:"Getting started with object detection in YOLOv7"},s=void 0,a={id:"IoT-and-Machine-Learning/ML/2023-01-05-yolov7/index",title:"YOLOv7 Introduction",description:"Getting started with object detection in YOLOv7",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-01-05-yolov7/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-01-05-yolov7",slug:"/IoT-and-Machine-Learning/ML/2023-01-05-yolov7/2023-01-05",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-01-05-yolov7/2023-01-05",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-01-05-yolov7/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"YOLO",permalink:"/docs/tags/yolo"},{label:"Torch",permalink:"/docs/tags/torch"}],version:"current",sidebarPosition:4830,frontMatter:{sidebar_position:4830,slug:"2023-01-05",title:"YOLOv7 Introduction",authors:"mpolinowski",tags:["Python","Machine Learning","YOLO","Torch"],description:"Getting started with object detection in YOLOv7"},sidebar:"tutorialSidebar",previous:{title:"MiDaS Depth Vision",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-01-08-depth-vision-midas/2023-01-08"},next:{title:"Recurrent Neural Networks",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-31-tf-rnn-text-generation/2022-12-31"}},c={},l=[{value:"Run YOLOv7 on Arch LINUX",id:"run-yolov7-on-arch-linux",level:2},{value:"Clone the Repository",id:"clone-the-repository",level:3},{value:"Install all Dependencies",id:"install-all-dependencies",level:3},{value:"Download pre-trained Weights",id:"download-pre-trained-weights",level:3},{value:"Test the Model",id:"test-the-model",level:2},{value:"Image Files",id:"image-files",level:3},{value:"Video Files",id:"video-files",level:3},{value:"Videostreaming",id:"videostreaming",level:3}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.ah)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Guangzhou, China",src:t(64898).Z+"",width:"1500",height:"652"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#run-yolov7-on-arch-linux",children:"Run YOLOv7 on Arch LINUX"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#clone-the-repository",children:"Clone the Repository"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#install-all-dependencies",children:"Install all Dependencies"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#download-pre-trained-weights",children:"Download pre-trained Weights"})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#test-the-model",children:"Test the Model"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#image-files",children:"Image Files"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#video-files",children:"Video Files"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#videostreaming",children:"Videostreaming"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Citation"}),": ",(0,i.jsx)(n.a,{href:"https://arxiv.org/abs/2207.02696",children:"YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors"})," ",(0,i.jsx)(n.em,{children:"Chien-Yao Wang, Alexey Bochkovskiy, Hong-Yuan Mark Liao"}),": YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9% AP) outperforms both transformer-based detector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by 509% in speed and 2% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed and 0.7% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. ",(0,i.jsx)(n.a,{href:"https://github.com/WongKinYiu/yolov7",children:"Github"})]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"run-yolov7-on-arch-linux",children:"Run YOLOv7 on Arch LINUX"}),"\n",(0,i.jsx)(n.h3,{id:"clone-the-repository",children:"Clone the Repository"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/mpolinowski/yolov7.git\ncd yolov7\n"})}),"\n",(0,i.jsx)(n.h3,{id:"install-all-dependencies",children:"Install all Dependencies"}),"\n",(0,i.jsxs)(n.p,{children:["The repository comes with an ",(0,i.jsx)(n.code,{children:"Requirements.txt"})," that you can install globally or inside a virtEnvironment ",(0,i.jsx)(n.code,{children:"pip3 install -r requirements.txt"}),". But I am going to use ",(0,i.jsx)(n.a,{href:"/docs/Development/Python/2022-12-11-pipenv/2022-12-11",children:"PipEnv"})," instead:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pipenv --version\npipenv, version 2022.12.19\n\npipenv install -r requirements.txt\nCreating a Pipfile for this project...\nRequirements file provided! Importing into Pipfile...\nPipfile.lock not found, creating...\nInstalling dependencies from Pipfile.lock (9ec603)...\nTo activate this project's virtualenv, run pipenv shell.\nAlternatively, run a command inside the virtualenv with pipenv run.\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"pipfile"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'[[source]]\nurl = "https://pypi.org/simple"\nverify_ssl = true\nname = "pypi"\n\n[packages]\nmatplotlib = ">=3.2.2"\nnumpy = "<1.24.0,>=1.18.5"\nopencv-python = ">=4.1.1"\npillow = ">=7.1.2"\npyyaml = ">=5.3.1"\nrequests = ">=2.23.0"\nscipy = ">=1.4.1"\ntorch = ">=1.7.0,!=1.12.0"\ntorchvision = ">=0.8.1,!=0.13.0"\ntqdm = ">=4.41.0"\nprotobuf = "<4.21.3"\ntensorboard = ">=2.4.1"\npandas = ">=1.1.4"\nseaborn = ">=0.11.0"\nipython = "*"\npsutil = "*"\nthop = "*"\n\n[dev-packages]\n\n[requires]\npython_version = "3.10"\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"UPDATE"}),": I ran into some issues changing the torch and torchvision version here to their CUDA counterparts. Since I had all the dependencies globally installed anyway, I dropped the virtual environment and made sure that ",(0,i.jsx)(n.a,{href:"https://pytorch.org/",children:"PyTorch was installed with GPU support"}),":"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"YOLOv7 Introduction",src:t(785145).Z+"",width:"812",height:"307"})}),"\n",(0,i.jsxs)(n.p,{children:["You can empty the virtual environment with ",(0,i.jsx)(n.code,{children:"pipenv uninstall --all"})," and run all scripts without the ",(0,i.jsx)(n.code,{children:"pipenv run"})," prefix if you encounter the same issue:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"python detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --source hongkong.jpg\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"YOLOv7 Introduction",src:t(875179).Z+"",width:"2385",height:"1591"})}),"\n",(0,i.jsx)(n.h3,{id:"download-pre-trained-weights",children:"Download pre-trained Weights"}),"\n",(0,i.jsxs)(n.p,{children:["The weights we can use are ",(0,i.jsx)(n.a,{href:"https://github.com/mpolinowski/yolov7#testing",children:"linked in the repo README"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\n"})}),"\n",(0,i.jsx)(n.h2,{id:"test-the-model",children:"Test the Model"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.a,{href:"https://github.com/mpolinowski/yolov7#inference",children:"README"})," also holds the two commands we can use to try out the pre-trained model - one for video and one for still images:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"python detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --source yourvideo.mp4\npython detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --source inference/images/horses.jpg\n"})}),"\n",(0,i.jsx)(n.h3,{id:"image-files",children:"Image Files"}),"\n",(0,i.jsxs)(n.p,{children:["Since I used ",(0,i.jsx)(n.code,{children:"pipenv"})," I have to prepend the ",(0,i.jsx)(n.code,{children:"python"})," in this command:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pipenv run python detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --source harbin.jpeg\n\n21 persons, 7 cars, 1 bus, 1 truck, 4 traffic lights, 1 umbrella, 3 handbags, Done. (49.1ms) Inference, (11.6ms) NMS\n The image with the result is saved in: runs/detect/exp/harbin.jpeg\nDone. (0.786s)\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"YOLOv7 Introduction",src:t(442918).Z+"",width:"5362",height:"3029"})}),"\n",(0,i.jsx)(n.h3,{id:"video-files",children:"Video Files"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pipenv run python detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --source video.mp4\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"YOLOv7 Introduction",src:t(576757).Z+"",width:"485",height:"271"})}),"\n",(0,i.jsx)(n.h3,{id:"videostreaming",children:"Videostreaming"}),"\n",(0,i.jsxs)(n.p,{children:["According to the documentation you can select a connected webcam from your system as a video source by selected the video source, e.g. ",(0,i.jsx)(n.code,{children:"--source 0"}),", ",(0,i.jsx)(n.code,{children:"--source 1"})," etc.:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pipenv run python detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --view-img --nosave --source 0\n"})}),"\n",(0,i.jsxs)(n.p,{children:["But looking at the ",(0,i.jsx)(n.code,{children:"detect.py"})," file shows me that it also accepts RTSP streams from IP cameras as source:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-py",children:" webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n        ('rtsp://', 'rtmp://', 'http://', 'https://'))\n"})}),"\n",(0,i.jsx)(n.p,{children:"So let't try this with an INSTAR WQHD IP camera:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pipenv run python detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --view-img --nosave --source rtsp://admin:instar@192.168.2.120:554/livestream/13\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"YOLOv7 Introduction",src:t(239430).Z+"",width:"1007",height:"572"})})]})}function h(e={}){const{wrapper:n}={...(0,r.ah)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},603905:(e,n,t)=>{t.d(n,{ah:()=>l});var i=t(667294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,i)}return t}function s(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function a(e,n){if(null==e)return{};var t,i,r=function(e,n){if(null==e)return{};var t,i,r={},o=Object.keys(e);for(i=0;i<o.length;i++)t=o[i],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(i=0;i<o.length;i++)t=o[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var c=i.createContext({}),l=function(e){var n=i.useContext(c),t=n;return e&&(t="function"==typeof e?e(n):s(s({},n),e)),t},d={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},h=i.forwardRef((function(e,n){var t=e.components,r=e.mdxType,o=e.originalType,c=e.parentName,h=a(e,["components","mdxType","originalType","parentName"]),p=l(t),u=r,g=p["".concat(c,".").concat(u)]||p[u]||d[u]||o;return t?i.createElement(g,s(s({ref:n},h),{},{components:t})):i.createElement(g,s({ref:n},h))}));h.displayName="MDXCreateElement"},875179:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/YOLOv7_Introduction_00-384c3de34bcedbf8432d4aad47f5ecc2.jpg"},785145:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/YOLOv7_Introduction_00-0746a1b6adab6f65d30ca7fee9d1b169.png"},442918:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/YOLOv7_Introduction_01-77eeea431addd10589622e75691f74e6.jpg"},576757:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/YOLOv7_Introduction_02-8f61d47cbb03764072abb1c2af14954b.gif"},239430:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/YOLOv7_Introduction_03-46f40120c7d47fbdfc92dac93c97b574.png"},64898:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-ba3b23aa3d5392c02b451d1b2b911721.jpg"}}]);