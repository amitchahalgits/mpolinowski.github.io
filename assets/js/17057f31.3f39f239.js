"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[19558],{452209:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>t,default:()=>h,frontMatter:()=>a,metadata:()=>l,toc:()=>o});var i=r(785893),s=r(603905);const a={sidebar_position:7e3,slug:"2021-12-03",title:"OpenCV Image Operations",authors:"mpolinowski",tags:["Machine Learning","Python","OpenCV"]},t=void 0,l={id:"IoT-and-Machine-Learning/ML/2021-12-03--opencv-image-operations/index",title:"OpenCV Image Operations",description:"Shenzhen, China",source:"@site/docs/IoT-and-Machine-Learning/ML/2021-12-03--opencv-image-operations/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2021-12-03--opencv-image-operations",slug:"/IoT-and-Machine-Learning/ML/2021-12-03--opencv-image-operations/2021-12-03",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-12-03--opencv-image-operations/2021-12-03",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2021-12-03--opencv-image-operations/index.md",tags:[{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Python",permalink:"/docs/tags/python"},{label:"OpenCV",permalink:"/docs/tags/open-cv"}],version:"current",sidebarPosition:7e3,frontMatter:{sidebar_position:7e3,slug:"2021-12-03",title:"OpenCV Image Operations",authors:"mpolinowski",tags:["Machine Learning","Python","OpenCV"]},sidebar:"tutorialSidebar",previous:{title:"OpenCV Image Objects",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-12-04--opencv-image-objects/2021-12-04"},next:{title:"OpenCV, Streams and Video Files",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-12-02--opencv-with-videos/2021-12-02"}},c={},o=[{value:"Setup OpenCV",id:"setup-opencv",level:2},{value:"Image Operations",id:"image-operations",level:2},{value:"Basic Procedures",id:"basic-procedures",level:3},{value:"Region of Interest",id:"region-of-interest",level:3},{value:"Image Splitting and Joining",id:"image-splitting-and-joining",level:3},{value:"Image Blending",id:"image-blending",level:3},{value:"Apply Filters",id:"apply-filters",level:3},{value:"Sharpening",id:"sharpening",level:4},{value:"Bluring",id:"bluring",level:4},{value:"Outline Edge Detection",id:"outline-edge-detection",level:4},{value:"Embossing",id:"embossing",level:4},{value:"Image Thresholding",id:"image-thresholding",level:3},{value:"Canny Edge Detection",id:"canny-edge-detection",level:5}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",h4:"h4",h5:"h5",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.ah)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Shenzhen, China",src:r(557430).Z+"",width:"2385",height:"919"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#setup-opencv",children:"Setup OpenCV"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#image-operations",children:"Image Operations"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#basic-procedures",children:"Basic Procedures"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#region-of-interest",children:"Region of Interest"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#image-splitting-and-joining",children:"Image Splitting and Joining"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#image-blending",children:"Image Blending"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#apply-filters",children:"Apply Filters"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#sharpening",children:"Sharpening"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#bluring",children:"Bluring"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#outline-edge-detection",children:"Outline Edge Detection"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#embossing",children:"Embossing"})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#image-thresholding",children:"Image Thresholding"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#canny-edge-detection",children:"Canny Edge Detection"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://github.com/mpolinowski/opencv-image-editing",children:"Github Repo"})}),"\n",(0,i.jsx)(n.h2,{id:"setup-opencv",children:"Setup OpenCV"}),"\n",(0,i.jsx)(n.p,{children:"Create and activate a virtual work environment:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"python -m venv .env\r\nsource .env/bin/activate\r\npython -m pip install --upgrade pip\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Add a file ",(0,i.jsx)(n.code,{children:"dependencies.txt"})," with all project ",(0,i.jsx)(n.strong,{children:"pip dependencies"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"opencv-python\r\nnumpy\n"})}),"\n",(0,i.jsx)(n.p,{children:"Install all dependencies with:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pip install -r dependencies.txt\n"})}),"\n",(0,i.jsx)(n.h2,{id:"image-operations",children:"Image Operations"}),"\n",(0,i.jsx)(n.h3,{id:"basic-procedures",children:"Basic Procedures"}),"\n",(0,i.jsx)(n.p,{children:"Read pixel values:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import cv2\r\n\r\nimage = cv2.imread('resources/test-image.jpg')\r\n\r\n# Read pixel value\r\npixel = image[100, 100]\r\nprint(pixel)\n"})}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.strong,{children:"BGR values"})," of the pixel in position 100x100 are ",(0,i.jsx)(n.code,{children:"[99 104 159]"}),". To return only the red value you can specify the location inside the BGR array - red = 2 - which returns ",(0,i.jsx)(n.code,{children:"159"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"redValue = image[100, 100, 2]\r\nprint(redValue)\n"})}),"\n",(0,i.jsx)(n.p,{children:"To change the colour value of a pixel:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"whitePixel = image[100, 100] = [255, 255, 255]\r\nprint(whitePixel)\n"})}),"\n",(0,i.jsx)(n.p,{children:"Print image properties:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"print(image.shape)\r\nprint(image.dtype)\r\nprint(image.size)\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Shape (height, width, channels: colours + alpha) - ",(0,i.jsx)(n.code,{children:"(476, 715, 3)"})]}),"\n",(0,i.jsxs)(n.li,{children:["Data Type - ",(0,i.jsx)(n.code,{children:"uint8"})]}),"\n",(0,i.jsxs)(n.li,{children:["Size (height*width*channels) - ",(0,i.jsx)(n.code,{children:"1021020"})]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"region-of-interest",children:"Region of Interest"}),"\n",(0,i.jsxs)(n.p,{children:["Setting a ",(0,i.jsx)(n.strong,{children:"ROI"})," - ",(0,i.jsx)(n.code,{children:"cv2.selectROI(windowName, image, showCrosshair, fromCenter)"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'roi = cv2.selectROI("ROI Selector", image)\r\n# Crop image from x/y position of top left corner of selector to bottom right x/y position\r\ncropped_image = image[int(roi[1]): int(roi[1]) + int(roi[3]), int(roi[0]): int(roi[0]) + int(roi[2])]\r\ncrop_resized = cv2.resize(cropped_image, (600, 600))\r\ncv2.imshow("Region of Interest", crop_resized)\r\ncv2.imwrite("processed/cropped_image.jpg", crop_resized)\r\n\r\ncv2.waitKey(5000)\r\ncv2.destroyAllWindows()\r\nprint(roi)\n'})}),"\n",(0,i.jsx)(n.p,{children:"Manually select a region of interest and have it cropped and saved:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"OpenCV Image Operations",src:r(393098).Z+"",width:"1369",height:"688"})}),"\n",(0,i.jsx)(n.p,{children:"The image is cropped around the crosshair:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"image[int(roi[1]): int(roi[1]) + int(roi[3]), int(roi[0]): int(roi[0]) + int(roi[2])]\n"})}),"\n",(0,i.jsxs)(n.p,{children:["If you ",(0,i.jsx)(n.code,{children:"print(roi)"})," you get ",(0,i.jsx)(n.code,{children:"(383, 249, 123, 134)"}),". Where ",(0,i.jsx)(n.strong,{children:"383x249"})," are the x and y coordinates of the starting point of the region of interest (top, left corner) and ",(0,i.jsx)(n.strong,{children:"123x134"})," is the distance from this starting point at the bottom, right corner ",(0,i.jsx)(n.strong,{children:"506x383"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"383 + 123 = 506"}),"\n",(0,i.jsx)(n.li,{children:"249 + 134 = 383"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"image-splitting-and-joining",children:"Image Splitting and Joining"}),"\n",(0,i.jsx)(n.p,{children:"Splitting the image into color channels:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'b, g, r = cv2.split(image)\r\n\r\ncv2.imshow("Blue Channel", b)\r\ncv2.imshow("Green Channel", g)\r\ncv2.imshow("Red Channel", r)\r\n\r\ncv2.waitKey(5000)\r\ncv2.destroyAllWindows()\n'})}),"\n",(0,i.jsx)(n.p,{children:"You can re-merge them into one image and end up with the original colour image:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'image_color_channels = cv2.merge((b, g, r))\r\ncv2.imshow("All Colour Channels", image_color_channels)\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"OpenCV Image Operations",src:r(117968).Z+"",width:"1429",height:"1013"})}),"\n",(0,i.jsx)(n.p,{children:"Changing the Colourspace (RGB, BGR, HSV, LUV, LAB, etc.)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'image_luv = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\r\ncv2.imshow("LUV Colour Space", image_luv)\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"OpenCV Image Operations",src:r(492710).Z+"",width:"1138",height:"493"})}),"\n",(0,i.jsx)(n.h3,{id:"image-blending",children:"Image Blending"}),"\n",(0,i.jsxs)(n.p,{children:["Alpha-blending two independent images ",(0,i.jsx)(n.code,{children:"img1"})," and ",(0,i.jsx)(n.code,{children:"img2"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"cv2.addWeighted(img1,alpha,img2,beta,gamma)\n"})}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"dst = \u03b1.img1 + \xdf.img2 + \u03b3"})}),"\n"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"alpha"}),"/ ",(0,i.jsx)(n.strong,{children:"beta"}),": Weight of the first and second array element"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"gamma"}),": Scalar added to each sum (usually makes resulting image darker)"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import cv2\r\n\r\nbackground = cv2.imread('resources/test-image.jpg')\r\nforeground = cv2.imread('processed/red_channel.jpg')\r\n\r\nblurred_background = cv2.GaussianBlur(background, (1,1), 0)\r\n\r\nblended_image = cv2.addWeighted(blurred_background, 0.2, foreground, 0.8, 0.0)\r\ncv2.imshow(\"Blended Image\", blended_image)\r\n\r\ncv2.waitKey(5000)\r\ncv2.destroyAllWindows()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"apply-filters",children:"Apply Filters"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"filter2D()"})," function is used to change the pixel intensity value of an image based on the surrounding pixel intensity values. This method can enhance or remove certain features of an image to create a new image."]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"resulting_image = cv2.filter2D(src, ddepth, kernel)"})}),"\n"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"src"}),": The source image matrix on which to apply the fitler."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ddepth"}),": It is the desirable depth of destination image. ",(0,i.jsx)(n.code,{children:"-1"})," represents that the resulting image will have same depth as the source image."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"kernel"}),": Kernel is the filter matrix applied on the image."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"OpenCV Image Operations",src:r(707088).Z+"",width:"2131",height:"1048"})}),"\n",(0,i.jsx)(n.h4,{id:"sharpening",children:"Sharpening"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import cv2\r\nimport numpy as np\r\n\r\nimage = cv2.imread(\'resources/test-image.jpg\')\r\n\r\n# Sharpening\r\nkernel = np.array([\r\n    [0, -1, 0],\r\n    [-1, 5, -1],\r\n    [0, -1, 0]\r\n])\r\n\r\nsharpened_image = cv2.filter2D(image, -1, kernel)\r\n\r\ncv2.imshow("Original Image", image)\r\ncv2.imshow("Sharpened Image", sharpened_image)\r\ncv2.imwrite("processed/sharpened_image.jpg", sharpened_image)\n'})}),"\n",(0,i.jsx)(n.h4,{id:"bluring",children:"Bluring"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'kernel = np.array([\r\n    [1, 1, 1],\r\n    [1, 1, 1],\r\n    [1, 1, 1]\r\n]) / 9\r\n\r\nblurred_image = cv2.filter2D(image, -1, kernel)\r\n\r\ncv2.imshow("Blurred Image", blurred_image)\r\ncv2.imwrite("processed/blurred_image.jpg", blurred_image)\n'})}),"\n",(0,i.jsx)(n.h4,{id:"outline-edge-detection",children:"Outline Edge Detection"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'kernel = np.array([\r\n    [-1, -1, -1],\r\n    [-1, 8, -1],\r\n    [-1, -1, -1]\r\n])\r\n\r\noutlined_image = cv2.filter2D(image, -1, kernel)\r\n\r\ncv2.imshow("Outlined Image", outlined_image)\r\ncv2.imwrite("processed/outlined_image.jpg", outlined_image)\n'})}),"\n",(0,i.jsx)(n.h4,{id:"embossing",children:"Embossing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'kernel = np.array([\r\n    [-2, -1, 0],\r\n    [-1, 1, 1],\r\n    [0, 1, 2]\r\n])\r\n\r\nembossed_image = cv2.filter2D(image, -1, kernel)\r\n\r\ncv2.imshow("Embossed Image", embossed_image)\r\ncv2.imwrite("processed/embossed_image.jpg", embossed_image)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"image-thresholding",children:"Image Thresholding"}),"\n",(0,i.jsxs)(n.p,{children:["For every pixel, the same threshold value is applied. If the pixel value is smaller than the threshold, it is set to 0, otherwise it is set to a maximum value. The first argument is the source image, which should be a ",(0,i.jsx)(n.strong,{children:"grayscale image"}),". The second argument is the threshold value which is used to classify the pixel values. The third argument is the maximum value which is assigned to pixel values exceeding the threshold. OpenCV provides different types of thresholding which is given by the fourth parameter of the function. Basic thresholding as described above is done by using the type ",(0,i.jsx)(n.code,{children:"cv.THRESH_BINARY"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"th, dst = cv2.threshold(image, thresh, maxValue, cv2.THRESH_BINARY)\n"})}),"\n",(0,i.jsx)(n.p,{children:"All simple thresholding types are:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"cv.THRESH_BINARY"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"cv.THRESH_BINARY_INV"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"cv.THRESH_TRUNC"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"cv.THRESH_TOZERO"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"cv.THRESH_TOZERO_INV"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"OpenCV Image Operations",src:r(13167).Z+"",width:"432",height:"288"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import cv2\r\n\r\nimage = cv2.imread(\'resources/test-image.jpg\', cv2.IMREAD_GRAYSCALE)\r\n\r\n# Set threshold and maxValue\r\nthresh = 127\r\nmaxValue = 255\r\n\r\nret, thresh_binary = cv2.threshold(image, thresh, maxValue, cv2.THRESH_BINARY)\r\ncv2.imshow("Threshold Binary", thresh_binary)\r\n\r\nret, thresh_binary_inv = cv2.threshold(image, thresh, maxValue, cv2.THRESH_BINARY_INV)\r\ncv2.imshow("Threshold Binary Inverted", thresh_binary_inv)\r\n\r\nret, thresh_trunc = cv2.threshold(image, thresh, maxValue, cv2.THRESH_TRUNC)\r\ncv2.imshow("Threshold Truncated", thresh_trunc)\r\n\r\nret, thresh_tozero = cv2.threshold(image, thresh, maxValue, cv2.THRESH_TOZERO)\r\ncv2.imshow("Threshold toZero", thresh_tozero)\r\n\r\nret, thresh_tozero_inv = cv2.threshold(image, thresh, maxValue, cv2.THRESH_TOZERO_INV)\r\ncv2.imshow("Threshold toZero Inverted", thresh_tozero_inv)\n'})}),"\n",(0,i.jsx)(n.h5,{id:"canny-edge-detection",children:"Canny Edge Detection"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"Canny( detected_edges, detected_edges, lowThreshold, lowThreshold*ratio, kernel_size )"})}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'canny_image_src = cv2.Canny(image, 85, 255)\r\ncanny_image = cv2.Canny(thresh_binary, 85, 255)\r\n\r\ncv2.imshow("Canny Image from Source", canny_image_src)\r\ncv2.imshow("Canny Image", canny_image)\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"OpenCV Image Operations",src:r(918365).Z+"",width:"2143",height:"1039"})})]})}function h(e={}){const{wrapper:n}={...(0,s.ah)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},603905:(e,n,r)=>{r.d(n,{ah:()=>o});var i=r(667294);function s(e,n,r){return n in e?Object.defineProperty(e,n,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[n]=r,e}function a(e,n){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),r.push.apply(r,i)}return r}function t(e){for(var n=1;n<arguments.length;n++){var r=null!=arguments[n]?arguments[n]:{};n%2?a(Object(r),!0).forEach((function(n){s(e,n,r[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):a(Object(r)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(r,n))}))}return e}function l(e,n){if(null==e)return{};var r,i,s=function(e,n){if(null==e)return{};var r,i,s={},a=Object.keys(e);for(i=0;i<a.length;i++)r=a[i],n.indexOf(r)>=0||(s[r]=e[r]);return s}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(i=0;i<a.length;i++)r=a[i],n.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(s[r]=e[r])}return s}var c=i.createContext({}),o=function(e){var n=i.useContext(c),r=n;return e&&(r="function"==typeof e?e(n):t(t({},n),e)),r},d={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},h=i.forwardRef((function(e,n){var r=e.components,s=e.mdxType,a=e.originalType,c=e.parentName,h=l(e,["components","mdxType","originalType","parentName"]),g=o(r),p=s,m=g["".concat(c,".").concat(p)]||g[p]||d[p]||a;return r?i.createElement(m,t(t({ref:n},h),{},{components:r})):i.createElement(m,t({ref:n},h))}));h.displayName="MDXCreateElement"},393098:(e,n,r)=>{r.d(n,{Z:()=>i});const i=r.p+"assets/images/opencv_image_operations_01-a64aee1b9b70226cd796dc73d3aa17f8.png"},117968:(e,n,r)=>{r.d(n,{Z:()=>i});const i=r.p+"assets/images/opencv_image_operations_02-47c0e90b9eebd411dd19e867c2ddca8e.png"},492710:(e,n,r)=>{r.d(n,{Z:()=>i});const i=r.p+"assets/images/opencv_image_operations_03-5c135d861c4213fad7a110811b50cc8b.png"},707088:(e,n,r)=>{r.d(n,{Z:()=>i});const i=r.p+"assets/images/opencv_image_operations_04-adc169f2946aa57255d297099c328ce1.png"},918365:(e,n,r)=>{r.d(n,{Z:()=>i});const i=r.p+"assets/images/opencv_image_operations_05-da81782d6fda470a54eb7b0650cbbdef.png"},557430:(e,n,r)=>{r.d(n,{Z:()=>i});const i=r.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-5a0b68587d9242bbb46a1f1aaab44216.jpg"},13167:(e,n,r)=>{r.d(n,{Z:()=>i});const i=r.p+"assets/images/threshold-ec028108d74a79505200fb6e3c134fb1.jpg"}}]);