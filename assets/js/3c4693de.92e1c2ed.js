"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[27776],{70702:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>d});var a=i(785893),r=i(603905);const s={sidebar_position:5060,slug:"2022-04-01",title:"Deep Audio",authors:"mpolinowski",tags:["Tensorflow","Machine Learning","Python"],description:"Deep Audio Classifier with Tensorflow"},t=void 0,o={id:"IoT-and-Machine-Learning/ML/2022-04-01-tensorflow-audio-classifier/index",title:"Deep Audio",description:"Deep Audio Classifier with Tensorflow",source:"@site/docs/IoT-and-Machine-Learning/ML/2022-04-01-tensorflow-audio-classifier/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2022-04-01-tensorflow-audio-classifier",slug:"/IoT-and-Machine-Learning/ML/2022-04-01-tensorflow-audio-classifier/2022-04-01",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-04-01-tensorflow-audio-classifier/2022-04-01",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2022-04-01-tensorflow-audio-classifier/index.md",tags:[{label:"Tensorflow",permalink:"/docs/tags/tensorflow"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Python",permalink:"/docs/tags/python"}],version:"current",sidebarPosition:5060,frontMatter:{sidebar_position:5060,slug:"2022-04-01",title:"Deep Audio",authors:"mpolinowski",tags:["Tensorflow","Machine Learning","Python"],description:"Deep Audio Classifier with Tensorflow"},sidebar:"tutorialSidebar",previous:{title:"Super Resolution with ESRGAN",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-04-02-pytorch-super-resolution/2022-04-02"},next:{title:"Yolo App - YOLOv5 Data Preparation",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-02-20--yolo-app-yolov5-data-prep/2022-02-20"}},l={},d=[{value:"Signal Processing with Tensorflow",id:"signal-processing-with-tensorflow",level:2},{value:"Project Setup",id:"project-setup",level:2},{value:"Data Loading",id:"data-loading",level:2},{value:"Creating the Dataset",id:"creating-the-dataset",level:2},{value:"Calculating Average Length of a Birdcall",id:"calculating-average-length-of-a-birdcall",level:2},{value:"Converting Data into a Spectrogram",id:"converting-data-into-a-spectrogram",level:2},{value:"Test Function",id:"test-function",level:3},{value:"Preparing a Testing &amp; Training Dataset",id:"preparing-a-testing--training-dataset",level:2},{value:"Training Set",id:"training-set",level:3},{value:"Testing Set",id:"testing-set",level:3},{value:"Build the Deep Learning Model",id:"build-the-deep-learning-model",level:2},{value:"Sequential Model",id:"sequential-model",level:3},{value:"Training the Model",id:"training-the-model",level:2},{value:"Making Predictions",id:"making-predictions",level:2},{value:"Putting the Model to Work",id:"putting-the-model-to-work",level:2},{value:"Test Function",id:"test-function-1",level:3},{value:"Data Preprocessing",id:"data-preprocessing",level:3},{value:"Running a Prediction",id:"running-a-prediction",level:3},{value:"Grouping Consecutive Hits",id:"grouping-consecutive-hits",level:3},{value:"Processing the RAW Data",id:"processing-the-raw-data",level:2},{value:"Export the Results",id:"export-the-results",level:3}];function c(e){const n={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.ah)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Victoria Harbour, Hongkong",src:i(423970).Z+"",width:"1500",height:"565"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#signal-processing-with-tensorflow",children:"Signal Processing with Tensorflow"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#project-setup",children:"Project Setup"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#data-loading",children:"Data Loading"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#creating-the-dataset",children:"Creating the Dataset"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#calculating-average-length-of-a-birdcall",children:"Calculating Average Length of a Birdcall"})}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"#converting-data-into-a-spectrogram",children:"Converting Data into a Spectrogram"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#test-function",children:"Test Function"})}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"#preparing-a-testing--training-dataset",children:"Preparing a Testing & Training Dataset"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#training-set",children:"Training Set"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#testing-set",children:"Testing Set"})}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"#build-the-deep-learning-model",children:"Build the Deep Learning Model"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#sequential-model",children:"Sequential Model"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#training-the-model",children:"Training the Model"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#making-predictions",children:"Making Predictions"})}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"#putting-the-model-to-work",children:"Putting the Model to Work"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#test-function-1",children:"Test Function"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#data-preprocessing",children:"Data Preprocessing"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#running-a-prediction",children:"Running a Prediction"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#grouping-consecutive-hits",children:"Grouping Consecutive Hits"})}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"#processing-the-raw-data",children:"Processing the RAW Data"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#export-the-results",children:"Export the Results"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://github.com/mpolinowski/signal-processing-tensorflow",children:"Github Repository"})}),"\n",(0,a.jsx)(n.h2,{id:"signal-processing-with-tensorflow",children:"Signal Processing with Tensorflow"}),"\n",(0,a.jsxs)(n.p,{children:["The Challenge is to build a Machine Learning model and ",(0,a.jsx)(n.a,{href:"https://www.kaggle.com/datasets/kenjee/z-by-hp-unlocked-challenge-3-signal-processing",children:"code to count the number of Capuchinbird calls within a given clip"}),". This can be done in a variety of ways and we would recommend that you do some research into ",(0,a.jsx)(n.a,{href:"https://www.kaggle.com/datasets/kenjee/z-by-hp-unlocked-challenge-3-signal-processing#where-to-start",children:"various methods of audio recognition"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"project-setup",children:"Project Setup"}),"\n",(0,a.jsxs)(n.p,{children:["The Dataset is provided on ",(0,a.jsx)(n.a,{href:"https://www.kaggle.com/datasets/kenjee/z-by-hp-unlocked-challenge-3-signal-processing",children:"kaggle.com"})," and contains of three sets of bird call recordings:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"raw (RAW Data)"}),"\n",(0,a.jsx)(n.li,{children:"positives (Cut out positive bird calls)"}),"\n",(0,a.jsx)(n.li,{children:"negatives (Cut out negatives)"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Copy them into your ",(0,a.jsx)(n.code,{children:"data"})," folder. Open a Jupyter Notebook and create a notebook called ",(0,a.jsx)(n.code,{children:"signal-processing"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"jupyter notebook\n"})}),"\n",(0,a.jsx)(n.p,{children:"Now we can install the Python dependencies from inside the notebook:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"!pip install tensorflow tensorflow-gpu tensorflow-io matplotlib\n"})}),"\n",(0,a.jsx)(n.p,{children:"Once installed import them into your notebook:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import os\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf \nimport tensorflow_io as tfio\n"})}),"\n",(0,a.jsx)(n.h2,{id:"data-loading",children:"Data Loading"}),"\n",(0,a.jsx)(n.p,{children:"To work with the recorded audio files we can select the corresponding data paths - in the example below we pick both an example file that contains the signal we are looking for and one that only comes with background noise:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"CAPUCHIN_FILE = os.path.join('data', 'positives', 'XC3776-3.wav')\nNOT_CAPUCHIN_FILE = os.path.join('data', 'negatives', 'afternoon-birds-song-in-forest-0.wav')\n"})}),"\n",(0,a.jsxs)(n.p,{children:["And read a single audio stream (mono) resampled to 16kHz from those files by feeding the ",(0,a.jsx)(n.code,{children:"filepath"}),"  into the following function:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"https://www.tensorflow.org/api_docs/python/tf/audio/decode_wav",children:"decode_wav"})," : Read single channel from stereo file"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"https://www.tensorflow.org/tutorials/audio/simple_audio",children:"squeeze"})," : Since all the data is single channel (mono), drop the ",(0,a.jsx)(n.code,{children:"channels"})," axis from array"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"https://www.tensorflow.org/io/api_docs/python/tfio/audio/resample",children:"resample"})," : Reduce audio data to 16kHz"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"def load_wav_16k_mono(filename):\n    # Load encoded wav file\n    file_contents = tf.io.read_file(filename)\n    # Decode wav (tensors by channels) \n    wav, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1)\n    # Removes trailing axis\n    wav = tf.squeeze(wav, axis=-1)\n    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n    # Goes from 44100Hz to 16000hz - amplitude of the audio signal\n    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n    return wav\n"})}),"\n",(0,a.jsx)(n.p,{children:"We can run the function over both the positive and negative file:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"wave = load_wav_16k_mono(CAPUCHIN_FILE)\nnwave = load_wav_16k_mono(NOT_CAPUCHIN_FILE)\n"})}),"\n",(0,a.jsx)(n.p,{children:"And overlay both waves in a plot:"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Tensorflow Audio Signal Classifier",src:i(788757).Z+"",width:"978",height:"396"})}),"\n",(0,a.jsx)(n.p,{children:"The blue plot represents the positive signal - the correct bird call. While the orange plot is a negative representing our baseline background noise. This now gives us a image representation of our audio that we can use to train our neural network with."}),"\n",(0,a.jsx)(n.h2,{id:"creating-the-dataset",children:"Creating the Dataset"}),"\n",(0,a.jsx)(n.p,{children:"First we need to define the path to all our positive and negative audio files:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"POS = os.path.join('data', 'positives')\nNEG = os.path.join('data', 'negatives')\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Now we can store all the data paths inside ",(0,a.jsx)(n.a,{href:"https://www.tensorflow.org/api_docs/python/tf/data/Dataset",children:"Tensorflow Datasets"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-Python",children:"pos = tf.data.Dataset.list_files(POS+'/*.wav')\nneg = tf.data.Dataset.list_files(NEG+'/*.wav')\n"})}),"\n",(0,a.jsx)(n.p,{children:"We can now label our data by adding ones and zeros to each filepath inside the dataset depending on whether it is a positive or negative sample:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"positives = tf.data.Dataset.zip((pos, tf.data.Dataset.from_tensor_slices(tf.ones(len(pos)))))\n\nnegatives = tf.data.Dataset.zip((neg, tf.data.Dataset.from_tensor_slices(tf.zeros(len(neg)))))\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Tensorflow Audio Signal Classifier",src:i(940523).Z+"",width:"980",height:"205"})}),"\n",(0,a.jsx)(n.p,{children:"After successfully labelling our data we can now merge everything into a single dataset:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"data = positives.concatenate(negatives)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"calculating-average-length-of-a-birdcall",children:"Calculating Average Length of a Birdcall"}),"\n",(0,a.jsx)(n.p,{children:"To be able to count bird calls in our RAW data we first have to know the average length of a single call. We can do this by taking all our parsed positive recordings and load them into their waveform:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"lengths = []\nfor file in os.listdir(os.path.join('data', 'positives')):\n    tensor_wave = load_wav_16k_mono(os.path.join('data', 'positives', file))\n    lengths.append(len(tensor_wave))\n"})}),"\n",(0,a.jsxs)(n.p,{children:["By appending each file length to the ",(0,a.jsx)(n.code,{children:"lengths"})," array we can now do some basic maths to calculate the ",(0,a.jsx)(n.strong,{children:"Min"}),", ",(0,a.jsx)(n.strong,{children:"Max"})," and ",(0,a.jsx)(n.strong,{children:"AVG"})," length of a positive birdcall:"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Tensorflow Audio Signal Classifier",src:i(220174).Z+"",width:"981",height:"473"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Tensorflow Audio Signal Classifier",src:i(615688).Z+"",width:"981",height:"304"})}),"\n",(0,a.jsxs)(n.p,{children:["This means that the average birdcall ist ",(0,a.jsx)(n.code,{children:"54156 / 16000 Hz = 3.38s"}),". And the calls are in between ",(0,a.jsx)(n.strong,{children:"Min"})," ",(0,a.jsx)(n.code,{children:"2s"})," and ",(0,a.jsx)(n.strong,{children:"Max"})," ",(0,a.jsx)(n.code,{children:"5s"})," in length."]}),"\n",(0,a.jsx)(n.h2,{id:"converting-data-into-a-spectrogram",children:"Converting Data into a Spectrogram"}),"\n",(0,a.jsx)(n.p,{children:"The following function takes an audio file and converts it to 16 kHz Mono. Since we know that the average call is about 3s in length we can limit our data to the first 48000 units of each parsed audio file."}),"\n",(0,a.jsxs)(n.p,{children:["Since our minimum file length was 32000 we need to make sure that every file is padded up with zeros to a length of 48000 using the ",(0,a.jsx)(n.code,{children:"tf.zeros"})," function:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"def preprocess(file_path, label):\n\t# Load files into 16kHz mono\n    wav = load_wav_16k_mono(file_path)\n    # Only read the first 3 secs\n    wav = wav[:48000]\n    # If file < 3s add zeros\n    zero_padding = tf.zeros([48000] - tf.shape(wav), dtype=tf.float32)\n    wav = tf.concat([zero_padding, wav],0)\n    # Use Short-time Fourier Transform\n    spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)\n    # Convert to absolut values (no negatives)\n    spectrogram = tf.abs(spectrogram)\n    # Add channel dimension (needed by CNN later)\n    spectrogram = tf.expand_dims(spectrogram, axis=2)\n    return spectrogram, label\n"})}),"\n",(0,a.jsxs)(n.p,{children:["To create the spectrogram we can use the ",(0,a.jsx)(n.a,{href:"https://www.tensorflow.org/api_docs/python/tf/signal/stft",children:"Short-time Fourier Transformation"})," provided by Tensorflow."]}),"\n",(0,a.jsxs)(n.p,{children:["_",(0,a.jsx)(n.strong,{children:"Note"}),": the dimensions of each spectrogram are ",(0,a.jsx)(n.code,{children:"1491,257,1"})," - which represents the height and width of the image representation + an additional channel that was added with the ",(0,a.jsx)(n.code,{children:"tf.expand_dims"})," function. This channel does not hold any information here but is expected to exists by the Tensorflow model we are going to use later."]}),"\n",(0,a.jsx)(n.h3,{id:"test-function",children:"Test Function"}),"\n",(0,a.jsxs)(n.p,{children:["Pick a random audio file from the ",(0,a.jsx)(n.code,{children:"positives"})," dataset:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"filepath, label = positives.shuffle(buffer_size=10000).as_numpy_iterator().next()\n"})}),"\n",(0,a.jsx)(n.p,{children:"And run it through the pre-processing function:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"spectrogram, label = preprocess(filepath, label)\n"})}),"\n",(0,a.jsx)(n.p,{children:"Let's see what the spectrum actually looks like:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"plt.figure(figsize=(30,20))\nplt.imshow(tf.transpose(spectrogram)[0])\nplt.show()\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Tensorflow Audio Signal Classifier",src:i(776139).Z+"",width:"985",height:"304"})}),"\n",(0,a.jsx)(n.p,{children:"For comparison, this is the spectrogram of a negative sample:"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Tensorflow Audio Signal Classifier",src:i(791826).Z+"",width:"982",height:"304"})}),"\n",(0,a.jsx)(n.p,{children:"Now all we have to do, is to train a Tensorflow model that is able to distinguish between those two image representation of our audio data."}),"\n",(0,a.jsx)(n.h2,{id:"preparing-a-testing--training-dataset",children:"Preparing a Testing & Training Dataset"}),"\n",(0,a.jsxs)(n.p,{children:["We already wrapped all of our data - positives and negatives - into the ",(0,a.jsx)(n.code,{children:"data"})," variable. So now we can map through this data and feed each file to the ",(0,a.jsx)(n.code,{children:"preprocessing"})," function."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"data = data.map(preprocess)\ndata = data.cache()\ndata = data.shuffle(buffer_size=1000)\ndata = data.batch(16)\ndata = data.prefetch(8)\n"})}),"\n",(0,a.jsxs)(n.p,{children:["To optimize the training we will first shuffle the data and limit the amount of simultaneous files being processed to ",(0,a.jsx)(n.code,{children:"16"})," and the pre-fetch to ",(0,a.jsx)(n.code,{children:"8"})," files. This can be increased if your CPU/GPU can handle it."]}),"\n",(0,a.jsx)(n.p,{children:"To split our data into training and testing data we can run:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"train = data.take(36)\ntest = data.skip(36).take(15)\n"})}),"\n",(0,a.jsxs)(n.p,{children:["With ",(0,a.jsx)(n.code,{children:"len(data) = 51"})," the split ration is close to ",(0,a.jsx)(n.strong,{children:"70:30"}),". We can verify the content of each array by:"]}),"\n",(0,a.jsx)(n.h3,{id:"training-set",children:"Training Set"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"samples, labels = train.as_numpy_iterator().next\nlabels\n\narray([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n      dtype=float32)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"testing-set",children:"Testing Set"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"samples, labels = test.as_numpy_iterator().next()\nlabels\n\narray([0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0.],\n      dtype=float32)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"build-the-deep-learning-model",children:"Build the Deep Learning Model"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Dense, Flatten\n"})}),"\n",(0,a.jsx)(n.h3,{id:"sequential-model",children:"Sequential Model"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"model = Sequential()\nmodel.add(Conv2D(8, (3,3), activation='relu', input_shape=(1491,257,1)))\nmodel.add(Conv2D(8, (3,3), activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"model.compile('Adam', loss='BinaryCrossentropy', metrics=[tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.em,{children:"model.summary()"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'Model: "sequential"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 1489, 255, 8)      80        \n                                                                 \n conv2d_1 (Conv2D)           (None, 1487, 253, 8)      584       \n                                                                 \n flatten (Flatten)           (None, 3009688)           0         \n                                                                 \n dense (Dense)               (None, 32)                96310048  \n                                                                 \n dense_1 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 96,310,745\nTrainable params: 96,310,745\nNon-trainable params: 0\n_________________________________________________________________\n'})}),"\n",(0,a.jsx)(n.h2,{id:"training-the-model",children:"Training the Model"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"hist = model.fit(train, epochs=4, validation_data=test)\n"})}),"\n",(0,a.jsx)(n.p,{children:"Already after 4 epochs we are starting to get getting a precision and recall value around 100%:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"Epoch 4/4\n36/36 [==============================] - 6s 177ms/step - loss: 0.0091 - recall: 0.9870 - precision: 1.0000 - val_loss: 0.0157 - val_recall: 0.9844 - val_precision: 1.0000\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Tensorflow Audio Signal Classifier",src:i(335344).Z+"",width:"980",height:"436"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Tensorflow Audio Signal Classifier",src:i(420088).Z+"",width:"984",height:"437"})}),"\n",(0,a.jsx)(n.h2,{id:"making-predictions",children:"Making Predictions"}),"\n",(0,a.jsx)(n.p,{children:"We can now use our testing data - the 16 files we excluded from the training dataset - and use them to run a prediction against:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"X_test, y_test = test.as_numpy_iterator().next()\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Tensorflow Audio Signal Classifier",src:i(460828).Z+"",width:"980",height:"251"})}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"yhat"})," parameter gives us the probabilities of a file being the recording of a birdcall:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"yhat = model.predict(X_test)\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Every value that is close to ",(0,a.jsx)(n.code,{children:"1.00000000e+00"})," is a recording that, with a very high probability, contains the birdcall we are looking for:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"array([[9.99920249e-01],\n       [3.07901189e-08],\n       [1.00000000e+00],\n       [2.84484369e-25],\n       [9.62874225e-09],\n       [9.99707282e-01],\n       [1.53503152e-05],\n       [2.34340667e-03],\n       [8.77155067e-28],\n       [1.00000000e+00],\n       [1.14530545e-08],\n       [5.61734714e-06],\n       [3.27430301e-20],\n       [1.99461836e-11],\n       [0.00000000e+00],\n       [0.00000000e+00]], dtype=float32)\n"})}),"\n",(0,a.jsx)(n.p,{children:"To make this more readable we can make this - within a selected range of confidence - binary:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"yhat = [1 if prediction > 0.5 else 0 for prediction in yhat]\n"})}),"\n",(0,a.jsx)(n.p,{children:"Which looks like this:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"[1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n"})}),"\n",(0,a.jsxs)(n.p,{children:["We can verify that this result is correct by calculating the sum of ",(0,a.jsx)(n.code,{children:"yhat"})," and comparing it with the sum of ",(0,a.jsx)(n.code,{children:"y_test"})," - which are our labels that we set to ",(0,a.jsx)(n.code,{children:"1"})," for positives and ",(0,a.jsx)(n.code,{children:"0"})," for negatives:"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Tensorflow Audio Signal Classifier",src:i(805108).Z+"",width:"982",height:"252"})}),"\n",(0,a.jsx)(n.p,{children:"And it seems that we were able to identify all 4 of them! But we should also rule out false-positives/negatives by comparing both arrays:"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Tensorflow Audio Signal Classifier",src:i(827065).Z+"",width:"982",height:"228"})}),"\n",(0,a.jsx)(n.p,{children:"And we have a match - all the zeros and ones are where they belong."}),"\n",(0,a.jsx)(n.h2,{id:"putting-the-model-to-work",children:"Putting the Model to Work"}),"\n",(0,a.jsxs)(n.p,{children:["We now have a well performing model that is able to recognize bird calls. So we can now let it loose on the RAW forest recordings in the ",(0,a.jsx)(n.code,{children:"data/raw"})," directory. First we need a function that loads the recordings - that are this time in ",(0,a.jsx)(n.code,{children:"mp3"})," containers. So instead of the ",(0,a.jsx)(n.code,{children:"tf.io.read_file"})," function we now need to use ",(0,a.jsx)(n.code,{children:"tfio.audio.AudioIOTensor"}),". We will handle the stereo recording by adding both channels into a single channel and divide every value by ",(0,a.jsx)(n.code,{children:"2"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"def load_mp3_16k_mono(filename):\n    # Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio.\n    res = tfio.audio.AudioIOTensor(filename)\n    # Convert to tensor and combine channels \n    tensor = res.to_tensor()\n    tensor = tf.math.reduce_sum(tensor, axis=1) / 2 \n    # Extract sample rate and cast\n    sample_rate = res.rate\n    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n    # Resample to 16 kHz\n    wav = tfio.audio.resample(tensor, rate_in=sample_rate, rate_out=16000)\n    return wav\n"})}),"\n",(0,a.jsx)(n.h3,{id:"test-function-1",children:"Test Function"}),"\n",(0,a.jsx)(n.p,{children:"We can test the loading function by loading a single file:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"mp3 = os.path.join('data', 'Forest Recordings', 'recording_00.mp3')\nwav = load_mp3_16k_mono(mp3)\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Since the RAW recordings are much longer than the neatly parsed training recordings that were already cut down to the length of a typical bird call we can now slice up the file into short sequences of the same length ",(0,a.jsx)(n.code,{children:"48000"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"audio_slices = tf.keras.utils.timeseries_dataset_from_array(wav, wav, sequence_length=48000, sequence_stride=48000, batch_size=1)\n"})}),"\n",(0,a.jsxs)(n.p,{children:["We can check the amount of slices that were generated with ",(0,a.jsx)(n.code,{children:"len(audio_slices)"})," - and in case of the file ",(0,a.jsx)(n.code,{children:"recording_00.mp3"})," we end up with ",(0,a.jsx)(n.strong,{children:"60"})," sequences."]}),"\n",(0,a.jsx)(n.h3,{id:"data-preprocessing",children:"Data Preprocessing"}),"\n",(0,a.jsx)(n.p,{children:"The pre-processing step is identical to before. We can pick the first sequence out of the 60 slices that were generated, make sure that it is the correct length (which of course in case of the first slice - it will be). And then continue generating the spectrogram:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"def preprocess_mp3(sample, index):\n    sample = sample[0]\n    zero_padding = tf.zeros([48000] - tf.shape(sample), dtype=tf.float32)\n    wav = tf.concat([zero_padding, sample],0)\n    spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)\n    spectrogram = tf.abs(spectrogram)\n    spectrogram = tf.expand_dims(spectrogram, axis=2)\n    return spectrogram\n"})}),"\n",(0,a.jsx)(n.p,{children:"Now we can create the audio slices and run them through the preprocessing function:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"audio_slices = tf.keras.utils.timeseries_dataset_from_array(wav, wav, sequence_length=48000, sequence_stride=48000, batch_size=1)\naudio_slices = audio_slices.map(preprocess_mp3)\naudio_slices = audio_slices.batch(64)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"running-a-prediction",children:"Running a Prediction"}),"\n",(0,a.jsx)(n.p,{children:"With the 60 slices in place we can now run our prediction model against them - this time I will bump up the confidence to 90% to make sure we don't catch any background noise:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"yhat = model.predict(audio_slices)\nyhat = [1 if prediction > 0.9 else 0 for prediction in yhat]\n"})}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"yhat"})," variable now returns 60 zeros and ones depending on wether the tested sequence contained a bird call or not:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"len(yhat)\n60\n\nyhat\n[0,0,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0]\n"})}),"\n",(0,a.jsx)(n.p,{children:"But we can see that there are consecutive detections that are most likely caused by us slicing a single bird call into two. Currently we would count't 8 bird calls:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"tf.math.reduce_sum(yhat)\n<tf.Tensor: shape=(), dtype=int32, numpy=8>\n"})}),"\n",(0,a.jsx)(n.h3,{id:"grouping-consecutive-hits",children:"Grouping Consecutive Hits"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from itertools import groupby\n\nyhat = [key for key, group in groupby(yhat)]\ncalls = tf.math.reduce_sum(yhat).numpy()\n"})}),"\n",(0,a.jsxs)(n.p,{children:["This reduces the number of detections to 5 - which we can confirm by listening to the ",(0,a.jsx)(n.code,{children:"recording_00.mp3"})," file and counting using finger technology:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"calls\n5\n"})}),"\n",(0,a.jsx)(n.h2,{id:"processing-the-raw-data",children:"Processing the RAW Data"}),"\n",(0,a.jsxs)(n.p,{children:["Now with everything in place we can loop through every file in the ",(0,a.jsx)(n.code,{children:"raw"})," directory and count our detections:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"results = {}\nfor file in os.listdir(os.path.join('data', 'raw')):\n    FILEPATH = os.path.join('data','raw', file)\n    \n    wav = load_mp3_16k_mono(FILEPATH)\n    audio_slices = tf.keras.utils.timeseries_dataset_from_array(wav, wav, sequence_length=48000, sequence_stride=48000, batch_size=1)\n    audio_slices = audio_slices.map(preprocess_mp3)\n    audio_slices = audio_slices.batch(64)\n    \n    yhat = model.predict(audio_slices)\n    \n    results[file] = yhat\n"})}),"\n",(0,a.jsx)(n.p,{children:"This returns predictions for every slice in every recording:"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Tensorflow Audio Signal Classifier",src:i(959731).Z+"",width:"983",height:"474"})}),"\n",(0,a.jsx)(n.p,{children:"Again we can make this more readable by turning everything with a 99% confidence into ones and the rest into zeros:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class_preds = {}\nfor file, logits in results.items():\n    class_preds[file] = [1 if prediction > 0.99 else 0 for prediction in logits]\nclass_preds\n"})}),"\n",(0,a.jsx)(n.p,{children:"And group all consecutive detections to make sure we don't double-count them:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"postprocessed = {}\nfor file, scores in class_preds.items():\n    postprocessed[file] = tf.math.reduce_sum([key for key, group in groupby(scores)]).numpy()\npostprocessed\n"})}),"\n",(0,a.jsx)(n.p,{children:"Now we end up with the sum of all calls inside each file:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"{'recording_00.mp3': 5,\n 'recording_01.mp3': 0,\n 'recording_02.mp3': 0,\n 'recording_03.mp3': 0,\n 'recording_04.mp3': 4,\n 'recording_05.mp3': 0,\n 'recording_06.mp3': 5,\n 'recording_07.mp3': 2,\n 'recording_08.mp3': 25,\n 'recording_09.mp3': 0,\n 'recording_10.mp3': 5,\n 'recording_11.mp3': 3,\n 'recording_12.mp3': 0,\n 'recording_13.mp3': 0,\n 'recording_14.mp3': 0,\n 'recording_15.mp3': 2,\n 'recording_17.mp3': 3,\n 'recording_18.mp3': 5,\n 'recording_19.mp3': 0,\n 'recording_20.mp3': 0,\n 'recording_21.mp3': 1,\n 'recording_22.mp3': 2,\n 'recording_23.mp3': 5,\n 'recording_24.mp3': 0,\n 'recording_25.mp3': 16,\n 'recording_26.mp3': 2,\n 'recording_27.mp3': 0,\n 'recording_28.mp3': 16,\n 'recording_29.mp3': 0,\n 'recording_30.mp3': 2,\n 'recording_31.mp3': 1,\n 'recording_32.mp3': 2,\n 'recording_34.mp3': 4,\n 'recording_35.mp3': 0,\n 'recording_36.mp3': 0,\n 'recording_37.mp3': 3,\n 'recording_38.mp3': 1,\n 'recording_39.mp3': 9,\n 'recording_40.mp3': 1,\n 'recording_41.mp3': 0,\n 'recording_42.mp3': 0,\n 'recording_43.mp3': 5,\n 'recording_44.mp3': 1,\n 'recording_45.mp3': 3,\n 'recording_46.mp3': 17,\n 'recording_47.mp3': 16,\n 'recording_48.mp3': 4,\n 'recording_49.mp3': 0,\n 'recording_51.mp3': 3,\n 'recording_52.mp3': 0,\n 'recording_53.mp3': 0,\n 'recording_54.mp3': 3,\n 'recording_55.mp3': 0,\n 'recording_56.mp3': 16,\n 'recording_57.mp3': 3,\n 'recording_58.mp3': 0,\n 'recording_59.mp3': 15,\n 'recording_60.mp3': 4,\n 'recording_61.mp3': 11,\n 'recording_62.mp3': 0,\n 'recording_63.mp3': 17,\n 'recording_64.mp3': 2,\n 'recording_65.mp3': 5,\n 'recording_66.mp3': 0,\n 'recording_16.mp3': 5,\n 'recording_33.mp3': 0,\n 'recording_50.mp3': 0,\n 'recording_67.mp3': 0,\n 'recording_68.mp3': 1,\n 'recording_69.mp3': 1,\n 'recording_70.mp3': 4,\n 'recording_71.mp3': 5,\n 'recording_72.mp3': 4,\n 'recording_73.mp3': 0,\n 'recording_74.mp3': 0,\n 'recording_75.mp3': 1,\n 'recording_76.mp3': 0,\n 'recording_77.mp3': 3,\n 'recording_78.mp3': 10,\n 'recording_79.mp3': 0,\n 'recording_80.mp3': 1,\n 'recording_81.mp3': 5,\n 'recording_82.mp3': 0,\n 'recording_83.mp3': 0,\n 'recording_84.mp3': 16,\n 'recording_85.mp3': 0,\n 'recording_86.mp3': 17,\n 'recording_87.mp3': 24,\n 'recording_88.mp3': 0,\n 'recording_89.mp3': 5,\n 'recording_90.mp3': 0,\n 'recording_91.mp3': 0,\n 'recording_92.mp3': 0,\n 'recording_93.mp3': 5,\n 'recording_94.mp3': 3,\n 'recording_95.mp3': 4,\n 'recording_96.mp3': 1,\n 'recording_97.mp3': 4,\n 'recording_98.mp3': 21,\n 'recording_99.mp3': 5}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"export-the-results",children:"Export the Results"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"with open('results.csv', 'w', newline='') as f:\n    writer = csv.writer(f, delimiter=',')\n    writer.writerow(['recording', 'capuchin_calls'])\n    for key, value in postprocessed.items():\n        writer.writerow([key, value])\n"})})]})}function h(e={}){const{wrapper:n}={...(0,r.ah)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},603905:(e,n,i)=>{i.d(n,{ah:()=>d});var a=i(667294);function r(e,n,i){return n in e?Object.defineProperty(e,n,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[n]=i,e}function s(e,n){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),i.push.apply(i,a)}return i}function t(e){for(var n=1;n<arguments.length;n++){var i=null!=arguments[n]?arguments[n]:{};n%2?s(Object(i),!0).forEach((function(n){r(e,n,i[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):s(Object(i)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(i,n))}))}return e}function o(e,n){if(null==e)return{};var i,a,r=function(e,n){if(null==e)return{};var i,a,r={},s=Object.keys(e);for(a=0;a<s.length;a++)i=s[a],n.indexOf(i)>=0||(r[i]=e[i]);return r}(e,n);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(a=0;a<s.length;a++)i=s[a],n.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(r[i]=e[i])}return r}var l=a.createContext({}),d=function(e){var n=a.useContext(l),i=n;return e&&(i="function"==typeof e?e(n):t(t({},n),e)),i},c={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},h=a.forwardRef((function(e,n){var i=e.components,r=e.mdxType,s=e.originalType,l=e.parentName,h=o(e,["components","mdxType","originalType","parentName"]),p=d(i),g=r,u=p["".concat(l,".").concat(g)]||p[g]||c[g]||s;return i?a.createElement(u,t(t({ref:n},h),{},{components:i})):a.createElement(u,t({ref:n},h))}));h.displayName="MDXCreateElement"},788757:(e,n,i)=>{i.d(n,{Z:()=>a});const a=i.p+"assets/images/Tensorflow_Audio_Signal_Classifier_01-9099cae18ea318d913deb288536936c2.png"},940523:(e,n,i)=>{i.d(n,{Z:()=>a});const a=i.p+"assets/images/Tensorflow_Audio_Signal_Classifier_02-38f5644096c17c03dd63c34e9e7d20c6.png"},220174:(e,n,i)=>{i.d(n,{Z:()=>a});const a=i.p+"assets/images/Tensorflow_Audio_Signal_Classifier_03-3d5becd95368c543379396673698f8fa.png"},615688:(e,n,i)=>{i.d(n,{Z:()=>a});const a=i.p+"assets/images/Tensorflow_Audio_Signal_Classifier_04-24d83d821ae7c9435921faca625b02a0.png"},776139:(e,n,i)=>{i.d(n,{Z:()=>a});const a=i.p+"assets/images/Tensorflow_Audio_Signal_Classifier_05-0793a83ec3cb42c4e99b53d967f27d5c.png"},791826:(e,n,i)=>{i.d(n,{Z:()=>a});const a=i.p+"assets/images/Tensorflow_Audio_Signal_Classifier_06-7b30d9ce088cdf7c33271b021e2e7553.png"},335344:(e,n,i)=>{i.d(n,{Z:()=>a});const a=i.p+"assets/images/Tensorflow_Audio_Signal_Classifier_07-637f322258cd342f7752ff6a15b2b8fa.png"},420088:(e,n,i)=>{i.d(n,{Z:()=>a});const a=i.p+"assets/images/Tensorflow_Audio_Signal_Classifier_08-d226b68924dddb2fa409188b2b389c32.png"},460828:(e,n,i)=>{i.d(n,{Z:()=>a});const a=i.p+"assets/images/Tensorflow_Audio_Signal_Classifier_09-273cc1be5ce851dc0821d7994bad2089.png"},805108:(e,n,i)=>{i.d(n,{Z:()=>a});const a=i.p+"assets/images/Tensorflow_Audio_Signal_Classifier_10-109f59222cc37bb53ab05515e1ae9b96.png"},827065:(e,n,i)=>{i.d(n,{Z:()=>a});const a=i.p+"assets/images/Tensorflow_Audio_Signal_Classifier_11-3af10ccca631fc66050260fc7b2ce559.png"},959731:(e,n,i)=>{i.d(n,{Z:()=>a});const a=i.p+"assets/images/Tensorflow_Audio_Signal_Classifier_12-65d58afdadefd9af5980968181fe5207.png"},423970:(e,n,i)=>{i.d(n,{Z:()=>a});const a=i.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-23d027067cc9016279f834178a642545.jpg"}}]);