"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[92128],{3905:(e,n,t)=>{t.d(n,{Zo:()=>g,kt:()=>d});var a=t(67294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function c(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=a.createContext({}),r=function(e){var n=a.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},g=function(e){var n=r(e.components);return a.createElement(s.Provider,{value:n},e.children)},m={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},p=a.forwardRef((function(e,n){var t=e.components,i=e.mdxType,o=e.originalType,s=e.parentName,g=c(e,["components","mdxType","originalType","parentName"]),p=r(t),d=i,_=p["".concat(s,".").concat(d)]||p[d]||m[d]||o;return t?a.createElement(_,l(l({ref:n},g),{},{components:t})):a.createElement(_,l({ref:n},g))}));function d(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var o=t.length,l=new Array(o);l[0]=p;var c={};for(var s in n)hasOwnProperty.call(n,s)&&(c[s]=n[s]);c.originalType=e,c.mdxType="string"==typeof e?e:i,l[1]=c;for(var r=2;r<o;r++)l[r]=t[r];return a.createElement.apply(null,l)}return a.createElement.apply(null,t)}p.displayName="MDXCreateElement"},61638:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>l,default:()=>m,frontMatter:()=>o,metadata:()=>c,toc:()=>r});var a=t(87462),i=(t(67294),t(3905));const o={sidebar_position:4080,slug:"2023-10-01",title:"DLIB Face Recognition",authors:"mpolinowski",tags:["Python","Machine Learning"],description:"Detect faces in images and compare their feature vector to known entities"},l=void 0,c={unversionedId:"IoT-and-Machine-Learning/ML/2023-10-01--delib-face-detection/index",id:"IoT-and-Machine-Learning/ML/2023-10-01--delib-face-detection/index",title:"DLIB Face Recognition",description:"Detect faces in images and compare their feature vector to known entities",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-10-01--delib-face-detection/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-10-01--delib-face-detection",slug:"/IoT-and-Machine-Learning/ML/2023-10-01--delib-face-detection/2023-10-01",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-10-01--delib-face-detection/2023-10-01",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-10-01--delib-face-detection/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"}],version:"current",sidebarPosition:4080,frontMatter:{sidebar_position:4080,slug:"2023-10-01",title:"DLIB Face Recognition",authors:"mpolinowski",tags:["Python","Machine Learning"],description:"Detect faces in images and compare their feature vector to known entities"},sidebar:"tutorialSidebar",previous:{title:"Machine Learning",permalink:"/docs/category/machine-learning"},next:{title:"Audio Classification with Computer Vision",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-09-23--yolo8-listen/2023-09-23"}},s={},r=[{value:"Detect Face Location (CPU)",id:"detect-face-location-cpu",level:2},{value:"Crop Location",id:"crop-location",level:3},{value:"Get all the Training Images",id:"get-all-the-training-images",level:3},{value:"Face Recognition",id:"face-recognition",level:2},{value:"Compare Faces",id:"compare-faces",level:3},{value:"Test Image 1",id:"test-image-1",level:4},{value:"Test Image 2",id:"test-image-2",level:4},{value:"Test Image 3",id:"test-image-3",level:4},{value:"Test Image 4",id:"test-image-4",level:4},{value:"Test Image 5",id:"test-image-5",level:4},{value:"Test Image 6",id:"test-image-6",level:4},{value:"Detect Face Location (GPU + Batch Processing)",id:"detect-face-location-gpu--batch-processing",level:2},{value:"Draw Bounding Boxes",id:"draw-bounding-boxes",level:2},{value:"Save Feature Vector",id:"save-feature-vector",level:2}],g={toc:r};function m(e){let{components:n,...o}=e;return(0,i.kt)("wrapper",(0,a.Z)({},g,o,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"TST, Hongkong",src:t(27170).Z,width:"1500",height:"620"})),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#detect-face-location-cpu"},"Detect Face Location (CPU)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#crop-location"},"Crop Location")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#get-all-the-training-images"},"Get all the Training Images")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#face-recognition"},"Face Recognition"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#compare-faces"},"Compare Faces"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#test-image-1"},"Test Image 1")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#test-image-2"},"Test Image 2")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#test-image-3"},"Test Image 3")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#test-image-4"},"Test Image 4")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#test-image-5"},"Test Image 5")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#test-image-6"},"Test Image 6")))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#detect-face-location-gpu--batch-processing"},"Detect Face Location (GPU + Batch Processing)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#draw-bounding-boxes"},"Draw Bounding Boxes")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#save-feature-vector"},"Save Feature Vector"))),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/yolo-listen"},"Github Repository"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("a",{parentName:"p",href:"https://github.com/ageitgey/face_recognition"},"Face Recognition")))),(0,i.kt)("p",null,"Recognize and manipulate faces from Python or from the command line with the world's simplest face recognition library. Built using ",(0,i.kt)("a",{parentName:"p",href:"http://dlib.net/"},"dlib"),"'s state-of-the-art face recognition built with deep learning. The model has an accuracy of 99.38% on the ",(0,i.kt)("a",{parentName:"p",href:"http://vis-www.cs.umass.edu/lfw/"},"Labeled Faces in the Wild")," benchmark."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"!pip install face_recognition\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import face_recognition\nimport numpy as np\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'img_bobby ="faces/bobbie_w_draper.jpg"\nimg_jim ="faces/jim_holden.jpg"\nimg_amos ="faces/amos_burton.jpg"\nimg_camina ="faces/camina_drummer.jpg"\nimg_naomi ="faces/naomi_nagata.jpg"\nimg_chrisjen ="faces/chrisjen_avasarala.jpg"\n\nimage_path = img_bobby\n')),(0,i.kt)("h2",{id:"detect-face-location-cpu"},"Detect Face Location (CPU)"),(0,i.kt)("p",null,"Re-run the following steps for all training images above:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"image = face_recognition.load_image_file(image_path)\nface_locations = face_recognition.face_locations(image)\n")),(0,i.kt)("h3",{id:"crop-location"},"Crop Location"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import cv2 as cv\nimport matplotlib.pyplot as plt\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"img = cv.imread(image_path)\nimg = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"plt.imshow(img)\nplt.axis('off')\n\nfor face_location in face_locations:  \n    plt.plot(face_location[3], face_location[0], 'ro') \n    plt.plot(face_location[1], face_location[0], 'r+')     \n    plt.plot(face_location[3], face_location[2], 'bo')\n    plt.plot(face_location[1], face_location[2], 'b+')\n\nplt.show()\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(2202).Z,width:"373",height:"389"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"for face_location in face_locations: \n    x1, y1 = face_location[3], face_location[2]\n    x2, y2 = face_location[1], face_location[2]\n    x3, y3 = face_location[1], face_location[0]\n    x4, y4 = face_location[3], face_location[0]\n\ntop_left_x = min([x1,x2,x3,x4])\ntop_left_y = min([y1,y2,y3,y4])\nbot_right_x = max([x1,x2,x3,x4])\nbot_right_y = max([y1,y2,y3,y4])\n\ncropped_image = img[top_left_y:bot_right_y, top_left_x:bot_right_x]\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"plt.imshow(cropped_image)\nplt.axis('off')\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"(-0.5, 267.5, 266.5, -0.5)\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(22473).Z,width:"390",height:"389"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"cv.imwrite('faces/cut/bobbie_w_draper.jpg', cv.cvtColor(cropped_image, cv.COLOR_RGB2BGR))\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"True\n")),(0,i.kt)("h3",{id:"get-all-the-training-images"},"Get all the Training Images"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"bobby_train = face_recognition.load_image_file(img_bobby)\njim_train = face_recognition.load_image_file(img_jim)\namos_train = face_recognition.load_image_file(img_amos)\ncamina_train = face_recognition.load_image_file(img_camina)\nnaomi_train = face_recognition.load_image_file(img_naomi)\nchrisjen_train = face_recognition.load_image_file(img_chrisjen)\n\nbobby_encoding = face_recognition.face_encodings(bobby_train)[0]\njim_encoding = face_recognition.face_encodings(jim_train)[0]\namos_encoding = face_recognition.face_encodings(amos_train)[0]\ncamina_encoding = face_recognition.face_encodings(camina_train)[0]\nnaomi_encoding = face_recognition.face_encodings(naomi_train)[0]\nchrisjen_encoding = face_recognition.face_encodings(chrisjen_train)[0]\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from glob import glob\n\ncropped_images = glob('./faces/cut/*.jpg')\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"plt.figure(figsize=(12, 8))\nplt.suptitle('Training Images')\n\nax = plt.subplot(2, 3, 1)\nimg_path = cropped_images[0]\nimg_title = 'face: ' + cropped_images[0][12:-4]\nplt.title(img_title, fontsize='medium')\nimage = plt.imread(img_path)\nplt.imshow(image, cmap=plt.cm.binary)\n\nax = plt.subplot(2, 3, 2)\nimg_path = cropped_images[1]\nimg_title = 'face: ' + cropped_images[1][12:-4]\nplt.title(img_title, fontsize='medium')\nimage = plt.imread(img_path)\nplt.imshow(image, cmap=plt.cm.binary)\n\nax = plt.subplot(2, 3, 3)\nimg_path = cropped_images[2]\nimg_title = 'face: ' + cropped_images[2][12:-4]\nplt.title(img_title, fontsize='medium')\nimage = plt.imread(img_path)\nplt.imshow(image, cmap=plt.cm.binary)\n\nax = plt.subplot(2, 3, 4)\nimg_path = cropped_images[3]\nimg_title = 'face: ' + cropped_images[3][12:-4]\nplt.title(img_title, fontsize='medium')\nimage = plt.imread(img_path)\nplt.imshow(image, cmap=plt.cm.binary)\n\nax = plt.subplot(2, 3, 5)\nimg_path = cropped_images[4]\nimg_title = 'face: ' + cropped_images[4][12:-4]\nplt.title(img_title, fontsize='medium')\nimage = plt.imread(img_path)\nplt.imshow(image, cmap=plt.cm.binary)\n\nax = plt.subplot(2, 3, 6)\nimg_path = cropped_images[5]\nimg_title = 'face: ' + cropped_images[5][12:-4]\nplt.title(img_title, fontsize='medium')\nimage = plt.imread(img_path)\nplt.imshow(image, cmap=plt.cm.binary)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"<matplotlib.image.AxesImage at 0x7f22cb783430>\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(68708).Z,width:"986",height:"737"})),(0,i.kt)("h2",{id:"face-recognition"},"Face Recognition"),(0,i.kt)("p",null,'Loading a bunch of test images with "unknown" faces:'),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'test_image1 = face_recognition.load_image_file("faces/test/unknown_01.jpg")\ntest_image2 = face_recognition.load_image_file("faces/test/unknown_02.jpg")\ntest_image3 = face_recognition.load_image_file("faces/test/unknown_03.jpg")\ntest_image4 = face_recognition.load_image_file("faces/test/unknown_04.jpg")\ntest_image5 = face_recognition.load_image_file("faces/test/unknown_05.jpg")\ntest_image6 = face_recognition.load_image_file("faces/test/unknown_06.jpg")\n\ntest1_encoding = face_recognition.face_encodings(test_image1)\ntest2_encoding = face_recognition.face_encodings(test_image2)\ntest3_encoding = face_recognition.face_encodings(test_image3)\ntest4_encoding = face_recognition.face_encodings(test_image4)\ntest5_encoding = face_recognition.face_encodings(test_image5)\ntest6_encoding = face_recognition.face_encodings(test_image6)\n')),(0,i.kt)("h3",{id:"compare-faces"},"Compare Faces"),(0,i.kt)("p",null,"Compare all detected images in the test dataset to the training images:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'trained_images = [bobby_encoding, jim_encoding, amos_encoding, camina_encoding, naomi_encoding, chrisjen_encoding]\ntrained_faces = np.array(["bobbie_w_draper", "jim_holden", "amos_burton", "camina_drummer", "naomi_nagata", "chrisjen_avasarala"])\n')),(0,i.kt)("h4",{id:"test-image-1"},"Test Image 1"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"test1_results = []\n\nfor detection in test1_encoding:\n    result = face_recognition.compare_faces(trained_images, detection)\n    test1_results.append(trained_faces[result])\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"test_img1 = plt.imread('faces/test/unknown_01.jpg')\nplt.title('detected faces: \\n' + str(test1_results), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img1)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"<matplotlib.image.AxesImage at 0x7f22c838e680>\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(32473).Z,width:"515",height:"321"})),(0,i.kt)("h4",{id:"test-image-2"},"Test Image 2"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"test2_results = []\n\nfor detection in test2_encoding:\n    result = face_recognition.compare_faces(trained_images, detection)\n    test1_results.append(trained_faces[result])\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"test_img2 = plt.imread('faces/test/unknown_02.jpg')\nplt.title('detected faces: \\n' + str(test2_results), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img2)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"<matplotlib.image.AxesImage at 0x7f22c831e950>\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(63713).Z,width:"515",height:"332"})),(0,i.kt)("h4",{id:"test-image-3"},"Test Image 3"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"test3_results = []\n\nfor detection in test3_encoding:\n    result = face_recognition.compare_faces(trained_images, detection)\n    test3_results.append(trained_faces[result])\n\ntest3_results\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"[array(['naomi_nagata'], dtype='<U18'), array(['amos_burton'], dtype='<U18')]\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"test_img3 = plt.imread('faces/test/unknown_03.jpg')\nplt.title('detected faces: \\n' + str(test3_results), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img3)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"<matplotlib.image.AxesImage at 0x7f22c8400c40>\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(68603).Z,width:"512",height:"422"})),(0,i.kt)("h4",{id:"test-image-4"},"Test Image 4"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"test4_results = []\n\nfor detection in test4_encoding:\n    result = face_recognition.compare_faces(trained_images, detection)\n    test4_results.append(trained_faces[result])\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"test_img4 = plt.imread('faces/test/unknown_04.jpg')\nplt.title('detected faces: \\n' + str(test4_results), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img4)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"<matplotlib.image.AxesImage at 0x7f22c86f7f40>\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(91344).Z,width:"702",height:"306"})),(0,i.kt)("h4",{id:"test-image-5"},"Test Image 5"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"test5_results = []\n\nfor detection in test5_encoding:\n    result = face_recognition.compare_faces(trained_images, detection)\n    test5_results.append(trained_faces[result])\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"test_img5 = plt.imread('faces/test/unknown_05.jpg')\nplt.title('detected faces: \\n' + str(test5_results), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img5)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"<matplotlib.image.AxesImage at 0x7f22c8246d40>\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(59906).Z,width:"664",height:"332"})),(0,i.kt)("h4",{id:"test-image-6"},"Test Image 6"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"test6_results = []\n\nfor detection in test6_encoding:\n    result = face_recognition.compare_faces(trained_images, detection)\n    test6_results.append(trained_faces[result])\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"test_img6 = plt.imread('faces/test/unknown_06.jpg')\nplt.title('detected faces: \\n' + str(test6_results), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img6)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"<matplotlib.image.AxesImage at 0x7f22c829f1c0>\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(1390).Z,width:"515",height:"384"})),(0,i.kt)("h2",{id:"detect-face-location-gpu--batch-processing"},"Detect Face Location (GPU + Batch Processing)"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"all_training_images = glob('./faces/*.jpg')\nlen(all_training_images)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"60\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"ran_gen = np.random.default_rng()\n\nplt.figure(figsize=(14, 12))\nplt.suptitle('Training Images')\n\nfor i in range(16):\n    ax = plt.subplot(4, 4, i+1)\n    random_index = ran_gen.integers(low=0, high=59, size=1)\n    i = random_index[0]\n    img_loc = all_training_images[i]\n    img_title = 'label: ' + all_training_images[i][8:-4]\n    image = plt.imread(img_loc)\n    plt.imshow(image)\n    plt.title(img_title, fontsize='small')\n    plt.axis(False)\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(14061).Z,width:"1048",height:"1064"})),(0,i.kt)("p",null,"For this experiment I collected 10 images from all faces that I used before. All training images only contain one face - so I expect only getting one location that I can map to the image label:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'image_labels = []\nface_locations = []\n\nfor image_path in all_training_images:\n    image_labels.append(image_path[8:-5])\n    image = face_recognition.load_image_file(image_path)\n    location = face_recognition.face_locations(image, model="cnn")\n    face_locations.append(location)\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"print(len(image_labels), len(face_locations))\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"60 60\n")),(0,i.kt)("p",null,"Now I can get the feature vector for every detected face by it's bounding box:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"face_encodings = []\ni = 0\n\nfor location in face_locations:\n    image = face_recognition.load_image_file(all_training_images[i])\n    encoding = face_recognition.face_encodings(image, location)[0]\n\n    face_encodings.append(encoding)\n    i+=1\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"len(face_encodings)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"60\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"testcnn1_results = []\n\nfor detection in test1_encoding:\n    result = face_recognition.compare_faces(face_encodings, detection)\n    testcnn1_results.append(np.array(image_labels)[result])\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"classes = []\n\ntest_img1 = plt.imread('faces/test/unknown_01.jpg')\n\nfor result in testcnn1_results:\n    label, count = np.unique(result, return_counts=True)\n    classes.append(\n        (\n            # check for unlabeled faces\n            count[0] if 0 < len(count) else None, \n            label[0] if 0 < len(label) else None\n        )\n    )\n    \nplt.title(str(classes), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img1)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"<matplotlib.image.AxesImage at 0x7fd8a747abf0>\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(74260).Z,width:"515",height:"317"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"testcnn2_results = []\n\nfor detection in test2_encoding:\n    result = face_recognition.compare_faces(face_encodings, detection)\n    testcnn2_results.append(np.array(image_labels)[result])\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"classes = []\n\ntest_img2 = plt.imread('faces/test/unknown_02.jpg')\n\nfor result in testcnn2_results:\n    label, count = np.unique(result, return_counts=True)\n    classes.append(\n        (\n            # check for unlabeled faces\n            count[0] if 0 < len(count) else None, \n            label[0] if 0 < len(label) else None\n        )\n    )\n    \nplt.title(str(classes), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img2)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"<matplotlib.image.AxesImage at 0x7fd8a75ecb20>\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(65543).Z,width:"515",height:"317"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"testcnn3_results = []\n\nfor detection in test3_encoding:\n    result = face_recognition.compare_faces(face_encodings, detection)\n    testcnn3_results.append(np.array(image_labels)[result])\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"classes = []\ntest_img3 = plt.imread('faces/test/unknown_03.jpg')\n\nfor result in testcnn3_results:\n    label, count = np.unique(result, return_counts=True)\n    classes.append(\n        (\n            # check for unlabeled faces\n            count[0] if 0 < len(count) else None, \n            label[0] if 0 < len(label) else None\n        )\n    )\n    \nplt.title(str(classes), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img3)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"<matplotlib.image.AxesImage at 0x7fd8a74cbbb0>\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(98469).Z,width:"512",height:"407"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"testcnn4_results = []\n\nfor detection in test4_encoding:\n    result = face_recognition.compare_faces(face_encodings, detection)\n    testcnn4_results.append(np.array(image_labels)[result])\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"classes = []\ntest_img4 = plt.imread('faces/test/unknown_04.jpg')\n\nfor result in testcnn4_results:\n    label, count = np.unique(result, return_counts=True)\n    classes.append(\n        (\n            # check for unlabeled faces\n            count[0] if 0 < len(count) else None, \n            label[0] if 0 < len(label) else None\n        )\n    )\n    \nplt.title(str(classes), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img4)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"<matplotlib.image.AxesImage at 0x7fd8ad1f74c0>\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(6514).Z,width:"515",height:"291"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"testcnn5_results = []\n\nfor detection in test5_encoding:\n    result = face_recognition.compare_faces(face_encodings, detection)\n    testcnn5_results.append(np.array(image_labels)[result])\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"classes = []\ntest_img5 = plt.imread('faces/test/unknown_05.jpg')\n\nfor result in testcnn5_results:\n    label, count = np.unique(result, return_counts=True)\n    classes.append(\n        (\n            # check for unlabeled faces\n            count[0] if 0 < len(count) else None, \n            label[0] if 0 < len(label) else None\n        )\n    )\n    \nplt.title(str(classes), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img5)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"<matplotlib.image.AxesImage at 0x7fd8a749a0e0>\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(31805).Z,width:"515",height:"317"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"testcnn6_results = []\n\nfor detection in test6_encoding:\n    result = face_recognition.compare_faces(face_encodings, detection)\n    testcnn6_results.append(np.array(image_labels)[result])\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"classes = []\ntest_img6 = plt.imread('faces/test/unknown_06.jpg')\n\nfor result in testcnn6_results:\n    label, count = np.unique(result, return_counts=True)\n    classes.append(\n        (\n            # check for unlabeled faces\n            count[0] if 0 < len(count) else None, \n            label[0] if 0 < len(label) else None\n        )\n    )\n    \nplt.title(str(classes), fontsize='small')\nplt.axis('off')\nplt.imshow(test_img6)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"<matplotlib.image.AxesImage at 0x7fd8ad20a1d0>\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(67516).Z,width:"515",height:"369"})),(0,i.kt)("h2",{id:"draw-bounding-boxes"},"Draw Bounding Boxes"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from PIL import Image, ImageDraw\n")),(0,i.kt)("p",null,"I'm using the encodings/labels for the 60 training images generated above:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"print(len(face_encodings), len(image_labels))\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"60 60\n")),(0,i.kt)("p",null,"Load a test image that contains one of the faces above:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"unknown_image = face_recognition.load_image_file('faces/test/unknown_01.jpg')\n\n# Find all the faces and face encodings in the unknown image\nfound_faces = face_recognition.face_locations(unknown_image)\nfound_face_encodings = face_recognition.face_encodings(unknown_image, found_faces)\n\n# Convert the image to a PIL-format image so that we can draw on top of it with the Pillow library\npil_image = Image.fromarray(unknown_image)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# Create a Pillow ImageDraw Draw instance to draw with\ndraw = ImageDraw.Draw(pil_image)\n\n# Loop through each face found in the unknown image\nfor (top, right, bottom, left), face_encoding in zip(found_faces, found_face_encodings):\n    # See if the face is a match for the known face(s)\n    matches = face_recognition.compare_faces(face_encodings, face_encoding)\n\n    name = "Unknown"\n\n    # If a match was found in face_encodings, just use the first one.\n    # if True in matches:\n    #     first_match_index = matches.index(True)\n    #     name = image_labels[first_match_index]\n\n    # Or instead, use the known face with the smallest distance to the new face\n    face_distances = face_recognition.face_distance(face_encodings, face_encoding)\n    best_match_index = np.argmin(face_distances)\n    if matches[best_match_index]:\n        name = image_labels[best_match_index]\n    # Draw a box around the face using the Pillow module\n    draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n    # Draw a label with a name below the face\n    text_width, text_height = draw.textsize(name)\n    draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n    draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n\n# Remove the drawing library from memory as per the Pillow docs\ndel draw\n\n# Display the resulting image\npil_image.show()\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"/tmp/ipykernel_11211/4178765491.py:25: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n  text_width, text_height = draw.textsize(name)\n/tmp/ipykernel_11211/4178765491.py:25: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n  text_width, text_height = draw.textsize(name)\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(12250).Z,width:"1280",height:"720"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"unknown_image2 = face_recognition.load_image_file('faces/test/unknown_02.jpg')\n\n# Find all the faces and face encodings in the unknown image\nfound_faces2 = face_recognition.face_locations(unknown_image2)\nfound_face_encodings2 = face_recognition.face_encodings(unknown_image2, found_faces2)\n\n# Convert the image to a PIL-format image so that we can draw on top of it with the Pillow library\npil_image2 = Image.fromarray(unknown_image2)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# Create a Pillow ImageDraw Draw instance to draw with\ndraw = ImageDraw.Draw(pil_image2)\n\n# Loop through each face found in the unknown image\nfor (top, right, bottom, left), face_encoding in zip(found_faces2, found_face_encodings2):\n    # See if the face is a match for the known face(s)\n    matches = face_recognition.compare_faces(face_encodings, face_encoding)\n\n    name = "Unknown"\n\n    # If a match was found in face_encodings, just use the first one.\n    # if True in matches:\n    #     first_match_index = matches.index(True)\n    #     name = image_labels[first_match_index]\n\n    # Or instead, use the known face with the smallest distance to the new face\n    face_distances = face_recognition.face_distance(face_encodings, face_encoding)\n    best_match_index = np.argmin(face_distances)\n    if matches[best_match_index]:\n        name = image_labels[best_match_index]\n\n    # Draw a box around the face using the Pillow module\n    draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n    # Draw a label with a name below the face\n    text_width, text_height = draw.textsize(name)\n    draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n    draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n\n# Remove the drawing library from memory as per the Pillow docs\ndel draw\n\n# Display the resulting image\npil_image2.show()\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"/tmp/ipykernel_11211/1125746434.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n  text_width, text_height = draw.textsize(name)\n/tmp/ipykernel_11211/1125746434.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n  text_width, text_height = draw.textsize(name)\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(73145).Z,width:"780",height:"439"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"unknown_image3 = face_recognition.load_image_file('faces/test/unknown_03.jpg')\n\n# Find all the faces and face encodings in the unknown image\nfound_faces3 = face_recognition.face_locations(unknown_image3)\nfound_face_encodings3 = face_recognition.face_encodings(unknown_image3, found_faces3)\n\n# Convert the image to a PIL-format image so that we can draw on top of it with the Pillow library\npil_image3 = Image.fromarray(unknown_image3)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# Create a Pillow ImageDraw Draw instance to draw with\ndraw = ImageDraw.Draw(pil_image3)\n\n# Loop through each face found in the unknown image\nfor (top, right, bottom, left), face_encoding in zip(found_faces3, found_face_encodings3):\n    # See if the face is a match for the known face(s)\n    matches = face_recognition.compare_faces(face_encodings, face_encoding)\n\n    name = "Unknown"\n\n    # If a match was found in face_encodings, just use the first one.\n    # if True in matches:\n    #     first_match_index = matches.index(True)\n    #     name = image_labels[first_match_index]\n\n    # Or instead, use the known face with the smallest distance to the new face\n    face_distances = face_recognition.face_distance(face_encodings, face_encoding)\n    best_match_index = np.argmin(face_distances)\n    if matches[best_match_index]:\n        name = image_labels[best_match_index]\n\n    # Draw a box around the face using the Pillow module\n    draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n    # Draw a label with a name below the face\n    text_width, text_height = draw.textsize(name)\n    draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n    draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n\n# Remove the drawing library from memory as per the Pillow docs\ndel draw\n\n# Display the resulting image\npil_image3.show()\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"/tmp/ipykernel_11211/3035399979.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n  text_width, text_height = draw.textsize(name)\n/tmp/ipykernel_11211/3035399979.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n  text_width, text_height = draw.textsize(name)\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(80904).Z,width:"2400",height:"1800"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"unknown_image4 = face_recognition.load_image_file('faces/test/unknown_04.jpg')\n\n# Find all the faces and face encodings in the unknown image\nfound_faces4 = face_recognition.face_locations(unknown_image4)\nfound_face_encodings4 = face_recognition.face_encodings(unknown_image4, found_faces4)\n\n# Convert the image to a PIL-format image so that we can draw on top of it with the Pillow library\npil_image4 = Image.fromarray(unknown_image4)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# Create a Pillow ImageDraw Draw instance to draw with\ndraw = ImageDraw.Draw(pil_image4)\n\n# Loop through each face found in the unknown image\nfor (top, right, bottom, left), face_encoding in zip(found_faces4, found_face_encodings4):\n    # See if the face is a match for the known face(s)\n    matches = face_recognition.compare_faces(face_encodings, face_encoding)\n\n    name = "Unknown"\n\n    # If a match was found in face_encodings, just use the first one.\n    # if True in matches:\n    #     first_match_index = matches.index(True)\n    #     name = image_labels[first_match_index]\n\n    # Or instead, use the known face with the smallest distance to the new face\n    face_distances = face_recognition.face_distance(face_encodings, face_encoding)\n    best_match_index = np.argmin(face_distances)\n    if matches[best_match_index]:\n        name = image_labels[best_match_index]\n\n    # Draw a box around the face using the Pillow module\n    draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n    # Draw a label with a name below the face\n    text_width, text_height = draw.textsize(name)\n    draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n    draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n\n# Remove the drawing library from memory as per the Pillow docs\ndel draw\n\n# Display the resulting image\npil_image4.show()\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"/tmp/ipykernel_11211/329699440.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n  text_width, text_height = draw.textsize(name)\n/tmp/ipykernel_11211/329699440.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n  text_width, text_height = draw.textsize(name)\n/tmp/ipykernel_11211/329699440.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n  text_width, text_height = draw.textsize(name)\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(17572).Z,width:"3328",height:"1698"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"unknown_image5 = face_recognition.load_image_file('faces/test/unknown_05.jpg')\n\n# Find all the faces and face encodings in the unknown image\nfound_faces5 = face_recognition.face_locations(unknown_image5)\nfound_face_encodings5 = face_recognition.face_encodings(unknown_image5, found_faces5)\n\n# Convert the image to a PIL-format image so that we can draw on top of it with the Pillow library\npil_image5 = Image.fromarray(unknown_image5)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# Create a Pillow ImageDraw Draw instance to draw with\ndraw = ImageDraw.Draw(pil_image5)\n\n# Loop through each face found in the unknown image\nfor (top, right, bottom, left), face_encoding in zip(found_faces5, found_face_encodings5):\n    # See if the face is a match for the known face(s)\n    matches = face_recognition.compare_faces(face_encodings, face_encoding)\n\n    name = "Unknown"\n\n    # If a match was found in face_encodings, just use the first one.\n    # if True in matches:\n    #     first_match_index = matches.index(True)\n    #     name = image_labels[first_match_index]\n\n    # Or instead, use the known face with the smallest distance to the new face\n    face_distances = face_recognition.face_distance(face_encodings, face_encoding)\n    best_match_index = np.argmin(face_distances)\n    if matches[best_match_index]:\n        name = image_labels[best_match_index]\n\n    # Draw a box around the face using the Pillow module\n    draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n    # Draw a label with a name below the face\n    text_width, text_height = draw.textsize(name)\n    draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n    draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n\n# Remove the drawing library from memory as per the Pillow docs\ndel draw\n\n# Display the resulting image\npil_image5.show()\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"/tmp/ipykernel_11211/864981556.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n  text_width, text_height = draw.textsize(name)\n/tmp/ipykernel_11211/864981556.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n  text_width, text_height = draw.textsize(name)\n/tmp/ipykernel_11211/864981556.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n  text_width, text_height = draw.textsize(name)\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(43421).Z,width:"1920",height:"1080"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"unknown_image6 = face_recognition.load_image_file('faces/test/unknown_06.jpg')\n\n# Find all the faces and face encodings in the unknown image\nfound_faces6 = face_recognition.face_locations(unknown_image6)\nfound_face_encodings6 = face_recognition.face_encodings(unknown_image6, found_faces6)\n\n# Convert the image to a PIL-format image so that we can draw on top of it with the Pillow library\npil_image6 = Image.fromarray(unknown_image6)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# Create a Pillow ImageDraw Draw instance to draw with\ndraw = ImageDraw.Draw(pil_image6)\n\n# Loop through each face found in the unknown image\nfor (top, right, bottom, left), face_encoding in zip(found_faces6, found_face_encodings6):\n    # See if the face is a match for the known face(s)\n    matches = face_recognition.compare_faces(face_encodings, face_encoding)\n\n    name = "Unknown"\n\n    # If a match was found in face_encodings, just use the first one.\n    # if True in matches:\n    #     first_match_index = matches.index(True)\n    #     name = image_labels[first_match_index]\n\n    # Or instead, use the known face with the smallest distance to the new face\n    face_distances = face_recognition.face_distance(face_encodings, face_encoding)\n    best_match_index = np.argmin(face_distances)\n    if matches[best_match_index]:\n        name = image_labels[best_match_index]\n\n    # Draw a box around the face using the Pillow module\n    draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n    # Draw a label with a name below the face\n    text_width, text_height = draw.textsize(name)\n    draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n    draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n\n# Remove the drawing library from memory as per the Pillow docs\ndel draw\n\n# Display the resulting image\npil_image6.show()\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"/tmp/ipykernel_11211/1615432601.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n  text_width, text_height = draw.textsize(name)\n/tmp/ipykernel_11211/1615432601.py:26: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n  text_width, text_height = draw.textsize(name)\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"DLIB Face Recognition",src:t(30107).Z,width:"1500",height:"1002"})),(0,i.kt)("h2",{id:"save-feature-vector"},"Save Feature Vector"),(0,i.kt)("p",null,"Export encodings and labels to use them in a Flask App -> see ",(0,i.kt)("inlineCode",{parentName:"p"},"./main.py")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"with open('features/face_encodings.npy', 'wb') as f:\n    np.save(f, face_encodings)\n\nwith open('features/image_labels.npy', 'wb') as f:\n    np.save(f, image_labels)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"with open('features/face_encodings.npy', 'rb') as f:\n    feature_vectors = np.load(f)\n\nwith open('features/image_labels.npy', 'rb') as f:\n    feature_labels = np.load(f)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"np.unique(feature_labels, return_counts=True)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"(array(['amos_burton', 'bobbie_w_draper', 'camina_drummer',\n        'chrisjen_avasarala', 'jim_holden', 'naomi_nagata'], dtype='<U18'),\n array([10, 10, 10, 10, 10, 10]))\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"feature_vectors.shape\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"(60, 128)\n")))}m.isMDXComponent=!0},2202:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_10_0-3fc99340460a5394b27cc4f3f5c9d7fd.png"},22473:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_12_1-34ededf10548acc3e3dac3d0586e6145.png"},68708:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_17_1-4418223b167876be1fa8babce9252dc7.png"},32473:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_24_1-f63226da303d670adfec0d4b4026bacc.png"},63713:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_27_1-a0008dd355ec2171e6e929f44c33f48e.png"},68603:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_30_1-351af2c686b85232dc0209d055fe7c2b.png"},91344:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_33_1-d8ba9c2f02a6b87fab02cf050cda3194.png"},59906:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_36_1-8138e3bdc663ad0b2245d5a557597a43.png"},1390:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_39_1-194c400485fe4b708793d0be5d119a6a.png"},14061:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_42_0-0f8929362920e806619fe16c01b757ed.png"},74260:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_50_1-73608f4bd670f7c287a2dbbe52e8e996.png"},65543:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_52_1-e3f4ebb76c4e94450785d19ebc6cb6f1.png"},98469:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_54_1-5574a5145b291dbfda722e0e69be9a1b.png"},6514:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_56_1-0820cc189b3250cc40e7b6a8b70733ab.png"},31805:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_58_1-e6bd78a118a0ea887a49ee266a3ebd7a.png"},67516:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_60_1-e1999a249a86692234578015252719de.png"},12250:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_67_1-4c30cad6378124c03a4f69e1b4cba729.png"},73145:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_69_1-ddebae2f57dfecd2dd614c48db18fae5.png"},80904:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_71_1-4074a578690c43746bfa44c511d13b1c.png"},17572:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_73_1-6e49aa6a5523c16699e27bb1ffab0384.png"},43421:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_75_1-2de8685de457822eb43a1158aa3a8e82.png"},30107:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_77_1-3809e9b204faa60015065cced483eac6.png"},27170:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-252551beac0b36b4ba53ccd380897f8e.jpg"}}]);