"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[34921],{790452:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>d});var t=r(785893),a=r(603905);const s={sidebar_position:4700,slug:"2023-02-18",title:"Keras for Tensorflow - Recurrent Neural Networks",authors:"mpolinowski",tags:["Python","Machine Learning","Keras","Tensorflow"],description:"Recurrent Neural Networks are widely used to work with sequence data such as time series or natural language."},i=void 0,o={id:"IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-rnn/index",title:"Keras for Tensorflow - Recurrent Neural Networks",description:"Recurrent Neural Networks are widely used to work with sequence data such as time series or natural language.",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-rnn/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-rnn",slug:"/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-rnn/2023-02-18",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-rnn/2023-02-18",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-rnn/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Keras",permalink:"/docs/tags/keras"},{label:"Tensorflow",permalink:"/docs/tags/tensorflow"}],version:"current",sidebarPosition:4700,frontMatter:{sidebar_position:4700,slug:"2023-02-18",title:"Keras for Tensorflow - Recurrent Neural Networks",authors:"mpolinowski",tags:["Python","Machine Learning","Keras","Tensorflow"],description:"Recurrent Neural Networks are widely used to work with sequence data such as time series or natural language."},sidebar:"tutorialSidebar",previous:{title:"Keras for Tensorflow - VGG16 Network Architecture",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-vgg16/2023-02-18"},next:{title:"Keras for Tensorflow - Convolutional Neural Networks",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-02-17-keras-introduction-cnn/2023-02-17"}},l={},d=[{value:"Long Short-Term Memory Models (LSTM)",id:"long-short-term-memory-models-lstm",level:2},{value:"Time Series",id:"time-series",level:3},{value:"Natural Language",id:"natural-language",level:3}];function c(e){const n={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.ah)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Guangzhou, China",src:r(847241).Z+"",width:"2830",height:"1272"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#long-short-term-memory-models-lstm",children:"Long Short-Term Memory Models (LSTM)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#time-series",children:"Time Series"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#natural-language",children:"Natural Language"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://github.com/mpolinowski/tf-keras-2023",children:"Github Repository"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://keras.io/getting_started/",children:"Keras"})," is built on top of TensorFlow 2 and provides an API designed for human beings. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear & actionable error messages."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.em,{children:"See also:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-02-14-keras-introduction/2023-02-14",children:"Keras for Tensorflow - An (Re)Introduction 2023"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-02-16-keras-introduction-ann/2023-02-16",children:"Keras for Tensorflow - Artificial Neural Networks"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-02-17-keras-introduction-cnn/2023-02-17",children:"Keras for Tensorflow - Convolutional Neural Networks"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-vgg16/2023-02-18",children:"Keras for Tensorflow - VGG16 Network Architecture"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-rnn/2023-02-18",children:"Keras for Tensorflow - Recurrent Neural Networks"})}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"long-short-term-memory-models-lstm",children:"Long Short-Term Memory Models (LSTM)"}),"\n",(0,t.jsxs)(n.p,{children:["Recurrent Neural Networks (",(0,t.jsx)(n.strong,{children:"RNN"}),") are widely used to work with sequence data such as time series or natural language. Unlike ",(0,t.jsx)(n.strong,{children:"CNN"}),"'s ",(0,t.jsx)(n.strong,{children:"RNN"}),"'s persist data by introducing loops in their data flow - allowing you them to make predictions based on past events. Types of ",(0,t.jsx)(n.strong,{children:"RNN"}),"'s are (",(0,t.jsx)(n.a,{href:"https://medium.com/explore-artificial-intelligence/an-introduction-to-recurrent-neural-networks-72c97bf0912",children:"Image Source"}),"):"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Keras for Tensorflow - Convolutional Neural Networks",src:r(448258).Z+"",width:"700",height:"229"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"One-to-One"}),": A network with a single in- and output like the image classification we had before."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"One-to-Many"}),": A network with a single in- and multiple outputs. E.g. a caption generator for an input image."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Many-to-One"}),": A network with multiple in- and a single output. E.g. a sentiment analyzer."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Many-to-Many"}),": A network with multiple in- and multiple outputs. E.g. a language translator."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"time-series",children:"Time Series"}),"\n",(0,t.jsxs)(n.p,{children:["We can use the ",(0,t.jsx)(n.a,{href:"https://raw.githubusercontent.com/kartikdube/Datasets/master/international-airline-passengers.csv",children:"International Airline Passengers"})," dataset to build and train an RNN prediction model in Keras:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# loading the passenger into a dataframe\ndf = pd.read_csv('data/international-airline-passengers.csv', usecols=[1], skipfooter=2, engine='python')\ndf.plot()\nplt.show()\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Keras for Tensorflow - Convolutional Neural Networks",src:r(693016).Z+"",width:"2560",height:"982"})}),"\n",(0,t.jsxs)(n.p,{children:["We need to first normalize the dataset so that all values range in between ",(0,t.jsx)(n.code,{children:"0"})," and ",(0,t.jsx)(n.code,{children:"1"})," and split the set into training and testing data:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"from sklearn.preprocessing import MinMaxScaler\n\n# pre-processing\ndataset = df.values\ndataset = dataset.astype('float32')\n\n## normalize data to 0->1 range\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(dataset)\n\n## data split 70/30\ntrain_size = int(len(dataset)*.70)\ntest_size = len(dataset) - train_size\n\ntrain, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\nprint(train.shape, test.shape)\n# (100, 1) (44, 1)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# creating the timeseries datasets\ndef create_dataset(dataset, look_back=1):\n      dataX, dataY = [], []\n      for i in range(len(dataset) - look_back - 1):\n            # input data\n            a = dataset[i:(i+look_back), 0]\n            dataX.append(a)\n            # output data\n            b = dataset[(i+look_back), 0]\n            dataY.append(b)\n      return np.array(dataX), np.array(dataY)\n\ntrainX, trainY = create_dataset(train)\ntestX, testY = create_dataset(test)\n\ntrainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\ntestX = testX.reshape((testX.shape[0], 1, testX.shape[1]))\n"})}),"\n",(0,t.jsxs)(n.p,{children:["With our data pre-processed we can now compile our simple ",(0,t.jsx)(n.strong,{children:"Long Short-Term Memory"})," model:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# building the model\nmodel = Sequential()\n\nmodel.add(LSTM(4, input_shape=(1,1)))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.summary()\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'Model: "sequential"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm (LSTM)                 (None, 4)                 96        \n                                                                 \n dense (Dense)               (None, 1)                 5         \n                                                                 \n=================================================================\nTotal params: 101\nTrainable params: 101\nNon-trainable params: 0\n'})}),"\n",(0,t.jsx)(n.p,{children:"The compiled model now need to be fitted to our dataset:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# fitting the model\nmodel.fit(trainX, trainY, batch_size=1, epochs=100)\n"})}),"\n",(0,t.jsx)(n.p,{children:"To run predictions we first have to undo the normalization we have applied to our values:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# run prediction\ntrainPred = model.predict(trainX)\ntestPred = model.predict(testX)\n\n# undo normalization\ntrainPred = scaler.inverse_transform(trainPred)\ntestPred = scaler.inverse_transform(testPred)\n\ntrainY = scaler.inverse_transform([trainY])\ntrainX = scaler.inverse_transform([testY])\n"})}),"\n",(0,t.jsx)(n.p,{children:"And we can plot the predicted values on top of the training dataset:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Keras for Tensorflow - Convolutional Neural Networks",src:r(756152).Z+"",width:"2558",height:"349"})}),"\n",(0,t.jsxs)(n.p,{children:["From left to right we see three training runs with ",(0,t.jsx)(n.strong,{children:"100"}),", ",(0,t.jsx)(n.strong,{children:"1000"})," and ",(0,t.jsx)(n.strong,{children:"10000 Epochs"}),". We can see that the predicted value start to fit the earlier passenger numbers better and better. But the fit for later years continues to decrease. Thinking that this might be an overfitting issue I added a dropout layer to my model:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# building the model\nmodel = Sequential()\n\nmodel.add(LSTM(4, input_shape=(1,1)))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(1))\n"})}),"\n",(0,t.jsx)(n.p,{children:"And I can see a much better fit with a Dropout - even though there is still some more room for improvement:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Keras for Tensorflow - Convolutional Neural Networks",src:r(739307).Z+"",width:"1601",height:"332"})}),"\n",(0,t.jsxs)(n.p,{children:["Both those graph show the prediction validation after ",(0,t.jsx)(n.strong,{children:"1000 epochs"})," of training. On the ",(0,t.jsx)(n.strong,{children:"left is the previous model without"})," and on the ",(0,t.jsx)(n.strong,{children:"right the model with a Dropout layer"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"natural-language",children:"Natural Language"}),"\n",(0,t.jsx)(n.p,{children:"To work with a natural language problem we can use the IMDB dataset provided by Keras:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"from keras.datasets import imdb\n\n# loading an excerpt from the imdb dataset\ntop_words = 5000\n(Xtrain, Ytrain), (Xtest, Ytest) = imdb.load_data(num_words=top_words)\n\n# truncate movie reviews > 500 words\nmax_review_size = 500\nXtrain = sequence.pad_sequences(Xtrain, maxlen=max_review_size)\nXtest = sequence.pad_sequences(Xtest, maxlen=max_review_size)\n"})}),"\n",(0,t.jsx)(n.p,{children:"As LSTM model we need a binary classifier - is the review positive or negative:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# building the model\n## number of feature outputs\nfeature_length = 32\n\nmodel = Sequential()\nmodel.add(Embedding(top_words, feature_length, input_length=max_review_size))\nmodel.add(LSTM(100))\n##output binary classification\nmodel.add(Dense(1, activation=sigmoid))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'Model: "sequential"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 500, 32)           160000    \n                                                                 \n lstm (LSTM)                 (None, 100)               53200     \n                                                                 \n dense (Dense)               (None, 1)                 101       \n                                                                 \n=================================================================\nTotal params: 213,301\nTrainable params: 213,301\nNon-trainable params: 0\n_________________________________________________________________\n'})}),"\n",(0,t.jsx)(n.p,{children:"Now we can fit this model to our review dataset and validate the training:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# training the model\nmodel.fit(Xtrain, Ytrain, epoch=2, batch_size=118)\n\n## training validation\nval_loss, val_score = model.evaluate(Xtest, Ytest)\nprint(val_loss, val_score)\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Already after ",(0,t.jsx)(n.strong,{children:"2 epochs"})," I am seeing an accuracy of ",(0,t.jsx)(n.strong,{children:"83%"})," - ",(0,t.jsx)(n.code,{children:"0.3782317042350769"})," ",(0,t.jsx)(n.code,{children:"0.8289600014686584"}),". Going to ",(0,t.jsx)(n.strong,{children:"86%"})," after ",(0,t.jsx)(n.strong,{children:"10 epochs"})," - ",(0,t.jsx)(n.code,{children:"0.4176311194896698"})," ",(0,t.jsx)(n.code,{children:"0.8595200181007385"}),". So this seems to be a problem that can be solved with patience :)"]})]})}function h(e={}){const{wrapper:n}={...(0,a.ah)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},603905:(e,n,r)=>{r.d(n,{ah:()=>d});var t=r(667294);function a(e,n,r){return n in e?Object.defineProperty(e,n,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[n]=r,e}function s(e,n){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),r.push.apply(r,t)}return r}function i(e){for(var n=1;n<arguments.length;n++){var r=null!=arguments[n]?arguments[n]:{};n%2?s(Object(r),!0).forEach((function(n){a(e,n,r[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):s(Object(r)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(r,n))}))}return e}function o(e,n){if(null==e)return{};var r,t,a=function(e,n){if(null==e)return{};var r,t,a={},s=Object.keys(e);for(t=0;t<s.length;t++)r=s[t],n.indexOf(r)>=0||(a[r]=e[r]);return a}(e,n);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(t=0;t<s.length;t++)r=s[t],n.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var l=t.createContext({}),d=function(e){var n=t.useContext(l),r=n;return e&&(r="function"==typeof e?e(n):i(i({},n),e)),r},c={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},h=t.forwardRef((function(e,n){var r=e.components,a=e.mdxType,s=e.originalType,l=e.parentName,h=o(e,["components","mdxType","originalType","parentName"]),u=d(r),_=a,p=u["".concat(l,".").concat(_)]||u[_]||c[_]||s;return r?t.createElement(p,i(i({ref:n},h),{},{components:r})):t.createElement(p,i({ref:n},h))}));h.displayName="MDXCreateElement"},448258:(e,n,r)=>{r.d(n,{Z:()=>t});const t=r.p+"assets/images/Keras_Introduction_RNN_Model_01-4c5ccf6e4611c2e3fbb3749f275f5183.png"},693016:(e,n,r)=>{r.d(n,{Z:()=>t});const t=r.p+"assets/images/Keras_Introduction_RNN_Model_02-52bf45774dadb769dae7c296dcdc839e.png"},756152:(e,n,r)=>{r.d(n,{Z:()=>t});const t=r.p+"assets/images/Keras_Introduction_RNN_Model_03-1162ccf4ee70e7ec41898bd85c09a006.png"},739307:(e,n,r)=>{r.d(n,{Z:()=>t});const t=r.p+"assets/images/Keras_Introduction_RNN_Model_04-d18cb6ebdd27c83402ba7fa6cf337481.png"},847241:(e,n,r)=>{r.d(n,{Z:()=>t});const t=r.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-f80e63ee872dae25129198058ac93b4e.jpg"}}]);