"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[45682],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>h});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function s(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?s(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):s(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},s=Object.keys(e);for(n=0;n<s.length;n++)a=s[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(n=0;n<s.length;n++)a=s[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),d=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},p=function(e){var t=d(e.components);return n.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,s=e.originalType,l=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),u=d(a),h=r,g=u["".concat(l,".").concat(h)]||u[h]||c[h]||s;return a?n.createElement(g,i(i({ref:t},p),{},{components:a})):n.createElement(g,i({ref:t},p))}));function h(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var s=a.length,i=new Array(s);i[0]=u;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o.mdxType="string"==typeof e?e:r,i[1]=o;for(var d=2;d<s;d++)i[d]=a[d];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},64659:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>c,frontMatter:()=>s,metadata:()=>o,toc:()=>d});var n=a(87462),r=(a(67294),a(3905));const s={sidebar_position:4690,slug:"2023-02-05",title:"Apache Airflow DAG Scheduling",authors:"mpolinowski",tags:["Python","Machine Learning","Airflow"],description:"Airflow is a platform to author, schedule and monitor workflows."},i=void 0,o={unversionedId:"IoT-and-Machine-Learning/AIOps/2023-02-05-apache-airflow-scheduler/index",id:"IoT-and-Machine-Learning/AIOps/2023-02-05-apache-airflow-scheduler/index",title:"Apache Airflow DAG Scheduling",description:"Airflow is a platform to author, schedule and monitor workflows.",source:"@site/docs/IoT-and-Machine-Learning/AIOps/2023-02-05-apache-airflow-scheduler/index.md",sourceDirName:"IoT-and-Machine-Learning/AIOps/2023-02-05-apache-airflow-scheduler",slug:"/IoT-and-Machine-Learning/AIOps/2023-02-05-apache-airflow-scheduler/2023-02-05",permalink:"/docs/IoT-and-Machine-Learning/AIOps/2023-02-05-apache-airflow-scheduler/2023-02-05",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/AIOps/2023-02-05-apache-airflow-scheduler/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Airflow",permalink:"/docs/tags/airflow"}],version:"current",sidebarPosition:4690,frontMatter:{sidebar_position:4690,slug:"2023-02-05",title:"Apache Airflow DAG Scheduling",authors:"mpolinowski",tags:["Python","Machine Learning","Airflow"],description:"Airflow is a platform to author, schedule and monitor workflows."},sidebar:"tutorialSidebar",previous:{title:"AIOps",permalink:"/docs/category/aiops"},next:{title:"Apache Airflow Data Pipelines",permalink:"/docs/IoT-and-Machine-Learning/AIOps/2023-02-04-apache-airflow-data-pipelines/2023-02-04"}},l={},d=[{value:"Basic Scheduling",id:"basic-scheduling",level:2},{value:"Use Dataset Updates as Worflow Trigger",id:"use-dataset-updates-as-worflow-trigger",level:2},{value:"Debugging DAGs",id:"debugging-dags",level:3},{value:"Importing Datasets",id:"importing-datasets",level:3},{value:"Airflow Sensors",id:"airflow-sensors",level:2}],p={toc:d};function c(e){let{components:t,...s}=e;return(0,r.kt)("wrapper",(0,n.Z)({},p,s,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Guangzhou, China",src:a(52985).Z,width:"1061",height:"405"})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#basic-scheduling"},"Basic Scheduling")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#use-dataset-updates-as-worflow-trigger"},"Use Dataset Updates as Worflow Trigger"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#debugging-dags"},"Debugging DAGs")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#importing-datasets"},"Importing Datasets")))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#airflow-sensors"},"Airflow Sensors"))),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/mpolinowski/apache-airflow-intro"},"Github Repository"))),(0,r.kt)("h2",{id:"basic-scheduling"},"Basic Scheduling"),(0,r.kt)("p",null,"As we have seen before every pipeline we build is defined with a ",(0,r.kt)("inlineCode",{parentName:"p"},"start_date")," and a scheduled interval in  which the pipeline should be run. The ",(0,r.kt)("inlineCode",{parentName:"p"},"catchup")," parameter tells the DAG if it should try to run all the non-triggered DAG-runs since the start. Another, optional parameter is ",(0,r.kt)("inlineCode",{parentName:"p"},"end_date"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},'with DAG("example_dag", start_date=datetime(2023,2,5), schedule="@daily", catchup=False):\n')),(0,r.kt)("h2",{id:"use-dataset-updates-as-worflow-trigger"},"Use Dataset Updates as Worflow Trigger"),(0,r.kt)("p",null,"You can use ",(0,r.kt)("strong",{parentName:"p"},"Dataset Scheduler")," e.g. when you are working with multiple Airflow DAGs that depend on the results from the previous pipeline. So that pipeline 2 is automatically triggered once pipeline 1 finished updating a dataset."),(0,r.kt)("p",null,"So we first write a DAG that pre-processes our dataset - simulated by adding some text to a simple text file:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"from airflow import DAG, Dataset\nfrom airflow.decorators import task\n\nfrom datetime import datetime\n\ndataset = Dataset('/tmp/preprocessed_data.txt') \nstart_date = datetime(2023,2,5)\ndescription = \"Data pre-processing\"\ntags = ['scheduler', 'producer']\ndag_id = 'producer'\nschedule = '@daily'\n\nwith DAG(\n    dag_id=dag_id,\n    start_date=start_date,\n    schedule=schedule,\n    catchup=False,\n    description=description,\n    tags=tags\n    ):\n\n    ## decorator with outlet so updates\n    ## to dataset can be used as trigger\n    @task(outlets=[dataset])\n    ## preprocess data and write to file\n    def preprocess_dataset():\n        with open(dataset.uri, 'a+') as file:\n            file.write('preprocessed data ready for consumption')\n\n    preprocess_dataset()\n")),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"@task")," decorator now has an ",(0,r.kt)("inlineCode",{parentName:"p"},"outlet")," that tells Airflow that once this task ran the dataset ",(0,r.kt)("inlineCode",{parentName:"p"},"dataset")," was updated and can use this event to trigger the following DAG:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"from airflow import DAG, Dataset\nfrom airflow.decorators import task\n\nfrom datetime import datetime\n\ndataset = Dataset('/tmp/preprocessed_data.txt') \nstart_date = datetime(2023,2,5)\ndescription = \"Start this pipeline when new data arrives\"\ntags = ['scheduler', 'consumer']\ndag_id = 'consumer'\n\nwith DAG(\n    start_date=start_date,\n    dag_id=dag_id,\n    schedule=[dataset],\n    catchup=False,\n    description=description,\n    tags=tags\n    ):\n\n    @task\n    ## process prepared data and write to file\n    def process_dataset():\n        with open(dataset.uri, 'r') as file:\n            print(file.read())\n\n    process_dataset()\n")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Apache Airflow DAG Scheduling",src:a(89887).Z,width:"1331",height:"374"})),(0,r.kt)("p",null,"Now as soon as we trigger the ",(0,r.kt)("strong",{parentName:"p"},"Producer DAG")," the dataset in form of the text file will be updated which in turn triggers the ",(0,r.kt)("strong",{parentName:"p"},"Consumer DAG"),":"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Apache Airflow DAG Scheduling",src:a(79297).Z,width:"1337",height:"372"})),(0,r.kt)("h3",{id:"debugging-dags"},"Debugging DAGs"),(0,r.kt)("p",null,"I am certain that I was able to find the execution logs in the Airflow UI before... hmm not sure where they are hiding now. But you can enter the Airflow container and get the task log from there:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"docker exec -ti airflow /bin/bash\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"airflow info\n\nApache Airflow\nversion                | 2.5.1                                              \nexecutor               | SequentialExecutor                                 \ntask_logging_handler   | airflow.utils.log.file_task_handler.FileTaskHandler\nsql_alchemy_conn       | sqlite:////opt/airflow/airflow.db                  \ndags_folder            | /opt/airflow/dags                                  \nplugins_folder         | /opt/airflow/plugins                               \nbase_log_folder        | /opt/airflow/logs\n\n...\n")),(0,r.kt)("p",null,"I was able to find the log file in:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"/opt/airflow/logs/dag_id=producer/run_id=manual__2023-02-05T13:10:07.002907+00:00/task_id=preprocess_dataset\n")),(0,r.kt)("p",null,"And catch the Python error that was crashing my flow."),(0,r.kt)("h3",{id:"importing-datasets"},"Importing Datasets"),(0,r.kt)("p",null,"In the example above we needed to define the dataset in both DAGs. A cleaner solution is to have a dataset definition file instead:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"mkdir /opt/airflow/dags/include\nnano /opt/airflow/dags/include/datasets.py\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"from airflow import Dataset\n\nDATASET_01 = Dataset('/tmp/preprocessed_data.txt') \n")),(0,r.kt)("p",null,"Now we can import the dataset where ever we need it:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"from airflow import DAG\nfrom airflow.decorators import task\n\nfrom datetime import datetime\n\nfrom include.datasets import DATASET_01\n\n\nstart_date = datetime(2023,2,5)\ndescription = \"Data pre-processing\"\ntags = ['scheduler', 'producer']\ndag_id = 'producer'\nschedule = '@daily'\n\nwith DAG(\n    dag_id=dag_id,\n    start_date=start_date,\n    schedule=schedule,\n    catchup=False,\n    description=description,\n    tags=tags\n    ):\n\n    ## decorator with outlet so updates\n    ## to dataset can be used as trigger\n    @task(outlets=[DATASET_01])\n    ## preprocess data and write to file\n    def preprocess_dataset():\n        with open(DATASET_01.uri, 'a+') as file:\n            file.write('preprocessed data ready for consumption')\n\n    preprocess_dataset()\n")),(0,r.kt)("p",null,"All your datasets and DAGs that depend on it can be monitored under the ",(0,r.kt)("strong",{parentName:"p"},"Datasets")," tab:"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Apache Airflow DAG Scheduling",src:a(44675).Z,width:"1075",height:"301"})),(0,r.kt)("h2",{id:"airflow-sensors"},"Airflow Sensors"),(0,r.kt)("p",null,"With ",(0,r.kt)("strong",{parentName:"p"},"Dataset Schedulers")," Airflow monitors the successful completion of a task to trigger the next DAG. If we want to be able to react to other programs updating datasets we need to use ",(0,r.kt)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/sensors.html"},"Airflow Sensors")," instead. Sensors are a special type of Operator that are designed to do exactly one thing - wait for something to occur. It can be time-based, or waiting for a file, or an external event, but all they do is wait until something happens, and then succeed so their downstream tasks can run."),(0,r.kt)("p",null,"In the following example we receive data from 3 actors, have to pre-process it and ",(0,r.kt)("strong",{parentName:"p"},"after all data is collected")," process the prepared data:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"Data Producer A -> Pre-Processing ->\nData Producer B -> Pre-Processing -> Processing\nData Producer C -> Pre-Processing ->\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"from airflow.models import DAG\nfrom airflow.sensors.filesystem import FileSensor\n\nfrom datetime import datetime\n\nstart_date = datetime(2023,2,5)\ndescription = \"Data pre-processing\"\ntags = ['sensor']\nschedule = '@daily'\n\n\nwith DAG('dag_sensor',\n    schedule=schedule,\n    start_date=start_date,\n    description=description,\n    tags=tags\n    catchup=False\n    ):\n\n    sensor_task = FileSensor(\n        task_id='waiting_for_change',\n        poke_interval=30,\n        # kill sensor if data does\n        # not arrive\n        timeout= 60 * 5,\n        # 'reschedule' or 'poke'\n        # the first releases the\n        # worker slot in between pokes\n        mode='reschedule',\n        # skip sensor after timeout\n        soft_fail=True,\n        filepath= '/tmp/input_data.txt'\n    )\n\n    ...\n")))}c.isMDXComponent=!0},89887:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/Apache_Airflow_Scheduler_01-9b2336eb794e1616c3f526d14392b818.png"},79297:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/Apache_Airflow_Scheduler_02-b8dee20bc96860bbe696a128d0c20114.png"},44675:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/Apache_Airflow_Scheduler_03-98baca4db3fa4aeb786fd58ec9eb7278.png"},52985:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-fe9bbb57ea8da08fea2f3fef2bf2515b.jpg"}}]);