"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[20436],{568063:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var t=a(785893),i=a(603905);const r={sidebar_position:4970,slug:"2022-12-11",title:"Breast Histopathology Image Segmentation Part 2",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"Balancing skewed datasets and data augmentation"},s=void 0,o={id:"IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part2/index",title:"Breast Histopathology Image Segmentation Part 2",description:"Balancing skewed datasets and data augmentation",source:"@site/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part2/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part2",slug:"/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part2/2022-12-11",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part2/2022-12-11",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part2/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Tensorflow",permalink:"/docs/tags/tensorflow"}],version:"current",sidebarPosition:4970,frontMatter:{sidebar_position:4970,slug:"2022-12-11",title:"Breast Histopathology Image Segmentation Part 2",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"Balancing skewed datasets and data augmentation"},sidebar:"tutorialSidebar",previous:{title:"Breast Histopathology Image Segmentation Part 3",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part3/2022-12-11"},next:{title:"Breast Histopathology Image Segmentation Part 1",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-10-tf-breast-cancer-classification-part1/2022-12-10"}},l={},c=[{value:"Skewed Datasets",id:"skewed-datasets",level:2},{value:"Adding Weights to balance Data Classes",id:"adding-weights-to-balance-data-classes",level:3},{value:"Plotting Training Progress",id:"plotting-training-progress",level:3},{value:"Data Augmentation",id:"data-augmentation",level:2},{value:"Data Generators",id:"data-generators",level:3}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.ah)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Guangzhou, China",src:a(211633).Z+"",width:"1500",height:"383"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-10-tf-breast-cancer-classification-part1/2022-12-10",children:"Part 1: Data Inspection and Pre-processing"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part2/2022-12-11",children:"Part 2: Weights, Data Augmentations and Generators"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part3/2022-12-11",children:"Part 3: Model creation based on a pre-trained and a custom model"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part4/2022-12-11",children:"Part 4: Train our model to fit the dataset"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part5/2022-12-12",children:"Part 5: Evaluate the performance of your trained model"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6/2022-12-12",children:"Part 6: Running Predictions"})}),"\n"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://github.com/mpolinowski/tf-bc-classification",children:"Github"})}),"\n"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#skewed-datasets",children:"Skewed Datasets"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#adding-weights-to-balance-data-classes",children:"Adding Weights to balance Data Classes"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#plotting-training-progress",children:"Plotting Training Progress"})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#data-augmentation",children:"Data Augmentation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#data-generators",children:"Data Generators"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:["Based on ",(0,t.jsx)(n.a,{href:"https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images",children:"Breast Histopathology Images"})," by ",(0,t.jsx)(n.a,{href:"https://www.kaggle.com/paultimothymooney",children:"Paul Mooney"}),".\n",(0,t.jsx)(n.code,{children:"Invasive Ductal Carcinoma (IDC) is the most common subtype of all breast cancers. To assign an aggressiveness grade to a whole mount sample, pathologists typically focus on the regions which contain the IDC. As a result, one of the common pre-processing steps for automatic aggressiveness grading is to delineate the exact regions of IDC inside of a whole mount slide."}),"\n",(0,t.jsx)(n.a,{href:"https://youtu.be/8XsiMQQ-4mM",children:"Can recurring breast cancer be spotted with AI tech? - BBC News"})]}),"\n"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Citation: ",(0,t.jsx)(n.a,{href:"https://pubmed.ncbi.nlm.nih.gov/27563488/",children:"Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases"})]}),"\n",(0,t.jsx)(n.li,{children:"Dataset: 198,738 IDC(negative) image patches; 78,786 IDC(positive) image patches"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"skewed-datasets",children:"Skewed Datasets"}),"\n",(0,t.jsxs)(n.p,{children:["Before we can start the training we have to provide a few helper functions. These are identical for the custom model and the pre-trained ",(0,t.jsx)(n.strong,{children:"ResNet50"}),":"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.em,{children:"./train_CustomModel_32_conv_20k.py"}),"\n",(0,t.jsx)(n.em,{children:"./train_ResNet50_32_20k.py"})]}),"\n",(0,t.jsx)(n.h3,{id:"adding-weights-to-balance-data-classes",children:"Adding Weights to balance Data Classes"}),"\n",(0,t.jsx)(n.p,{children:"Get number of image files from a path:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# Method to get the number of files given a path\ndef retrieveNumberOfFiles(path):\n    list1 = []\n    for file_name in glob.iglob(path+'/**/*.png', recursive=True):\n        list1.append(file_name)\n    return len(list1)\n\n# Defining the paths to the training, validation, and testing directories\ntrainPath = config.TRAIN_PATH\nvalPath = config.VAL_PATH\ntestPath = config.TEST_PATH\n\n# Checking for the total number of images\ntotalTrain = retrieveNumberOfFiles(config.TRAIN_PATH)\ntotalVal = retrieveNumberOfFiles(config.VAL_PATH)\ntotalTest = retrieveNumberOfFiles(config.TEST_PATH)\n"})}),"\n",(0,t.jsx)(n.p,{children:"Get list of image files from a path:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# Defining a method to get the list of files given a path\ndef getAllFiles(path):\n    list1 = []\n    for file_name in glob.iglob(path+'/**/*.png', recursive=True):\n        list1.append(file_name)\n    return list1\n\n# Retrieving all files from train directory\nallTrainFiles = getAllFiles(config.TRAIN_PATH)\n"})}),"\n",(0,t.jsx)(n.p,{children:"Get number of benign and malignant images and create a weight for both classes. This helps to prevent overfitting the model to a dominant class (as we have seen in part 1 - the number of benign cases is far greater then the number of malignant):"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# Calculating the total number of training images against each class and then store the class weights in a dictionary\ntrainLabels = [int(p.split(os.path.sep)[-2]) for p in allTrainFiles]\ntrainLabels = to_categorical(trainLabels)\nclassSumTotals = trainLabels.sum(axis=0)\nclassWeight = dict()\n\n# Looping over all classes and calculate the class weights\nfor i in range(0, len(classSumTotals)):\n    classWeight[i] = classSumTotals.max() / classSumTotals[i]\n"})}),"\n",(0,t.jsx)(n.h3,{id:"plotting-training-progress",children:"Plotting Training Progress"}),"\n",(0,t.jsx)(n.p,{children:"Method to plot accuracy and loss of the training to visualize how the training is progressing:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:'# Defining a method to plot training and validation accuracy and loss\n# H - model fit\n# N - number of training epochs\n# plotPath - where to store the output file\ndef training_plot(H, N, plotPath):\n    plt.style.use("ggplot")\n    plt.figure()\n    plt.plot(np.arange(0, N), H.history["loss"], label="train_loss")\n    plt.plot(np.arange(0, N), H.history["val_loss"], label="val_loss")\n    plt.plot(np.arange(0, N), H.history["accuracy"], label="train_acc")\n    plt.plot(np.arange(0, N), H.history["val_accuracy"], label="val_acc")\n    plt.title("Training Loss and Accuracy")\n    plt.xlabel("Epoch")\n    plt.ylabel("Loss/Accuracy")\n    plt.legend(loc="lower left")\n    plt.savefig(plotPath)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"data-augmentation",children:"Data Augmentation"}),"\n",(0,t.jsx)(n.p,{children:"To improve the performance of an ML model the training data source we are working with should be big and diverse. Augmentation is used to increase the amount of data by adding slightly modified copies of existing data to the train dataset."}),"\n",(0,t.jsx)(n.p,{children:"In case of images this can be achieved by padding, random rotating, re-scaling. vertical and horizontal flipping, translating, cropping, zooming, darkening/brightening, adding noise or colour modifications."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.em,{children:"./train_CustomModel_32_conv_20k.py"}),"\n",(0,t.jsx)(n.em,{children:"./train_ResNet50_32_20k.py"})]}),"\n",(0,t.jsx)(n.p,{children:"Training augmentation:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:'# Initialize the training data augmentation object\n## preprocess_input will scale input pixels between -1 and 1\n## rotation_range is a value in degrees (0-180), a range within which to randomly rotate pictures\n## zoom_range is for randomly zooming inside pictures\n## width_shift and height_shift are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally\n## shear_range is for randomly applying shearing transformations\n## horizontal_flip and vertical_flip is for randomly flipping half of the images horizontally and vertically resp\n## fill_mode is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift\n\ntrainAug = ImageDataGenerator(\n\trescale=1 / 255.0,\n\trotation_range=30,\n\tzoom_range=0.15,\n\twidth_shift_range=0.2,\n\theight_shift_range=0.2,\n\tshear_range=0.15,\n\thorizontal_flip=True,\n\tvertical_flip=True,\n\tfill_mode="nearest")\n'})}),"\n",(0,t.jsx)(n.p,{children:"But only use re-scaling for the validation augmentation:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# Initialize the validation data augmentation object\nvalAug = ImageDataGenerator(rescale=1/255.0)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"data-generators",children:"Data Generators"}),"\n",(0,t.jsxs)(n.p,{children:["Use ",(0,t.jsx)(n.strong,{children:"Data Generators"})," like ",(0,t.jsx)(n.strong,{children:"Keras"})," ",(0,t.jsx)(n.code,{children:"ImageDataGenerator"})," to limit the amount of ",(0,t.jsx)(n.strong,{children:"Memory"})," your dataset is occupying. data generators allow you to augment your data in real-time while your model is still training. This limits the amount of data that needs to be loaded into GPU memory - as a large part of it is still being generated in parallel by your CPU."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:'trainAug = ImageDataGenerator(\n\trescale=1 / 255.0,\n\trotation_range=30,\n\tzoom_range=0.15,\n\twidth_shift_range=0.2,\n\theight_shift_range=0.2,\n\tshear_range=0.15,\n\thorizontal_flip=True,\n\tvertical_flip=True,\n\tfill_mode="nearest")\n\n# Initialize the training generator\ntrainGen = trainAug.flow_from_directory(\n\ttrainPath,\n\tclass_mode="categorical",\n\ttarget_size=(48, 48),\n\tcolor_mode="rgb",\n\tshuffle=True,\n\tbatch_size=config.BATCH_SIZE)\n\n\n# Initialize the validation data augmentation object\nvalAug = ImageDataGenerator(rescale=1/255.0)\n\n\n# Initialize the validation generator\nvalGen = valAug.flow_from_directory(\n\tvalPath,\n\tclass_mode="categorical",\n\ttarget_size=(48, 48),\n\tcolor_mode="rgb",\n\tshuffle=False,\n\tbatch_size=config.BATCH_SIZE)\n\n# Initialize the testing generator\ntestGen = valAug.flow_from_directory(\n\ttestPath,\n\tclass_mode="categorical",\n\ttarget_size=(48, 48),\n\tcolor_mode="rgb",\n\tshuffle=False,\n\tbatch_size=config.BATCH_SIZE)\n'})})]})}function h(e={}){const{wrapper:n}={...(0,i.ah)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},603905:(e,n,a)=>{a.d(n,{ah:()=>c});var t=a(667294);function i(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function r(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function s(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?r(Object(a),!0).forEach((function(n){i(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function o(e,n){if(null==e)return{};var a,t,i=function(e,n){if(null==e)return{};var a,t,i={},r=Object.keys(e);for(t=0;t<r.length;t++)a=r[t],n.indexOf(a)>=0||(i[a]=e[a]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)a=r[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var l=t.createContext({}),c=function(e){var n=t.useContext(l),a=n;return e&&(a="function"==typeof e?e(n):s(s({},n),e)),a},d={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},h=t.forwardRef((function(e,n){var a=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,h=o(e,["components","mdxType","originalType","parentName"]),g=c(a),p=i,f=g["".concat(l,".").concat(p)]||g[p]||d[p]||r;return a?t.createElement(f,s(s({ref:n},h),{},{components:a})):t.createElement(f,s({ref:n},h))}));h.displayName="MDXCreateElement"},211633:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-918471126c0472aad97358a725e1a399.jpg"}}]);