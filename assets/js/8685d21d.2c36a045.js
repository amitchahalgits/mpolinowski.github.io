"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[8570],{3905:(e,t,a)=>{a.d(t,{Zo:()=>_,kt:()=>u});var n=a(67294);function l(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){l(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,l=function(e,t){if(null==e)return{};var a,n,l={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(l[a]=e[a]);return l}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(l[a]=e[a])}return l}var d=n.createContext({}),s=function(e){var t=n.useContext(d),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},_=function(e){var t=s(e.components);return n.createElement(d.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,l=e.mdxType,r=e.originalType,d=e.parentName,_=o(e,["components","mdxType","originalType","parentName"]),m=s(a),u=l,c=m["".concat(d,".").concat(u)]||m[u]||p[u]||r;return a?n.createElement(c,i(i({ref:t},_),{},{components:a})):n.createElement(c,i({ref:t},_))}));function u(e,t){var a=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var r=a.length,i=new Array(r);i[0]=m;var o={};for(var d in t)hasOwnProperty.call(t,d)&&(o[d]=t[d]);o.originalType=e,o.mdxType="string"==typeof e?e:l,i[1]=o;for(var s=2;s<r;s++)i[s]=a[s];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},76888:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>i,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>s});var n=a(87462),l=(a(67294),a(3905));const r={sidebar_position:4410,slug:"2023-04-19",title:"(Re) Introduction to Tensorflow Natural Language Processing",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow","Keras"],description:"Using Tensorflow to classify Disaster Tweet."},i=void 0,o={unversionedId:"IoT-and-Machine-Learning/ML/2023-04-19-tensorflow-natural-language-processing/index",id:"IoT-and-Machine-Learning/ML/2023-04-19-tensorflow-natural-language-processing/index",title:"(Re) Introduction to Tensorflow Natural Language Processing",description:"Using Tensorflow to classify Disaster Tweet.",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-04-19-tensorflow-natural-language-processing/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-04-19-tensorflow-natural-language-processing",slug:"/IoT-and-Machine-Learning/ML/2023-04-19-tensorflow-natural-language-processing/2023-04-19",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-04-19-tensorflow-natural-language-processing/2023-04-19",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-04-19-tensorflow-natural-language-processing/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Tensorflow",permalink:"/docs/tags/tensorflow"},{label:"Keras",permalink:"/docs/tags/keras"}],version:"current",sidebarPosition:4410,frontMatter:{sidebar_position:4410,slug:"2023-04-19",title:"(Re) Introduction to Tensorflow Natural Language Processing",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow","Keras"],description:"Using Tensorflow to classify Disaster Tweet."},sidebar:"tutorialSidebar",previous:{title:"Machine Learning",permalink:"/docs/category/machine-learning"},next:{title:"3D Image Classification",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-04-16-deep-3d-image-segmentation/2023-04-16"}},d={},s=[{value:"Abstract",id:"abstract",level:2},{value:"Dataset",id:"dataset",level:2},{value:"Exploration",id:"exploration",level:3},{value:"Train Test Split",id:"train-test-split",level:3},{value:"Tokenization and Embedding",id:"tokenization-and-embedding",level:3},{value:"Tokenization",id:"tokenization",level:4},{value:"Embedding",id:"embedding",level:4},{value:"Experiments",id:"experiments",level:2},{value:"Model 0: Naive Bayes tf-ids",id:"model-0-naive-bayes-tf-ids",level:3},{value:"Tokenization and Modelling Pipeline",id:"tokenization-and-modelling-pipeline",level:4},{value:"Evaluation",id:"evaluation",level:4},{value:"Predictions",id:"predictions",level:4},{value:"Model 0 Metrics",id:"model-0-metrics",level:4},{value:"Model 1: Simple Dense",id:"model-1-simple-dense",level:3},{value:"Model Building and Training",id:"model-building-and-training",level:4},{value:"Model Evaluation",id:"model-evaluation",level:4},{value:"Model 1 Metrics",id:"model-1-metrics",level:4},{value:"Visualize the Embedding",id:"visualize-the-embedding",level:4},{value:"Model 2: LSTM Long-term Short-term Memory RNN",id:"model-2-lstm-long-term-short-term-memory-rnn",level:3},{value:"Model Building and Training",id:"model-building-and-training-1",level:4},{value:"Model Evaluation",id:"model-evaluation-1",level:4},{value:"Model 2 Metrics",id:"model-2-metrics",level:4},{value:"Model 3: GRU Gated Recurrent Unit RNN",id:"model-3-gru-gated-recurrent-unit-rnn",level:3},{value:"Model Building and Training",id:"model-building-and-training-2",level:4},{value:"Model Evaluation",id:"model-evaluation-2",level:4},{value:"Model 3 Metrics",id:"model-3-metrics",level:4},{value:"Model 4: Bi-Directional RNN",id:"model-4-bi-directional-rnn",level:3},{value:"Model Building and Training",id:"model-building-and-training-3",level:4},{value:"Model Evaluation",id:"model-evaluation-3",level:4},{value:"Model 4 Metrics",id:"model-4-metrics",level:4},{value:"Model 5: Conv1D",id:"model-5-conv1d",level:3},{value:"Model Building and Training",id:"model-building-and-training-4",level:4},{value:"Model Evaluation",id:"model-evaluation-4",level:4},{value:"Model 5 Metrics",id:"model-5-metrics",level:4},{value:"Model 6: Transfer Learning Feature Extractor",id:"model-6-transfer-learning-feature-extractor",level:3},{value:"Model Building and Training",id:"model-building-and-training-5",level:4},{value:"Model Evaluation",id:"model-evaluation-5",level:4},{value:"Model 6 Metrics",id:"model-6-metrics",level:4},{value:"Model 6a (added Dense Layer)",id:"model-6a-added-dense-layer",level:3},{value:"Model Evaluation",id:"model-evaluation-6",level:4},{value:"Model 6a Metrics",id:"model-6a-metrics",level:4},{value:"Model 6b: Transfer Learning Feature Extractor (10% Dataset)",id:"model-6b-transfer-learning-feature-extractor-10-dataset",level:3},{value:"Dataset",id:"dataset-1",level:4},{value:"Model Building and Training",id:"model-building-and-training-6",level:4},{value:"Model Evaluation",id:"model-evaluation-7",level:4},{value:"Model 6b Metrics",id:"model-6b-metrics",level:4},{value:"Model 6c: USE Data Leakage Issue (10% Dataset)",id:"model-6c-use-data-leakage-issue-10-dataset",level:3},{value:"Data Leakage Issue",id:"data-leakage-issue",level:4},{value:"Training 10% Split",id:"training-10-split",level:5},{value:"Model Evaluation",id:"model-evaluation-8",level:4},{value:"Model 6c Metrics",id:"model-6c-metrics",level:4},{value:"Compare Experiments",id:"compare-experiments",level:2},{value:"Saving &amp; Loading Trained Model",id:"saving--loading-trained-model",level:2},{value:"HDF5 Format (Higher Compatibility to 3rd Parties)",id:"hdf5-format-higher-compatibility-to-3rd-parties",level:3},{value:"Saved Model Format (Tensorflow Default)",id:"saved-model-format-tensorflow-default",level:3},{value:"Best Model Evaluation",id:"best-model-evaluation",level:2},{value:"False Positives",id:"false-positives",level:4},{value:"False Negatives",id:"false-negatives",level:4},{value:"Test Dataset Predictions",id:"test-dataset-predictions",level:2},{value:"Speed/Score Tradeoff",id:"speedscore-tradeoff",level:2},{value:"Comparing the Performance of all Models",id:"comparing-the-performance-of-all-models",level:4}],_={toc:s};function p(e){let{components:t,...r}=e;return(0,l.kt)("wrapper",(0,n.Z)({},_,r,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"Victoria Harbour, Hongkong",src:a(76806).Z,width:"2385",height:"823"})),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#re-introduction-to-tensorflow-natural-language-processing"},"(Re) Introduction to Tensorflow Natural Language Processing"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#abstract"},"Abstract")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#dataset"},"Dataset"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#exploration"},"Exploration")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#train-test-split"},"Train Test Split")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#tokenization-and-embedding"},"Tokenization and Embedding"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#tokenization"},"Tokenization")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#embedding"},"Embedding")))))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#experiments"},"Experiments"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-0-naive-bayes-tf-ids"},"Model 0: Naive Bayes tf-ids"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#tokenization-and-modelling-pipeline"},"Tokenization and Modelling Pipeline")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#evaluation"},"Evaluation")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#predictions"},"Predictions")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-0-metrics"},"Model 0 Metrics")))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-1-simple-dense"},"Model 1: Simple Dense"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-building-and-training"},"Model Building and Training")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-evaluation"},"Model Evaluation")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-1-metrics"},"Model 1 Metrics")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#visualize-the-embedding"},"Visualize the Embedding")))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-2-lstm-long-term-short-term-memory-rnn"},"Model 2: LSTM Long-term Short-term Memory RNN"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-building-and-training-1"},"Model Building and Training")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-evaluation-1"},"Model Evaluation")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-2-metrics"},"Model 2 Metrics")))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-3-gru-gated-recurrent-unit-rnn"},"Model 3: GRU Gated Recurrent Unit RNN"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-building-and-training-2"},"Model Building and Training")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-evaluation-2"},"Model Evaluation")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-3-metrics"},"Model 3 Metrics")))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-4-bi-directional-rnn"},"Model 4: Bi-Directional RNN"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-building-and-training-3"},"Model Building and Training")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-evaluation-3"},"Model Evaluation")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-4-metrics"},"Model 4 Metrics")))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-5-conv1d"},"Model 5: Conv1D"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-building-and-training-4"},"Model Building and Training")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-evaluation-4"},"Model Evaluation")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-5-metrics"},"Model 5 Metrics")))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-6-transfer-learning-feature-extractor"},"Model 6: Transfer Learning Feature Extractor"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-building-and-training-5"},"Model Building and Training")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-evaluation-5"},"Model Evaluation")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-6-metrics"},"Model 6 Metrics")))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-6a-added-dense-layer"},"Model 6a (added Dense Layer)"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-evaluation-6"},"Model Evaluation")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-6a-metrics"},"Model 6a Metrics")))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-6b-transfer-learning-feature-extractor-10-dataset"},"Model 6b: Transfer Learning Feature Extractor (10% Dataset)"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#dataset-1"},"Dataset")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-building-and-training-6"},"Model Building and Training")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-evaluation-7"},"Model Evaluation")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-6b-metrics"},"Model 6b Metrics")))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-6c-use-data-leakage-issue-10-dataset"},"Model 6c: USE Data Leakage Issue (10% Dataset)"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#data-leakage-issue"},"Data Leakage Issue"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#training-10-split"},"Training 10% Split")))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-evaluation-8"},"Model Evaluation")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#model-6c-metrics"},"Model 6c Metrics")))))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#compare-experiments"},"Compare Experiments")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#saving--loading-trained-model"},"Saving \\& Loading Trained Model"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#hdf5-format-higher-compatibility-to-3rd-parties"},"HDF5 Format (Higher Compatibility to 3rd Parties)")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#saved-model-format-tensorflow-default"},"Saved Model Format (Tensorflow Default)")))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#best-model-evaluation"},"Best Model Evaluation"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#false-positives"},"False Positives")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#false-negatives"},"False Negatives")))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#test-dataset-predictions"},"Test Dataset Predictions")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#speedscore-tradeoff"},"Speed/Score Tradeoff"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#comparing-the-performance-of-all-models"},"Comparing the Performance of all Models"))))))),(0,l.kt)("p",null,(0,l.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/tf-nlp-2023"},"Github Repository")),(0,l.kt)("h1",{id:"re-introduction-to-tensorflow-natural-language-processing"},"(Re) Introduction to Tensorflow Natural Language Processing"),(0,l.kt)("h2",{id:"abstract"},"Abstract"),(0,l.kt)("p",null,"Twitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies)."),(0,l.kt)("p",null,"But, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster. Here Machine Learning can help us segmenting large quantities of incoming messages. But what type Machine Learning is the best suited?"),(0,l.kt)("p",null,"I am running a couple of experiments to show the performace of different solutions and their trade-off - how long does it take to get a prediction from a given algorithm? Is a higher accuracy worth the wait?"),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"(Re) Introduction to Tensorflow Natural Language Processing",src:a(12253).Z,width:"1010",height:"701"})),(0,l.kt)("h2",{id:"dataset"},"Dataset"),(0,l.kt)("p",null,(0,l.kt)("a",{parentName:"p",href:"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip"},"https://www.kaggle.com/competitions/nlp-getting-started/data")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},".\n\u251c\u2500\u2500 data\n\u2502\xa0\xa0 \u251c\u2500\u2500 test.csv\n\u2502\xa0\xa0 \u2514\u2500\u2500 train.csv\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"import datetime\nimport io\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport random\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nimport tensorflow as tf\n# tf-hub bug https://stackoverflow.com/questions/69339917/importerror-cannot-import-name-dnn-logit-fn-builder-from-partially-initialize\nfrom tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\nimport tensorflow_hub as hub\nfrom tensorflow.keras.layers import (\n    TextVectorization,\n    Embedding,\n    Input,\n    Dense,\n    GlobalAveragePooling1D,\n    GlobalMaxPool1D,\n    LSTM,\n    GRU,\n    Bidirectional,\n    Conv1D\n)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from helper_functions import (\n    create_tensorboard_callback,\n    plot_loss_curves,\n    compare_histories,\n    calculate_metrics,\n    time_to_prediction\n)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"SEED = 42\nLOG_DIR = 'tensorboad'\n")),(0,l.kt)("h3",{id:"exploration"},"Exploration"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"train_df = pd.read_csv('data/train.csv')\ntest_df = pd.read_csv('data/test.csv')\n\ntrain_df.head(5)\n# target 1 = disaster / 0 = not a disaster\n")),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null}),(0,l.kt)("th",{parentName:"tr",align:null},"id"),(0,l.kt)("th",{parentName:"tr",align:null},"keyword"),(0,l.kt)("th",{parentName:"tr",align:null},"location"),(0,l.kt)("th",{parentName:"tr",align:null},"text"),(0,l.kt)("th",{parentName:"tr",align:null},"target"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"0"),(0,l.kt)("td",{parentName:"tr",align:null},"1"),(0,l.kt)("td",{parentName:"tr",align:null},"NaN"),(0,l.kt)("td",{parentName:"tr",align:null},"NaN"),(0,l.kt)("td",{parentName:"tr",align:null},"Our Deeds are the Reason of this #earthquake M..."),(0,l.kt)("td",{parentName:"tr",align:null},"1")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"1"),(0,l.kt)("td",{parentName:"tr",align:null},"4"),(0,l.kt)("td",{parentName:"tr",align:null},"NaN"),(0,l.kt)("td",{parentName:"tr",align:null},"NaN"),(0,l.kt)("td",{parentName:"tr",align:null},"Forest fire near La Ronge Sask. Canada"),(0,l.kt)("td",{parentName:"tr",align:null},"1")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"2"),(0,l.kt)("td",{parentName:"tr",align:null},"5"),(0,l.kt)("td",{parentName:"tr",align:null},"NaN"),(0,l.kt)("td",{parentName:"tr",align:null},"NaN"),(0,l.kt)("td",{parentName:"tr",align:null},"All residents asked to 'shelter in place' are ..."),(0,l.kt)("td",{parentName:"tr",align:null},"1")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"3"),(0,l.kt)("td",{parentName:"tr",align:null},"6"),(0,l.kt)("td",{parentName:"tr",align:null},"NaN"),(0,l.kt)("td",{parentName:"tr",align:null},"NaN"),(0,l.kt)("td",{parentName:"tr",align:null},"13,000 people receive #wildfires evacuation or..."),(0,l.kt)("td",{parentName:"tr",align:null},"1")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"4"),(0,l.kt)("td",{parentName:"tr",align:null},"7"),(0,l.kt)("td",{parentName:"tr",align:null},"NaN"),(0,l.kt)("td",{parentName:"tr",align:null},"NaN"),(0,l.kt)("td",{parentName:"tr",align:null},"Just got sent this photo from Ruby #Alaska as ..."),(0,l.kt)("td",{parentName:"tr",align:null},"1")))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# checking if the dataset is balanced\nprint(train_df.target.value_counts())\n# 0    4342\n# 1    3271\n\nprint(len(train_df), len(test_df))\n# 7613 3263\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"train_df_shuffle = train_df.sample(frac=1, random_state=SEED)\n\ntrain_df_shuffle.head(5)\n")),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null}),(0,l.kt)("th",{parentName:"tr",align:null},"id"),(0,l.kt)("th",{parentName:"tr",align:null},"keyword"),(0,l.kt)("th",{parentName:"tr",align:null},"location"),(0,l.kt)("th",{parentName:"tr",align:null},"text"),(0,l.kt)("th",{parentName:"tr",align:null},"target"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"2644"),(0,l.kt)("td",{parentName:"tr",align:null},"3796"),(0,l.kt)("td",{parentName:"tr",align:null},"destruction"),(0,l.kt)("td",{parentName:"tr",align:null},"NaN"),(0,l.kt)("td",{parentName:"tr",align:null},"So you have a new weapon that can cause un-ima..."),(0,l.kt)("td",{parentName:"tr",align:null},"1")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"2227"),(0,l.kt)("td",{parentName:"tr",align:null},"3185"),(0,l.kt)("td",{parentName:"tr",align:null},"deluge"),(0,l.kt)("td",{parentName:"tr",align:null},"NaN"),(0,l.kt)("td",{parentName:"tr",align:null},"The f$","&","@ing things I do for #GISHWHES Just..."),(0,l.kt)("td",{parentName:"tr",align:null},"0")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"5448"),(0,l.kt)("td",{parentName:"tr",align:null},"7769"),(0,l.kt)("td",{parentName:"tr",align:null},"police"),(0,l.kt)("td",{parentName:"tr",align:null},"UK"),(0,l.kt)("td",{parentName:"tr",align:null},"DT @georgegalloway: RT @Galloway4Mayor: \x89\xdb\xcfThe..."),(0,l.kt)("td",{parentName:"tr",align:null},"1")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"132"),(0,l.kt)("td",{parentName:"tr",align:null},"191"),(0,l.kt)("td",{parentName:"tr",align:null},"aftershock"),(0,l.kt)("td",{parentName:"tr",align:null},"NaN"),(0,l.kt)("td",{parentName:"tr",align:null},"Aftershock back to school kick off was great. ..."),(0,l.kt)("td",{parentName:"tr",align:null},"0")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"6845"),(0,l.kt)("td",{parentName:"tr",align:null},"9810"),(0,l.kt)("td",{parentName:"tr",align:null},"trauma"),(0,l.kt)("td",{parentName:"tr",align:null},"Montgomery County, MD"),(0,l.kt)("td",{parentName:"tr",align:null},"in response to trauma Children of Addicts deve..."),(0,l.kt)("td",{parentName:"tr",align:null},"0")))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'random_index = random.randint(0, len(train_df)-5)\n\nfor row in train_df_shuffle[[\'text\', \'target\']][random_index:random_index+5].itertuples():\n    _, text, target = row\n    print(f"Target: {target}","(disaster)" if target>0 else "(not a disaster)")\n    print(f"Text: {text}\\n")\n    print("---\\n")\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"Target: 1 (disaster)\nText: @WesleyLowery ?????? how are you going to survive this devastation?\n\n---\n\nTarget: 0 (not a disaster)\nText: Hollywood movie about trapped miners released in Chile http://t.co/xe0EE1Fzfh\n\n---\n\nTarget: 0 (not a disaster)\nText: #sing #tsunami Beginners #computer tutorial.: http://t.co/ukQYbhxMQI Everyone Wants To Learn To Build A Pc. Re http://t.co/iDWS2ZgYsa\n\n---\n\nTarget: 1 (disaster)\nText: #Reddit updates #content #policy promises to quarantine \x89\xdb\xf7extremely offensive\x89\xdb\xaa communities http://t.co/EHGtZhKAn4\n\n---\n\nTarget: 1 (disaster)\nText: Japan Marks 70th Anniversary of Hiroshima Atomic Bombing http://t.co/cQLM9jOJOP\n\n---\n")),(0,l.kt)("h3",{id:"train-test-split"},"Train Test Split"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"train_tweets, val_tweets, train_labels, val_labels = train_test_split(\n    train_df_shuffle['text'].to_numpy(),\n    train_df_shuffle['target'].to_numpy(),\n    test_size=0.1,\n    random_state=SEED)\n\nprint(len(train_tweets), len(val_tweets))\n# 6851 762\n")),(0,l.kt)("h3",{id:"tokenization-and-embedding"},"Tokenization and Embedding"),(0,l.kt)("p",null,"Machine learning models take vectors (arrays of numbers) as input. When working with text, the first thing you must do is come up with a strategy to convert strings to numbers (or to ",(0,l.kt)("a",{parentName:"p",href:"https://www.tensorflow.org/text/guide/word_embeddings"},'"vectorize" the text'),") before feeding it to the model."),(0,l.kt)("h4",{id:"tokenization"},"Tokenization"),(0,l.kt)("p",null,(0,l.kt)("a",{parentName:"p",href:"https://www.tensorflow.org/tutorials/keras/text_classification"},"TextVectorization layer")," is a Keras layer to standardize, tokenize, and vectorize the dataset."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# find average number of words in tweets\naverage_tokens_per_tweet=round(sum([len(i.split()) for i in train_tweets])/len(train_tweets))\nprint(average_tokens_per_tweet)\n# 15\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# create text vectorizer\nmax_features = 10000 # limit to most common words\nsequence_length = 15 # limit to average number of words in tweets\n\ntext_vectorizer = TextVectorization(\n    max_tokens=max_features, # set a value to only include most common words\n    standardize='lower_and_strip_punctuation',\n    split='whitespace',\n    ngrams=None, # set value to form common word groups\n    output_mode='int',\n    output_sequence_length=sequence_length, # set value to limit tweet size\n    pad_to_max_tokens=True # fluff tweets that are shorter than set max length\n)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# fit text vectorizer to training data\ntext_vectorizer.adapt(train_tweets)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'# test fitted vectorizer\nsample_sentence1 = "Next I\'m buying Coca-Cola to put the cocaine back in"\nsample_sentence2 = "Hey guys, wanna feel old? I\'m 40. You\'re welcome."\nsample_sentence3 = "Beef chicken pork bacon chuck shortloin sirloin shank eu, bresaola voluptate in enim ea kielbasa laboris brisket laborum, jowl labore id porkchop elit ad commodo."\ntext_vectorizer([sample_sentence1,sample_sentence2,sample_sentence3])\n\n# <tf.Tensor: shape=(3, 15), dtype=int64, numpy=\n# array([[ 274,   32, 4046,    1,    5,  370,    2, 5962,   88,    4,    0,\n#            0,    0,    0,    0],\n#        [ 706,  576,  473,  214,  206,   32,  354,  172, 1569,    0,    0,\n#            0,    0,    0,    0],\n#        [   1, 4013,    1,    1,    1,    1,    1,    1, 3878,    1,    1,\n#            4,    1,    1,    1]])>\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"random_tweet = random.choice(train_tweets)\nvector = text_vectorizer([random_tweet])\n\nprint(\n    f'Tweet: {random_tweet}\\\n    \\n\\nVector: {vector}'\n)\n\n# Tweet: Ignition Knock (Detonation) Sensor ACDelco GM Original Equipment 213-4678\n# Vector: [[ 888  885  580 1767    1 1671 1623 1863    1    1    1    0    0    0\n#      0]]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# get unique vocabulary\nwords_in_vocab = text_vectorizer.get_vocabulary()\ntop_10_words = words_in_vocab[:10]\nbottom_10_words = words_in_vocab[-10:]\n\nprint(\n    len(words_in_vocab),\n    top_10_words,\n    bottom_10_words\n)\n# 10000\n# ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is']\n# ['painthey', 'painful', 'paine', 'paging', 'pageshi', 'pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n\n# The [UNK] stands for unknown - meaning outside of the 10.000 tokens limit\n")),(0,l.kt)("h4",{id:"embedding"},"Embedding"),(0,l.kt)("p",null,"The now vectorize data can be used as the first layer of the classification model, feeding transformed strings into an ",(0,l.kt)("a",{parentName:"p",href:"https://www.tensorflow.org/text/guide/word_embeddings"},"Embedding layer"),". The Embedding layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# create embedding layer\nembedding = Embedding(\n    input_dim = max_features,\n    output_dim = 128,\n    input_length = sequence_length\n)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"random_tweet = random.choice(train_tweets)\nsample_embedd = embedding(text_vectorizer([random_tweet]))\n\nprint(\n    f'Tweet: {random_tweet}\\\n    \\n\\nEmbedding: {sample_embedd}, {sample_embedd.shape}'\n)\n\n# Tweet: @biggangVH1 looks like George was having a panic attack. LOL.    \n\n# Embedding: [[[-0.02811491 -0.02710991 -0.04273632 ...  0.01480064 -0.02413664\n#     0.02612327]\n#   [-0.013403   -0.04941868  0.03431542 ...  0.00432001 -0.03614474\n#     0.04559914]\n#   [ 0.00161045 -0.02501463  0.02461291 ... -0.02123032  0.02596099\n#    -0.02626952]\n#   ...\n#   [ 0.03742747  0.03854593 -0.02052871 ...  0.01287705 -0.04228047\n#    -0.02316147]\n#   [ 0.03742747  0.03854593 -0.02052871 ...  0.01287705 -0.04228047\n#    -0.02316147]\n#   [ 0.03742747  0.03854593 -0.02052871 ...  0.01287705 -0.04228047\n#    -0.02316147]]], (1, 15, 128)\n\n# the shape tells us the the layer received 1 input with the length of\n# 15 words (as set before and filled up with zeros if tweet is shorter)\n# and each of those words is now represented by a 128dim vector\n")),(0,l.kt)("h2",{id:"experiments"},"Experiments"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Model 0: Naive Bayes (baseline)"),(0,l.kt)("li",{parentName:"ul"},"Model 1: Feed-forward neural network (dense model)"),(0,l.kt)("li",{parentName:"ul"},"Model 2: LSTM model (RNN)"),(0,l.kt)("li",{parentName:"ul"},"Model 3: GRU model (RNN)"),(0,l.kt)("li",{parentName:"ul"},"Model 4: Bidirectional-LSTM model (RNN)"),(0,l.kt)("li",{parentName:"ul"},"Model 5: 1D-Convolutional Neural Network (CNN)"),(0,l.kt)("li",{parentName:"ul"},"Model 6: TensorFlow Hub pre-trained NLP feature extractor"),(0,l.kt)("li",{parentName:"ul"},"Model 6a: Model 6 with added complexity (add dense layer)"),(0,l.kt)("li",{parentName:"ul"},"Model 6b: Model 6a with 10% training data"),(0,l.kt)("li",{parentName:"ul"},"Model 6c: Model 6a with 10% training data (fixed sampling)")),(0,l.kt)("h3",{id:"model-0-naive-bayes-tf-ids"},"Model 0: Naive Bayes tf-ids"),(0,l.kt)("h4",{id:"tokenization-and-modelling-pipeline"},"Tokenization and Modelling Pipeline"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'model_0 = Pipeline([\n    ("tfidf", TfidfVectorizer()),\n    ("clf", MultinomialNB())\n])\n\n# fit to training data\nmodel_0.fit(train_tweets, train_labels)\n')),(0,l.kt)("h4",{id:"evaluation"},"Evaluation"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'baseline_score = model_0.score(val_tweets, val_labels)\nprint(f"Baseline accuracy: {baseline_score*100:.2f}%")\n# Baseline accuracy: 79.27%\n')),(0,l.kt)("h4",{id:"predictions"},"Predictions"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"baseline_preds = model_0.predict(val_tweets)\n\nprint(val_tweets[:10])\nprint(baseline_preds[:10])\n# array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0])\n")),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},(0,l.kt)("inlineCode",{parentName:"p"},"[1 1 1 0 0 1 1 1 1 0]"))),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"'DFR EP016 Monthly Meltdown - On Dnbheaven 2015.08.06 ",(0,l.kt)("a",{parentName:"li",href:"http://t.co/EjKRf8N8A8"},"http://t.co/EjKRf8N8A8")," #Drum and Bass #heavy #nasty ",(0,l.kt)("a",{parentName:"li",href:"http://t.co/SPHWE6wFI5'"},"http://t.co/SPHWE6wFI5'")),(0,l.kt)("li",{parentName:"ol"},"'FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps ",(0,l.kt)("a",{parentName:"li",href:"http://t.co/qZQc8WWwcN"},"http://t.co/qZQc8WWwcN")," via @usatoday'"),(0,l.kt)("li",{parentName:"ol"},"'Gunmen kill four in El Salvador bus attack: Suspected Salvadoran gang members killed four people and wounded s... ",(0,l.kt)("a",{parentName:"li",href:"http://t.co/CNtwB6ScZj'"},"http://t.co/CNtwB6ScZj'")),(0,l.kt)("li",{parentName:"ol"},"'@camilacabello97 Internally and externally screaming'"),(0,l.kt)("li",{parentName:"ol"},"'Radiation emergency #preparedness starts with knowing to: get inside stay inside and stay tuned ",(0,l.kt)("a",{parentName:"li",href:"http://t.co/RFFPqBAz2F"},"http://t.co/RFFPqBAz2F")," via @CDCgov'"),(0,l.kt)("li",{parentName:"ol"},"'Investigators rule catastrophic structural failure resulted in 2014 Virg.. Related Articles: ",(0,l.kt)("a",{parentName:"li",href:"http://t.co/Cy1LFeNyV8'"},"http://t.co/Cy1LFeNyV8'")),(0,l.kt)("li",{parentName:"ol"},"'How the West was burned: Thousands of wildfires ablaze in #California alone ",(0,l.kt)("a",{parentName:"li",href:"http://t.co/iCSjGZ9tE1"},"http://t.co/iCSjGZ9tE1")," #climate #energy ",(0,l.kt)("a",{parentName:"li",href:"http://t.co/9FxmN0l0Bd'"},"http://t.co/9FxmN0l0Bd'")),(0,l.kt)("li",{parentName:"ol"},"\"Map: Typhoon Soudelor's predicted path as it approaches Taiwan; expected to make landfall over southern China by S\\x89\xdb_ ",(0,l.kt)("a",{parentName:"li",href:"http://t.co/JDVSGVhlIs%22"},'http://t.co/JDVSGVhlIs"')),(0,l.kt)("li",{parentName:"ol"},"'\\x89\xdb\xaa93 blasts accused Yeda Yakub dies in Karachi of heart attack ",(0,l.kt)("a",{parentName:"li",href:"http://t.co/mfKqyxd8XG"},"http://t.co/mfKqyxd8XG")," #Mumbai'"),(0,l.kt)("li",{parentName:"ol"},"'My ears are bleeding  ",(0,l.kt)("a",{parentName:"li",href:"https://t.co/k5KnNwugwT'%5D"},"https://t.co/k5KnNwugwT']"))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"baseline_metrics = calculate_metrics(\n    y_true=val_labels,\n    y_pred=baseline_preds\n)\n\nprint(f\"Accuracy: {baseline_metrics['accuracy']}, Precision: {baseline_metrics['precision']}, Recall: {baseline_metrics['recall']}, F1-Score: {baseline_metrics['f1']}\")\n\n")),(0,l.kt)("h4",{id:"model-0-metrics"},"Model 0 Metrics"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Accuracy: 79.26509186351706,"),(0,l.kt)("li",{parentName:"ul"},"Precision: 0.8111390004213173,"),(0,l.kt)("li",{parentName:"ul"},"Recall: 0.7926509186351706,"),(0,l.kt)("li",{parentName:"ul"},"F1-Score: 0.7862189758049549")),(0,l.kt)("h3",{id:"model-1-simple-dense"},"Model 1: Simple Dense"),(0,l.kt)("h4",{id:"model-building-and-training"},"Model Building and Training"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# build model\n## inputs are single 1-dimensional strings\ninputs = Input(shape=(1,), dtype=tf.string)\n## turn strings into numbers\nx = text_vectorizer(inputs)\n## create embedding from vectorized input\nx = embedding(x)\n## instead of returning a prediction for every token/word\n## condense all to a single prediction for entire input string\nx = GlobalAveragePooling1D()(x)\n## sigmoid activated output for binary classification\noutputs = Dense(1, activation='sigmoid')(x)\n\nmodel_1 = tf.keras.Model(inputs, outputs, name='model_1_dense')\nmodel_1.summary()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},'Model: "model_1_dense"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 1)]               0         \n                                                                 \n text_vectorization (TextVec  (None, 15)               0         \n torization)                                                     \n                                                                 \n embedding (Embedding)       (None, 15, 128)           1280000   \n                                                                 \n global_average_pooling1d (G  (None, 128)              0         \n lobalAveragePooling1D)                                          \n                                                                 \n dense_2 (Dense)             (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 1,280,129\nTrainable params: 1,280,129\nNon-trainable params: 0\n_________________________________________________________________\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# compile model\nmodel_1.compile(\n    loss='binary_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(),\n    metrics=['accuracy']\n)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'# there seems to be an issue with the tb callback\n# https://github.com/keras-team/keras/issues/15163\n# changed histogram_freq=0\n## create a callback to track experiments in TensorBoard\ndef create_tensorboard_callback_bugged(dir_name, experiment_name):\n    # log progress to log directory\n    log_dir = dir_name + "/" + experiment_name + "/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)\n    print(f"INFO :: Saving TensorBoard Log to: {log_dir}")\n    return tensorboard_callback\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# model training\nmodel_1_history = model_1.fit(\n    x=train_tweets,\n    y=train_labels,\n    epochs=5,\n    validation_data=(val_tweets, val_labels),\n    callbacks=[create_tensorboard_callback_bugged(\n        dir_name=LOG_DIR,\n        experiment_name='model_1_dense'\n    )]\n)\n")),(0,l.kt)("h4",{id:"model-evaluation"},"Model Evaluation"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_1.evaluate(val_tweets, val_labels)\n# loss: 0.4830 - accuracy: 0.7887\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'model_1_preds = model_1.predict(val_tweets)\nsample_prediction=model_1_preds[0]\nprint(f"Prediction: {sample_prediction}","(disaster)" if sample_prediction>0.5 else "(not a disaster)")\n# Prediction: [0.32197773] (not a disaster)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# convert model prediction probabilities to binary label format\nmodel_1_preds = tf.squeeze(tf.round(model_1_preds))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_1_metrics = calculate_metrics(\n    y_true=val_labels,\n    y_pred=model_1_preds\n)\n\nprint(model_1_metrics)\n")),(0,l.kt)("h4",{id:"model-1-metrics"},"Model 1 Metrics"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Accuracy: 78.87139107611549"),(0,l.kt)("li",{parentName:"ul"},"Precision: 0.7969619064252174"),(0,l.kt)("li",{parentName:"ul"},"Recall: 0.7887139107611548"),(0,l.kt)("li",{parentName:"ul"},"F1: 0.7847294282013199")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# the model performs worse than the baseline model\nnp.array(list(model_1_metrics.values())) > np.array(list(baseline_metrics.values()))\n# array([False, False, False, False])\n")),(0,l.kt)("h4",{id:"visualize-the-embedding"},"Visualize the Embedding"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# get vocab from text vectorizer layer\nwords_in_vocab = text_vectorizer.get_vocabulary()\ntop_10_words = words_in_vocab[:10]\nprint(top_10_words)\n# ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is']\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# get embedding weight matrix\nembed_weights = model_1.get_layer('embedding').get_weights()[0]\nprint(embed_weights.shape)\n# (10000, 128) same size as max_features - one weight for every word in vocabulary\n")),(0,l.kt)("p",null,"The ",(0,l.kt)("a",{parentName:"p",href:"https://www.tensorflow.org/tutorials/text/word2vec"},"embeddeding weights")," started as random numbers assigned to each token/word in our dataset. By fitting this embedding space to our dataset these weights can now be used to group the words in our dataset. Words that belong to the same class should also have similar vectors representing them."),(0,l.kt)("p",null,"We can use the ",(0,l.kt)("a",{parentName:"p",href:"https://projector.tensorflow.org/"},"Tensorflow Projector")," to display our embedding space:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"out_v = io.open('embedding_weights/vectors_model1.tsv', 'w', encoding='utf-8')\nout_m = io.open('embedding_weights/metadata_model1.tsv', 'w', encoding='utf-8')\n\nfor index, word in enumerate(words_in_vocab):\n    if index == 0:\n        continue # skip padding\n    vec = embed_weights[index]\n    out_v.write('\\t'.join([str(x) for x in vec]) + '\\n')\n    out_m.write(word + '\\n')\n    \nout_v.close()\nout_m.close()\n\n# Upload both files to the [Tensorflow Projector](https://projector.tensorflow.org/)\n")),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"(Re) Introduction to Tensorflow Natural Language Processing",src:a(60045).Z,width:"866",height:"713"})),(0,l.kt)("p",null,"The projection shows a clear separation between our two classes showing each word in our vocabulary as a member of one of two clusters."),(0,l.kt)("h3",{id:"model-2-lstm-long-term-short-term-memory-rnn"},"Model 2: LSTM Long-term Short-term Memory RNN"),(0,l.kt)("h4",{id:"model-building-and-training-1"},"Model Building and Training"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"inputs = Input(shape=(1,), dtype='string')\nx = text_vectorizer(inputs)\nx = embedding(x)\n# x = LSTM(64, return_sequences=True)(x)\nx = LSTM(64)(x)\nx = Dense(64, activation='relu')(x)\noutputs = Dense(1, activation='sigmoid')(x)\n\nmodel_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")\nmodel_2.summary()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},'Model: "model_2_LSTM"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 1)]               0         \n                                                                 \n text_vectorization (TextVec  (None, 15)               0         \n torization)                                                     \n                                                                 \n embedding (Embedding)       (None, 15, 128)           1280000   \n                                                                 \n lstm_2 (LSTM)               (None, 64)                49408     \n                                                                 \n dense_3 (Dense)             (None, 64)                4160      \n                                                                 \n dense_4 (Dense)             (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 1,333,633\nTrainable params: 1,333,633\nNon-trainable params: 0\n_________________________________________________________________\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_2.compile(\n    loss='binary_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(),\n    metrics=['accuracy']\n)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_2_history = model_2.fit(\n    train_tweets,\n    train_labels,\n    epochs=5,\n    validation_data=(val_tweets, val_labels),\n    callbacks=[create_tensorboard_callback_bugged(\n        dir_name=LOG_DIR,\n        experiment_name='model_2_lstm'\n    )]\n)\n")),(0,l.kt)("h4",{id:"model-evaluation-1"},"Model Evaluation"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_2.evaluate(val_tweets, val_labels)\n# loss: 1.6905 - accuracy: 0.7743\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'# make predictions\nmodel_2_preds = model_2.predict(val_tweets)\nsample_prediction=model_2_preds[0]\nprint(f"Prediction: {sample_prediction}","(disaster)" if sample_prediction>0.5 else "(not a disaster)")\n# Prediction: [0.04310093] (not a disaster)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# convert model prediction probabilities to binary label format\nmodel_2_preds = tf.squeeze(tf.round(model_2_preds))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_2_metrics = calculate_metrics(\n    y_true=val_labels,\n    y_pred=model_2_preds\n)\n\nprint(model_2_metrics)\n")),(0,l.kt)("h4",{id:"model-2-metrics"},"Model 2 Metrics"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Accuracy: 76.77165354330708"),(0,l.kt)("li",{parentName:"ul"},"Precision: 0.7674723453090632"),(0,l.kt)("li",{parentName:"ul"},"Recall: 0.7677165354330708"),(0,l.kt)("li",{parentName:"ul"},"F1: 0.7668863186407149")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# the model performs worse than the previous model\nnp.array(list(model_2_metrics.values())) > np.array(list(model_1_metrics.values()))\n# array([False, False, False, False])\n")),(0,l.kt)("h3",{id:"model-3-gru-gated-recurrent-unit-rnn"},"Model 3: GRU Gated Recurrent Unit RNN"),(0,l.kt)("h4",{id:"model-building-and-training-2"},"Model Building and Training"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"inputs = Input(shape=(1,), dtype=tf.string)\nx = text_vectorizer(inputs)\nx = embedding(x)\n# x = GRU(64, return_sequences=True)(x)\nx = GRU(64)(x)\nx = Dense(64, activation='relu')(x)\noutputs = Dense(1, activation='sigmoid')(x)\n\nmodel_3 = tf.keras.Model(inputs, outputs, name=\"model_3_GRU\")\nmodel_3.summary()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},'Model: "model_3_GRU"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, 1)]               0         \n                                                                 \n text_vectorization (TextVec  (None, 15)               0         \n torization)                                                     \n                                                                 \n embedding (Embedding)       (None, 15, 128)           1280000   \n                                                                 \n gru (GRU)                   (None, 64)                37248     \n                                                                 \n dense_5 (Dense)             (None, 64)                4160      \n                                                                 \n dense_6 (Dense)             (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 1,321,473\nTrainable params: 1,321,473\nNon-trainable params: 0\n_________________________________________________________________\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_3.compile(\n    loss='binary_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(),\n    metrics=['accuracy']\n)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_3_history = model_3.fit(\n    train_tweets,\n    train_labels,\n    epochs=5,\n    validation_data=(val_tweets, val_labels),\n    callbacks=[create_tensorboard_callback_bugged(\n        dir_name=LOG_DIR,\n        experiment_name='model_3_gru'\n    )]\n)\n")),(0,l.kt)("h4",{id:"model-evaluation-2"},"Model Evaluation"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_3.evaluate(val_tweets, val_labels)\n# loss: 1.2467 - accuracy: 0.7703\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'# make predictions\nmodel_3_preds = model_3.predict(val_tweets)\nsample_prediction=model_3_preds[0]\nprint(f"Prediction: {sample_prediction}","(disaster)" if sample_prediction>0.5 else "(not a disaster)")\n# Prediction: [0.00013416] (not a disaster)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# convert model prediction probabilities to binary label format\nmodel_3_preds = tf.squeeze(tf.round(model_3_preds))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_3_metrics = calculate_metrics(\n    y_true=val_labels,\n    y_pred=model_3_preds\n)\n\nprint(model_3_metrics)\n")),(0,l.kt)("h4",{id:"model-3-metrics"},"Model 3 Metrics"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Accuracy: 76.9028871391076"),(0,l.kt)("li",{parentName:"ul"},"Precision: 0.7768747910576218"),(0,l.kt)("li",{parentName:"ul"},"Recall: 0.7690288713910761"),(0,l.kt)("li",{parentName:"ul"},"F1: 0.7643954892702002")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# the model performs (sometimes) better than the lstm but still worse than the baseline model\nprint(np.array(list(model_3_metrics.values())) > np.array(list(model_2_metrics.values())))\n# array([ True,  True,  True, False])\nprint(np.array(list(model_3_metrics.values())) > np.array(list(baseline_metrics.values())))\n# array([False, False, False, False])\n")),(0,l.kt)("h3",{id:"model-4-bi-directional-rnn"},"Model 4: Bi-Directional RNN"),(0,l.kt)("h4",{id:"model-building-and-training-3"},"Model Building and Training"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"inputs = Input(shape=(1,), dtype=tf.string)\nx = text_vectorizer(inputs)\nx = embedding(x)\nx = Bidirectional(LSTM(64, return_sequences=True))(x)\nx = Bidirectional(GRU(64))(x)\noutputs = Dense(1, activation='sigmoid')(x)\n\nmodel_4 = tf.keras.Model(inputs, outputs, name='model_4_bidirectional')\nmodel_4.summary()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},'Model: "model_4_bidirectional"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_6 (InputLayer)        [(None, 1)]               0         \n                                                                 \n text_vectorization (TextVec  (None, 15)               0         \n torization)                                                     \n                                                                 \n embedding (Embedding)       (None, 15, 128)           1280000   \n                                                                 \n bidirectional_2 (Bidirectio  (None, 15, 128)          98816     \n nal)                                                            \n                                                                 \n bidirectional_3 (Bidirectio  (None, 128)              74496     \n nal)                                                            \n                                                                 \n dense_8 (Dense)             (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 1,453,441\nTrainable params: 1,453,441\nNon-trainable params: 0\n_________________________________________________________________\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_4.compile(\n    loss='binary_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(),\n    metrics=['accuracy']\n)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_4_history = model_4.fit(\n    train_tweets,\n    train_labels,\n    epochs=5,\n    validation_data=(val_tweets, val_labels),\n    callbacks=[create_tensorboard_callback_bugged(\n        dir_name=LOG_DIR,\n        experiment_name='model_4_bidirectional'\n    )]\n)\n")),(0,l.kt)("h4",{id:"model-evaluation-3"},"Model Evaluation"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_4.evaluate(val_tweets, val_labels)\n# loss: 1.7367 - accuracy: 0.7756\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'# make predictions\nmodel_4_preds = model_4.predict(val_tweets)\nsample_prediction=model_4_preds[0]\nprint(f"Prediction: {sample_prediction}","(disaster)" if sample_prediction>0.5 else "(not a disaster)")\n# Prediction: [0.00025736] (not a disaster)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# convert model prediction probabilities to binary label format\nmodel_4_preds = tf.squeeze(tf.round(model_4_preds))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_4_metrics = calculate_metrics(\n    y_true=val_labels,\n    y_pred=model_4_preds\n)\n\nprint(model_4_metrics)\n")),(0,l.kt)("h4",{id:"model-4-metrics"},"Model 4 Metrics"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Accuracy: 77.55905511811024"),(0,l.kt)("li",{parentName:"ul"},"Precision: 0.7777490986405654"),(0,l.kt)("li",{parentName:"ul"},"Recall: 0.7755905511811023"),(0,l.kt)("li",{parentName:"ul"},"F1: 0.7733619560087615")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# the model performs better than the gru but worse than the baseline model\nprint(np.array(list(model_4_metrics.values())) > np.array(list(model_3_metrics.values())))\n# [ True  True  True  True]\nprint(np.array(list(model_4_metrics.values())) > np.array(list(baseline_metrics.values())))\n# [False False False False]\n")),(0,l.kt)("h3",{id:"model-5-conv1d"},"Model 5: Conv1D"),(0,l.kt)("h4",{id:"model-building-and-training-4"},"Model Building and Training"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"inputs = Input(shape=(1,), dtype=tf.string)\nx = text_vectorizer(inputs)\nx = embedding(x)\nx = Conv1D(\n    filters=64,\n    kernel_size=5, # check 5 words at a time\n    activation='relu',\n    padding='valid',\n    strides=1\n)(x)\nx = GlobalMaxPool1D()(x)\noutputs = Dense(1, activation='sigmoid')(x)\n\nmodel_5 = tf.keras.Model(inputs, outputs, name='model_5_conv1d')\nmodel_5.summary()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},'Model: "model_5_conv1d"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_14 (InputLayer)       [(None, 1)]               0         \n                                                                 \n text_vectorization (TextVec  (None, 15)               0         \n torization)                                                     \n                                                                 \n embedding (Embedding)       (None, 15, 128)           1280000   \n                                                                 \n conv1d_6 (Conv1D)           (None, 11, 64)            41024     \n                                                                 \n global_max_pooling1d_5 (Glo  (None, 64)               0         \n balMaxPooling1D)                                                \n                                                                 \n dense_14 (Dense)            (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 1,321,089\nTrainable params: 1,321,089\nNon-trainable params: 0\n_________________________________________________________________\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_5.compile(\n    loss='binary_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(),\n    metrics=['accuracy']\n)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_5_history = model_5.fit(\n    train_tweets,\n    train_labels,\n    epochs=5,\n    validation_data=(val_tweets, val_labels),\n    callbacks=[create_tensorboard_callback_bugged(\n        dir_name=LOG_DIR,\n        experiment_name='model_5_conv1d'\n    )]\n)\n")),(0,l.kt)("h4",{id:"model-evaluation-4"},"Model Evaluation"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_5.evaluate(val_tweets, val_labels)\n# loss: 1.3018 - accuracy: 0.7454\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'# make predictions\nmodel_5_preds = model_5.predict(val_tweets)\nsample_prediction=model_5_preds[0]\nprint(f"Prediction: {sample_prediction}","(disaster)" if sample_prediction>0.5 else "(not a disaster)")\n# Prediction: [0.05387583] (not a disaster)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# convert model prediction probabilities to binary label format\nmodel_5_preds = tf.squeeze(tf.round(model_5_preds))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_5_metrics = calculate_metrics(\n    y_true=val_labels,\n    y_pred=model_5_preds\n)\n\nprint(model_5_metrics)\n")),(0,l.kt)("h4",{id:"model-5-metrics"},"Model 5 Metrics"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Accuracy: 74.67191601049869"),(0,l.kt)("li",{parentName:"ul"},"Precision: 0.7465631385096111"),(0,l.kt)("li",{parentName:"ul"},"Recall: 0.7467191601049868"),(0,l.kt)("li",{parentName:"ul"},"F1: 0.7453858813570734")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# the model performs worse than the gru and worse than the baseline model\nprint(np.array(list(model_5_metrics.values())) > np.array(list(model_4_metrics.values())))\n# [False False False False]\nprint(np.array(list(model_5_metrics.values())) > np.array(list(baseline_metrics.values())))\n# [False False False False]\n")),(0,l.kt)("h3",{id:"model-6-transfer-learning-feature-extractor"},"Model 6: Transfer Learning Feature Extractor"),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"USE")," Feature Extractor - ",(0,l.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1803.11175"},"Universal Sentence Encoder")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'# https://tfhub.dev/google/collections/universal-sentence-encoder/1\nembed = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'# test the encoder\nsample_sentence1 = "Next I\'m buying Coca-Cola to put the cocaine back in"\nsample_sentence2 = "Hey guys, wanna feel old? I\'m 40. You\'re welcome."\nsample_sentence3 = "Beef chicken pork bacon chuck shortloin sirloin shank eu, bresaola voluptate in enim ea kielbasa laboris brisket laborum, jowl labore id porkchop elit ad commodo."\n\nembed_sample = embed([\n    sample_sentence1,\n    sample_sentence2,\n    sample_sentence3\n])\n\nprint(embed_sample)\n\n# the encoder turns each input into size-512 feature vectors\n# [[ 0.06844153 -0.0325974  -0.01901028 ... -0.03307429 -0.04625704\n#  -0.08149158]\n# [ 0.02810995 -0.06714624  0.02414106 ... -0.02519046  0.03197665\n#   0.02462349]\n# [ 0.03188843 -0.0167392  -0.03194157 ... -0.04541751 -0.05822486\n#  -0.07237621]], shape=(3, 512), dtype=float32)\n')),(0,l.kt)("h4",{id:"model-building-and-training-5"},"Model Building and Training"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_6 = tf.keras.models.Sequential(name='model_6_use')\n\nmodel_6.add(hub.KerasLayer(\n    'https://tfhub.dev/google/universal-sentence-encoder/4', \n    input_shape=[], # layer excepts string of variable length and returns a 512 feature vector\n    dtype=tf.string, \n    trainable=False,\n    name='sentence-encoder'\n))\n\nmodel_6.add(Dense(1, activation='sigmoid'))\n\nmodel_6.summary()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},'Model: "model_6_use"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n sentence-encoder (KerasLaye  (None, 512)              256797824 \n r)                                                              \n                                                                 \n dense_5 (Dense)             (None, 1)                 513       \n                                                                 \n=================================================================\nTotal params: 256,798,337\nTrainable params: 513\nNon-trainable params: 256,797,824\n_________________________________________________________________\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_6.compile(\n    loss='binary_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(),\n    metrics=['accuracy']\n)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_6_history = model_6.fit(\n    train_tweets,\n    train_labels,\n    epochs=5,\n    validation_data=(val_tweets, val_labels),\n    callbacks=[create_tensorboard_callback_bugged(\n        dir_name=LOG_DIR,\n        experiment_name='model_6_use'\n    )]\n)\n")),(0,l.kt)("h4",{id:"model-evaluation-5"},"Model Evaluation"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_6.evaluate(val_tweets, val_labels)\n# loss: 0.4982 - accuracy: 0.7887\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'# make predictions\nmodel_6_preds = model_6.predict(val_tweets)\nsample_prediction=model_6_preds[0]\nprint(f"Prediction: {sample_prediction}","(disaster)" if sample_prediction>0.5 else "(not a disaster)")\n# Prediction: [0.36132362] (not a disaster)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# convert model prediction probabilities to binary label format\nmodel_6_preds = tf.squeeze(tf.round(model_6_preds))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_6_metrics = calculate_metrics(\n    y_true=val_labels,\n    y_pred=model_6_preds\n)\n\nprint(model_6_metrics)\n")),(0,l.kt)("h4",{id:"model-6-metrics"},"Model 6 Metrics"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Accuracy: 78.87139107611549"),(0,l.kt)("li",{parentName:"ul"},"Precision: 0.7891485217486439"),(0,l.kt)("li",{parentName:"ul"},"Recall: 0.7887139107611548"),(0,l.kt)("li",{parentName:"ul"},"F1: 0.7876016937745534")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# the model performs better than the conv1d and gettin close to the baseline model\nprint(np.array(list(model_6_metrics.values())) > np.array(list(model_5_metrics.values())))\n# [ True  True  True  True]\nprint(np.array(list(model_6_metrics.values())) > np.array(list(baseline_metrics.values())))\n# [False False False  True]\n")),(0,l.kt)("h3",{id:"model-6a-added-dense-layer"},"Model 6a (added Dense Layer)"),(0,l.kt)("p",null,"The model get's close to the baseline model - I will try to add another Dense layer and see if this improves the performance."),(0,l.kt)("p",null,"Otherwise the model is identical - only written for functional API (no reason, just for practise :) )"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"inputs = Input(shape=[], dtype=tf.string)\n\nx = hub.KerasLayer(\n    'https://tfhub.dev/google/universal-sentence-encoder/4', \n    trainable=False,\n    dtype=tf.string,\n    name='sentence-encoder'\n)(inputs)\n\nx = Dense(64, activation='relu')(x)\noutput = Dense(1, activation='sigmoid')(x)\n\nmodel_6a = tf.keras.models.Model(inputs, output, name='model_6_use')\nmodel_6a.summary()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},'Model: "model_6_use"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_6 (InputLayer)        [(None,)]                 0         \n                                                                 \n sentence-encoder (KerasLaye  (None, 512)              256797824 \n r)                                                              \n                                                                 \n dense_13 (Dense)            (None, 64)                32832     \n                                                                 \n dense_14 (Dense)            (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 256,830,721\nTrainable params: 32,897\nNon-trainable params: 256,797,824\n_________________________________________________________________\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_6a.compile(\n    loss='binary_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(),\n    metrics=['accuracy']\n)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_6a_history = model_6a.fit(\n    train_tweets,\n    train_labels,\n    epochs=5,\n    validation_data=(val_tweets, val_labels),\n    callbacks=[create_tensorboard_callback_bugged(\n        dir_name=LOG_DIR,\n        experiment_name='model_6a_use'\n    )]\n)\n")),(0,l.kt)("h4",{id:"model-evaluation-6"},"Model Evaluation"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_6a.evaluate(val_tweets, val_labels)\n# loss: 0.4305 - accuracy: 0.8123\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'# make predictions\nmodel_6a_preds = model_6a.predict(val_tweets)\nsample_prediction=model_6a_preds[0]\nprint(f"Prediction: {sample_prediction}","(disaster)" if sample_prediction>0.5 else "(not a disaster)")\n# Prediction: [0.15243901] (not a disaster)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# convert model prediction probabilities to binary label format\nmodel_6a_preds = tf.squeeze(tf.round(model_6a_preds))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_6a_metrics = calculate_metrics(\n    y_true=val_labels,\n    y_pred=model_6a_preds\n)\n\nprint(model_6a_metrics)\n")),(0,l.kt)("h4",{id:"model-6a-metrics"},"Model 6a Metrics"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Accuracy: 81.23359580052494"),(0,l.kt)("li",{parentName:"ul"},"Precision: 0.8148798668657973"),(0,l.kt)("li",{parentName:"ul"},"Recall: 0.8123359580052494"),(0,l.kt)("li",{parentName:"ul"},"F1: 0.810686575717776")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# the model performs better than the conv1d AND better than the baseline model~!\nprint(np.array(list(model_6a_metrics.values())) > np.array(list(model_5_metrics.values())))\n# [ True  True  True  True]\nprint(np.array(list(model_6a_metrics.values())) > np.array(list(baseline_metrics.values())))\n# [ True  True  True  True]\n")),(0,l.kt)("h3",{id:"model-6b-transfer-learning-feature-extractor-10-dataset"},"Model 6b: Transfer Learning Feature Extractor (10% Dataset)"),(0,l.kt)("h4",{id:"dataset-1"},"Dataset"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# create data subset with 10% of the training data\ntrain_10_percent = train_df_shuffle[['text', 'target']].sample(frac=0.1, random_state=SEED)\nprint(len(train_df_shuffle), len(train_10_percent))\n# 7613 761\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"train_tweets_10_percent = train_10_percent['text'].to_list()\ntrain_labels_10_percent = train_10_percent['target'].to_list()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# check label distribution in randomized subset\ndist_full_dataset = train_df_shuffle['target'].value_counts()\ndist_10_percent_subset = train_10_percent['target'].value_counts()\n\nprint(\n    (dist_full_dataset[0]/dist_full_dataset[1]),\n    (dist_10_percent_subset[0]/dist_10_percent_subset[1])\n)\n\n# the full dataset has 33% more \"no-desaster\" tweets\n# in the subset the overhang is lower with 19%\n# 1.3274228064811984 1.1867816091954022\n")),(0,l.kt)("h4",{id:"model-building-and-training-6"},"Model Building and Training"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_6b = tf.keras.models.Sequential(name='model_6b_use_10_percent')\n\nmodel_6b.add(hub.KerasLayer(\n    'https://tfhub.dev/google/universal-sentence-encoder/4', \n    input_shape=[], # layer excepts string of variable length and returns a 512 feature vector\n    dtype=tf.string, \n    trainable=False,\n    name='sentence-encoder'\n))\n\nmodel_6b.add(Dense(64, activation='relu'))\nmodel_6b.add(Dense(1, activation='sigmoid'))\n\nmodel_6b.summary()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},'Model: "model_6b_use_10_percent"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n sentence-encoder (KerasLaye  (None, 512)              256797824 \n r)                                                              \n                                                                 \n dense_10 (Dense)            (None, 64)                32832     \n                                                                 \n dense_11 (Dense)            (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 256,830,721\nTrainable params: 32,897\nNon-trainable params: 256,797,824\n_________________________________________________________________\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_6b.compile(\n    loss='binary_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(),\n    metrics=['accuracy']\n)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_6b_history = model_6b.fit(\n    train_tweets_10_percent,\n    train_labels_10_percent,\n    epochs=5,\n    validation_data=(val_tweets, val_labels),\n    callbacks=[create_tensorboard_callback_bugged(\n        dir_name=LOG_DIR,\n        experiment_name='model_6b_use'\n    )]\n)\n")),(0,l.kt)("h4",{id:"model-evaluation-7"},"Model Evaluation"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_6b.evaluate(val_tweets, val_labels)\n# loss: 0.3375 - accuracy: 0.8675\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'# make predictions\nmodel_6b_preds = model_6b.predict(val_tweets)\nsample_prediction=model_6b_preds[0]\nprint(f"Prediction: {sample_prediction}","(disaster)" if sample_prediction>0.5 else "(not a disaster)")\n# Prediction: [0.16887127] (not a disaster)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# convert model prediction probabilities to binary label format\nmodel_6b_preds = tf.squeeze(tf.round(model_6b_preds))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_6b_metrics = calculate_metrics(\n    y_true=val_labels,\n    y_pred=model_6b_preds\n)\n\nprint(model_6b_metrics)\n")),(0,l.kt)("h4",{id:"model-6b-metrics"},"Model 6b Metrics"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Accuracy: 86.74540682414698"),(0,l.kt)("li",{parentName:"ul"},"Precision: 0.8695676801900293"),(0,l.kt)("li",{parentName:"ul"},"Recall: 0.8674540682414699"),(0,l.kt)("li",{parentName:"ul"},"F1: 0.8666326892977956")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# the model performs even better with only 10% AND better than the baseline model ?\nprint(np.array(list(model_6b_metrics.values())) > np.array(list(model_6_metrics.values())))\n# [ True  True  True  True]\nprint(np.array(list(model_6b_metrics.values())) > np.array(list(baseline_metrics.values())))\n# [ True  True  True  True]\n")),(0,l.kt)("h3",{id:"model-6c-use-data-leakage-issue-10-dataset"},"Model 6c: USE Data Leakage Issue (10% Dataset)"),(0,l.kt)("h4",{id:"data-leakage-issue"},"Data Leakage Issue"),(0,l.kt)("p",null,"Both the random 10% subset and the validation data was taken from the shuffled data ",(0,l.kt)("inlineCode",{parentName:"p"},"train_df_shuffle"),". This means that the new training dataset might contain a small amount of data entries that are also part of the validation set. Since we are now only working with 10% of the original dataset this data leakage can affect our training massively."),(0,l.kt)("p",null,"The model needs to be retrained with a clean training dataset..."),(0,l.kt)("h5",{id:"training-10-split"},"Training 10% Split"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# create data subset with 10% of the training data\ntrain_10_percent_split = int(0.1 * len(train_tweets))\ntrain_tweets_10_percent_clean = train_tweets[:train_10_percent_split]\ntrain_labels_10_percent_clean = train_labels[:train_10_percent_split]\nprint(len(train_tweets), len(train_tweets_10_percent_clean))\n# 6851 685\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'# check label distribution in randomized subset\ndist_train_dataset = pd.Series(train_labels).value_counts()\ndist_10_percent_train_subset = pd.Series(train_labels_10_percent_clean).value_counts()\n\n\nprint(dist_train_dataset, dist_10_percent_train_subset)\n# 0    3928\n# 1    2923\n# ---------\n# 406\n# 279\n\nprint(\n    (dist_train_dataset[0]/dist_train_dataset[1]),\n    (dist_10_percent_train_subset[0]/dist_10_percent_train_subset[1])\n)\n\n# the full dataset has 34% more "no-desaster" tweets\n# in the subset the overhang is lower with 46% - not ideal\n# 1.3438248374957236 1.4551971326164874\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# use the same model as before but with cleaned weights\nmodel_6c = tf.keras.models.clone_model(model_6b)\n\nmodel_6c.compile(\n    loss='binary_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(),\n    metrics=['accuracy']\n)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_6c_history = model_6c.fit(\n    train_tweets_10_percent_clean,\n    train_labels_10_percent_clean,\n    epochs=5,\n    validation_data=(val_tweets, val_labels),\n    callbacks=[create_tensorboard_callback_bugged(\n        dir_name=LOG_DIR,\n        experiment_name='model_6c_use'\n    )]\n)\n")),(0,l.kt)("h4",{id:"model-evaluation-8"},"Model Evaluation"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_6b.evaluate(val_tweets, val_labels)\n# evaluation with data leakage - performance suspiciously high\n# loss: 0.3375 - accuracy: 0.8675\nmodel_6c.evaluate(val_tweets, val_labels)\n# the model trained on the cleaned-up 10% dataset - as expected -\n# performs worse than the model trained on the full dataset\n# loss: 0.4904 - accuracy: 0.7822\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'# make predictions\nmodel_6c_preds = model_6c.predict(val_tweets)\nsample_prediction=model_6c_preds[0]\nprint(f"Prediction: {sample_prediction}","(disaster)" if sample_prediction>0.5 else "(not a disaster)")\n# Prediction: [0.22155239] (not a disaster)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# convert model prediction probabilities to binary label format\nmodel_6c_preds = tf.squeeze(tf.round(model_6c_preds))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_6c_metrics = calculate_metrics(\n    y_true=val_labels,\n    y_pred=model_6c_preds\n)\n\nprint(model_6c_metrics)\n")),(0,l.kt)("h4",{id:"model-6c-metrics"},"Model 6c Metrics"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Accuracy: 78.21522309711287"),(0,l.kt)("li",{parentName:"ul"},"Precision: 0.7827333187650908"),(0,l.kt)("li",{parentName:"ul"},"Recall: 0.7821522309711286"),(0,l.kt)("li",{parentName:"ul"},"F1: 0.7808542546238821")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# the model performs worse than the baseline but gets close\n# considering that we only used 10% of the data\nprint(np.array(list(model_6c_metrics.values())) > np.array(list(model_6b_metrics.values())))\n# [False False False False]\nprint(np.array(list(model_6c_metrics.values())) > np.array(list(baseline_metrics.values())))\n# [False False False False]\n")),(0,l.kt)("h2",{id:"compare-experiments"},"Compare Experiments"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'# combine training results in dataframe\nmodel_metrics_all = pd.DataFrame({\n    "0_baseline": baseline_metrics,\n    "1_simple_dense": model_1_metrics,\n    "2_lstm": model_2_metrics,\n    "3_gru": model_3_metrics,\n    "4_bidirectional": model_4_metrics,\n    "5_conv1d": model_5_metrics,\n    "6_use": model_6_metrics,\n    "6a_use_added_dense": model_6a_metrics,\n    "6b_use_10_percent": model_6b_metrics,\n    "6c_use_10_percent_cleaned": model_6c_metrics\n})\n\n# transpose\nmodel_metrics_all_transposed = model_metrics_all.transpose()\n\n# rescale accuracy\nmodel_metrics_all_transposed[\'accuracy\'] = model_metrics_all_transposed[\'accuracy\']/100\n\n# sort by F1-score\nmodel_metrics_all_transposed_sorted = model_metrics_all_transposed.sort_values(by=[\'f1\'], ascending=False)\nmodel_metrics_all_transposed_sorted\n# The best performing 6b_use_10_percent is faulty\n# The next best is the USE with 100% of training data 6a_use_added_dense\n# The USE with 10% training data performs closely to the baseline and simple dense model\n')),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null}),(0,l.kt)("th",{parentName:"tr",align:null},"accuracy"),(0,l.kt)("th",{parentName:"tr",align:null},"precision"),(0,l.kt)("th",{parentName:"tr",align:null},"recall"),(0,l.kt)("th",{parentName:"tr",align:null},"f1"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"6b_use_10_percent"),(0,l.kt)("td",{parentName:"tr",align:null},"0.867454"),(0,l.kt)("td",{parentName:"tr",align:null},"0.869568"),(0,l.kt)("td",{parentName:"tr",align:null},"0.867454"),(0,l.kt)("td",{parentName:"tr",align:null},"0.866633")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"6a_use_added_dense"),(0,l.kt)("td",{parentName:"tr",align:null},"0.813648"),(0,l.kt)("td",{parentName:"tr",align:null},"0.814169"),(0,l.kt)("td",{parentName:"tr",align:null},"0.813648"),(0,l.kt)("td",{parentName:"tr",align:null},"0.812791")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"0_baseline"),(0,l.kt)("td",{parentName:"tr",align:null},"0.792651"),(0,l.kt)("td",{parentName:"tr",align:null},"0.811139"),(0,l.kt)("td",{parentName:"tr",align:null},"0.792651"),(0,l.kt)("td",{parentName:"tr",align:null},"0.786219")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"1_simple_dense"),(0,l.kt)("td",{parentName:"tr",align:null},"0.788714"),(0,l.kt)("td",{parentName:"tr",align:null},"0.796962"),(0,l.kt)("td",{parentName:"tr",align:null},"0.788714"),(0,l.kt)("td",{parentName:"tr",align:null},"0.784729")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"6_use"),(0,l.kt)("td",{parentName:"tr",align:null},"0.784777"),(0,l.kt)("td",{parentName:"tr",align:null},"0.785083"),(0,l.kt)("td",{parentName:"tr",align:null},"0.784777"),(0,l.kt)("td",{parentName:"tr",align:null},"0.783692")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"6c_use_10_percent_cleaned"),(0,l.kt)("td",{parentName:"tr",align:null},"0.782152"),(0,l.kt)("td",{parentName:"tr",align:null},"0.782733"),(0,l.kt)("td",{parentName:"tr",align:null},"0.782152"),(0,l.kt)("td",{parentName:"tr",align:null},"0.780854")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"2_lstm"),(0,l.kt)("td",{parentName:"tr",align:null},"0.775591"),(0,l.kt)("td",{parentName:"tr",align:null},"0.776962"),(0,l.kt)("td",{parentName:"tr",align:null},"0.775591"),(0,l.kt)("td",{parentName:"tr",align:null},"0.773741")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"3_gru"),(0,l.kt)("td",{parentName:"tr",align:null},"0.770341"),(0,l.kt)("td",{parentName:"tr",align:null},"0.773598"),(0,l.kt)("td",{parentName:"tr",align:null},"0.770341"),(0,l.kt)("td",{parentName:"tr",align:null},"0.767494")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"4_bidirectional"),(0,l.kt)("td",{parentName:"tr",align:null},"0.765092"),(0,l.kt)("td",{parentName:"tr",align:null},"0.764730"),(0,l.kt)("td",{parentName:"tr",align:null},"0.765092"),(0,l.kt)("td",{parentName:"tr",align:null},"0.764430")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"5_conv1d"),(0,l.kt)("td",{parentName:"tr",align:null},"0.759843"),(0,l.kt)("td",{parentName:"tr",align:null},"0.759995"),(0,l.kt)("td",{parentName:"tr",align:null},"0.759843"),(0,l.kt)("td",{parentName:"tr",align:null},"0.758468")))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# plot model results\nmodel_metrics_all_transposed_sorted.plot(\n    kind='bar',\n    rot=30,\n    figsize=(12,8)).legend(bbox_to_anchor=(1.0, 1.0)\n)\n")),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"(Re) Introduction to Tensorflow Natural Language Processing",src:a(74816).Z,width:"1101",height:"755"})),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# Load TensorBoard\n%load_ext tensorboard\n%tensorboard --logdir './tensorboad/'\n")),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"(Re) Introduction to Tensorflow Natural Language Processing",src:a(57116).Z,width:"847",height:"649"})),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"(Re) Introduction to Tensorflow Natural Language Processing",src:a(82441).Z,width:"844",height:"581"})),(0,l.kt)("h2",{id:"saving--loading-trained-model"},"Saving & Loading Trained Model"),(0,l.kt)("h3",{id:"hdf5-format-higher-compatibility-to-3rd-parties"},"HDF5 Format (Higher Compatibility to 3rd Parties)"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# save best performing model to HDF5 format\nmodel_6a.save('./models/6a_use_added_dense.h5')\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# restore model with custom TF Hub layer (only HDF5 format)\n# restoring requires import tensorflow_hub as hub\nloaded_model_6a = tf.keras.models.load_model(\n    './models/6a_use_added_dense.h5',\n    custom_objects={'KerasLayer': hub.KerasLayer}\n)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# verify loaded model\nloaded_model_6a.evaluate(val_tweets, val_labels) == model_6a.evaluate(val_tweets, val_labels)\n# True\n")),(0,l.kt)("h3",{id:"saved-model-format-tensorflow-default"},"Saved Model Format (Tensorflow Default)"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# save best performing model to saved_model format\nmodel_6a.save('./models/6a_use_added_dense')\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"loaded_model_6a_saved_model = tf.keras.models.load_model(\n    './models/6a_use_added_dense',\n)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# verify loaded model\nloaded_model_6a_saved_model.evaluate(val_tweets, val_labels) == model_6a.evaluate(val_tweets, val_labels)\n# True\n")),(0,l.kt)("h2",{id:"best-model-evaluation"},"Best Model Evaluation"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# find most wrong predictions\n## create dataframe with validation tweets, labels and model predictions\nloaded_model_pred_probs = tf.squeeze(loaded_model_6a_saved_model.predict(val_tweets))\nloaded_model_preds = tf.round(loaded_model_preds)\nprint(loaded_model_preds[:5])\n# [0. 1. 1. 0. 1.]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'pred_df = pd.DataFrame({\n    "text": val_tweets,\n    "target": val_labels,\n    "pred": loaded_model_preds,\n    "pred_prob": loaded_model_pred_probs\n})\n\npred_df\n')),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null}),(0,l.kt)("th",{parentName:"tr",align:null},"text"),(0,l.kt)("th",{parentName:"tr",align:null},"target"),(0,l.kt)("th",{parentName:"tr",align:null},"pred"),(0,l.kt)("th",{parentName:"tr",align:null},"pred_prob"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"0"),(0,l.kt)("td",{parentName:"tr",align:null},"DFR EP016 Monthly Meltdown - On Dnbheaven 2015..."),(0,l.kt)("td",{parentName:"tr",align:null},"0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.191866")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"1"),(0,l.kt)("td",{parentName:"tr",align:null},"FedEx no longer to transport bioterror germs i..."),(0,l.kt)("td",{parentName:"tr",align:null},"0"),(0,l.kt)("td",{parentName:"tr",align:null},"1.0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.793415")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"2"),(0,l.kt)("td",{parentName:"tr",align:null},"Gunmen kill four in El Salvador bus attack: Su..."),(0,l.kt)("td",{parentName:"tr",align:null},"1"),(0,l.kt)("td",{parentName:"tr",align:null},"1.0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.992559")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"3"),(0,l.kt)("td",{parentName:"tr",align:null},"@camilacabello97 Internally and externally scr..."),(0,l.kt)("td",{parentName:"tr",align:null},"1"),(0,l.kt)("td",{parentName:"tr",align:null},"0.0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.255227")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"4"),(0,l.kt)("td",{parentName:"tr",align:null},"Radiation emergency #preparedness starts with ..."),(0,l.kt)("td",{parentName:"tr",align:null},"1"),(0,l.kt)("td",{parentName:"tr",align:null},"1.0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.722030")))),(0,l.kt)("p",null,"...\n| 757 | That's the ultimate road to destruction | 0 | 0.0 | 0.141685 |\n| 758 | @SetZorah dad why dont you claim me that mean ... | 0 | 0.0 | 0.111817 |\n| 759 | FedEx will no longer transport bioterror patho... | 0 | 1.0 | 0.898317 |\n| 760 | Crack in the path where I wiped out this morni... | 0 | 1.0 | 0.671607 |\n| 761 | I liked a @YouTube video from @dannyonpc http:... | 0 | 0.0 | 0.127992 |"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# create another datframe that only contains wrong predictions\nmost_wrong = pred_df[pred_df['target'] != pred_df['pred']].sort_values(\n    'pred_prob',\n    ascending=False\n)\n\nmost_wrong\n")),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null}),(0,l.kt)("th",{parentName:"tr",align:null},"text"),(0,l.kt)("th",{parentName:"tr",align:null},"target"),(0,l.kt)("th",{parentName:"tr",align:null},"pred"),(0,l.kt)("th",{parentName:"tr",align:null},"pred_prob"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"31"),(0,l.kt)("td",{parentName:"tr",align:null},"? High Skies - Burning Buildings ? ",(0,l.kt)("a",{parentName:"td",href:"http://t.co"},"http://t.co"),"..."),(0,l.kt)("td",{parentName:"tr",align:null},"0"),(0,l.kt)("td",{parentName:"tr",align:null},"1.0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.936771")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"628"),(0,l.kt)("td",{parentName:"tr",align:null},"@noah_anyname That's where the concentration c..."),(0,l.kt)("td",{parentName:"tr",align:null},"0"),(0,l.kt)("td",{parentName:"tr",align:null},"1.0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.907564")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"759"),(0,l.kt)("td",{parentName:"tr",align:null},"FedEx will no longer transport bioterror patho..."),(0,l.kt)("td",{parentName:"tr",align:null},"0"),(0,l.kt)("td",{parentName:"tr",align:null},"1.0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.898317")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"251"),(0,l.kt)("td",{parentName:"tr",align:null},"@AshGhebranious civil rights continued in the ..."),(0,l.kt)("td",{parentName:"tr",align:null},"0"),(0,l.kt)("td",{parentName:"tr",align:null},"1.0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.854326")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"49"),(0,l.kt)("td",{parentName:"tr",align:null},"@madonnamking RSPCA site multiple 7 story high..."),(0,l.kt)("td",{parentName:"tr",align:null},"0"),(0,l.kt)("td",{parentName:"tr",align:null},"1.0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.853781")))),(0,l.kt)("p",null,"...\n| 233 | I get to smoke my shit in peace | 1 | 0.0 | 0.052748 |\n| 411 | @SoonerMagic",(0,l.kt)("em",{parentName:"p"}," I mean I'm a fan but I don't nee... | 1 | 0.0 | 0.046553 |\n| 244 | Reddit Will Now Quarantine\x89\xdb")," ",(0,l.kt)("a",{parentName:"p",href:"http://t.co/pkUA"},"http://t.co/pkUA"),"... | 1 | 0.0 | 0.044474 |\n| 23 | Ron ","&"," Fez - Dave's High School Crush https... | 1 | 0.0 | 0.040853 |\n| 38 | Why are you deluged with low self-image? Take ... | 1 | 0.0 | 0.040229 |"),(0,l.kt)("h4",{id:"false-positives"},"False Positives"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'for row in most_wrong[:10].itertuples():\n    _, text, target, pred, pred_prob = row\n    print(f"Target: {target}, Pred: {pred}, Probability: {pred_prob}")\n    print(f"Tweet: {text}")\n    print("----\\n")\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"Target: 0, Pred: 1.0, Probability: 0.936771035194397\nTweet: ? High Skies - Burning Buildings ? http://t.co/uVq41i3Kx2 #nowplaying\n----\n\nTarget: 0, Pred: 1.0, Probability: 0.9075638055801392\nTweet: @noah_anyname That's where the concentration camps and mass murder come in. \n \nEVERY. FUCKING. TIME.\n----\n\nTarget: 0, Pred: 1.0, Probability: 0.8983168601989746\nTweet: FedEx will no longer transport bioterror pathogens in wake of anthrax lab mishaps http://t.co/lHpgxc4b8J\n----\n\nTarget: 0, Pred: 1.0, Probability: 0.8543258309364319\nTweet: @AshGhebranious civil rights continued in the 60s. And what about trans-generational trauma? if anything we should listen to the Americans.\n----\n\nTarget: 0, Pred: 1.0, Probability: 0.8537805676460266\nTweet: @madonnamking RSPCA site multiple 7 story high rise buildings next to low density character residential in an area that floods\n----\n\nTarget: 0, Pred: 1.0, Probability: 0.8336963057518005\nTweet: [55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES WITH MAGNE-TRACTION INSTRUCTIONS http://t.co/xEZBs3sq0y http://t.co/C2x0QoKGlY\n----\n\nTarget: 0, Pred: 1.0, Probability: 0.833143413066864\nTweet: Ashes 2015: Australia\x89\xdb\xaas collapse at Trent Bridge among worst in history: England bundled out Australia for 60 ... http://t.co/t5TrhjUAU0\n----\n\nTarget: 0, Pred: 1.0, Probability: 0.8305423855781555\nTweet: @SonofLiberty357 all illuminated by the brightly burning buildings all around the town!\n----\n\nTarget: 0, Pred: 1.0, Probability: 0.8277301788330078\nTweet: Deaths 3 http://t.co/nApviyGKYK\n----\n\nTarget: 0, Pred: 1.0, Probability: 0.8111591935157776\nTweet: The Sound of Arson\n----\n")),(0,l.kt)("h4",{id:"false-negatives"},"False Negatives"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'for row in most_wrong[-10:].itertuples():\n    _, text, target, pred, pred_prob = row\n    print(f"Target: {target}, Pred: {pred}, Probability: {pred_prob}")\n    print(f"Tweet: {text}")\n    print("----\\n")\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"Target: 1, Pred: 0.0, Probability: 0.07530134916305542\nTweet: going to redo my nails and watch behind the scenes of desolation of smaug ayyy\n----\n\nTarget: 1, Pred: 0.0, Probability: 0.07096730172634125\nTweet: @DavidVonderhaar At least you were sincere ??\n----\n\nTarget: 1, Pred: 0.0, Probability: 0.06341199576854706\nTweet: Lucas Duda is Ghost Rider. Not the Nic Cage version but an actual 'engulfed in flames' badass. #Mets\n----\n\nTarget: 1, Pred: 0.0, Probability: 0.06147787347435951\nTweet: You can never escape me. Bullets don't harm me. Nothing harms me. But I know pain. I know pain. Sometimes I share it. With someone like you.\n----\n\nTarget: 1, Pred: 0.0, Probability: 0.05444430932402611\nTweet: @willienelson We need help! Horses will die!Please RT &amp; sign petition!Take a stand &amp; be a voice for them! #gilbert23 https://t.co/e8dl1lNCVu\n----\n\nTarget: 1, Pred: 0.0, Probability: 0.0527476891875267\nTweet: I get to smoke my shit in peace\n----\n\nTarget: 1, Pred: 0.0, Probability: 0.04655275493860245\nTweet: @SoonerMagic_ I mean I'm a fan but I don't need a girl sounding off like a damn siren\n----\n\nTarget: 1, Pred: 0.0, Probability: 0.04447368532419205\nTweet: Reddit Will Now Quarantine\x89\xdb_ http://t.co/pkUAMXw6pm #onlinecommunities #reddit #amageddon #freespeech #Business http://t.co/PAWvNJ4sAP\n----\n\nTarget: 1, Pred: 0.0, Probability: 0.04085317254066467\nTweet: Ron &amp; Fez - Dave's High School Crush https://t.co/aN3W16c8F6 via @YouTube\n----\n\nTarget: 1, Pred: 0.0, Probability: 0.0402291864156723\nTweet: Why are you deluged with low self-image? Take the quiz: http://t.co/XsPqdOrIqj http://t.co/CQYvFR4UCy\n----\n")),(0,l.kt)("h2",{id:"test-dataset-predictions"},"Test Dataset Predictions"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# get the test dataset and randomize\ntest_df_shuffle = test_df.sample(frac=1, random_state=SEED)\n\ntest_df_shuffle.head(5)\n")),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null}),(0,l.kt)("th",{parentName:"tr",align:null},"id"),(0,l.kt)("th",{parentName:"tr",align:null},"keyword"),(0,l.kt)("th",{parentName:"tr",align:null},"location"),(0,l.kt)("th",{parentName:"tr",align:null},"text"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"2406"),(0,l.kt)("td",{parentName:"tr",align:null},"8051"),(0,l.kt)("td",{parentName:"tr",align:null},"refugees"),(0,l.kt)("td",{parentName:"tr",align:null},"NaN"),(0,l.kt)("td",{parentName:"tr",align:null},"Refugees as citizens - The Hindu ",(0,l.kt)("a",{parentName:"td",href:"http://t.co/G"},"http://t.co/G"),"...")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"134"),(0,l.kt)("td",{parentName:"tr",align:null},"425"),(0,l.kt)("td",{parentName:"tr",align:null},"apocalypse"),(0,l.kt)("td",{parentName:"tr",align:null},"Currently Somewhere On Earth"),(0,l.kt)("td",{parentName:"tr",align:null},"@5SOStag honestly he could say an apocalypse i...")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"411"),(0,l.kt)("td",{parentName:"tr",align:null},"1330"),(0,l.kt)("td",{parentName:"tr",align:null},"blown%20up"),(0,l.kt)("td",{parentName:"tr",align:null},"Scout Team"),(0,l.kt)("td",{parentName:"tr",align:null},"If you bored as shit don't nobody fuck wit you...")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"203"),(0,l.kt)("td",{parentName:"tr",align:null},"663"),(0,l.kt)("td",{parentName:"tr",align:null},"attack"),(0,l.kt)("td",{parentName:"tr",align:null},"NaN"),(0,l.kt)("td",{parentName:"tr",align:null},"@RealTwanBrown Yesterday I Had A Heat Attack ?...")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"889"),(0,l.kt)("td",{parentName:"tr",align:null},"2930"),(0,l.kt)("td",{parentName:"tr",align:null},"danger"),(0,l.kt)("td",{parentName:"tr",align:null},"Leeds"),(0,l.kt)("td",{parentName:"tr",align:null},"The Devil Wears Prada is still one of my favou...")))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"test_tweets = test_df_shuffle['text']\ntest_tweets = np.array(test_tweets.values.tolist())\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"loaded_model_pred_probs_test = tf.squeeze(loaded_model_6a_saved_model.predict(test_tweets))\nloaded_model_preds_test = tf.round(loaded_model_pred_probs_test)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'pred_test_df = pd.DataFrame({\n    "text": test_tweets,\n    "pred": loaded_model_preds_test,\n    "pred_prob": loaded_model_pred_probs_test\n})\n\npred_test_df\n')),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null}),(0,l.kt)("th",{parentName:"tr",align:null},"text"),(0,l.kt)("th",{parentName:"tr",align:null},"pred"),(0,l.kt)("th",{parentName:"tr",align:null},"pred_prob"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"0"),(0,l.kt)("td",{parentName:"tr",align:null},"Refugees as citizens - The Hindu ",(0,l.kt)("a",{parentName:"td",href:"http://t.co/G"},"http://t.co/G"),"..."),(0,l.kt)("td",{parentName:"tr",align:null},"1.0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.717548")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"1"),(0,l.kt)("td",{parentName:"tr",align:null},"@5SOStag honestly he could say an apocalypse i..."),(0,l.kt)("td",{parentName:"tr",align:null},"0.0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.093809")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"2"),(0,l.kt)("td",{parentName:"tr",align:null},"If you bored as shit don't nobody fuck wit you..."),(0,l.kt)("td",{parentName:"tr",align:null},"0.0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.082749")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"3"),(0,l.kt)("td",{parentName:"tr",align:null},"@RealTwanBrown Yesterday I Had A Heat Attack ?..."),(0,l.kt)("td",{parentName:"tr",align:null},"0.0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.097784")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"4"),(0,l.kt)("td",{parentName:"tr",align:null},"The Devil Wears Prada is still one of my favou..."),(0,l.kt)("td",{parentName:"tr",align:null},"0.0"),(0,l.kt)("td",{parentName:"tr",align:null},"0.028455")))),(0,l.kt)("p",null,"...\n| 3258 | Free Kindle Book - Aug 3-7 - Thriller - Desola... | 0.0 | 0.080420 |\n| 3259 | HitchBot travels Europe and greeted with open ... | 1.0 | 0.603396 |\n| 3260 | If you told me you was drowning. I would not l... | 0.0 | 0.208108 |\n| 3261 | First time for everything! @ Coney Island Cycl... | 1.0 | 0.617582 |\n| 3262 | Rocky Fire #cali #SCFD #wildfire #LakeCounty h... | 1.0 | 0.794876 |"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'# make random prediction on sub-samples of the test dataset\ntest_samples = random.sample(test_tweets.tolist(), 10)\n\nfor test_sample in test_samples:\n    pred_prob = tf.squeeze(loaded_model_6a_saved_model.predict([test_sample]))\n    pred = tf.round(pred_prob)\n    \n    print(f"Pred: {int(pred)}, Probability: {pred_prob}")\n    print(f"Text: {test_sample}")\n    print("-------\\n")\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"1/1 [==============================] - 0s 26ms/step\nPred: 0, Probability: 0.15592874586582184\nText: @EllaEMusic_ You should have just simply let on that you had electrocuted yourself while plugging in your phone charger. It works for me...\n-------\n\n1/1 [==============================] - 0s 25ms/step\nPred: 1, Probability: 0.5131102800369263\nText: Evacuation drill at work. The fire doors wouldn't open so i got to smash the emergency release glass #feelingmanly\n-------\n\n1/1 [==============================] - 0s 33ms/step\nPred: 1, Probability: 0.9751215577125549\nText: Rare photographs show the nightmare aftermath of #Hiroshima | #NoNukes #Amerikkka #WhiteTerrorism #Nuclear #Disaster  http://t.co/8tWLAKdaBf\n-------\n\n1/1 [==============================] - 0s 28ms/step\nPred: 1, Probability: 0.676400899887085\nText: kayaking about killed us so mom and grandma came to the rescue.\n-------\n\n1/1 [==============================] - 0s 26ms/step\nPred: 1, Probability: 0.9690964818000793\nText: Pasco officials impressed by drone video showing floods: Drone videos have given an up-close view of flooded areas\x89\xdb_ http://t.co/PrUunEDids\n-------\n\n1/1 [==============================] - 0s 29ms/step\nPred: 0, Probability: 0.07793092727661133\nText: just trying to smoke and get Taco Bell\n-------\n\n1/1 [==============================] - 0s 26ms/step\nPred: 1, Probability: 0.9799614548683167\nText: RT AbbsWinston: #Zionist #Terrorist kidnapped 15 #Palestinians in overnight terror on Palestinian Villages \x89\xdb_ http://t.co/J5mKcbKcov\n-------\n\n1/1 [==============================] - 0s 24ms/step\nPred: 1, Probability: 0.8005449771881104\nText: ME: gun shot wounds 3 4 6 7 'rapidly lethal' would have killed in 30-60 seconds or few minutes max. #kerricktrial\n-------\n\n1/1 [==============================] - 0s 24ms/step\nPred: 1, Probability: 0.9902410507202148\nText: Three Israeli soldiers wounded in West Bank terrorist attack - Haaretz http://t.co/Mwd1iPMoWT #world\n-------\n\n1/1 [==============================] - 0s 29ms/step\nPred: 0, Probability: 0.03447313234210014\nText: @_Souuul * gains super powers im now lava girl throws you ina chest wrapped in chains &amp; sinks you down the the bottom of the ocean*\n-------\n")),(0,l.kt)("h2",{id:"speedscore-tradeoff"},"Speed/Score Tradeoff"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def time_to_prediction(model, samples):\n    start_time = time.perf_counter()\n    model.predict(samples)\n    end_time = time.perf_counter()\n    time_to_prediction = end_time - start_time\n    prediction_time_weighted = time_to_prediction / len(samples)\n\n    return time_to_prediction, prediction_time_weighted\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_6_time_to_prediction, model_6_prediction_time_weighted = time_to_prediction(\n    model = loaded_model_6a_saved_model,\n    samples = test_tweets\n)\n\nprint(model_6_time_to_prediction, model_6_prediction_time_weighted)\n# 0.4922969689941965 0.0001508725004579211\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_0_time_to_prediction, model_0_prediction_time_weighted = time_to_prediction(\n    model = model_0,\n    samples = test_tweets\n)\n\nprint(model_0_time_to_prediction, model_0_prediction_time_weighted)\n# 0.05065811199892778 1.5525011338929752e-05\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"TimeToPrediction = [model_6_time_to_prediction, model_0_time_to_prediction]\nTimeToPredictionWeighted = [model_6_prediction_time_weighted, model_0_prediction_time_weighted]\nindex = ['Model 6', 'Model 0']\n\nprediction_times_df = pd.DataFrame({\n    'TimeToPrediction': TimeToPrediction,\n    'TimeToPrediction (weighted)': TimeToPredictionWeighted\n}, index=index)\n\nprediction_times_df.plot(\n    kind='bar',\n    rot=0,\n    subplots=True,\n    figsize=(12,8)\n)\n")),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"(Re) Introduction to Tensorflow Natural Language Processing",src:a(43054).Z,width:"1016",height:"682"})),(0,l.kt)("h4",{id:"comparing-the-performance-of-all-models"},"Comparing the Performance of all Models"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_1_time_to_prediction, model_1_prediction_time_weighted = time_to_prediction(\n    model = model_1,\n    samples = test_tweets\n)\n\nmodel_2_time_to_prediction, model_2_prediction_time_weighted = time_to_prediction(\n    model = model_2,\n    samples = test_tweets\n)\n\nmodel_3_time_to_prediction, model_3_prediction_time_weighted = time_to_prediction(\n    model = model_3,\n    samples = test_tweets\n)\n\nmodel_4_time_to_prediction, model_4_prediction_time_weighted = time_to_prediction(\n    model = model_4,\n    samples = test_tweets\n)\n\nmodel_5_time_to_prediction, model_5_prediction_time_weighted = time_to_prediction(\n    model = model_5,\n    samples = test_tweets\n)\n\nmodel_6_time_to_prediction, model_6_prediction_time_weighted = time_to_prediction(\n    model = model_6,\n    samples = test_tweets\n)\n\nmodel_6a_time_to_prediction, model_6a_prediction_time_weighted = time_to_prediction(\n    model = model_6a,\n    samples = test_tweets\n)\n\nmodel_6b_time_to_prediction, model_6b_prediction_time_weighted = time_to_prediction(\n    model = model_6b,\n    samples = test_tweets\n)\n\nmodel_6c_time_to_prediction, model_6c_prediction_time_weighted = time_to_prediction(\n    model = model_6c,\n    samples = test_tweets\n)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"plt.figure(figsize=(12, 8))\nplt.scatter(model_6c_time_to_prediction, model_6c_metrics['f1'], label='Pre-trained USE (10%)')\nplt.scatter(model_6b_time_to_prediction, model_6b_metrics['f1'], label='Pre-trained USE (10% / data leakage)')\nplt.scatter(model_6a_time_to_prediction, model_6a_metrics['f1'], label='Pre-trained USE (added Dense)')\nplt.scatter(model_6_time_to_prediction, model_6_metrics['f1'], label='Pre-trained USE')\nplt.scatter(model_5_time_to_prediction, model_5_metrics['f1'], label='1D-Convolutional Neural Network (CNN)')\nplt.scatter(model_4_time_to_prediction, model_4_metrics['f1'], label='Bidirectional-LSTM model (RNN)')\nplt.scatter(model_3_time_to_prediction, model_3_metrics['f1'], label='GRU model (RNN)')\nplt.scatter(model_2_time_to_prediction, model_2_metrics['f1'], label='LSTM model (RNN)')\nplt.scatter(model_1_time_to_prediction, model_1_metrics['f1'], label='Feed-forward neural network (dense model)')\nplt.scatter(model_0_time_to_prediction, baseline_metrics['f1'], label='Naive Bayes (baseline)')\nplt.legend()\nplt.title(\"F1-Score vs Time to Prediction\")\nplt.xlabel('Time to Prediction')\nplt.ylabel('F1-Score')\n")),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"(Re) Introduction to Tensorflow Natural Language Processing",src:a(12253).Z,width:"1010",height:"701"})))}p.isMDXComponent=!0},76806:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-a3f1c3cf7621dc9c70d8bc62dec4a9d5.jpg"},60045:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/tf_nlp_desaster_tweets_01-9df7623eb1b424d4d16ff3f6b9bd98d4.png"},74816:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/tf_nlp_desaster_tweets_02-7188185c31d900af80997ad49889a931.png"},57116:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/tf_nlp_desaster_tweets_03-ff7555ff2b5009fd2a5cdc4fbb40f8dd.png"},82441:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/tf_nlp_desaster_tweets_04-a633f2d0c4e08c1fdf653d5c1ac6e644.png"},43054:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/tf_nlp_desaster_tweets_05-2ecc125f1cbd52d556aaf8c288d36525.png"},12253:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/tf_nlp_desaster_tweets_06-458287cf210faec316cfb51972d6c1a9.png"}}]);