"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[67398],{3905:(e,t,a)=>{a.d(t,{Zo:()=>m,kt:()=>u});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),p=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},m=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,m=l(e,["components","mdxType","originalType","parentName"]),d=p(a),u=r,g=d["".concat(s,".").concat(u)]||d[u]||c[u]||i;return a?n.createElement(g,o(o({ref:t},m),{},{components:a})):n.createElement(g,o({ref:t},m))}));function u(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,o[1]=l;for(var p=2;p<i;p++)o[p]=a[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},9304:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>c,frontMatter:()=>i,metadata:()=>l,toc:()=>p});var n=a(87462),r=(a(67294),a(3905));const i={sidebar_position:4140,slug:"2023-08-27",title:"Image Segmentation with PyTorch",authors:"mpolinowski",tags:["Python","Machine Learning","PyTorch"],description:"Food item segmentation from images of the Tray Food Segmentation dataset"},o=void 0,l={unversionedId:"IoT-and-Machine-Learning/ML/2023-08-27--image-segmentation-with-pytorch/index",id:"IoT-and-Machine-Learning/ML/2023-08-27--image-segmentation-with-pytorch/index",title:"Image Segmentation with PyTorch",description:"Food item segmentation from images of the Tray Food Segmentation dataset",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-08-27--image-segmentation-with-pytorch/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-08-27--image-segmentation-with-pytorch",slug:"/IoT-and-Machine-Learning/ML/2023-08-27--image-segmentation-with-pytorch/2023-08-27",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-08-27--image-segmentation-with-pytorch/2023-08-27",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-08-27--image-segmentation-with-pytorch/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"PyTorch",permalink:"/docs/tags/py-torch"}],version:"current",sidebarPosition:4140,frontMatter:{sidebar_position:4140,slug:"2023-08-27",title:"Image Segmentation with PyTorch",authors:"mpolinowski",tags:["Python","Machine Learning","PyTorch"],description:"Food item segmentation from images of the Tray Food Segmentation dataset"},sidebar:"tutorialSidebar",previous:{title:"Image Segmentation with PyTorch (RCNN)",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-08-28--semantic-segmentation-detectron2-model-zoo/2023-08-28"},next:{title:"Containerized PyTorch Dev Workflow",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-08-21--pytorch-development-in-docker/2023-08-21"}},s={},p=[{value:"Creating Label Annotations",id:"creating-label-annotations",level:2},{value:"Dataset",id:"dataset",level:2},{value:"Preparing the Dataset",id:"preparing-the-dataset",level:3},{value:"Dataset Visualization",id:"dataset-visualization",level:4},{value:"Data Augmentation",id:"data-augmentation",level:4},{value:"Visualize Augmented Data",id:"visualize-augmented-data",level:5},{value:"Data Loading",id:"data-loading",level:3},{value:"Performance Metrics",id:"performance-metrics",level:2},{value:"Building the Segmentation Model",id:"building-the-segmentation-model",level:2},{value:"Data Loader",id:"data-loader",level:3},{value:"Model Training",id:"model-training",level:3},{value:"Model Evaluation",id:"model-evaluation",level:2},{value:"Visualize Segmentation",id:"visualize-segmentation",level:3},{value:"Predicted Segmentation Map",id:"predicted-segmentation-map",level:4}],m={toc:p};function c(e){let{components:t,...i}=e;return(0,r.kt)("wrapper",(0,n.Z)({},m,i,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"TST, Hong Kong",src:a(32995).Z,width:"1500",height:"811"})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#image-segmentation-with-pytorch"},"Image Segmentation with PyTorch"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#creating-label-annotations"},"Creating Label Annotations")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#dataset"},"Dataset"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#preparing-the-dataset"},"Preparing the Dataset"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#dataset-visualization"},"Dataset Visualization")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#data-augmentation"},"Data Augmentation"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#visualize-augmented-data"},"Visualize Augmented Data")))))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#data-loading"},"Data Loading")))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#performance-metrics"},"Performance Metrics")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#building-the-segmentation-model"},"Building the Segmentation Model"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#data-loader"},"Data Loader")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#model-training"},"Model Training")))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#model-evaluation"},"Model Evaluation"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#visualize-segmentation"},"Visualize Segmentation"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#predicted-segmentation-map"},"Predicted Segmentation Map"))))))))),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/mpolinowski/pt-seg-i-see-you"},"Github Repository"))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Related"),":"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Image Segmentation with PyTorch"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-08-28--semantic-segmentation-detectron2-model-zoo/2023-08-28"},"Semantic Segmentation Detectron2 Model Zoo")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-08-29--semantic-segmentation-detectron2-model-zoo-faster-rcnn/2023-08-29"},"Semantic Segmentation Detectron2 Model Zoo: Faster RCNN")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-08-30--instance_segmentation_detectron2_model_zoo_mask_rcnn/2023-08-30"},"Semantic Segmentation Detectron2 Model Zoo: Mask RCNN")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-08-31--semantic_segmentation_detectron2_openimages_dataset/2023-08-31"},"Detectron Object Detection with OpenImages Dataset (WIP)"))),(0,r.kt)("h1",{id:"image-segmentation-with-pytorch"},"Image Segmentation with PyTorch"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.kaggle.com/datasets/thezaza102/tray-food-segmentation"},"Tray Food Segmentation"),":\n",(0,r.kt)("em",{parentName:"li"},"Food item segmentation from images of trays")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/thezaza101/Meal-Compliance-Project"},"Meal-Compliance-Project"))),(0,r.kt)("h2",{id:"creating-label-annotations"},"Creating Label Annotations"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://pypi.org/project/labelme/"},"labelme "))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"!pip install matplotlib opencv-python albumentations tqdm\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"!pip install git+https://github.com/qubvel/segmentation_models.pytorch\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import lr_scheduler\n\nimport segmentation_models_pytorch as smp\n\nimport torchvision\nfrom torchvision import datasets, models, transforms\n\nimport albumentations as albu\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from helper.metrics import IoU, Accuracy, Fscore, Recall, Precision, DiceLoss\nfrom helper.train import BaseDataset, VisualizeDataset, VisualizeResult, TestEpoch, TrainEpoch\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"LR = 0.0001\nDLR_STEPS = 7\nDLR_GAMMA = 0.1\nEPOCHS = 20\nMODEL_PATH = '../../saved_models/MobileNetV3Encoder_UnetPlusPlus.pth'\nDATA_DIR = '../../dataset/TrayDataset/'\nBATCH = 8\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n")),(0,r.kt)("h2",{id:"dataset"},"Dataset"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"CLASS_LABELS = ['background','tray','cutlery','form','straw','meatball','beef','roastlamb','beeftomatocasserole','ham','bean','cucumber','leaf','tomato','boiledrice','beefmexicanmeatballs','spinachandpumpkinrisotto','bakedfish','gravy','zucchini','carrot','broccoli','pumpkin','celery','sandwich','sidesalad','tartaresauce','jacketpotato','creamedpotato','bread','margarine','soup','apple','cannedfruit','milk','vanillayogurt','jelly','custard','lemonsponge','juice','applejuice','orangejuice','water']\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"len(CLASS_LABELS)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"CLASS_COLOUR_MAP = np.array([\n    (0, 0, 0), (100, 127, 150),(50, 0, 0),(0, 0, 255),(100, 0, 0),(0, 100, 0),\n    (0, 100, 0),(0, 50, 50),(50, 100, 0),(0, 250, 0),(180, 0, 0),\n    (100, 100, 0),(128, 0, 100),(100, 128, 0),(0, 100, 128),(100, 0, 100),\n    (150, 100, 0),(0, 100, 200), (100, 50, 50),(50, 100, 250),(100, 250, 50),(180, 100, 0),\n    (100, 50, 218),(200, 128, 100),(100, 0, 128),(10, 100, 128),(100, 150, 75),\n    (175, 100, 90),(30, 100, 128),(100, 250, 125),(50, 10, 50), (175, 10, 175),(25, 225, 50),\n    (100, 128, 218),(128, 0, 100),(128, 128, 0),(90, 100, 0),(100, 200, 0),(175, 100, 150),\n    (200, 100, 200),(200, 50, 50),(250, 100, 50),(100, 25, 50),(150, 100, 100)\n])\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"x_train_dir = os.path.join(DATA_DIR, 'XTrain')\ny_train_dir = os.path.join(DATA_DIR, 'yTrain')\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"x_test_dir = os.path.join(DATA_DIR, 'XTest')\ny_test_dir = os.path.join(DATA_DIR, 'yTest')\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"path, dirs, files = next(os.walk(x_train_dir))\nfile_count = len(files)\n\nprint('Test Dataset: ',file_count)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Test Dataset:  1241\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"path, dirs, files = next(os.walk(x_test_dir))\nfile_count = len(files)\n\nprint('Training Dataset: ',file_count)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Training Dataset:  8\n")),(0,r.kt)("h3",{id:"preparing-the-dataset"},"Preparing the Dataset"),(0,r.kt)("h4",{id:"dataset-visualization"},"Dataset Visualization"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"for label  in CLASS_LABELS:\n    dataset = BaseDataset(x_test_dir, y_test_dir, classes=[label])\n    image, mask = dataset[2]\n    VisualizeDataset(\n        image = image, mask=mask.squeeze(),\n        label = label\n    )\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"/opt/app/notebook/MobileNetV3Encoder_UnetPlusPlus/helper/train.py:20: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n  plt.figure(figsize=(14, 20))\n")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Image Segmentation with PyTorch",src:a(64456).Z,width:"3740",height:"3693"})),(0,r.kt)("h4",{id:"data-augmentation"},"Data Augmentation"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"def apply_train_aug():\n    train_transform = [\n        # apply to all\n        albu.Resize(256, 416, p=1),\n        # apply to 50% of images\n        albu.HorizontalFlip(p=0.5),\n        # apply one randomly to 90% of images\n        albu.OneOf([\n            albu.RandomBrightnessContrast(\n                  brightness_limit=0.4, contrast_limit=0.4, p=1),\n            albu.CLAHE(p=1),\n            albu.HueSaturationValue(p=1)\n            ], p=0.9,),\n        # add noise to 20% of images\n        albu.GaussNoise(var_limit=(10.0, 50.0), mean=0, per_channel=True, always_apply=False, p=0.2),\n    ]\n    \n    return albu.Compose(train_transform)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'def apply_test_aug():\n    """Add paddings to make image shape divisible by 32"""\n    test_transform = [\n        albu.PadIfNeeded(256, 416)\n    ]\n\n    return albu.Compose(test_transform)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"def to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"def apply_preprocessing(preprocessing_fn):\n    _transform = [\n        albu.Lambda(image=preprocessing_fn),\n        albu.Lambda(image=to_tensor, mask=to_tensor),\n    ]\n\n    return albu.Compose(_transform)\n")),(0,r.kt)("h5",{id:"visualize-augmented-data"},"Visualize Augmented Data"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"train_dataset = BaseDataset(\n    x_train_dir,\n    y_train_dir,\n    augmentation=apply_train_aug(),\n    classes=['tray'],\n)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"test_dataset = BaseDataset(\n    x_test_dir,\n    y_test_dir,\n    augmentation=apply_test_aug(),\n    classes=['tray'],\n)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# show image with 5 different augmentations\nfor i in range(5):\n    image, mask = train_dataset[8]\n    VisualizeResult(image=image, mask=mask.squeeze(-1))\n")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Image Segmentation with PyTorch",src:a(75427).Z,width:"536",height:"944"})),(0,r.kt)("h3",{id:"data-loading"},"Data Loading"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"train_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH,\n    shuffle=True,\n    num_workers=0)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"test_loader = DataLoader(\n    test_dataset,\n    batch_size=1,\n    shuffle=False,\n    num_workers=0)\n")),(0,r.kt)("h2",{id:"performance-metrics"},"Performance Metrics"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"Intersection-over-Union")," (",(0,r.kt)("strong",{parentName:"li"},"IOU"),"): Calculated from the overlap of the ground truth and predicted area divided by the overall area of both: ",(0,r.kt)("inlineCode",{parentName:"li"},"(Area of Overlap) / (Area of Union)")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"Mean-Intersection-over-Union")," (",(0,r.kt)("strong",{parentName:"li"},"mIOU"),"): Mean ",(0,r.kt)("strong",{parentName:"li"},"IOU")," over all classes.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"metrics = [\n    IoU(threshold=0.5),\n    Accuracy(threshold=0.5),\n    Fscore(threshold=0.5),\n    Recall(threshold=0.5),\n    Precision(threshold=0.5),\n]\n")),(0,r.kt)("h2",{id:"building-the-segmentation-model"},"Building the Segmentation Model"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# https://github.com/qubvel/segmentation_models.pytorch\n# resnet50, resnext50_32x4d, resnext101_32x8d, xception, timm-gernet_s, mobileone_s0, timm-efficientnet-b0, timm-mobilenetv3_small_100, resnet152, vgg13\nENCODER = 'timm-mobilenetv3_small_100'\nENCODER_WEIGHTS = 'imagenet'\nACTIVATION = 'softmax'\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# https://github.com/qubvel/segmentation_models.pytorch/tree/master/segmentation_models_pytorch/decoders\n# FPN, PAN, PSPNet, MAnet, Linknet, FPN, DeepLabV3, DeepLabV3Plus, Unet\nmodel = smp.UnetPlusPlus(\n    encoder_name=ENCODER,\n    encoder_weights=ENCODER_WEIGHTS,\n    classes=len(CLASS_LABELS),\n    activation=ACTIVATION,\n)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"#Normalize your data the same way as during encoder weight pre-training\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"print(model)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# Define Optimization algorithm with Learning rate\noptimizer = torch.optim.Adam([\n    dict(params=model.parameters(), lr=LR),\n])\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# Define Loss Function\nloss = DiceLoss()\n")),(0,r.kt)("h3",{id:"data-loader"},"Data Loader"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"train_dataset = BaseDataset(\n    x_train_dir,\n    y_train_dir,\n    augmentation=apply_train_aug(),\n    preprocessing=apply_preprocessing(preprocessing_fn),\n    classes=CLASS_LABELS,\n)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"test_dataset = BaseDataset(\n    x_test_dir,\n    y_test_dir,\n    augmentation=apply_test_aug(),\n    preprocessing=apply_preprocessing(preprocessing_fn),\n    classes=CLASS_LABELS,\n)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"train_loader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True, num_workers=0)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n")),(0,r.kt)("h3",{id:"model-training"},"Model Training"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"train_epoch = TrainEpoch(\n    model,\n    loss=loss,\n    metrics=metrics,\n    optimizer=optimizer,\n    device=DEVICE,\n    verbose=True,\n)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"test_epoch = TestEpoch(\n    model,\n    loss=loss,\n    metrics=metrics,\n    device=DEVICE,\n    verbose=True,\n)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"max_score = 0\n\nfor i in range(0, 40):\n\n    print('\\nEpoch: {}'.format(i))\n    train_logs = train_epoch.run(train_loader)\n    test_logs = test_epoch.run(test_loader)\n    \n    # Save the model with best iou score\n    if max_score < test_logs['iou_score']:\n        max_score = test_logs['iou_score']\n        torch.save(model, MODEL_PATH)\n        print('Model saved!')\n\n    if i == 50:\n        optimizer.param_groups[0]['lr'] = 1e-5\n        print('Decrease decoder learning rate to 1e-5!')\n")),(0,r.kt)("p",null,"Epoch: 39\ntrain: 100%|\u2588| 156/156 [06:10<00:00,  2.37s/it, dice_loss - 0.03158, iou_score - 0.9396, accuracy - 0.9985, fscore - 0.9688, recall - 0.9686, precision\ntest: 100%|\u2588| 8/8 [00:01<00:00,  4.94it/s, dice_loss - 0.04502, iou_score - 0.9164, accuracy - 0.9979, fscore - 0.9555, recall - 0.9552, precision - 0."),(0,r.kt)("h2",{id:"model-evaluation"},"Model Evaluation"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"trained_model = torch.load(MODEL_PATH)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"logs = test_epoch.run(test_loader)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"test:  12%|\u258f| 1/8 [00:02<00:15,  2.16s/it, dice_loss - 0.03128, iou_score - 0.9401, accuracy - 0.9986, fscore - 0.9691, recall - 0.9689, precision - 0.\n\n/opt/conda/lib/python3.10/site-packages/segmentation_models_pytorch/base/modules.py:116: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self.activation(x)\n\n\ntest: 100%|\u2588| 8/8 [00:03<00:00,  2.27it/s, dice_loss - 0.04451, iou_score - 0.9178, accuracy - 0.998, fscore - 0.9563, recall - 0.9558, precision - 0.9\n")),(0,r.kt)("p",null,"test: 100%|\u2588| 8/8 [00:01<00:00,  4.21it/s, dice_loss - 0.2597, iou_score - 0.8069, accuracy - 0.9953, fscore - 0.8928, recall - 0.839, precision - 0.95"),(0,r.kt)("h3",{id:"visualize-segmentation"},"Visualize Segmentation"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"#Get orignial image and mask from test dataset\nimage, gt_mask = test_dataset[3]\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\npredicted_mask = trained_model.predict(x_tensor)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"pr_mask = (predicted_mask.squeeze().cpu().numpy().round())\npr_mask = pr_mask[1,:,:]\ngt_mask = gt_mask[1,:,:]\nimage_t = image.transpose(1, 2, 0)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"VisualizeResult(\n    image=image_t,\n    ground_truth_mask=gt_mask,\n    predicted_mask=pr_mask\n)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"png",src:a(79820).Z,width:"1260",height:"266"})),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"#Convert the predicted mask to numpy and get the predicted class indices\npredicted_output = torch.argmax(predicted_mask.squeeze(), dim=0).detach().cpu().numpy()\nindices = np.unique(predicted_output)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"for i in indices:\n  print(CLASS_LABELS[i])\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"background\ntray\ncutlery\nboiledrice\nbeefmexicanmeatballs\nzucchini\npumpkin\nwater\n")),(0,r.kt)("h4",{id:"predicted-segmentation-map"},"Predicted Segmentation Map"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# Define function to convert 2D segmentation to RGB Image\n\ndef decode_segmentation_map(image, classesLength=43):\n\n  r = np.zeros_like(image).astype(np.uint8)\n  g = np.zeros_like(image).astype(np.uint8)\n  b = np.zeros_like(image).astype(np.uint8)\n\n  for l in range(0, classesLength):\n    idx = image == l\n    r[idx] = CLASS_COLOUR_MAP[l, 0]\n    g[idx] = CLASS_COLOUR_MAP[l, 1]\n    b[idx] = CLASS_COLOUR_MAP[l, 2]\n\n  rgb = np.stack([r, g, b], axis=2)\n\n  return rgb\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"rgb_map = decode_segmentation_map(predicted_output,43)\nplt.imshow(rgb_map);\n")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"png",src:a(66689).Z,width:"552",height:"353"})))}c.isMDXComponent=!0},64456:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/Model_Eval_01-b566a283bce965027cf79e1e81b83298.webp"},75427:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/Model_Eval_02-0cb1ef77ae4a34912b4961a50b507619.webp"},79820:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/output_55_1-6aeaad3a8be76479038d3053a3b8604f.png"},66689:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/output_60_0-47d9520cd54c9e9c918a7b288c7ea8f1.png"},32995:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-f940fa4541ff8a00764cf3f41cd6b985.jpg"}}]);