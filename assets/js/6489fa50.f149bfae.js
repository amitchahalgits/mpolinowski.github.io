"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[82230],{3905:(e,n,t)=>{t.d(n,{Zo:()=>d,kt:()=>_});var a=t(67294);function s(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){s(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,a,s=function(e,n){if(null==e)return{};var t,a,s={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(s[t]=e[t]);return s}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(s[t]=e[t])}return s}var i=a.createContext({}),m=function(e){var n=a.useContext(i),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},d=function(e){var n=m(e.components);return a.createElement(i.Provider,{value:n},e.children)},c={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},p=a.forwardRef((function(e,n){var t=e.components,s=e.mdxType,o=e.originalType,i=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),p=m(t),_=s,u=p["".concat(i,".").concat(_)]||p[_]||c[_]||o;return t?a.createElement(u,r(r({ref:n},d),{},{components:t})):a.createElement(u,r({ref:n},d))}));function _(e,n){var t=arguments,s=n&&n.mdxType;if("string"==typeof e||s){var o=t.length,r=new Array(o);r[0]=p;var l={};for(var i in n)hasOwnProperty.call(n,i)&&(l[i]=n[i]);l.originalType=e,l.mdxType="string"==typeof e?e:s,r[1]=l;for(var m=2;m<o;m++)r[m]=t[m];return a.createElement.apply(null,r)}return a.createElement.apply(null,t)}p.displayName="MDXCreateElement"},93:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>i,contentTitle:()=>r,default:()=>c,frontMatter:()=>o,metadata:()=>l,toc:()=>m});var a=t(87462),s=(t(67294),t(3905));const o={sidebar_position:4128,slug:"2023-08-30",title:"Instance Segmentation with PyTorch (Mask RCNN)",authors:"mpolinowski",tags:["Python","Machine Learning","PyTorch"],description:"Detectron2 is a platform for object detection, segmentation and other visual recognition tasks."},r=void 0,l={unversionedId:"IoT-and-Machine-Learning/ML/2023-08-30--instance_segmentation_detectron2_model_zoo_mask_rcnn/index",id:"IoT-and-Machine-Learning/ML/2023-08-30--instance_segmentation_detectron2_model_zoo_mask_rcnn/index",title:"Instance Segmentation with PyTorch (Mask RCNN)",description:"Detectron2 is a platform for object detection, segmentation and other visual recognition tasks.",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-08-30--instance_segmentation_detectron2_model_zoo_mask_rcnn/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-08-30--instance_segmentation_detectron2_model_zoo_mask_rcnn",slug:"/IoT-and-Machine-Learning/ML/2023-08-30--instance_segmentation_detectron2_model_zoo_mask_rcnn/2023-08-30",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-08-30--instance_segmentation_detectron2_model_zoo_mask_rcnn/2023-08-30",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-08-30--instance_segmentation_detectron2_model_zoo_mask_rcnn/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"PyTorch",permalink:"/docs/tags/py-torch"}],version:"current",sidebarPosition:4128,frontMatter:{sidebar_position:4128,slug:"2023-08-30",title:"Instance Segmentation with PyTorch (Mask RCNN)",authors:"mpolinowski",tags:["Python","Machine Learning","PyTorch"],description:"Detectron2 is a platform for object detection, segmentation and other visual recognition tasks."},sidebar:"tutorialSidebar",previous:{title:"Detectron Object Detection with OpenImages Dataset (WIP)",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-08-31--semantic_segmentation_detectron2_openimages_dataset/2023-08-31"},next:{title:"Image Segmentation with PyTorch (Faster RCNN)",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-08-29--semantic-segmentation-detectron2-model-zoo-faster-rcnn/2023-08-29"}},i={},m=[{value:"Detectron2 :: Faster RCNN R101 FPN",id:"detectron2--faster-rcnn-r101-fpn",level:2},{value:"Custom Dataset",id:"custom-dataset",level:3},{value:"Model Training",id:"model-training",level:2},{value:"Inference &amp; Evaluation",id:"inference--evaluation",level:3}],d={toc:m};function c(e){let{components:n,...o}=e;return(0,s.kt)("wrapper",(0,a.Z)({},d,o,{components:n,mdxType:"MDXLayout"}),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"TST, Hong Kong",src:t(37835).Z,width:"1500",height:"811"})),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"#object-detection-with-instance-segmentation"},"Object Detection with Instance Segmentation"),(0,s.kt)("ul",{parentName:"li"},(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"#detectron2--faster-rcnn-r101-fpn"},"Detectron2 :: Faster RCNN R101 FPN"),(0,s.kt)("ul",{parentName:"li"},(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"#custom-dataset"},"Custom Dataset")))),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"#model-training"},"Model Training"),(0,s.kt)("ul",{parentName:"li"},(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"#inference--evaluation"},"Inference \\& Evaluation"))))))),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/mpolinowski/pt-seg-i-see-you"},"Github Repository"))),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Related"),":"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-08-27--image-segmentation-with-pytorch/2023-08-27"},"Image Segmentation with PyTorch")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-08-28--semantic-segmentation-detectron2-model-zoo/2023-08-28"},"Semantic Segmentation Detectron2 Model Zoo")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-08-29--semantic-segmentation-detectron2-model-zoo-faster-rcnn/2023-08-29"},"Semantic Segmentation Detectron2 Model Zoo: Faster RCNN")),(0,s.kt)("li",{parentName:"ul"},"Semantic Segmentation Detectron2 Model Zoo: Mask RCNN"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2023-08-31--semantic_segmentation_detectron2_openimages_dataset/2023-08-31"},"Detectron Object Detection with OpenImages Dataset (WIP)"))),(0,s.kt)("h1",{id:"object-detection-with-instance-segmentation"},"Object Detection with Instance Segmentation"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://detectron2.readthedocs.io/en/latest/"},"Detectron2")," is a platform for object detection, segmentation and other visual recognition tasks."),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Includes new capabilities such as panoptic segmentation, Densepose, Cascade R-CNN, rotated bounding boxes, PointRend, DeepLab, ViTDet, MViTv2 etc."),(0,s.kt)("li",{parentName:"ul"},"Used as a library to support building research projects on top of it."),(0,s.kt)("li",{parentName:"ul"},"Models can be exported to TorchScript format or Caffe2 format for deployment.")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"!pip install opencv-python matplotlib\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n")),(0,s.kt)("h2",{id:"detectron2--faster-rcnn-r101-fpn"},"Detectron2 :: Faster RCNN R101 FPN"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"import detectron2\n\n#https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md\nfrom detectron2 import model_zoo\nfrom detectron2.data import DatasetCatalog, build_detection_test_loader\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.engine import DefaultTrainer, DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.structures import BoxMode\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch, os, json, cv2, random\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'# Setup detectron2 \n\n\n\n\n\n    <Logger detectron2 (DEBUG)>\n\n\n\n\n```python\nCLASS_LABELS = ["balloon"]\n')),(0,s.kt)("h3",{id:"custom-dataset"},"Custom Dataset"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip"},"Balloon Dataset"))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"dataset_path = '../datasets/balloon/'\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'# # if your dataset is in COCO format:\n# from detectron2.data.datasets import register_coco_instances\n# register_coco_instances("dataset_train", {}, "../datasets/balloon/train/via_region_data.json", "../datasets/balloon/train/")\n# register_coco_instances("dataset_val", {}, "../datasets/balloon/val/via_region_data.json", "../datasets/balloon/val/")\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'def get_ds_dicts(dataset_path):\n    json_file = os.path.join(dataset_path, "via_region_data.json")\n    \n    with open(json_file) as f:\n        imgs_anns = json.load(f)\n\n    dataset_dicts = []\n\n    for idx, v in enumerate(imgs_anns.values()):\n        record = {}\n        filename = os.path.join(dataset_path, v["filename"])\n        height, width = cv2.imread(filename).shape[:2]\n        \n        record["file_name"] = filename\n        record["image_id"] = idx\n        record["height"] = height\n        record["width"] = width\n\n        annos = v["regions"]\n        objs = []\n\n        for _, anno in annos.items():\n            assert not anno["region_attributes"]\n            anno = anno["shape_attributes"]\n            px = anno["all_points_x"]\n            py = anno["all_points_y"]\n            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n            poly = [p for x in poly for p in x]\n\n            obj = {\n                "bbox": [np.min(px), np.min(py), np.max(px), np.max(py)],\n                "bbox_mode": BoxMode.XYXY_ABS,\n                "segmentation": [poly],\n                "category_id": 0,\n            }\n\n            objs.append(obj)\n\n        record["annotations"] = objs\n        dataset_dicts.append(record)\n\n    return dataset_dicts\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'for d in ["train", "val"]:\n    DatasetCatalog.register("balloon_" + d, lambda d=d: get_ds_dicts(dataset_path + d))\n    MetadataCatalog.get("balloon_" + d).set(thing_classes=CLASS_LABELS)\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'metadata = MetadataCatalog.get("balloon_train")\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'dataset_dicts = get_ds_dicts(dataset_path + "train")\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'dataset_dicts[0]["file_name"]\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"'../datasets/balloon/train/34020010494_e5cb88e1c4_k.jpg'\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'for d in random.sample(dataset_dicts, 1):\n    img = plt.imread(d["file_name"])\n    # plt.imshow(img)\n    visualizer = Visualizer(img[:, :, ::-1], metadata=metadata, scale=0.5)\n    out = visualizer.draw_dataset_dict(d)\n    plt.imshow(out.get_image()[:, :, ::-1])\n')),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"png",src:t(74199).Z,width:"554",height:"377"})),(0,s.kt)("h2",{id:"model-training"},"Model Training"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'# https://github.com/facebookresearch/detectron2/tree/main/configs/COCO-InstanceSegmentation\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml"))\ncfg.DATASETS.TRAIN = ("balloon_train",)\ncfg.DATASETS.TEST = ()\ncfg.DATALOADER.NUM_WORKERS = 2\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml")  # initialize from model zoo\ncfg.SOLVER.IMS_PER_BATCH = 2\ncfg.SOLVER.BASE_LR = 0.00025\ncfg.SOLVER.MAX_ITER = 500\ncfg.SOLVER.STEPS = []  # do not decay learning rate\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128 # faster, and good enough for this toy dataset\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon)\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n\ntrainer = DefaultTrainer(cfg)\ntrainer.resume_or_load(resume=False)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"\x1b[32m[08/26 11:59:22 d2.engine.defaults]: \x1b[0mModel:\nGeneralizedRCNN(\n  (backbone): FPN(\n    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (top_block): LastLevelMaxPool()\n    (bottom_up): ResNet(\n      (stem): BasicStem(\n        (conv1): Conv2d(\n          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n      )\n      (res2): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n      )\n      (res3): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (3): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n      )\n      (res4): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (3): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (4): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (5): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (6): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (7): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (8): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (9): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (10): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (11): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (12): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (13): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (14): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (15): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (16): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (17): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (18): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (19): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (20): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (21): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (22): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n      )\n      (res5): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n      )\n    )\n  )\n  (proposal_generator): RPN(\n    (rpn_head): StandardRPNHead(\n      (conv): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (anchor_generator): DefaultAnchorGenerator(\n      (cell_anchors): BufferList()\n    )\n  )\n  (roi_heads): StandardROIHeads(\n    (box_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (box_head): FastRCNNConvFCHead(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc_relu1): ReLU()\n      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n      (fc_relu2): ReLU()\n    )\n    (box_predictor): FastRCNNOutputLayers(\n      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n    )\n    (mask_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (mask_head): MaskRCNNConvUpsampleHead(\n      (mask_fcn1): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (mask_fcn2): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (mask_fcn3): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (mask_fcn4): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n      (deconv_relu): ReLU()\n      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n)\n\x1b[32m[08/26 11:59:23 d2.data.build]: \x1b[0mRemoved 0 images with no usable annotations. 61 images left.\n\x1b[32m[08/26 11:59:23 d2.data.build]: \x1b[0mDistribution of instances among all 1 categories:\n\x1b[36m|  category  | #instances   |\n|:----------:|:-------------|\n|  balloon   | 255          |\n|            |              |\x1b[0m\n\x1b[32m[08/26 11:59:23 d2.data.dataset_mapper]: \x1b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n\x1b[32m[08/26 11:59:23 d2.data.build]: \x1b[0mUsing training sampler TrainingSampler\n\x1b[32m[08/26 11:59:23 d2.data.common]: \x1b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n\x1b[32m[08/26 11:59:23 d2.data.common]: \x1b[0mSerializing 61 elements to byte tensors and concatenating them all ...\n\x1b[32m[08/26 11:59:23 d2.data.common]: \x1b[0mSerialized dataset takes 0.17 MiB\n\x1b[32m[08/26 11:59:23 d2.checkpoint.detection_checkpoint]: \x1b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x/138205316/model_final_a3ec72.pkl ...\n\n\nmodel_final_a3ec72.pkl: 254MB [00:49, 5.11MB/s]                                                                                                        \nSkip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\nSkip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\nSkip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\nSkip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\nSkip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\nSkip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\nSome model parameters or buffers are not found in the checkpoint:\n\x1b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\x1b[0m\n\x1b[34mroi_heads.box_predictor.cls_score.{bias, weight}\x1b[0m\n\x1b[34mroi_heads.mask_head.predictor.{bias, weight}\x1b[0m\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"trainer.train()\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"\x1b[32m[08/26 12:00:23 d2.engine.train_loop]: \x1b[0mStarting training from iteration 0\n\n\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/TensorShape.cpp:3483.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n\n\n\x1b[32m[08/26 12:00:44 d2.utils.events]: \x1b[0m eta: 0:07:59  iter: 19  total_loss: 2.08  loss_cls: 0.7676  loss_box_reg: 0.5217  loss_mask: 0.6938  loss_rpn_cls: 0.02692  loss_rpn_loc: 0.006267    time: 0.9673  last_time: 0.9997  data_time: 0.0144  last_data_time: 0.0050   lr: 9.7405e-06  max_mem: 3524M\n\x1b[32m[08/26 12:01:04 d2.utils.events]: \x1b[0m eta: 0:07:39  iter: 39  total_loss: 1.912  loss_cls: 0.6528  loss_box_reg: 0.6019  loss_mask: 0.6323  loss_rpn_cls: 0.03528  loss_rpn_loc: 0.006398    time: 0.9761  last_time: 0.8214  data_time: 0.0035  last_data_time: 0.0017   lr: 1.9731e-05  max_mem: 3524M\n\x1b[32m[08/26 12:01:23 d2.utils.events]: \x1b[0m eta: 0:07:09  iter: 59  total_loss: 1.78  loss_cls: 0.5066  loss_box_reg: 0.6542  loss_mask: 0.5377  loss_rpn_cls: 0.03933  loss_rpn_loc: 0.01158    time: 0.9698  last_time: 1.0864  data_time: 0.0040  last_data_time: 0.0035   lr: 2.972e-05  max_mem: 3524M\n\x1b[32m[08/26 12:01:41 d2.utils.events]: \x1b[0m eta: 0:06:44  iter: 79  total_loss: 1.585  loss_cls: 0.4236  loss_box_reg: 0.6475  loss_mask: 0.4356  loss_rpn_cls: 0.02608  loss_rpn_loc: 0.006511    time: 0.9526  last_time: 1.0615  data_time: 0.0037  last_data_time: 0.0026   lr: 3.9711e-05  max_mem: 3524M\n\x1b[32m[08/26 12:01:59 d2.utils.events]: \x1b[0m eta: 0:06:18  iter: 99  total_loss: 1.421  loss_cls: 0.3465  loss_box_reg: 0.6667  loss_mask: 0.3558  loss_rpn_cls: 0.03316  loss_rpn_loc: 0.01091    time: 0.9393  last_time: 0.9450  data_time: 0.0036  last_data_time: 0.0034   lr: 4.9701e-05  max_mem: 3524M\n\x1b[32m[08/26 12:02:17 d2.utils.events]: \x1b[0m eta: 0:05:52  iter: 119  total_loss: 1.31  loss_cls: 0.2972  loss_box_reg: 0.6948  loss_mask: 0.3011  loss_rpn_cls: 0.01337  loss_rpn_loc: 0.005912    time: 0.9303  last_time: 0.9456  data_time: 0.0044  last_data_time: 0.0041   lr: 5.9691e-05  max_mem: 3524M\n\x1b[32m[08/26 12:02:35 d2.utils.events]: \x1b[0m eta: 0:05:31  iter: 139  total_loss: 1.222  loss_cls: 0.2661  loss_box_reg: 0.6034  loss_mask: 0.2566  loss_rpn_cls: 0.02629  loss_rpn_loc: 0.01137    time: 0.9245  last_time: 1.0169  data_time: 0.0033  last_data_time: 0.0030   lr: 6.9681e-05  max_mem: 3524M\n\x1b[32m[08/26 12:02:53 d2.utils.events]: \x1b[0m eta: 0:05:13  iter: 159  total_loss: 1.02  loss_cls: 0.1901  loss_box_reg: 0.6373  loss_mask: 0.1804  loss_rpn_cls: 0.007702  loss_rpn_loc: 0.003213    time: 0.9206  last_time: 0.6985  data_time: 0.0033  last_data_time: 0.0008   lr: 7.9671e-05  max_mem: 3524M\n\x1b[32m[08/26 12:03:11 d2.utils.events]: \x1b[0m eta: 0:04:54  iter: 179  total_loss: 1.026  loss_cls: 0.1796  loss_box_reg: 0.6195  loss_mask: 0.1726  loss_rpn_cls: 0.01859  loss_rpn_loc: 0.009227    time: 0.9204  last_time: 0.8157  data_time: 0.0039  last_data_time: 0.0024   lr: 8.966e-05  max_mem: 3524M\n\x1b[32m[08/26 12:03:29 d2.utils.events]: \x1b[0m eta: 0:04:36  iter: 199  total_loss: 0.8615  loss_cls: 0.1591  loss_box_reg: 0.4736  loss_mask: 0.1335  loss_rpn_cls: 0.0301  loss_rpn_loc: 0.006674    time: 0.9184  last_time: 0.8636  data_time: 0.0041  last_data_time: 0.0031   lr: 9.9651e-05  max_mem: 3524M\n\x1b[32m[08/26 12:03:48 d2.utils.events]: \x1b[0m eta: 0:04:18  iter: 219  total_loss: 0.7652  loss_cls: 0.1127  loss_box_reg: 0.5319  loss_mask: 0.1008  loss_rpn_cls: 0.008044  loss_rpn_loc: 0.003744    time: 0.9203  last_time: 0.6884  data_time: 0.0036  last_data_time: 0.0044   lr: 0.00010964  max_mem: 3524M\n\x1b[32m[08/26 12:04:07 d2.utils.events]: \x1b[0m eta: 0:04:00  iter: 239  total_loss: 0.6049  loss_cls: 0.1049  loss_box_reg: 0.3995  loss_mask: 0.1042  loss_rpn_cls: 0.02107  loss_rpn_loc: 0.007915    time: 0.9224  last_time: 1.0892  data_time: 0.0036  last_data_time: 0.0033   lr: 0.00011963  max_mem: 3525M\n\x1b[32m[08/26 12:04:25 d2.utils.events]: \x1b[0m eta: 0:03:41  iter: 259  total_loss: 0.5345  loss_cls: 0.09611  loss_box_reg: 0.2977  loss_mask: 0.09788  loss_rpn_cls: 0.01481  loss_rpn_loc: 0.006961    time: 0.9207  last_time: 1.0230  data_time: 0.0041  last_data_time: 0.0032   lr: 0.00012962  max_mem: 3525M\n\x1b[32m[08/26 12:04:44 d2.utils.events]: \x1b[0m eta: 0:03:23  iter: 279  total_loss: 0.3356  loss_cls: 0.06542  loss_box_reg: 0.1683  loss_mask: 0.08724  loss_rpn_cls: 0.004802  loss_rpn_loc: 0.004928    time: 0.9237  last_time: 0.8594  data_time: 0.0035  last_data_time: 0.0059   lr: 0.00013961  max_mem: 3525M\n\x1b[32m[08/26 12:05:03 d2.utils.events]: \x1b[0m eta: 0:03:04  iter: 299  total_loss: 0.3859  loss_cls: 0.07795  loss_box_reg: 0.1921  loss_mask: 0.09685  loss_rpn_cls: 0.0128  loss_rpn_loc: 0.006977    time: 0.9233  last_time: 1.0000  data_time: 0.0042  last_data_time: 0.0036   lr: 0.0001496  max_mem: 3525M\n\x1b[32m[08/26 12:05:21 d2.utils.events]: \x1b[0m eta: 0:02:46  iter: 319  total_loss: 0.3095  loss_cls: 0.0648  loss_box_reg: 0.1446  loss_mask: 0.0731  loss_rpn_cls: 0.00463  loss_rpn_loc: 0.005734    time: 0.9231  last_time: 0.8351  data_time: 0.0034  last_data_time: 0.0017   lr: 0.00015959  max_mem: 3525M\n\x1b[32m[08/26 12:05:39 d2.utils.events]: \x1b[0m eta: 0:02:27  iter: 339  total_loss: 0.3478  loss_cls: 0.0756  loss_box_reg: 0.176  loss_mask: 0.09258  loss_rpn_cls: 0.01313  loss_rpn_loc: 0.006678    time: 0.9214  last_time: 0.9566  data_time: 0.0035  last_data_time: 0.0024   lr: 0.00016958  max_mem: 3525M\n\x1b[32m[08/26 12:05:58 d2.utils.events]: \x1b[0m eta: 0:02:09  iter: 359  total_loss: 0.3483  loss_cls: 0.06559  loss_box_reg: 0.1385  loss_mask: 0.08243  loss_rpn_cls: 0.006308  loss_rpn_loc: 0.005838    time: 0.9229  last_time: 1.0476  data_time: 0.0040  last_data_time: 0.0045   lr: 0.00017957  max_mem: 3525M\n\x1b[32m[08/26 12:06:17 d2.utils.events]: \x1b[0m eta: 0:01:51  iter: 379  total_loss: 0.2885  loss_cls: 0.06069  loss_box_reg: 0.1262  loss_mask: 0.06913  loss_rpn_cls: 0.006552  loss_rpn_loc: 0.006211    time: 0.9237  last_time: 0.8312  data_time: 0.0037  last_data_time: 0.0033   lr: 0.00018956  max_mem: 3525M\n\x1b[32m[08/26 12:06:35 d2.utils.events]: \x1b[0m eta: 0:01:32  iter: 399  total_loss: 0.2252  loss_cls: 0.04141  loss_box_reg: 0.1254  loss_mask: 0.05625  loss_rpn_cls: 0.006246  loss_rpn_loc: 0.002948    time: 0.9214  last_time: 0.9677  data_time: 0.0035  last_data_time: 0.0039   lr: 0.00019955  max_mem: 3525M\n\x1b[32m[08/26 12:06:54 d2.utils.events]: \x1b[0m eta: 0:01:14  iter: 419  total_loss: 0.2847  loss_cls: 0.06677  loss_box_reg: 0.1315  loss_mask: 0.06866  loss_rpn_cls: 0.006594  loss_rpn_loc: 0.005496    time: 0.9223  last_time: 0.8425  data_time: 0.0035  last_data_time: 0.0057   lr: 0.00020954  max_mem: 3528M\n\x1b[32m[08/26 12:07:12 d2.utils.events]: \x1b[0m eta: 0:00:55  iter: 439  total_loss: 0.2367  loss_cls: 0.04199  loss_box_reg: 0.1031  loss_mask: 0.07469  loss_rpn_cls: 0.008149  loss_rpn_loc: 0.004366    time: 0.9212  last_time: 0.9766  data_time: 0.0034  last_data_time: 0.0033   lr: 0.00021953  max_mem: 3528M\n\x1b[32m[08/26 12:07:30 d2.utils.events]: \x1b[0m eta: 0:00:37  iter: 459  total_loss: 0.3209  loss_cls: 0.05037  loss_box_reg: 0.136  loss_mask: 0.06822  loss_rpn_cls: 0.003209  loss_rpn_loc: 0.005442    time: 0.9206  last_time: 1.0487  data_time: 0.0032  last_data_time: 0.0026   lr: 0.00022952  max_mem: 3528M\n\x1b[32m[08/26 12:07:48 d2.utils.events]: \x1b[0m eta: 0:00:18  iter: 479  total_loss: 0.2499  loss_cls: 0.04811  loss_box_reg: 0.1068  loss_mask: 0.06462  loss_rpn_cls: 0.004312  loss_rpn_loc: 0.004274    time: 0.9199  last_time: 0.9412  data_time: 0.0035  last_data_time: 0.0028   lr: 0.00023951  max_mem: 3528M\n\x1b[32m[08/26 12:08:09 d2.utils.events]: \x1b[0m eta: 0:00:00  iter: 499  total_loss: 0.2681  loss_cls: 0.04987  loss_box_reg: 0.1311  loss_mask: 0.06432  loss_rpn_cls: 0.003482  loss_rpn_loc: 0.009058    time: 0.9189  last_time: 0.7537  data_time: 0.0041  last_data_time: 0.0046   lr: 0.0002495  max_mem: 3528M\n\x1b[32m[08/26 12:08:15 d2.engine.hooks]: \x1b[0mOverall training speed: 498 iterations in 0:07:37 (0.9189 s / it)\n\x1b[32m[08/26 12:08:15 d2.engine.hooks]: \x1b[0mTotal training time: 0:07:48 (0:00:10 on hooks)\n")),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"iter: 499  total_loss: 0.2681  loss_cls: 0.04987  loss_box_reg: 0.1311  loss_mask: 0.06432  loss_rpn_cls: 0.003482  loss_rpn_loc: 0.009058    time: 0.9189  last_time: 0.7537  data_time: 0.0041  last_data_time: 0.0046   lr: 0.0002495  max_mem: 3528M"),(0,s.kt)("li",{parentName:"ul"},"Overall training speed: 498 iterations in 0:07:37 (0.9189 s / it)"),(0,s.kt)("li",{parentName:"ul"},"Total training time: 0:07:48 (0:00:10 on hooks)")),(0,s.kt)("h3",{id:"inference--evaluation"},"Inference & Evaluation"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"%load_ext tensorboard\n%tensorboard --logdir output\n")),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"Detectron2 :: Faster RCNN R101 FPN",src:t(93934).Z,width:"898",height:"363"})),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\ncfg.DATASETS.TEST = ("balloon_val", )\npredictor = DefaultPredictor(cfg)\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"\x1b[32m[08/26 12:15:35 d2.checkpoint.detection_checkpoint]: \x1b[0m[DetectionCheckpointer] Loading from ./output/model_final.pth ...\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'dataset_dicts = get_ds_dicts(dataset_path + "val")\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'plt.figure(figsize=(14, 10))\n\nc = 1\n\nfor i in random.sample(dataset_dicts, 4):\n    im = cv2.imread(i["file_name"])\n    outputs = predictor(im)\n\n    v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=0.8)\n    v = v.draw_instance_predictions(outputs["instances"].to("cpu"))\n    \n    ax = plt.subplot(2, 2, c)\n    plt.imshow(cv2.cvtColor(v.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB))\n    plt.axis("off")\n\n    c += 1\n\nplt.savefig("./assets/Object_Detection_Detectron2_05.webp", bbox_inches=\'tight\')\n')),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"Detectron2 :: Faster RCNN R101 FPN",src:t(34114).Z,width:"1083",height:"790"})),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'evaluator = COCOEvaluator("balloon_val", ("bbox",), False, output_dir="./output/")\nval_loader = build_detection_test_loader(cfg, "balloon_val")\n\nprint(inference_on_dataset(trainer.model, val_loader, evaluator))\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"\x1b[32m[08/26 12:16:21 d2.evaluation.coco_evaluation]: \x1b[0mTrying to convert 'balloon_val' to COCO format ...\n\x1b[5m\x1b[31mWARNING\x1b[0m \x1b[32m[08/26 12:16:21 d2.data.datasets.coco]: \x1b[0mUsing previously cached COCO format annotations at './output/balloon_val_coco_format.json'. You need to clear the cache file if your dataset has been modified.\n\x1b[32m[08/26 12:16:21 d2.data.build]: \x1b[0mDistribution of instances among all 1 categories:\n\x1b[36m|  category  | #instances   |\n|:----------:|:-------------|\n|  balloon   | 50           |\n|            |              |\x1b[0m\n\x1b[32m[08/26 12:16:21 d2.data.dataset_mapper]: \x1b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\x1b[32m[08/26 12:16:21 d2.data.common]: \x1b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n\x1b[32m[08/26 12:16:21 d2.data.common]: \x1b[0mSerializing 13 elements to byte tensors and concatenating them all ...\n\x1b[32m[08/26 12:16:21 d2.data.common]: \x1b[0mSerialized dataset takes 0.04 MiB\n\x1b[32m[08/26 12:16:21 d2.evaluation.evaluator]: \x1b[0mStart inference on 13 batches\n\x1b[32m[08/26 12:16:25 d2.evaluation.evaluator]: \x1b[0mInference done 11/13. Dataloading: 0.0009 s/iter. Inference: 0.2404 s/iter. Eval: 0.0199 s/iter. Total: 0.2612 s/iter. ETA=0:00:00\n\x1b[32m[08/26 12:16:26 d2.evaluation.evaluator]: \x1b[0mTotal inference time: 0:00:02.096206 (0.262026 s / iter per device, on 1 devices)\n\x1b[32m[08/26 12:16:26 d2.evaluation.evaluator]: \x1b[0mTotal inference pure compute time: 0:00:01 (0.235122 s / iter per device, on 1 devices)\n\x1b[32m[08/26 12:16:26 d2.evaluation.coco_evaluation]: \x1b[0mPreparing results for COCO format ...\n\x1b[32m[08/26 12:16:26 d2.evaluation.coco_evaluation]: \x1b[0mSaving results to ./output/coco_instances_results.json\n\x1b[32m[08/26 12:16:26 d2.evaluation.coco_evaluation]: \x1b[0mEvaluating predictions with unofficial COCO API...\nLoading and preparing results...\nDONE (t=0.00s)\ncreating index...\nindex created!\n\x1b[32m[08/26 12:16:26 d2.evaluation.fast_eval_api]: \x1b[0mEvaluate annotation type *bbox*\n\x1b[32m[08/26 12:16:26 d2.evaluation.fast_eval_api]: \x1b[0mCOCOeval_opt.evaluate() finished in 0.00 seconds.\n\x1b[32m[08/26 12:16:26 d2.evaluation.fast_eval_api]: \x1b[0mAccumulating evaluation results...\n\x1b[32m[08/26 12:16:26 d2.evaluation.fast_eval_api]: \x1b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.801\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.934\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.900\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.379\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.665\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.909\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.240\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.810\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.834\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.500\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.729\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.927\n\x1b[32m[08/26 12:16:26 d2.evaluation.coco_evaluation]: \x1b[0mEvaluation results for bbox: \n|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n|:------:|:------:|:------:|:------:|:------:|:------:|\n| 80.116 | 93.379 | 89.982 | 37.850 | 66.528 | 90.944 |\nOrderedDict([('bbox', {'AP': 80.11559882221059, 'AP50': 93.37913947975808, 'AP75': 89.98231932673679, 'APs': 37.85007072135785, 'APm': 66.5283878216012, 'APl': 90.94365376771259})])\n")),(0,s.kt)("table",null,(0,s.kt)("thead",{parentName:"table"},(0,s.kt)("tr",{parentName:"thead"},(0,s.kt)("th",{parentName:"tr",align:null}),(0,s.kt)("th",{parentName:"tr",align:null}),(0,s.kt)("th",{parentName:"tr",align:null}),(0,s.kt)("th",{parentName:"tr",align:null}))),(0,s.kt)("tbody",{parentName:"table"},(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Average Precision"),(0,s.kt)("td",{parentName:"tr",align:null},"(AP) @[ IoU=0.50:0.95"),(0,s.kt)("td",{parentName:"tr",align:null},"area=   all"),(0,s.kt)("td",{parentName:"tr",align:null},"maxDets=100 ] = 0.801")),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Average Precision"),(0,s.kt)("td",{parentName:"tr",align:null},"(AP) @[ IoU=0.50"),(0,s.kt)("td",{parentName:"tr",align:null},"area=   all"),(0,s.kt)("td",{parentName:"tr",align:null},"maxDets=100 ] = 0.934")),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Average Precision"),(0,s.kt)("td",{parentName:"tr",align:null},"(AP) @[ IoU=0.75"),(0,s.kt)("td",{parentName:"tr",align:null},"area=   all"),(0,s.kt)("td",{parentName:"tr",align:null},"maxDets=100 ] = 0.900")),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Average Precision"),(0,s.kt)("td",{parentName:"tr",align:null},"(AP) @[ IoU=0.50:0.95"),(0,s.kt)("td",{parentName:"tr",align:null},"area= small"),(0,s.kt)("td",{parentName:"tr",align:null},"maxDets=100 ] = 0.379")),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Average Precision"),(0,s.kt)("td",{parentName:"tr",align:null},"(AP) @[ IoU=0.50:0.95"),(0,s.kt)("td",{parentName:"tr",align:null},"area=medium"),(0,s.kt)("td",{parentName:"tr",align:null},"maxDets=100 ] = 0.665")),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Average Precision"),(0,s.kt)("td",{parentName:"tr",align:null},"(AP) @[ IoU=0.50:0.95"),(0,s.kt)("td",{parentName:"tr",align:null},"area= large"),(0,s.kt)("td",{parentName:"tr",align:null},"maxDets=100 ] = 0.909")),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Average Recall"),(0,s.kt)("td",{parentName:"tr",align:null},"(AR) @[ IoU=0.50:0.95"),(0,s.kt)("td",{parentName:"tr",align:null},"area=   all"),(0,s.kt)("td",{parentName:"tr",align:null},"maxDets=  1 ] = 0.240")),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Average Recall"),(0,s.kt)("td",{parentName:"tr",align:null},"(AR) @[ IoU=0.50:0.95"),(0,s.kt)("td",{parentName:"tr",align:null},"area=   all"),(0,s.kt)("td",{parentName:"tr",align:null},"maxDets= 10 ] = 0.810")),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Average Recall"),(0,s.kt)("td",{parentName:"tr",align:null},"(AR) @[ IoU=0.50:0.95"),(0,s.kt)("td",{parentName:"tr",align:null},"area=   all"),(0,s.kt)("td",{parentName:"tr",align:null},"maxDets=100 ] = 0.834")),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Average Recall"),(0,s.kt)("td",{parentName:"tr",align:null},"(AR) @[ IoU=0.50:0.95"),(0,s.kt)("td",{parentName:"tr",align:null},"area= small"),(0,s.kt)("td",{parentName:"tr",align:null},"maxDets=100 ] = 0.500")),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Average Recall"),(0,s.kt)("td",{parentName:"tr",align:null},"(AR) @[ IoU=0.50:0.95"),(0,s.kt)("td",{parentName:"tr",align:null},"area=medium"),(0,s.kt)("td",{parentName:"tr",align:null},"maxDets=100 ] = 0.729")),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Average Recall"),(0,s.kt)("td",{parentName:"tr",align:null},"(AR) @[ IoU=0.50:0.95"),(0,s.kt)("td",{parentName:"tr",align:null},"area= large"),(0,s.kt)("td",{parentName:"tr",align:null},"maxDets=100 ] = 0.927")))),(0,s.kt)("p",null,"Evaluation results for bbox:\n|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n|:------:|:------:|:------:|:------:|:------:|:------:|\n| 80.116 | 93.379 | 89.982 | 37.850 | 66.528 | 90.944 |"))}c.isMDXComponent=!0},93934:(e,n,t)=>{t.d(n,{Z:()=>a});const a="data:image/webp;base64,UklGRmYlAABXRUJQVlA4IFolAADw/gCdASqCA2sBPjEYikQiIaEhIXDpEEAGCWlu+ASteeiW0/ZaqimWdotcq5jViRvgt+OLDXfrfrZwl/9DskdMI5/3G/85+ov+W+BP/v6wP+7L0frLA/tb4VfG8/m6v639ZfgG/G+EH2f+o9Bb3380/6P3d/ND+8/8X2yfaB7if6D/8X0b/2y99vmL/UX9uffd/uXph+pX/m/5F//+xw9BL9jPSS/cT4lP2v/4XqX//n/k67L53/zPbn/s/1t81/Hh6U9wvj6+Af8Lo1L0f8h+tf/f9Uv+B9xXdD+Qj/GfbF6O/+bzh/Gz+H/5b7pPiT8r/4HGGf5j/nfvP6hf+s7pPmI/233Ye53/t/ef6HXsbf0P+E/7v/q+w70AH//DSIBZVp8ft18MldHd7l4uCmdAB9YEgMmGPIJN+GdzSX8E38jfDbdHmvhtujzXw23R5r4bbo818Nt0OoNqobsBXG3Q5Og2TMPkh3NNBCNEQDx+a5ZQAK/BBWRqKm/I2MLHXWz79Ol2LSWM5Fbe3sQv+WNQgfNfDbdHmvhtujzXw23R5r4bbo81Dr/OGX63p2773QOH7UBG0M944ARj/MJJhXwxffw19TvHSorpJkAnRfg3smM/ScPNvCTMKc6KTM3Z+e3obBmoHzPOvBv3Sq4YjSrqPqLhkm+5LRmZ6CIVlJ+ge8w9clyfQ35Od+Jnzo3bQL6V3WDytFHipp/hsyjxmQXv1GVB/rfxFLnvC0ZhXiFbBYXB06VpJ9ztVZr25zOA4Am++WB2orslvdbvC26/2ARQVFc8uRCXsS8FnaEFX+6e0oTsKkOf4Ah1PMG2fj9LDbB5WqarS3Acyn1aau+QdmeaTLYhVOCTufh+v/0o0ZMVBSZ9rPahI/8/dmDuX/4J2Ju9Zfr6WLqQZye7U1gFuhZN9kB24JlKA/sDHeuT8V5S6TCkoeEf6VF4NhL48ZjjUJ14QzXXBPLosEexDQrX+bj31QAatB3uFO+QgClrPGyvB271XjdaKPFQpVebfeH1+ffOGovg3NQvbZv3o1G/qS8p/8M+X//05COOykFnRkmvch62zytWJhoti+w6P20kytEQLk9QaCpReB1lzHpQiOOK+lFOT4GI/4lfH5tKmU+xEASjCXDm21J3yMNEFX/bxzFnwOjEeSS43o30FVUNrP7VXm1ffH87GIqjiIv8fqhW5FsMw2XfV7KpII9cpK5Nz3hkqIf6IJG9XLrld6CA+Y+rT/icchdVh7fpviWG2DytU1k3z2nsYTb/4jWAmSbzx8Ju4nRYYqIX18BUqNvfdTgPXTf2X0sNrfp68a6PsH88MvpYEm6p4LVXm1BhKnjgj+Xq29AlSQkVftsHlaq8tIQjasLfeObUJjRO4l6AgfqfbyXm7Prelhtg8rVXm392fW9LDbBTwBEGyS5zp7ym6Rma1yMVSNg8rVXNNfhcQQxnHA99NEnpxz/FULmzSzaZuhUn6WG2DytVeNf3Z+P0rusHlaqQ6wSBGLggCRCAv4QvZ48F05tM71t/da7wOo7NE4iydmRiY8CbZTgaQa83Z+P0sNsHlaq82/uz63pYbYLF1uD9QVKKrBB0aw39Vz1Tq0rRxh9db8S7ftEFvprqJlHHRFD8Itp2uBXZLc74RAl4DQ1B1UK3I2DytVc1PYCFqrzb+7Px+lhtg8prjFdm5B1C/VlggQqD2NHVCEMvBiWJ5mrVUzZNQ1OZDaNE2MU5sXbdm/Ap85KZV+YBW3VnTl9vTPG82ZXDHFwlIrjxnXOH06QmMdCxT/okSDalaEmGMg66V7V0uP1eJKX8mAS0UnpnptMhCt5rYJbf8Ta3LDh6Bq8pC9gQWzClG9BYFNoMAolQOGmh7SyErVIPRlWKVs/QJvXdTKpefRBN0ftpB1ikPOrR649i8lHjQqoEOpByLHx1oCNlN7twWh0fZydPntS2mL9fv2PjytVebf3Z4F6cmtD/hS3I2DytVebf3Z0BSBgCvTFVwIWKKF6hiDEKTjHE4/Gw3w0GKHvp3wiGhCY3zbmqHlYgy+lhj48rVXm392fj9LDbB5WOPNv7r0w78tv5Xq9cy4j9qA5z9hQotbmbhcD/yawxqJC2xlJBBp+swZ9LDbB5WqvNv7s/H6WG2DytVebf3Z+PQGtrFhRyeiNwei+C1E5uL0y5I5pDPJz3r3VbI/5Aj3Ksz8wDv8X8MdK7XCZKtIGdvs+s6V2uEpFaQM7fZoZSoenKB3AgJMZKZkw7VXPHqnbdjA4+8LrdC0Qvd36Sa7LzdEBu9818dIrjFHm6IDej5r48rVXm390QDG9vY9QOgCucsAmjzBmxJugdNk8m2CWiKiHQ6npYbYPK1V5t/dn49ACubs/H6WG2DytVebf3XksFjnoH4gTQhQRmluoem5Kr+w2wsAGXY0vlJCDcKVR3lRn+0rVXm392BZTEE7bB5WqvNv7s/H6P22DpdVebfZp7hE9wo61PCuGiAIgwbqlpFc6fyg+VOm2Cosb6CV8vsP2snNtYf5aX2H7XQDpCBd5wliTueecUdAg0kXzi3zkLZG/ePnQ84pCy7wrs/zCal77BeoKngh0CDSQr7ZmSAIa/k4TkiaLp2nW/mZ7NAvk1dqXXfRF5U0qAnVyVDp5CJbPepNF9VPuVuZ4S68jU9xjljt7ayCDZFwrD2IZb9rNSxVMSrAaNP9xIe/S2aBltzf/51i84hbAXWgJqysTRippPBWJIaDlf84m3AAD++l6dU2eBtYtravuOmpORBJQ1lmRt376gU1pfS7uX9L4NVD3j7EaBBbCtESN2r9fa0PTOfCdhIZv7DhJob3HOdIPO1LM/v8lrhSuVSIOrOH978lAgQvkX6W9BDwvxVRoMVejQztaTteFmnDflvYvYQTTBUVAxdgO/mamN4zMUxxOKutZG0ibQzk6i4iuM+I5szXFN0Pbi/FNHdhcF3vriEdAvrLp4cxSiSe3glFIxwHmf+YrlWRruDbPIh3IJVIoge60dsrMEGAQHXotSmAALKZQX/uF1Ck2xhe3EAXFPinQGhzDUhgZePsKDd/6TGBbeWHjQkv3Df5N0lGlgmXWarQAFEt3dN4uFgDqJVmjCCeybmvnzzKhxNLMLzw9chNrQ6E3RirIG9uBEmc/xZZA17bg5jTKLB0KqjlgjTon9yKcsp51Xpd5DxpB1f8YfAyM4l73SMh2dsoLngi3m7K2v2ouiNAW6P5DVw4f7sx+fNpLsiYvfXTVT4gb9ig4dcQgQFx8ibtQO5A+lDASxluNdGPyBExqw3u53RnMWyXt1m+SPwqvFDpwSzD8i+XiSHluEnPKV6Yf+DofUtUyzR33O/dWfnyKv4tnbRzswaSebMxOMfZW6MuyGo9RIsz3ENPYiZB6eszf7rgVo2ZISZb/Udmm/o9wplWRkrvX9rLY5xhBNLsB6tVS8DmPJYwhbcq/CpIcK+ToQxKRpop2A25LEBN5bw5bS0FNICOcVqIkMTPwwptVOWXsnxd9RoUiGS/vOCsDohnykEwZ76B9uAlLm2eyxA99cHvwCt+/795x5w/5syEQ61o6qW8rw0Wv2z7z7ljX0Un8YHxgKm/44doC121EZlJupcWTt9sWfkub0M7tnyE7x8yfGXnu8T9dBil9Ge7qiIk7czXlws/+kJyn7d+NYVuKxhiKRs75mK8pRT6m+bt8eRzdIvEQnvw8+fZUsbKUNpCKkzblog6kFOERfzWnWqgwAe8TkqeSJ2LduoPx11I7/nugSXNOX7JvyrSqmG3eWmzYf53/BIGyf+uJbdr85AGjpiaHcrdqepGx01NsDdml6Vhk4ZeS2tAlThYpDGnlnFj9CjZG6kDjr6yQG0YAqRC2sK37Y917cA5ztS+V7OoM56b8Cj5f0//6KZnozvmSO164mBdlk16T0rqDXSffZ1R2tg3c3UqQnKC2ArVub0woxDdPrbJJv8/ClAsdZdwfKQkw/Px51ZsAd483k9F1cuh76NmiQgTE/Q94ZwwghOkHS1VXYRIZWvrP87oCSpWZ4Aa0MW5GR+Fzu0c/7il0CNtZgcIESUm4BjVhOGYxQIwAAA96v0vMwqEBxj3yCJu5PwSUQh6ealr7cehU8+WG+fLB3LTWHi9pOsEL6/ARfkdecOR33lIQl8Z1M27QgxxIIKiZknY+0yI3W31j2CbAJTMXN5zrBkKZwUk0qke12NyMM5Hl3I0mZFOjH+jRy1sqJhwBjDtmEaP2HXKCyd3fELABp/kJ2CT1vKjaASA6phFcSNeP5Y713ssmLM/tAJAdUwi23d8QrpR9+FvQci4KLZ7WHVI3j+GcsvxEAyRf22/KClZzUBImtl897sxF57XWRtt1TnkNehuHBtG7T8KYZegAM4aC+fGEIBox49U/XW6ueg3RmkvP5LZe2wsbSDjSxQ7RAn8C0SzL8Pk+X49QI9VAcSzNZZYtiq5MmSIdx0pWw4qw1wlUr+kC7Dxb7FunKyYhuVL2Ok3L+9yyJB6nR3xtaiSSL/P2NXTvs51Vhr0+wb3q4kPw9+GI0vgABjRa5aiYk0X51Eq6r/YVUPCDBFHjopJS/v1grRbqb3MHa/u4nvONUZaWYsHAPWVveEu6mXIFuqBRemV/XS5eidxLTOXaXCd0eV7FNhr7ua8QP9CXTlMgTHOKWcfEAdCeXc0tiuxtb9bErDZcX/7CDEUGuMxA9au3yTAqLgCqMsF7XDzvM4SEQjsyCOiHAgR4xHBhp40dYi5/F2Qaxxv3N7drk5AUojKwEEiHeMNAw8Iwr9xRmm3NQp/DH6xntTAGBuPOd1amnz1yFUrzmvKb03PEvtqAcfPS4MqFjnBMfXOG1WifzJbhvaUw+Zwl5UCLce+0szF/I6gwJbt+gn1rMpj04h2aXWrG7f/H0qOEOTpi2teoumV8m8ErVz1P8Q+gDBMGU3Ontc4m9qhoUAzPGKJqSC8PO5VAcplDtDypf1qjDg3d6fl0DCP/ptBGHd5lwerBkoYfYnIyf/6dOhSvs5vrwm1GCedOTmrCrVE7/wXk+1ScvmmLSPgG1hIoNczI4UFF79z5NIJRrInOvSHw9lfgIE/G27lVbA/WG1151suF6sJ4DeRI067lloXjN4WGtM48JBGjEiZFEi+csMqzQwjLZHnp5kHXAVngjTInpdDQhWJGX8jvhT2HKQIIQ7D9ICMXJ6CSi79/RuhM9azQiMADXUIRSwQ85TUv9CEUxBdFwpCdxc6862SpfYFzhug91oCzsQJLF4UUCvaZ0PYE8xJx9BrYIxWW06C9LBAuMMneQFCd+IXtodu8dBPEgz4gcjGGvhEzEglkSJPGZMA4DMns2MgiZtWj2IgD2986S1s6LIw46QcrtIgGJjz6ArHy74gBCjLyCxQOnmZTRf0vRRz2ezmk/sH+WOaQc3iVIC/6h0WnWbOzZYZdIDyHYAet895Q601hggt9kXyIaOSqrYSsHm8s5we9hpv0eN4ZDc5/jlMSCMRE1eeZfS0FUk9aG+s7SdgvXx54IfudHwDOB74Zh0oXImF3LhSPmspG6XgADC36yG4NocVrcWiS5FIhFwUVpYeFBzhaNE10aqPD77nxSjnBls/wC6R8L2z7PixjCyOiHGvJPf2hLeotmouG6s/qgZ37mkrzWHLuCkMNX+uA3WW6Abl++ooShnBw9nn5Ak8kTUogVEwdMUhOCKpt3/50gWVZY/iFe4Os0i6zBayrVrZoksnrQJRyDVGVjlyzhPtDEzP0Om+bJOLLPThEVq4TXtvGcyL36zVMdKQaEHzjLZT4Uh0i6UQGM9Tr1I/ZCYvIcFNx0gG3olKvu8kVkQlhti1H1+WpHGH7zDXQCJJZew5hvYIBEUz+xkNsvtVxCFog+mf9gQ2KVR5zhmIrqAwfPYzAUBix6Mvk3s4/Ky9eugQL21PPywGQ9EKoG/hX2IKE8cyZLBrXBBXGCRzTYAAGilxIVDShXJoBgJpRwZzDukzKGUYEbejHnDTEdbxUjgfDGgoigo90E3SwlNSEc9q/fJRYYCZryax/clvXI0yge+SAM/cE8BqNs+vEb95/hjZZaYBtNmPfUonM3cEVZfJsiGknHS3LueTk4f+vHNcm9A87nfSFqFcs+ETgSCkvUF0DOIKZTpNVsRH6q5P7soTwAeuasYBCclZDFtdpsVAt2FJ9NFjItagLop09qQYFcYDsFZwIKKEHdZNm5eF3hlEdx2kwkQfJ/zNPVvY1q+RXsuxfdaZc0Cl1p6lmX2e6k9/gl4AFJW8MvE1QxsjbBvoKTrDNkX6vH1zurLPOrGdkRXDbPEfvjFl3t8vDV6TuPhkYh7IkZc/TZIiChdin8XmZUaqcaSJAVxO0Lb0yaPRDoScf3jQAPdJnr5WDExFtjjauLlkijCLgE/YSGg4pnNMQHfi0azRYRp+P1r4XvlHQfAy9EZ/e24B/oLRPZ5grnv7HVa+qWFjukFgs/PM8GorzMiNYTbl2bWSBqo0Sftd16l4K7HxOAz9FmWTmMCjORsjHmyWsQhwUJAQTiJrcif3ZKpLgRNHM/JClGU43Dm4uy3iTiro7JcokE2mewHGb+tKonpuXUfbHCiDZm9KSXaMaW2t6PekvLi0VfLnoeJBFUU1QD6p/ZKAcXQ/ruK4rWn1yuzMGTy8f1WXQgx9ccITOdUNY0qAF3YRQQxwAVRbnDgHfRFfVdkb0eIHQANCCXpuxA3GU6caTtlspM7ziBHgpGvWjZ88mblJjJXIDpXAgH8b3Zv/yGtsw+4THFpU00H56yRrMMA7noP5+GyCOCubLdl83uDtdAFPxPUfUwabc8K6WVfXepsAwomf2ybY7pBwZGRp+3et5+3wyt/oOzeMyr0GhgfZHDaR8j6Z/t02f6SB18EWFTkKIbZmI2OY+IJ6L6oSdW4sZQgl6Rqkkfq8EGPUK+PfmRRcrRO/VaiBTuTEQ8zgerlYmvfCrj+l4+oCLwksTrHYXVcbZ2saG9eirj9anrBdwOAMkzRJq3lXGw7UJ9sUoq+/DBhlrL7IcOUSERfEzzhUgOUDg1A/PVRJElnsySlgSLLd0TOzPaKAkTI1I7++sbV23Ho5qXVuH4+R8LvfymCWTFxBaJDVjz3gKBUA0jwXgNT2AiMRYFYpkmNKPrFd7Vr/AFRkp/eqA62XFgAQmHD0QQ/cwmQDSJs1iIuI1u5Zrfw6xtLw8AwhF7PHx9uXDTyoU8+qKAT2aBCzUpCc20KO8YXFfKU600bJpfGeeBddtPGG9g8BQPZ2NT5T2BymE3+6ZduZxGe9a2dKmQSLaqAgYgp9HYQE4WoWWLKKqK2Z3FgkF+AgmL0ZOUzYDAWtGGAK0QusXkPVMqETyw55o2pmReTR2ksb2R4K1uW/W7ojPD9YvCqUThb7FeOknt0QXS0E1Q1LL9WLkE/h5JoAGFJw8RAhyLG9Ay/RsPaD4ztPgXjWJoiP5Q96k3b1WpRAY0nKrWjlG25pkrbFn9m+dGk0zbYEx5kU7s6EdVyM4glq5XITWXGpkqXLCRbZhVxQc5LXNV1k/V5qkAJbFq7ktc1YC+LUhzMJ3bhjzcrlxQeaugRNlLm1+ttzBP2lzD7Otyy13Ed+gcjYynQdWJLshzeM5+BKncA2wedzlQYpz4+7rnPCzEidh98XCsJ1iolfTv21VChBBD3NskWOapNxxa3dbQiTUt504nxFs7RXG3nfwlGureGJtxrsVhFBaEYN98EjbCXKU+7VnpuP9r+gBJ8PI+12PpjRYkagTEZx63tEo9NoUnyQRwf9VKILT6+e6zVpFHQKAoBpWhTTBNoy6p59t6QwxgovrNCrBCckCyRUaX11hbhmKUhQlerbBbTIjJ2KE2Wgo3e19cUOjaZuCUqXN26yVaWS4sVapWBW/ZSHRjW2wy0dQTdiK8Q7dKOR32VXA0dGLCy+Z/0g1cQgZAEKokFx+KTT4Q1/h7dRLCU15mQL/hU7Cjv/qVeFRt4HWuZqkBRdimyvYC76+1UcUoUphKRHuiBK9Xyult+EXshWq4HCXPJNfHi7nXLRzx8qYviTvNwOK73V6V+s0Mq7L7vmvoMFkyL1MWyZ1usD2feHLqSCb1Vmh0tTjHcwEbIVhIKLHnbjK3At18wAkw7dx6s831NtSAi+BseuPz4mAGyUUWOho84P7ET7X7Pp0d6xdz4G+W++Kix0nhcNiBJLTYT32BEhEaVeo93rNAIRPB4/fQt8ps8dnM1MGxbY0mtcAAmIJ4SjXjoO4XP2qkIA7w/Y/CfSq3Km7tWaB/+sIatBGVjLfYqao+4MD2E6dcm1vy4z3K9XW4RwDZ3ZbnAGH37CorMvnIRyTZDLn6LcPAyqkdRy4GMTSzymnsIYhibaAgfmP+WvC3CZFUszH5Ip5q+hW4w0ESMU2s6z2MmPC+kjcWbZOYdrBv6GUz0/ODwM4RkkaMmLNOfBVghb0X9lQRbXryJCKUgC5gt9fDo+X6x8VgKS1NQEEKDZm5bF79gVEDI4qGjlIuy8KF4ALjXbgVUE4cRV530vnzUz9rgHBpFXJeWE5gJ3BSBS+Q0P7qS6/jzMWwqatfunZX2g8FkNUKIjFuHeXCt5bi/tFCl7h5eeRNL6GEH58RA+mwZMcAwM3mGGH6Kbo3UUWsHhog4KrRfpV4Xcz3ewOCOr6KRVAPDec8EghG9lJPTb8fJxVKUWstSr+myvk2XfRGdgtjA0IWHglGoJD6gQIRy2k0EmF/3MyrULd4+XZRzOVC02BEAO/sYlfYiQ9dm44RUZyBahdV+EqXsQ3IbPGvfol5Eb65Zoua3Vpo3olhfLiW4HIN98rtwhTiGTn+FxGyz4bUTPEFpYQ3WKmAfON+eqcTHdwKfabQbk2fO+2d2jEbV7MqdLkrvOnQpVKMaQ5WzFpceoWGx7E2BBRF435HE28MpTeiLlyf2o/8Q6RtJEWe+G1DWj5Z0TejTmtGyUvZcH7iXeDQf72dLG3p445RXbt32D51rpWiIvc5CovXgdNwUZimLk9b2BoNkcz8qzMqaFl9qKqhNtoe/moPH2hB2Uk65JYkYkM3iWB9DBzwpo+NnLwLHlY4EKc8wvsgGHbjh+scbhkOVZXcKP16t/26FfiKN7eLCiShcUAHAetKsyvsWDeY0mNt7XIyTZTMwvDESOPAGGvH6UKm6QwCpj2PrWnOhyS2WKQN9blgXfexHvuXNi5LPICYHWSWyqsrhg5yn6wUrSzjLKOTC2qcXzBETgx2xF7J92FQja9CUn6ouGzGvOwAjSSbAtio8sHcVSVPdI7vIoazzQj0QhCZeTlDdN0GW0KVoW6a+1kWfq4PAP0tl36b0TEvz2Ja42ETwHNI3CmGkzsrwcS5a4FaCWzw/9Lizn7NlpMsspvsyO029RafDYck2SSrpaCqAUEgZ0oXp8/ZZJVTPZ4ZpKWKJTPWplk/f4aGXyCWhS6L/+sbD8BoFy4ZiBzkLNcTyZbGMu2b5xWaDYoo72MUC08VO05lvZ2FgVNhyr8RQcn3PEsIKwY4o23SMpfUEdjco+v1WkwCtWEBr36JeRGzjm/SCAFMu2AbyT57SiNOauGpZuJlVdHaKAixzW03SFYfxBHP7jaSVnQEB2UCqn/RxSDU+CqCbJY+oSvATLUDOJCAIEH+RYWa5/Yj4rof1XxaLvi4VeZDAbeDJ6o84k1ogAF7j6jZKxs0lo/aSQsQ34Rn2OGfkd6Bdlule0Qcv+B9PFDHBYRSsKJK6TIeRBDBYkMTBPWX5hKrX1klxpdxvJwxBONAFVMuKL8C1dF6k5USES80oNBK/ET9XpiDO2Lye6Nouy3eI3JYPd27Qf/1jFFBHITZnpnuWfwAVQ2AoVp5seml2qyNQnVvsKkiZ2h3ZkRELsZJjsMt+9PlZASUgIBC5ygGKSPtHf5b/ZboP22txiDplGcanvwMoYQ2JBBB56brvwBt8jrChadl3rc6W/5K9L8WsnqQu7scXE0GG456v354AlwDDsV8y6BqbvqstOYOtDjuYkiNpTWGYMrro+tqvT//fI4Utq9vMLc6jBEJwbiyLdS/DIGKw+Tjd/z7nHF+zk3i7TSxqQaPZkuloYB2DvNk3QwQlWjnbb6Oj7ShTMabkLrdA+Pbg34fYxlv/8kalyvH94ildpb03Rhafbf5OW/910n1Np6pTIRA2vAbMdrz8lhQ+gj3t2Otjs/nfLqCokMt9awXXuQL1M1mn4ycBImOSA/zEiwvydu2RT8LhAPmcjLhIiWfpOxdsj6RzKY3gJE99TEfklmrYnEp+j/6C94eZN/4qHqYK/E09JXPyRhM/fwKnvr6+oANhDmCYpuQVkuo5HfaYJfqMf59/c4/A5Uf16zA7j1toK8T1fy7rfaKHS8YtPj1iPJbWtL+FltdRz7FtFf39/JL5kV7oeQx+G32Rw2NTBWT23lJqIxY65B5fLM4DT8ss5AfpE63tkSzZCpuYkVeAR4RNA5XLKPhdqLxY+KNj4dUgq6JL/T9Vrynu76Ar2JJ0TsMA5N8tZQrltHP0BYBv2wa84TbXfezWJsw6pBVD8X2axNmHVIKh0mTlg5kVX86rWDhoPfiA8KjGeRHYzPxr73T6GGH/Ff5gMpdeHESfgtQnLoclVTPCPDT8caMeE+Zxm+/kf9A46VksF8m0XzLQ7KQuCBYhmSwKSIL2ooytZtjgTOlcOJXtZHp0A3UZ770sx8mgOEwqY4q3l4ks4+oFk68Ke/D6yr4Ep9Ga15w/sPc1OdYo7xmDO1pcQELWw7j3jlp13dFz+9hLKs3Vfqep+SN+J0ZIRJrbfDX0P/yyy1d4V2K5tv9hQxaeL0RYpGmnDzKKm85IqClzOA5SYV8geLB3yFPUuKQjeF69P9cnLyYMU7fOfFxuYvFQ0DHXRFblPAvKwE5jcxa8aqlr+lfu2LjWbuLw55syz9w0A1NqcXfXu03VhhiWMset+V3kY8mM2Tr8B2bFKRd9VZWhuXFUcMMqzMk0n3sVRFjQlcGU6qvHNtcdqR7JbCz06HNhdQlGeP2G+s40jYJgi6jng0govFspvt0FGisMeOceGTN8z89ggqDyciKQnmMmxCqnaMCGf4fDXZYI8//g6mUqb/iyo1eCGwfFtYepUwBZgAXrqA96sZBMKAAicplaeu2K1uVmXqDwThAPV7nztRo+A2mFu2f8ygL87Aoxk4+ySjq4CU/0EFyTUEgFz0D0X716rcKtp52E2X5Ol1ImvW/M+PXt+xZu/OcNOLlQmUR9YFJLPDgT5vOCttP1AYHmQl7AE8b7syL98kudKYDBuk1zG8m2FvUWGGL9xQON2t+Ri7O4ArFFkvjg8gnX6m09UpkIgWyxuCRUfGTUkFrwE8pRy3sDW0oWomkNLChZc7LkSxTM775qid2ZN4L03pJqE+E1SkSVOhBYXYdjEk3KQR+CTV8AWj8fIT5hHfFXCiYdGFcLFbOfuykCjHSGUqxFsdz7QsPscC5/sdWWlzKiUZkRfkNHI2PZ1GxpjULaa4Ux9gWgjP1+Pq8QhmthO23PsHF15Hy+T60+o8PLxjf6rQgmMVfOz/YEe0Vq/QEPp9EFjmtORpD4Y62qpu7ESJjGgVPMeOpGX6AP/LaNNS5/n/sUiFrDxKh3WFImCoxNgoFVdTmzSekGnDKZka/ETHzOuV7bvMvE6UVq0hGSzaVaXme6aUW7ViFfwxbbc8I7ZVYpu6XJI4QUHnfTLHGJjYPwA3/kBOUPw+XDmhcChVGOti0AIESZRG9Hbrtc8vasOgQxOmgmU+RkbVr3+xPxBH8bpVzDu2usSiTiwI3y0v0/PJws9N3D00OEp1tSKvEq5QyeZMVYf+nXH1PhOLEkCLPP8Yq2Vq5u7X1u6zvdp0w3748g30FPC4eEWf4kHVPqJevOrMC6/asfjT5iowqRcGB2RbLujp3KeJWqyCiOxurXv23BSy98BRGmHlMISMHZdnSEmOVxyB1qvBpraJY4o4Din2+4PizxhGTHIWOC8KtCWWkcwtLG5pGuURZw6vAryMvVVNkSBmAk+TKXWzvVBQAYWEBtdT9Q348jmQYc53tvRL222ET8mvMJvFIeV4E9OnWlSY+q64kcA/ImYwcy12XFdNRiLQEc8AKTLfKI9to1gGqZqI6+uDwVLuVyA54QmSTdfmLUN/6TQ0sxhrJVT46YHwcm9fpRPEQYIPSjZ1p1vi/6UgWJPICKOWTSAt1mdJ8Dt8VHQdUaJ1+s0skqVEFDjbnQWKK3jCT51y5/pXhKTXMxS5Bw+vfeL6HIPBXUJvPTx1N1wIm+Bu3ui3YIHYadjh5vvZ18RXu0rLJjvij60jdFIOUY1KRxv3ZXeuT5mulqXTQWFnEd+bHSYfO1+s99Ccy2zL9A8VKu1iiPXBaD/GX/PP8JXM2SDpU9499pPbXPyuq1HCyLKsLojAt/9JdMVDUl106anthYYvCa02uG3MaB19ba8e63AEMwJCzT5fak+Yk9UvYdW/99XR8KffaEdhhf/chXVOF3iOr0mer1pzOAh3FQOet05IQKMbS+y6xmrv0uZy940FKZWvmJ2Hm6d5b48EJ+F9r8Q7sY9yrdjIvFUHyhJOAzxq15pIOwqFiG72pTmGJxT6f2oUtbJ0pE/nZxcZMi2CqpSyZrD9RuhUYk+djgrZ1brBHdjb++AkbTvK+Xsm3z7Nc0KfYza2TDny1Jdfu47n2OUyfTMSMmaBvnOLFvIYbNTraJ4amBZ8VsiGebW+zMsajI2JgHAatRFYQciQHDuUerMx8RsM4mPyKq1Lp+rZSmjJxRRLxTVcCNy4YkExtgAAA"},34114:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/Object_Detection_Detectron2_05-56672b6decbda812825b47fb54692518.webp"},74199:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/output_15_0-898162873cc5853df639f17fe69d4217.png"},37835:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-f940fa4541ff8a00764cf3f41cd6b985.jpg"}}]);