"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[19167],{149257:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>l});var i=t(785893),a=t(603905);const o={sidebar_position:4950,slug:"2022-12-11",title:"Breast Histopathology Image Segmentation Part 4",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"Train our model to fit the dataset"},s=void 0,r={id:"IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part4/index",title:"Breast Histopathology Image Segmentation Part 4",description:"Train our model to fit the dataset",source:"@site/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part4/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part4",slug:"/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part4/2022-12-11",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part4/2022-12-11",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part4/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Tensorflow",permalink:"/docs/tags/tensorflow"}],version:"current",sidebarPosition:4950,frontMatter:{sidebar_position:4950,slug:"2022-12-11",title:"Breast Histopathology Image Segmentation Part 4",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"Train our model to fit the dataset"},sidebar:"tutorialSidebar",previous:{title:"Breast Histopathology Image Segmentation Part 5",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part5/2022-12-12"},next:{title:"Breast Histopathology Image Segmentation Part 3",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part3/2022-12-11"}},c={},l=[{value:"Model Compilation",id:"model-compilation",level:2},{value:"ResNet50 Model",id:"resnet50-model",level:3},{value:"Custom CNN Model",id:"custom-cnn-model",level:3},{value:"Model Checkpoints",id:"model-checkpoints",level:2},{value:"Model Fitting",id:"model-fitting",level:2},{value:"ResNet50 Model",id:"resnet50-model-1",level:3},{value:"Custom Model",id:"custom-model",level:3}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.ah)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Guangzhou, China",src:t(498683).Z+"",width:"1500",height:"383"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-10-tf-breast-cancer-classification-part1/2022-12-10",children:"Part 1: Data Inspection and Pre-processing"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part2/2022-12-11",children:"Part 2: Weights, Data Augmentations and Generators"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part3/2022-12-11",children:"Part 3: Model creation based on a pre-trained and a custom model"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part4/2022-12-11",children:"Part 4: Train our model to fit the dataset"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part5/2022-12-12",children:"Part 5: Evaluate the performance of your trained model"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6/2022-12-12",children:"Part 6: Running Predictions"})}),"\n"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://github.com/mpolinowski/tf-bc-classification",children:"Github"})}),"\n"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#model-compilation",children:"Model Compilation"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#resnet50-model",children:"ResNet50 Model"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#custom-cnn-model",children:"Custom CNN Model"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#model-checkpoints",children:"Model Checkpoints"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#model-fitting",children:"Model Fitting"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#resnet50-model-1",children:"ResNet50 Model"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#custom-model",children:"Custom Model"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["Based on ",(0,i.jsx)(n.a,{href:"https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images",children:"Breast Histopathology Images"})," by ",(0,i.jsx)(n.a,{href:"https://www.kaggle.com/paultimothymooney",children:"Paul Mooney"}),".\n",(0,i.jsx)(n.code,{children:"Invasive Ductal Carcinoma (IDC) is the most common subtype of all breast cancers. To assign an aggressiveness grade to a whole mount sample, pathologists typically focus on the regions which contain the IDC. As a result, one of the common pre-processing steps for automatic aggressiveness grading is to delineate the exact regions of IDC inside of a whole mount slide."}),"\n",(0,i.jsx)(n.a,{href:"https://youtu.be/8XsiMQQ-4mM",children:"Can recurring breast cancer be spotted with AI tech? - BBC News"})]}),"\n"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Citation: ",(0,i.jsx)(n.a,{href:"https://pubmed.ncbi.nlm.nih.gov/27563488/",children:"Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases"})]}),"\n",(0,i.jsx)(n.li,{children:"Dataset: 198,738 IDC(negative) image patches; 78,786 IDC(positive) image patches"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"model-compilation",children:"Model Compilation"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Optimizer"}),": An optimizer is a function that modifies e.g. weights and learning rates to help minimizing the loss function with each epoch. The default optimizer to use is ",(0,i.jsx)(n.strong,{children:"Adam"})," (",(0,i.jsx)(n.em,{children:"Adaptive Moment Estimation"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Loss Function"}),": A loss function is a method to evaluate how well an algorithm models a given dataset. If predictions deviate too much from actual results it will return a large number. The goal of learning epochs is it to minimize the outcome of the selected loss function. The ",(0,i.jsx)(n.strong,{children:"Binary Cross-Entropy"})," function compares the predicted probabilities to the actual class - ",(0,i.jsx)(n.code,{children:"0"})," or ",(0,i.jsx)(n.code,{children:"1"}),". Which in our case is either ",(0,i.jsx)(n.code,{children:"benign"})," or ",(0,i.jsx)(n.code,{children:"malignant"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"resnet50-model",children:"ResNet50 Model"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"./train_ResNet50_32_20k.py"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-py",children:'# Compiling the model\n## Decay updates the learning rate by a decreasing factor in each epoch\nprint("Compiling model")\nopt = Adam(learning_rate=config.INIT_LR, decay=config.INIT_LR / config.EPOCHS)\nmodel.compile(loss="binary_crossentropy", optimizer=opt, metrics=["accuracy"])\n'})}),"\n",(0,i.jsx)(n.h3,{id:"custom-cnn-model",children:"Custom CNN Model"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"./train_CustomModel_32_conv_20k.py"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-py",children:'# Compiling the model\n## Decay updates the learning rate by a decreasing factor in each epoch\nprint("Compiling the model")\nopt = Adam(learning_rate=config.INIT_LR, decay=config.INIT_LR / config.EPOCHS)\nmodel.compile(loss="binary_crossentropy", optimizer=opt, metrics=["accuracy"])\n'})}),"\n",(0,i.jsx)(n.h2,{id:"model-checkpoints",children:"Model Checkpoints"}),"\n",(0,i.jsx)(n.p,{children:"Model checkpoints are callbacks to save the Keras model or model weights in a given interval. Those can be loaded later to continue the training from this saved state. Configuration options are:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Save all epoch checkpoints or only hold on to the latest best result."}),"\n",(0,i.jsx)(n.li,{children:"Save after each epoch or only after a fixed number of training batches"}),"\n",(0,i.jsx)(n.li,{children:"Save the entire model or only it's weights"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"./train_ResNet50_32_20k.py"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-py",children:'# Using ModelCheckpoint to store the best performing model based on val_loss\nMCName = os.path.sep.join([config.OUTPUT_PATH, "resnet50_weights-{epoch:03d}-{val_loss:.4f}.hdf5"])\ncheckpoint = ModelCheckpoint(MCName, monitor="val_loss", mode="min", save_best_only=True, verbose=1)\ncallbacks = [checkpoint]\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"./train_CustomModel_32_conv_20k.py"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-py",children:'# Using ModelCheckpoint to store the best performing model based on val_loss\nMCName = os.path.sep.join([config.OUTPUT_PATH, "custom_weights-{epoch:03d}-{val_loss:.4f}.hdf5"])\ncheckpoint = ModelCheckpoint(MCName, monitor="val_loss", mode="min", save_best_only=True, verbose=1)\ncallbacks = [checkpoint]\n'})}),"\n",(0,i.jsx)(n.h2,{id:"model-fitting",children:"Model Fitting"}),"\n",(0,i.jsxs)(n.p,{children:["An ",(0,i.jsx)(n.strong,{children:"Epoch"})," refers to the number of passes the algorithm has made over the entire training dataset. The dataset is divided into small portions and not processed all at once. The amount of samples passing through the neural net at the same time is the ",(0,i.jsx)(n.strong,{children:"Batch Size"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"./train_ResNet50_32_20k.py"}),"\n",(0,i.jsx)(n.em,{children:"./train_CustomModel_32_conv_20k.py"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-py",children:'# Fitting the model on training data\nprint("Model Fitting")\nMF = model.fit(\n    x=trainGen,\n    steps_per_epoch=totalTrain // config.BATCH_SIZE,\n    validation_data=valGen,\n    validation_steps=totalVal // config.BATCH_SIZE,\n    class_weight=classWeight,\n    callbacks=callbacks,\n    epochs=config.EPOCHS)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"resnet50-model-1",children:"ResNet50 Model"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pipenv run python ./train_ResNet50_32_20k.py\n"})}),"\n",(0,i.jsx)(n.p,{children:"I kept the training running over night. But already after 25 epochs I could not see any improvements:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"Epoch 25/60\n6243/6244 [============================>.] - ETA: 0s - loss: 0.9039 - accuracy: 0.6454\nEpoch 25: val_loss improved from 0.63957 to 0.63328, saving model to ./output/weights-025-0.6333.hdf5\n6244/6244 [==============================] - 215s 34ms/step - loss: 0.9039 - accuracy: 0.6454 - val_loss: 0.6333 - val_accuracy: 0.6313\n"})}),"\n",(0,i.jsx)(n.p,{children:"Hmm that is interesting - I am coding along someone elses solution and I can see that my loss is a lot higher even though we are using the same data and augmentations - this training already reached a minimum after the 10th epoch:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"Epoch 10: val_loss did not improve from 0.39576\n7995/7995 [==============================] - 410s 51ms/step - loss: 0.5612 - accuracy: 0.8289 - val_loss: 0.4257 - val_accuracy: 0.8085\n"})}),"\n",(0,i.jsx)(n.h3,{id:"custom-model",children:"Custom Model"}),"\n",(0,i.jsx)(n.p,{children:"I now limit the number of epochs for the custom model to 10:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pipenv run python ./train_CustomModel_32_conv_20k.ipynb\n"})}),"\n",(0,i.jsx)(n.p,{children:"And given that most of the epochs before the last still showed an improvement - i assume that this is not yet the minimum:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"Epoch 10/10\n6243/6244 [============================>.] - ETA: 0s - loss: 0.5423 - accuracy: 0.8398  \nEpoch 10: val_loss did not improve from 0.42444\n6244/6244 [==============================] - 223s 36ms/step - loss: 0.5423 - accuracy: 0.8398 - val_loss: 0.4738 - val_accuracy: 0.8039\n"})})]})}function h(e={}){const{wrapper:n}={...(0,a.ah)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},603905:(e,n,t)=>{t.d(n,{ah:()=>l});var i=t(667294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,i)}return t}function s(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function r(e,n){if(null==e)return{};var t,i,a=function(e,n){if(null==e)return{};var t,i,a={},o=Object.keys(e);for(i=0;i<o.length;i++)t=o[i],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(i=0;i<o.length;i++)t=o[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var c=i.createContext({}),l=function(e){var n=i.useContext(c),t=n;return e&&(t="function"==typeof e?e(n):s(s({},n),e)),t},d={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},h=i.forwardRef((function(e,n){var t=e.components,a=e.mdxType,o=e.originalType,c=e.parentName,h=r(e,["components","mdxType","originalType","parentName"]),m=l(t),p=a,g=m["".concat(c,".").concat(p)]||m[p]||d[p]||o;return t?i.createElement(g,s(s({ref:n},h),{},{components:t})):i.createElement(g,s({ref:n},h))}));h.displayName="MDXCreateElement"},498683:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-918471126c0472aad97358a725e1a399.jpg"}}]);