"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[27953],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>h});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},m=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),d=p(n),h=o,u=d["".concat(l,".").concat(h)]||d[h]||c[h]||i;return n?a.createElement(u,r(r({ref:t},m),{},{components:n})):a.createElement(u,r({ref:t},m))}));function h(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,r=new Array(i);r[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:o,r[1]=s;for(var p=2;p<i;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},59550:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>c,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var a=n(87462),o=(n(67294),n(3905));const i={sidebar_position:4340,slug:"2023-07-21",title:"Introduction to Caffe2",authors:"mpolinowski",tags:["Python","Machine Learning"],description:"Deep Learning Framework with Python for flexibility and C++ for speed."},r=void 0,s={unversionedId:"IoT-and-Machine-Learning/ML/2023-07-21-introduction-to-pytorch-caffe2/index",id:"IoT-and-Machine-Learning/ML/2023-07-21-introduction-to-pytorch-caffe2/index",title:"Introduction to Caffe2",description:"Deep Learning Framework with Python for flexibility and C++ for speed.",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-07-21-introduction-to-pytorch-caffe2/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-07-21-introduction-to-pytorch-caffe2",slug:"/IoT-and-Machine-Learning/ML/2023-07-21-introduction-to-pytorch-caffe2/2023-07-21",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-07-21-introduction-to-pytorch-caffe2/2023-07-21",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-07-21-introduction-to-pytorch-caffe2/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"}],version:"current",sidebarPosition:4340,frontMatter:{sidebar_position:4340,slug:"2023-07-21",title:"Introduction to Caffe2",authors:"mpolinowski",tags:["Python","Machine Learning"],description:"Deep Learning Framework with Python for flexibility and C++ for speed."},sidebar:"tutorialSidebar",previous:{title:"Machine Learning",permalink:"/docs/category/machine-learning"},next:{title:"SQL in Data Science - Machine Learning",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-07-02-sql-in-data-science-ml/2023-07-02"}},l={},p=[{value:"Setup with Docker",id:"setup-with-docker",level:2},{value:"Testing Installation",id:"testing-installation",level:3},{value:"Caffe Tutorial",id:"caffe-tutorial",level:2},{value:"Caffe2 Basic Concepts - Operators &amp; Nets",id:"caffe2-basic-concepts---operators--nets",level:3},{value:"Workspaces",id:"workspaces",level:4},{value:"Operators",id:"operators",level:4},{value:"Nets",id:"nets",level:4},{value:"Loading Pre-Trained Models",id:"loading-pre-trained-models",level:2},{value:"Description",id:"description",level:3},{value:"Code",id:"code",level:3},{value:"Inputs",id:"inputs",level:4},{value:"Setup paths",id:"setup-paths",level:4},{value:"Image Preprocessing",id:"image-preprocessing",level:4},{value:"Prepare the CNN and run the net!",id:"prepare-the-cnn-and-run-the-net",level:3},{value:"Process Results",id:"process-results",level:4},{value:"Feeding Larger Batches",id:"feeding-larger-batches",level:4},{value:"Loading Datasets",id:"loading-datasets",level:2},{value:"Image Loading and Preprocessing",id:"image-loading-and-preprocessing",level:2},{value:"Caffe Uses BGR Order",id:"caffe-uses-bgr-order",level:3},{value:"Caffe Prefers CHW Order",id:"caffe-prefers-chw-order",level:3},{value:"Rotation and Mirroring",id:"rotation-and-mirroring",level:3},{value:"Sizing",id:"sizing",level:3},{value:"Rescaling",id:"rescaling",level:3},{value:"Cropping",id:"cropping",level:3},{value:"Upscaling",id:"upscaling",level:3},{value:"Batch Processing",id:"batch-processing",level:3}],m={toc:p};function c(e){let{components:t,...i}=e;return(0,o.kt)("wrapper",(0,a.Z)({},m,i,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Guangzhou, China",src:n(76369).Z,width:"1500",height:"581"})),(0,o.kt)("h1",{id:"introduction-to-caffe2"},"Introduction to Caffe2"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#introduction-to-caffe2"},"Introduction to Caffe2"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#setup-with-docker"},"Setup with Docker"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#testing-installation"},"Testing Installation")))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#caffe-tutorial"},"Caffe Tutorial"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#caffe2-basic-concepts---operators--nets"},"Caffe2 Basic Concepts - Operators \\& Nets"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#workspaces"},"Workspaces")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#operators"},"Operators")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#nets"},"Nets")))))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#loading-pre-trained-models"},"Loading Pre-Trained Models"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#description"},"Description")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#code"},"Code"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#inputs"},"Inputs")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#setup-paths"},"Setup paths")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#image-preprocessing"},"Image Preprocessing")))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#prepare-the-cnn-and-run-the-net"},"Prepare the CNN and run the net!"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#process-results"},"Process Results")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#feeding-larger-batches"},"Feeding Larger Batches")))))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#loading-datasets"},"Loading Datasets")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#image-loading-and-preprocessing"},"Image Loading and Preprocessing"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#caffe-uses-bgr-order"},"Caffe Uses BGR Order")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#caffe-prefers-chw-order"},"Caffe Prefers CHW Order")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#rotation-and-mirroring"},"Rotation and Mirroring")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#sizing"},"Sizing")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#rescaling"},"Rescaling")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#cropping"},"Cropping")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#upscaling"},"Upscaling")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#batch-processing"},"Batch Processing"))))))),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/morning-caffe2"},"Github Repository")),(0,o.kt)("h2",{id:"setup-with-docker"},"Setup with Docker"),(0,o.kt)("p",null,"There ",(0,o.kt)("a",{parentName:"p",href:"https://hub.docker.com/r/caffe2ai/caffe2"},"several images available")," with and without GPU support - the image tagged ",(0,o.kt)("inlineCode",{parentName:"p"},"latest")," comes with everything included:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"docker pull caffe2ai/caffe2:latest\n\ndocker run -it -v /opt/caffe:/home -p 8888:8888 caffe2ai/caffe2:latest jupyter notebook --no-browser --ip=0.0.0.0 --port=8888 --allow-root /home\n")),(0,o.kt)("p",null,"Make sure that ",(0,o.kt)("inlineCode",{parentName:"p"},"/opt/caffe")," exists and can be written into by the Docker user."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"Copy/paste this URL into your browser when you connect for the first time,\nto login with a token: http://0.0.0.0:8888/?token=9346cde0b9a37cda784d193e4e03a18c760847ace645f6cb\n")),(0,o.kt)("p",null,"You can now access the ",(0,o.kt)("strong",{parentName:"p"},"Jupyter Notebook")," on your servers IP address on port ",(0,o.kt)("inlineCode",{parentName:"p"},"8888")," with the generated token above."),(0,o.kt)("h3",{id:"testing-installation"},"Testing Installation"),(0,o.kt)("p",null,"Create a new notebook and verify that that Caffe2 is up and running:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},'from caffe2.python import workspace\nimport numpy as np\nprint ("Creating random data")\ndata = np.random.rand(3, 2)\nprint(data)\nprint ("Adding data to workspace ...")\nworkspace.FeedBlob("mydata", data)\nprint ("Retrieving data from workspace")\nmydata = workspace.FetchBlob("mydata")\nprint(mydata) \n')),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Introduction to Caffe2",src:n(72690).Z,width:"1057",height:"702"})),(0,o.kt)("p",null,"It works!"),(0,o.kt)("h2",{id:"caffe-tutorial"},"Caffe Tutorial"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"cd /opt/caffe\ngit clone --recursive https://github.com/caffe2/tutorials caffe2_tutorials\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Introduction to Caffe2",src:n(46527).Z,width:"999",height:"367"})),(0,o.kt)("h3",{id:"caffe2-basic-concepts---operators--nets"},"Caffe2 Basic Concepts - Operators & Nets"),(0,o.kt)("p",null,"In this tutorial we will go through a set of Caffe2 basics: the basic concepts including how operators and nets are being written."),(0,o.kt)("p",null,"First, let's import Caffe2. ",(0,o.kt)("inlineCode",{parentName:"p"},"core")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"workspace")," are usually the two that you need most. If you want to manipulate protocol buffers generated by Caffe2, you probably also want to import ",(0,o.kt)("inlineCode",{parentName:"p"},"caffe2_pb2")," from ",(0,o.kt)("inlineCode",{parentName:"p"},"caffe2.proto"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\n# We'll also import a few standard python libraries\nfrom matplotlib import pyplot\nimport numpy as np\nimport time\n\n# These are the droids you are looking for.\nfrom caffe2.python import core, workspace\nfrom caffe2.proto import caffe2_pb2\n\n# Let's show all plots inline.\n%matplotlib inline\n")),(0,o.kt)("p",null,"You might see a warning saying that caffe2 does not have GPU support. That means you are running a CPU-only build. Don't be alarmed - anything CPU is still runnable without a problem."),(0,o.kt)("h4",{id:"workspaces"},"Workspaces"),(0,o.kt)("p",null,"Let's cover workspaces first, where all the data resides."),(0,o.kt)("p",null,"Similar to Matlab, the Caffe2 workspace consists of blobs you create and store in memory. For now, consider a blob to be a N-dimensional Tensor similar to numpy's ndarray, but contiguous. Down the road, we will show you that a blob is actually a typed pointer that can store any type of C++ objects, but Tensor is the most common type stored in a blob. Let's show what the interface looks like."),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"Blobs()")," prints out all existing blobs in the workspace.\n",(0,o.kt)("inlineCode",{parentName:"p"},"HasBlob()")," queries if a blob exists in the workspace. As of now, we don't have any."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'print("Current blobs in the workspace: {}".format(workspace.Blobs()))\nprint("Workspace has blob \'X\'? {}".format(workspace.HasBlob("X")))\n')),(0,o.kt)("p",null,"We can feed blobs into the workspace using ",(0,o.kt)("inlineCode",{parentName:"p"},"FeedBlob()"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'X = np.random.randn(2, 3).astype(np.float32)\nprint("Generated X from numpy:\\n{}".format(X))\nworkspace.FeedBlob("X", X)\n')),(0,o.kt)("p",null,"Now, let's take a look at what blobs are in the workspace."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'print("Current blobs in the workspace: {}".format(workspace.Blobs()))\nprint("Workspace has blob \'X\'? {}".format(workspace.HasBlob("X")))\nprint("Fetched X:\\n{}".format(workspace.FetchBlob("X")))\n')),(0,o.kt)("p",null,"Let's verify that the arrays are equal."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'np.testing.assert_array_equal(X, workspace.FetchBlob("X"))\n')),(0,o.kt)("p",null,"Note that if you try to access a blob that does not exist, an error will be thrown:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'try:\n    workspace.FetchBlob("invincible_pink_unicorn")\nexcept RuntimeError as err:\n    print(err)\n')),(0,o.kt)("p",null,"One thing that you might not use immediately: you can have multiple workspaces in Python using different names, and switch between them. Blobs in different workspaces are separate from each other. You can query the current workspace using ",(0,o.kt)("inlineCode",{parentName:"p"},"CurrentWorkspace"),". Let's try switching the workspace by name (gutentag) and creating a new one if it doesn't exist."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'print("Current workspace: {}".format(workspace.CurrentWorkspace()))\nprint("Current blobs in the workspace: {}".format(workspace.Blobs()))\n\n# Switch the workspace. The second argument "True" means creating \n# the workspace if it is missing.\nworkspace.SwitchWorkspace("gutentag", True)\n\n# Let\'s print the current workspace. Note that there is nothing in the\n# workspace yet.\nprint("Current workspace: {}".format(workspace.CurrentWorkspace()))\nprint("Current blobs in the workspace: {}".format(workspace.Blobs()))\n')),(0,o.kt)("p",null,"Let's switch back to the default workspace."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'workspace.SwitchWorkspace("default")\nprint("Current workspace: {}".format(workspace.CurrentWorkspace()))\nprint("Current blobs in the workspace: {}".format(workspace.Blobs()))\n')),(0,o.kt)("p",null,"Finally, ",(0,o.kt)("inlineCode",{parentName:"p"},"ResetWorkspace()")," clears anything that is in the current workspace."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'workspace.ResetWorkspace()\nprint("Current blobs in the workspace after reset: {}".format(workspace.Blobs()))\n')),(0,o.kt)("h4",{id:"operators"},"Operators"),(0,o.kt)("p",null,"Operators in Caffe2 are kind of like functions. From the C++ side, they all derive from a common interface, and are registered by type, so that we can call different operators during runtime. The interface of operators is defined in ",(0,o.kt)("inlineCode",{parentName:"p"},"caffe2/proto/caffe2.proto"),". Basically, it takes in a bunch of inputs, and produces a bunch of outputs."),(0,o.kt)("p",null,'Remember, when we say "create an operator" in Caffe2 Python, nothing gets run yet. All it does is create the protocol buffer that specifies what the operator should be. At a later time it will be sent to the C++ backend for execution. If you are not familiar with protobuf, it is a json-like serialization tool for structured data. Find more about protocol buffers ',(0,o.kt)("a",{parentName:"p",href:"https://developers.google.com/protocol-buffers/"},"here"),"."),(0,o.kt)("p",null,"Let's see an actual example."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# Create an operator.\nop = core.CreateOperator(\n    "Relu", # The type of operator that we want to run\n    ["X"], # A list of input blobs by their names\n    ["Y"], # A list of output blobs by their names\n)\n# and we are done!\n')),(0,o.kt)("p",null,"As we mentioned, the created op is actually a protobuf object. Let's show the content."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'print("Type of the created op is: {}".format(type(op)))\nprint("Content:\\n")\nprint(str(op))\n')),(0,o.kt)("p",null,"Ok, let's run the operator. We first feed the input X to the workspace.\nThen the simplest way to run an operator is to do ",(0,o.kt)("inlineCode",{parentName:"p"},"workspace.RunOperatorOnce(operator)")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'workspace.FeedBlob("X", np.random.randn(2, 3).astype(np.float32))\nworkspace.RunOperatorOnce(op)\n')),(0,o.kt)("p",null,"After execution, let's see if the operator is doing the right thing."),(0,o.kt)("p",null,"In this case, the operator is a common activation function used in neural networks, called ","[ReLU]","(",(0,o.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"},"https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"),", or Rectified Linear Unit activation. ReLU activation helps to add necessary non-linear characteristics to the neural network classifier, and is defined as:"),(0,o.kt)("p",null,"$$ReLU(x) = max(0, x)$$"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'print("Current blobs in the workspace: {}\\n".format(workspace.Blobs()))\nprint("X:\\n{}\\n".format(workspace.FetchBlob("X")))\nprint("Y:\\n{}\\n".format(workspace.FetchBlob("Y")))\nprint("Expected:\\n{}\\n".format(np.maximum(workspace.FetchBlob("X"), 0)))\n')),(0,o.kt)("p",null,"This is working if your Expected output matches your Y output in this example."),(0,o.kt)("p",null,"Operators also take optional arguments if needed. They are specified as key-value pairs. Let's take a look at one simple example, which takes a tensor and fills it with Gaussian random variables."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'op = core.CreateOperator(\n    "GaussianFill",\n    [], # GaussianFill does not need any parameters.\n    ["Z"],\n    shape=[100, 100], # shape argument as a list of ints.\n    mean=1.0,  # mean as a single float\n    std=1.0, # std as a single float\n)\nprint("Content of op:\\n")\nprint(str(op))\n')),(0,o.kt)("p",null,"Let's run it and see if things are as intended."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'workspace.RunOperatorOnce(op)\ntemp = workspace.FetchBlob("Z")\npyplot.hist(temp.flatten(), bins=50)\npyplot.title("Distribution of Z")\n')),(0,o.kt)("p",null,"If you see a bell shaped curve then it worked!"),(0,o.kt)("h4",{id:"nets"},"Nets"),(0,o.kt)("p",null,"Nets are essentially computation graphs. We keep the name ",(0,o.kt)("inlineCode",{parentName:"p"},"Net")," for backward consistency (and also to pay tribute to neural nets). A Net is composed of multiple operators just like a program written as a sequence of commands. Let's take a look."),(0,o.kt)("p",null,"When we talk about nets, we will also talk about BlobReference, which is an object that wraps around a string so we can do easy chaining of operators."),(0,o.kt)("p",null,"Let's create a network that is essentially the equivalent of the following python math:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"X = np.random.randn(2, 3)\nW = np.random.randn(5, 3)\nb = np.ones(5)\nY = X * W^T + b\n")),(0,o.kt)("p",null,"We'll show the progress step by step. Caffe2's ",(0,o.kt)("inlineCode",{parentName:"p"},"core.Net")," is a wrapper class around a NetDef protocol buffer."),(0,o.kt)("p",null,"When creating a network, its underlying protocol buffer is essentially empty other than the network name. Let's create the net and then show the proto content."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'net = core.Net("my_first_net")\nprint("Current network proto:\\n\\n{}".format(net.Proto()))\n')),(0,o.kt)("p",null,"Let's create a blob called X, and use GaussianFill to fill it with some random data."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'X = net.GaussianFill([], ["X"], mean=0.0, std=1.0, shape=[2, 3], run_once=0)\nprint("New network proto:\\n\\n{}".format(net.Proto()))\n')),(0,o.kt)("p",null,"You might have observed a few differences from the earlier ",(0,o.kt)("inlineCode",{parentName:"p"},"core.CreateOperator")," call. Basically, when using a net, you can directly create an operator ",(0,o.kt)("em",{parentName:"p"},"and")," add it to the net at the same time by calling ",(0,o.kt)("inlineCode",{parentName:"p"},"net.SomeOp")," where SomeOp is a registered type string of an operator. This gets translated to:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'op = core.CreateOperator("SomeOp", ...)\nnet.Proto().op.append(op)\n')),(0,o.kt)("p",null,"Also, you might be wondering what X is. X is a ",(0,o.kt)("inlineCode",{parentName:"p"},"BlobReference")," which records two things:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"The blob's name, which is accessed with ",(0,o.kt)("inlineCode",{parentName:"p"},"str(X)"))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"The net it got created from, which is recorded by the internal variable ",(0,o.kt)("inlineCode",{parentName:"p"},"_from_net")))),(0,o.kt)("p",null,"Let's verify it. Also, remember, we are not actually running anything yet, so X contains nothing but a symbol. Don't expect to get any numerical values out of it right now :)"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'print("Type of X is: {}".format(type(X)))\nprint("The blob name is: {}".format(str(X)))\n')),(0,o.kt)("p",null,"Let's continue to create W and b."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'W = net.GaussianFill([], ["W"], mean=0.0, std=1.0, shape=[5, 3], run_once=0)\nb = net.ConstantFill([], ["b"], shape=[5,], value=1.0, run_once=0)\n')),(0,o.kt)("p",null,"Now, one simple code sugar: since the BlobReference objects know what net it is generated from, in addition to creating operators from net, you can also create operators from BlobReferences. Let's create the FC operator in this way."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'Y = X.FC([W, b], ["Y"])\n')),(0,o.kt)("p",null,"Under the hood, ",(0,o.kt)("inlineCode",{parentName:"p"},"X.FC(...)")," simply delegates to ",(0,o.kt)("inlineCode",{parentName:"p"},"net.FC")," by inserting ",(0,o.kt)("inlineCode",{parentName:"p"},"X")," as the first input of the corresponding operator, so what we did above is equivalent to"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'Y = net.FC([X, W, b], ["Y"])\n')),(0,o.kt)("p",null,"Let's take a look at the current network."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'print("Current network proto:\\n\\n{}".format(net.Proto()))\n')),(0,o.kt)("p",null,"Too verbose huh? Let's try to visualize it as a graph. Caffe2 ships with a very minimal graph visualization tool for this purpose."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from caffe2.python import net_drawer\nfrom IPython import display\ngraph = net_drawer.GetPydotGraph(net, rankdir="LR")\ndisplay.Image(graph.create_png(), width=800)\n')),(0,o.kt)("p",null,"So we have defined a Net, but nothing has been executed yet. Remember that the net above is essentially a protobuf that holds the definition of the network. When we actually run the network, what happens under the hood is:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A C++ net object is instantiated from the protobuf"),(0,o.kt)("li",{parentName:"ul"},"The instantiated net's Run() function is called")),(0,o.kt)("p",null,"Before we do anything, we should clear any earlier workspace variables with ",(0,o.kt)("inlineCode",{parentName:"p"},"ResetWorkspace()"),"."),(0,o.kt)("p",null,"Then there are two ways to run a net from Python. We will do the first option in the example below."),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Call ",(0,o.kt)("inlineCode",{parentName:"li"},"workspace.RunNetOnce()"),", which instantiates, runs and immediately destructs the network "),(0,o.kt)("li",{parentName:"ol"},"Call ",(0,o.kt)("inlineCode",{parentName:"li"},"workspace.CreateNet()")," to create the C++ net object owned by the workspace, then call ",(0,o.kt)("inlineCode",{parentName:"li"},"workspace.RunNet()"),", passing the name of the network to it")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'workspace.ResetWorkspace()\nprint("Current blobs in the workspace: {}".format(workspace.Blobs()))\nworkspace.RunNetOnce(net)\nprint("Blobs in the workspace after execution: {}".format(workspace.Blobs()))\n# Let\'s dump the contents of the blobs\nfor name in workspace.Blobs():\n    print("{}:\\n{}".format(name, workspace.FetchBlob(name)))\n')),(0,o.kt)("p",null,"Now let's try the second way to create the net, and run it. First, clear the variables with ",(0,o.kt)("inlineCode",{parentName:"p"},"ResetWorkspace()"),". Then create the net with the workspace's ",(0,o.kt)("inlineCode",{parentName:"p"},"net")," object that we created earlier using ",(0,o.kt)("inlineCode",{parentName:"p"},"CreateNet(net_object)"),". Finally, run the net with ",(0,o.kt)("inlineCode",{parentName:"p"},"RunNet(net_name)"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'workspace.ResetWorkspace()\nprint("Current blobs in the workspace: {}".format(workspace.Blobs()))\nworkspace.CreateNet(net)\nworkspace.RunNet(net.Proto().name)\nprint("Blobs in the workspace after execution: {}".format(workspace.Blobs()))\nfor name in workspace.Blobs():\n    print("{}:\\n{}".format(name, workspace.FetchBlob(name)))\n')),(0,o.kt)("p",null,"There are a few differences between ",(0,o.kt)("inlineCode",{parentName:"p"},"RunNetOnce")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"RunNet"),", but the main difference is the computational overhead. Since ",(0,o.kt)("inlineCode",{parentName:"p"},"RunNetOnce")," involves serializing the protobuf to pass between Python and C and instantiating the network, it may take longer to run. Let's run a test and see what the time overhead is."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# It seems that %timeit magic does not work well with\n# C++ extensions so we'll basically do for loops\nstart = time.time()\nfor i in range(1000):\n    workspace.RunNetOnce(net)\nend = time.time()\nprint('Run time per RunNetOnce: {}'.format((end - start) / 1000))\n\nstart = time.time()\nfor i in range(1000):\n    workspace.RunNet(net.Proto().name)\nend = time.time()\nprint('Run time per RunNet: {}'.format((end - start) / 1000))\n")),(0,o.kt)("p",null,"Congratulations, you now know the many of the key components of the Caffe2 Python API! Ready for more Caffe2? Check out the rest of the tutorials for a variety of interesting use-cases!"),(0,o.kt)("h2",{id:"loading-pre-trained-models"},"Loading Pre-Trained Models"),(0,o.kt)("h3",{id:"description"},"Description"),(0,o.kt)("p",null,"In this tutorial, we will use the pre-trained ",(0,o.kt)("inlineCode",{parentName:"p"},"squeezenet")," model from the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/caffe2/caffe2/wiki/Model-Zoo"},"ModelZoo")," to classify our own images. As input, we will provide the path (or URL) to an image we want to classify. It will also be helpful to know the ",(0,o.kt)("a",{parentName:"p",href:"https://gist.githubusercontent.com/aaronmarkham/cd3a6b6ac071eca6f7b4a6e40e6038aa/raw/9edb4038a37da6b5a44c3b5bc52e448ff09bfe5b/alexnet_codes"},"ImageNet object code"),' for the image so we can verify our results. The \'object code\' is nothing more than the integer label for the class used during training, for example "985" is the code for the class "daisy". Note, although we are using squeezenet here, this tutorial serves as a somewhat universal method for running inference on pretrained models.'),(0,o.kt)("p",null,"Note, assuming the last layer of the network is a softmax layer, the results come back as a multidimensional array of probabilities with length equal to the number of classes that the model was trained on. The probabilities may be indexed by the object code (integer type), so if you know the object code you can index the results array at that index to view the network's confidence that the input image is of that class."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Model Download Options")),(0,o.kt)("p",null,"Although we will use ",(0,o.kt)("inlineCode",{parentName:"p"},"squeezenet")," here, you can check out the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/caffe2/caffe2/wiki/Model-Zoo"},"Model Zoo for pre-trained models")," to browse/download a variety of pretrained models, or you can use Caffe2's ",(0,o.kt)("inlineCode",{parentName:"p"},"caffe2.python.models.download")," module to easily acquire pre-trained models from ",(0,o.kt)("a",{parentName:"p",href:"http://github.com/caffe2/models"},"Github caffe2/models"),". "),(0,o.kt)("p",null,"For our purposes, we will use the ",(0,o.kt)("inlineCode",{parentName:"p"},"models.download")," module to download ",(0,o.kt)("inlineCode",{parentName:"p"},"squeezenet")," into the ",(0,o.kt)("inlineCode",{parentName:"p"},"/caffe2/python/models")," folder of our local Caffe2 installation with the following command:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"python -m caffe2.python.models.download -i squeezenet\n")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Update"),": The repository has been archived - I am manually downloading this into the container ",(0,o.kt)("inlineCode",{parentName:"p"},"/caffe2/caffe2/python/models/squeezenet"),":"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"wget https://github.com/facebookarchive/models/raw/master/squeezenet/init_net.pb")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"wget https://github.com/facebookarchive/models/raw/master/squeezenet/predict_net.pb")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"wget https://github.com/facebookarchive/models/raw/master/squeezenet/value_info.json"))),(0,o.kt)("p",null,"If the above download worked then you should have a directory named squeezenet in your ",(0,o.kt)("inlineCode",{parentName:"p"},"/caffe2/python/models")," folder that contains ",(0,o.kt)("inlineCode",{parentName:"p"},"init_net.pb")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"predict_net.pb"),". Note, if you do not use the ",(0,o.kt)("inlineCode",{parentName:"p"},"-i")," flag, the model will be downloaded to your CWD, however it will still be a directory named squeezenet containing two protobuf files. Alternatively, if you wish to download all of the models, you can clone the entire repo using: "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"git clone https://github.com/caffe2/models\n")),(0,o.kt)("h3",{id:"code"},"Code"),(0,o.kt)("p",null,"Before we start, lets take care of the required imports."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n%matplotlib inline\nfrom caffe2.proto import caffe2_pb2\nimport numpy as np\nimport skimage.io\nimport skimage.transform\nfrom matplotlib import pyplot\nimport os\nfrom caffe2.python import core, workspace, models\nimport urllib2\nimport operator\nprint("Required modules imported.")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"!mkdir -p /caffe2/caffe2/python/models/squeezenet\n\n!wget https://github.com/facebookarchive/models/raw/master/squeezenet/init_net.pb -P /caffe2/caffe2/python/models/squeezenet\n!wget https://github.com/facebookarchive/models/raw/master/squeezenet/predict_net.pb -P /caffe2/caffe2/python/models/squeezenet\n!wget https://github.com/facebookarchive/models/raw/master/squeezenet/value_info.json -P /caffe2/caffe2/python/models/squeezenet\n\n!ls -la /caffe2/caffe2/python/models/squeezenet\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"total 6052\ndrwxr-xr-x 1 root root      80 Jul 24 06:17 .\ndrwxr-xr-x 1 root root      20 Jul 24 06:13 ..\n-rw-r--r-- 1 root root 6181001 Jul 24 06:15 init_net.pb\n-rw-r--r-- 1 root root    6175 Jul 24 06:15 predict_net.pb\n-rw-r--r-- 1 root root      32 Jul 24 06:17 value_info.json\n")),(0,o.kt)("h4",{id:"inputs"},"Inputs"),(0,o.kt)("p",null,"Here, we will specify the inputs to be used for this run, including the input image, the model location, the mean file (optional), the required size of the image, and the location of the label mapping file."),(0,o.kt)("p",null,"I downloaded an image ",(0,o.kt)("inlineCode",{parentName:"p"},"flower.jpg")," and placed it next to the Jupyter Notebook in ",(0,o.kt)("inlineCode",{parentName:"p"},"/opt/caffe/caffe2_tutorials"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# Configuration --- Change to your setup and preferences!\n# This directory should contain the models downloaded from the model zoo. To run this \n#   tutorial, make sure there is a 'squeezenet' directory at this location that \n#   contains both the 'init_net.pb' and 'predict_net.pb'\nCAFFE_MODELS = \"/caffe2/caffe2/python/models\"\n\n# Some sample images you can try, or use any URL to a regular image.\nIMAGE_LOCATION = \"flower.jpg\"\n\n# What model are we using?\n#    Format below is the model's: <folder, INIT_NET, predict_net, mean, input image size>\n#    You can switch 'squeezenet' out with 'bvlc_alexnet', 'bvlc_googlenet' or others that you have downloaded\nMODEL = 'squeezenet', 'init_net.pb', 'predict_net.pb', 'ilsvrc_2012_mean.npy', 227\n\n# labels - these help decypher the output and source from a list from ImageNet's object labels \n#    to provide an result like \"tabby cat\" or \"lemon\" depending on what's in the picture \n#   you submit to the CNN.\nlabels =  \"https://gist.githubusercontent.com/aaronmarkham/cd3a6b6ac071eca6f7b4a6e40e6038aa/raw/9edb4038a37da6b5a44c3b5bc52e448ff09bfe5b/alexnet_codes\"\nprint(\"Config set!\")\n")),(0,o.kt)("h4",{id:"setup-paths"},"Setup paths"),(0,o.kt)("p",null,"With the configs set, we can now load the mean file (if it exists), as well as the predict net and the init net."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# set paths and variables from model choice and prep image\nCAFFE_MODELS = os.path.expanduser(CAFFE_MODELS)\n\n# mean can be 128 or custom based on the model\n# gives better results to remove the colors found in all of the training images\nMEAN_FILE = os.path.join(CAFFE_MODELS, MODEL[0], MODEL[3])\nif not os.path.exists(MEAN_FILE):\n    print("No mean file found!")\n    mean = 128\nelse:\n    print ("Mean file found!")\n    mean = np.load(MEAN_FILE).mean(1).mean(1)\n    mean = mean[:, np.newaxis, np.newaxis]\nprint("mean was set to: ", mean)\n\n# some models were trained with different image sizes, this helps you calibrate your image\nINPUT_IMAGE_SIZE = MODEL[4]\n\n# make sure all of the files are around...\nINIT_NET = os.path.join(CAFFE_MODELS, MODEL[0], MODEL[1])\nPREDICT_NET = os.path.join(CAFFE_MODELS, MODEL[0], MODEL[2])\n\n# Check to see if the files exist\nif not os.path.exists(INIT_NET):\n    print("WARNING: " + INIT_NET + " not found!")\nelse:\n    if not os.path.exists(PREDICT_NET):\n        print("WARNING: " + PREDICT_NET + " not found!")\n    else:\n        print("All needed files found!")\n        \n')),(0,o.kt)("h4",{id:"image-preprocessing"},"Image Preprocessing"),(0,o.kt)("p",null,"Now that we have our inputs specified and verified the existance of the input network, we can load the image and pre-processing the image for ingestion into a Caffe2 convolutional neural network! This is a very important step as the trained CNN requires a specifically sized input image whose values are from a particular distribution."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# Function to crop the center cropX x cropY pixels from the input image\ndef crop_center(img,cropx,cropy):\n    y,x,c = img.shape\n    startx = x//2-(cropx//2)\n    starty = y//2-(cropy//2)    \n    return img[starty:starty+cropy,startx:startx+cropx]\n\n# Function to rescale the input image to the desired height and/or width. This function will preserve\n#   the aspect ratio of the original image while making the image the correct scale so we can retrieve\n#   a good center crop. This function is best used with center crop to resize any size input images into\n#   specific sized images that our model can use.\ndef rescale(img, input_height, input_width):\n    # Get original aspect ratio\n    aspect = img.shape[1]/float(img.shape[0])\n    if(aspect>1):\n        # landscape orientation - wide image\n        res = int(aspect * input_height)\n        imgScaled = skimage.transform.resize(img, (input_width, res))\n    if(aspect<1):\n        # portrait orientation - tall image\n        res = int(input_width/aspect)\n        imgScaled = skimage.transform.resize(img, (res, input_height))\n    if(aspect == 1):\n        imgScaled = skimage.transform.resize(img, (input_width, input_height))\n    return imgScaled\n\n# Load the image as a 32-bit float\n#    Note: skimage.io.imread returns a HWC ordered RGB image of some size\nimg = skimage.img_as_float(skimage.io.imread(IMAGE_LOCATION)).astype(np.float32)\nprint(\"Original Image Shape: \" , img.shape)\n\n# Rescale the image to comply with our desired input size. This will not make the image 227x227\n#    but it will make either the height or width 227 so we can get the ideal center crop.\nimg = rescale(img, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE)\nprint(\"Image Shape after rescaling: \" , img.shape)\npyplot.figure()\npyplot.imshow(img)\npyplot.title('Rescaled image')\n\n# Crop the center 227x227 pixels of the image so we can feed it to our model\nimg = crop_center(img, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE)\nprint(\"Image Shape after cropping: \" , img.shape)\npyplot.figure()\npyplot.imshow(img)\npyplot.title('Center Cropped')\n\n# switch to CHW (HWC --\x3e CHW)\nimg = img.swapaxes(1, 2).swapaxes(0, 1)\nprint(\"CHW Image Shape: \" , img.shape)\n\npyplot.figure()\nfor i in range(3):\n    # For some reason, pyplot subplot follows Matlab's indexing\n    # convention (starting with 1). Well, we'll just follow it...\n    pyplot.subplot(1, 3, i+1)\n    pyplot.imshow(img[i])\n    pyplot.axis('off')\n    pyplot.title('RGB channel %d' % (i+1))\n\n# switch to BGR (RGB --\x3e BGR)\nimg = img[(2, 1, 0), :, :]\n\n# remove mean for better results\nimg = img * 255 - mean\n\n# add batch size axis which completes the formation of the NCHW shaped input that we want\nimg = img[np.newaxis, :, :, :].astype(np.float32)\n\nprint(\"NCHW image (ready to be used as input): \", img.shape)\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Introduction to Caffe2",src:n(92001).Z,width:"382",height:"672"})),(0,o.kt)("h3",{id:"prepare-the-cnn-and-run-the-net"},"Prepare the CNN and run the net!"),(0,o.kt)("p",null,"Now that the image is ready to be ingested by the CNN, let's open the protobufs, load them into the workspace, and run the net. "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# Read the contents of the input protobufs into local variables\nwith open(INIT_NET, "rb") as f:\n    init_net = f.read()\nwith open(PREDICT_NET, "rb") as f:\n    predict_net = f.read()\n\n# Initialize the predictor from the input protobufs\np = workspace.Predictor(init_net, predict_net)\n\n# Run the net and return prediction\nNCHW_batch = np.zeros((1,3,227,227))\nNCHW_batch[0] = img\nresults = p.run([NCHW_batch.astype(np.float32)])\n# Turn it into something we can play with and examine which is in a multi-dimensional array\nresults = np.asarray(results)\nprint("results shape: ", results.shape)\n\n# Quick way to get the top-1 prediction result\n# Squeeze out the unnecessary axis. This returns a 1-D array of length 1000\npreds = np.squeeze(results)\n# Get the prediction and the confidence by finding the maximum value and index of maximum value in preds array\ncurr_pred, curr_conf = max(enumerate(preds), key=operator.itemgetter(1))\nprint("Prediction: ", curr_pred)\nprint("Confidence: ", curr_conf)\n')),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Prediction"),":  ",(0,o.kt)("inlineCode",{parentName:"li"},"723")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Confidence"),":  ",(0,o.kt)("inlineCode",{parentName:"li"},"0.910284"))),(0,o.kt)("h4",{id:"process-results"},"Process Results"),(0,o.kt)("p",null,"Recall ImageNet is a 1000 class dataset and observe that it is no coincidence that the third axis of results is length 1000. This axis is holding the probability for each category in the pre-trained model. So when you look at the results array at a specific index, the number can be interpreted as the probability that the input belongs to the class corresponding to that index. Now that we have run the predictor and collected the results, we can interpret them by matching them to their corresponding english labels."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# the rest of this is digging through the results \nresults = np.delete(results, 1)\nindex = 0\nhighest = 0\narr = np.empty((0,2), dtype=object)\narr[:,0] = int(10)\narr[:,1:] = float(10)\nfor i, r in enumerate(results):\n    # imagenet index begins with 1!\n    i=i+1\n    arr = np.append(arr, np.array([[i,r]]), axis=0)\n    if (r > highest):\n        highest = r\n        index = i \n\n# top N results\nN = 5\ntopN = sorted(arr, key=lambda x: x[1], reverse=True)[:N]\nprint("Raw top {} results: {}".format(N,topN))\n\n# Isolate the indexes of the top-N most likely classes\ntopN_inds = [int(x[0]) for x in topN]\nprint("Top {} classes in order: {}".format(N,topN_inds))\n\n# Now we can grab the code list and create a class Look Up Table\nresponse = urllib2.urlopen(labels)\nclass_LUT = []\nfor line in response:\n    code, result = line.partition(":")[::2]\n    code = code.strip()\n    result = result.replace("\'", "")\n    if code.isdigit():\n        class_LUT.append(result.split(",")[0][1:])\n        \n# For each of the top-N results, associate the integer result with an actual class\nfor n in topN:\n    print("Model predicts \'{}\' with {}% confidence".format(class_LUT[int(n[0])],float("{0:.2f}".format(n[1]*100))))\n\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"Raw top 5 results: [array([723.0, 0.9102839827537537], dtype=object), array([968.0, 0.017375782132148743], dtype=object), array([719.0, 0.010471619665622711], dtype=object), array([985.0, 0.009765725582838058], dtype=object), array([767.0, 0.006287392228841782], dtype=object)]\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Top 5")," classes in order: ",(0,o.kt)("inlineCode",{parentName:"li"},"[723, 968, 719, 985, 767]"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Model predicts"),": ",(0,o.kt)("inlineCode",{parentName:"li"},"pinwheel")," with ",(0,o.kt)("inlineCode",{parentName:"li"},"91.03%")," confidence"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Model predicts"),": ",(0,o.kt)("inlineCode",{parentName:"li"},"cup")," with ",(0,o.kt)("inlineCode",{parentName:"li"},"1.74%")," confidence"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Model predicts"),": ",(0,o.kt)("inlineCode",{parentName:"li"},"piggy"),"bank' with ",(0,o.kt)("inlineCode",{parentName:"li"},"1.05%")," confidence"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Model predicts"),": ",(0,o.kt)("inlineCode",{parentName:"li"},"daisy")," with ",(0,o.kt)("inlineCode",{parentName:"li"},"0.98%")," confidence"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Model predicts"),": ",(0,o.kt)("inlineCode",{parentName:"li"},"rubber eraser")," with ",(0,o.kt)("inlineCode",{parentName:"li"},"0.63%")," confidence")))),(0,o.kt)("h4",{id:"feeding-larger-batches"},"Feeding Larger Batches"),(0,o.kt)("p",null,"Above is an example of how to feed one image at a time. We can achieve higher throughput if we feed multiple images at a time in a single batch. Recall, the data fed into the classifier is in 'NCHW' order, so to feed multiple images, we will expand the 'N' axis."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# List of input images to be fed\nimages = ["images/cowboy-hat.jpg",\n            "images/cell-tower.jpg",\n            "images/Ducreux.jpg",\n            "images/pretzel.jpg",\n            "images/orangutan.jpg",\n            "images/aircraft-carrier.jpg",\n            "images/cat.jpg"]\n\n# Allocate space for the batch of formatted images\nNCHW_batch = np.zeros((len(images),3,227,227))\nprint ("Batch Shape: ",NCHW_batch.shape)\n\n# For each of the images in the list, format it and place it in the batch\nfor i,curr_img in enumerate(images):\n    img = skimage.img_as_float(skimage.io.imread(curr_img)).astype(np.float32)\n    img = rescale(img, 227, 227)\n    img = crop_center(img, 227, 227)\n    img = img.swapaxes(1, 2).swapaxes(0, 1)\n    img = img[(2, 1, 0), :, :]\n    img = img * 255 - mean\n    NCHW_batch[i] = img\n\nprint("NCHW image (ready to be used as input): ", NCHW_batch.shape)\n\n# Run the net on the batch\nresults = p.run([NCHW_batch.astype(np.float32)])\n\n# Turn it into something we can play with and examine which is in a multi-dimensional array\nresults = np.asarray(results)\n\n# Squeeze out the unnecessary axis\npreds = np.squeeze(results)\nprint("Squeezed Predictions Shape, with batch size {}: {}".format(len(images),preds.shape))\n\n# Describe the results\nfor i,pred in enumerate(preds):\n    print("Results for: \'{}\'".format(images[i]))\n    # Get the prediction and the confidence by finding the maximum value \n    #   and index of maximum value in preds array\n    curr_pred, curr_conf = max(enumerate(pred), key=operator.itemgetter(1))\n    print("\\tPrediction: ", curr_pred)\n    print("\\tClass Name: ", class_LUT[int(curr_pred)])\n    print("\\tConfidence: ", curr_conf)\n')),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Batch Shape"),":  ",(0,o.kt)("inlineCode",{parentName:"li"},"(8, 3, 227, 227)")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"NCHW image")," (ready to be used as input):  (8, 3, 227, 227)"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Squeezed Predictions Shape"),", with batch size 8: (8, 1000)"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Results for"),": 'images/cowboy-hat.jpg'",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Prediction:  515"),(0,o.kt)("li",{parentName:"ul"},"Class Name:  cowboy hat"),(0,o.kt)("li",{parentName:"ul"},"Confidence:  0.850092"))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Results for"),": ",(0,o.kt)("inlineCode",{parentName:"li"},"images/cell-tower.jpg"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Prediction:  645"),(0,o.kt)("li",{parentName:"ul"},"Class Name:  maypole"),(0,o.kt)("li",{parentName:"ul"},"Confidence:  0.185843"))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Results for"),": ",(0,o.kt)("inlineCode",{parentName:"li"},"images/Ducreux.jpg"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Prediction:  568"),(0,o.kt)("li",{parentName:"ul"},"Class Name:  fur coat"),(0,o.kt)("li",{parentName:"ul"},"Confidence:  0.102531"))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Results for"),": ",(0,o.kt)("inlineCode",{parentName:"li"},"images/pretzel.jpg"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Prediction:  932"),(0,o.kt)("li",{parentName:"ul"},"Class Name:  pretzel"),(0,o.kt)("li",{parentName:"ul"},"Confidence:  0.999622"))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Results for"),": ",(0,o.kt)("inlineCode",{parentName:"li"},"images/orangutan.jpg"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Prediction:  365"),(0,o.kt)("li",{parentName:"ul"},"Class Name:  orangutan"),(0,o.kt)("li",{parentName:"ul"},"Confidence:  0.992006"))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Results for"),": ",(0,o.kt)("inlineCode",{parentName:"li"},"images/aircraft-carrier.jpg"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Prediction:  403"),(0,o.kt)("li",{parentName:"ul"},"Class Name:  aircraft carrier"),(0,o.kt)("li",{parentName:"ul"},"Confidence:  0.999878"))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Results for"),": ",(0,o.kt)("inlineCode",{parentName:"li"},"images/cat.jpg"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Prediction:  281"),(0,o.kt)("li",{parentName:"ul"},"Class Name:  tabby"),(0,o.kt)("li",{parentName:"ul"},"Confidence:  0.513315"))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Results for"),": ",(0,o.kt)("inlineCode",{parentName:"li"},"images/flower.jpg"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Prediction:  985"),(0,o.kt)("li",{parentName:"ul"},"Class Name:  daisy"),(0,o.kt)("li",{parentName:"ul"},"Confidence:  0.982227")))),(0,o.kt)("h2",{id:"loading-datasets"},"Loading Datasets"),(0,o.kt)("p",null,"So Caffe2 uses a binary DB format to store the data that we would like to train models on. A Caffe2 DB is a glorified name of a key-value storage where the keys are usually randomized so that the batches are approximately i.i.d. The values are the real stuff here: they contain the serialized strings of the specific data formats that you would like your training algorithm to ingest. So, the stored DB would look (semantically) like this:"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"key1 value1"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"key2 value2"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"key3 value3")," ..."),(0,o.kt)("p",null,"To a DB, it treats the keys and values as strings, but you probably want structured contents. One way to do this is to use a TensorProtos protocol buffer: it essentially wraps Tensors, aka multi-dimensional arrays, together with the tensor data type and shape information. Then, one can use the TensorProtosDBInput operator to load the data into an SGD training fashion."),(0,o.kt)("p",null,"Here, we will show you one example of how to create your own dataset. To this end, we will use the UCI Iris dataset - which was a very popular classical dataset for classifying Iris flowers. It contains 4 real-valued features representing the dimensions of the flower, and classifies things into 3 types of Iris flowers. The dataset can be downloaded ",(0,o.kt)("a",{parentName:"p",href:"https://archive.ics.uci.edu/ml/datasets/Iris"},"here"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# First let's import some necessities\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\n%matplotlib inline\nimport urllib2 # for downloading the dataset from the web.\nimport numpy as np\nfrom matplotlib import pyplot\nfrom StringIO import StringIO\nfrom caffe2.python import core, utils, workspace\nfrom caffe2.proto import caffe2_pb2\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"f = urllib2.urlopen('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data')\nraw_data = f.read()\nprint('Raw data looks like this:')\nprint(raw_data[:100] + '...')\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"Raw data looks like this:\n5.1,3.5,1.4,0.2,Iris-setosa\n4.9,3.0,1.4,0.2,Iris-setosa\n4.7,3.2,1.3,0.2,Iris-setosa\n4.6,3.1,1.5,0.2,...\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# load the features to a feature matrix.\nfeatures = np.loadtxt(StringIO(raw_data), dtype=np.float32, delimiter=',', usecols=(0, 1, 2, 3))\n# load the labels to a feature matrix\nlabel_converter = lambda s : {'Iris-setosa':0, 'Iris-versicolor':1, 'Iris-virginica':2}[s]\nlabels = np.loadtxt(StringIO(raw_data), dtype=np.int, delimiter=',', usecols=(4,), converters={4: label_converter})\n")),(0,o.kt)("p",null,"Before we do training, one thing that is often beneficial is to separate the dataset into training and testing. In this case, let's randomly shuffle the data, use the first 100 data points to do training, and the remaining 50 to do testing. For more sophisticated approaches, you can use e.g. cross validation to separate your dataset into multiple training and testing splits. Read more about cross validation ",(0,o.kt)("a",{parentName:"p",href:"http://scikit-learn.org/stable/modules/cross_validation.html"},"here"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"random_index = np.random.permutation(150)\nfeatures = features[random_index]\nlabels = labels[random_index]\n\ntrain_features = features[:100]\ntrain_labels = labels[:100]\ntest_features = features[100:]\ntest_labels = labels[100:]\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# Let's plot the first two features together with the label.\n# Remember, while we are plotting the testing feature distribution\n# here too, you might not be supposed to do so in real research,\n# because one should not peek into the testing data.\nlegend = ['rx', 'b+', 'go']\npyplot.title(\"Training data distribution, feature 0 and 1\")\nfor i in range(3):\n    pyplot.plot(train_features[train_labels==i, 0], train_features[train_labels==i, 1], legend[i])\npyplot.figure()\npyplot.title(\"Testing data distribution, feature 0 and 1\")\nfor i in range(3):\n    pyplot.plot(test_features[test_labels==i, 0], test_features[test_labels==i, 1], legend[i])\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Introduction to Caffe2",src:n(67071).Z,width:"388",height:"540"})),(0,o.kt)("p",null,"Now, as promised, let's put things into a Caffe2 DB. In this DB, what would happen is that we will use \"train_xxx\" as the key, and use a TensorProtos object to store two tensors for each data point: one as the feature and one as the label. We will use Caffe2's Python DB interface to do so."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# First, let's see how one can construct a TensorProtos protocol buffer from numpy arrays.\nfeature_and_label = caffe2_pb2.TensorProtos()\nfeature_and_label.protos.extend([\n    utils.NumpyArrayToCaffe2Tensor(features[0]),\n    utils.NumpyArrayToCaffe2Tensor(labels[0])])\nprint('This is what the tensor proto looks like for a feature and its label:')\nprint(str(feature_and_label))\nprint('This is the compact string that gets written into the db:')\nprint(feature_and_label.SerializeToString())\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# Now, actually write the db.\n\ndef write_db(db_type, db_name, features, labels):\n    db = core.C.create_db(db_type, db_name, core.C.Mode.write)\n    transaction = db.new_transaction()\n    for i in range(features.shape[0]):\n        feature_and_label = caffe2_pb2.TensorProtos()\n        feature_and_label.protos.extend([\n            utils.NumpyArrayToCaffe2Tensor(features[i]),\n            utils.NumpyArrayToCaffe2Tensor(labels[i])])\n        transaction.put(\n            \'train_%03d\'.format(i),\n            feature_and_label.SerializeToString())\n    # Close the transaction, and then close the db.\n    del transaction\n    del db\n\nwrite_db("minidb", "iris_train.minidb", train_features, train_labels)\nwrite_db("minidb", "iris_test.minidb", test_features, test_labels)\n')),(0,o.kt)("p",null,"Now, let's create a very simple network that only consists of one single TensorProtosDBInput operator, to showcase how we load data from the DB that we created. For training, you might want to do something more complex: creating a network, train it, get the model, and run the prediction service. To this end you can look at the MNIST tutorial for details."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'net_proto = core.Net("example_reader")\ndbreader = net_proto.CreateDB([], "dbreader", db="iris_train.minidb", db_type="minidb")\nnet_proto.TensorProtosDBInput([dbreader], ["X", "Y"], batch_size=16)\n\nprint("The net looks like this:")\nprint(str(net_proto.Proto()))\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"workspace.CreateNet(net_proto)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# Let\'s run it to get batches of features.\nworkspace.RunNet(net_proto.Proto().name)\nprint("The first batch of feature is:")\nprint(workspace.FetchBlob("X"))\nprint("The first batch of label is:")\nprint(workspace.FetchBlob("Y"))\n\n# Let\'s run again.\nworkspace.RunNet(net_proto.Proto().name)\nprint("The second batch of feature is:")\nprint(workspace.FetchBlob("X"))\nprint("The second batch of label is:")\nprint(workspace.FetchBlob("Y"))\n')),(0,o.kt)("h2",{id:"image-loading-and-preprocessing"},"Image Loading and Preprocessing"),(0,o.kt)("p",null,"In this tutorial we're going to look at how we can load in images from a local file or a URL which you can then utilize in other tutorials or examples. Also, we're going to go in depth on the kinds of preprocessing that is necessary to utilize Caffe2 with images."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\n%matplotlib inline\nimport skimage\nimport skimage.io as io\nimport skimage.transform \nimport sys\nimport numpy as np\nimport math\nfrom matplotlib import pyplot\nimport matplotlib.image as mpimg\nprint("Required modules imported.")\n')),(0,o.kt)("h3",{id:"caffe-uses-bgr-order"},"Caffe Uses BGR Order"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Test an Image"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"In the code block below use IMAGE_LOCATION to load what you would like to test. Just change the comment flags to go through each round of the Tutorial. In this way, you'll get to see what happens with a variety of image formats and some tips on how you might preprocess them. If you want to try your own image, drop it in the images folder or use a remote URL. When you pick a remote URL, make it easy on yourself and try to find a URL that points to a common image file type and extension versus some long identifier or query string which might just break this next step."))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Color Issues"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Keep in mind when you load images from smartphone cameras that you may run into color formatting issues. Below we show an example of how flipping between RGB and BGR can impact an image. This would obviously throw off detection in your model. Due to legacy support of OpenCV in Caffe and how it handles images in Blue-Green-Red (BGR) order instead of the more commonly used Red-Green-Blue (RGB) order, Caffe2 also expects BGR order. In many ways this decision helps in the long run as you use different computer vision utilities and libraries, but it also can be the source of confusion.")))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# You can load either local IMAGE_FILE or remote URL\n# For Round 1 of this tutorial, try a local image.\nIMAGE_LOCATION = \'flower.jpg\'\n\n# For Round 2 of this tutorial, try a URL image with a flower: \n# IMAGE_LOCATION = "https://cdn.pixabay.com/photo/2015/02/10/21/28/flower-631765_1280.jpg"\n# IMAGE_LOCATION = "images/flower.jpg"\n\n# For Round 3 of this tutorial, try another URL image with lots of people:\n# IMAGE_LOCATION = "https://upload.wikimedia.org/wikipedia/commons/1/18/NASA_Astronaut_Group_15.jpg"\n# IMAGE_LOCATION = "images/astronauts.jpg"\n\n# For Round 4 of this tutorial, try a URL image with a portrait!\n# IMAGE_LOCATION = "https://upload.wikimedia.org/wikipedia/commons/9/9a/Ducreux1.jpg"\n# IMAGE_LOCATION = "images/Ducreux.jpg"\n\nimg = skimage.img_as_float(skimage.io.imread(IMAGE_LOCATION)).astype(np.float32)\n\n# test color reading\n# show the original image\npyplot.figure()\npyplot.subplot(1,2,1)\npyplot.imshow(img)\npyplot.axis(\'on\')\npyplot.title(\'Original image = RGB\')\n\n# show the image in BGR - just doing RGB->BGR temporarily for display\nimgBGR = img[:, :, (2, 1, 0)]\n#pyplot.figure()\npyplot.subplot(1,2,2)\npyplot.imshow(imgBGR)\npyplot.axis(\'on\')\npyplot.title(\'OpenCV, Caffe2 = BGR\')\n')),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Introduction to Caffe2",src:n(26059).Z,width:"386",height:"151"})),(0,o.kt)("p",null,"As you can see in the example above, the difference in order is very important to keep in mind. In the code block below we'll be taking the image and converting to BGR order for Caffe to process it appropriately."),(0,o.kt)("h3",{id:"caffe-prefers-chw-order"},"Caffe Prefers CHW Order"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"H"),": Height"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"W"),": Width"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"C"),": Channel (as in color)")),(0,o.kt)("p",null,"Digging even deeper into how image data can be stored is the memory allocation order. You might have noticed when we first loaded the image that we forced it through some interesting transformations. These were data transformations that let us play with the image as if it were a cube. What we see is on top of the cube, and manipulating the layers below can change what we view. We can tinker with it's underlying properties and as you saw above, swap colors quite easily. "),(0,o.kt)("p",null,'For GPU processing, which is what Caffe2 excels at, this order needs to be CHW. For CPU processing, this order is generally HWC. Essentially, you\'re going to want to use CHW and make sure that step is included in your image pipeline. Tweak RGB to be BGR, which is encapsulated as this "C" payload, then tweak HWC, the "C" being the very same colors you just switched around.'),(0,o.kt)("p",null,"You may ask why! And the reason points to cuDNN which is what helps accelerate processing on GPUs. It uses only CHW, and we'll sum it up by saying it is faster. "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"img_hcw = \"flower.jpg\"\nimg_hcw = skimage.img_as_float(skimage.io.imread(img_hcw)).astype(np.float32)\nprint(\"Image shape before HWC --\x3e CHW conversion: \", img_hcw.shape)\n# swapping the axes to go from HWC to CHW\n# uncomment the next line and run this block!\nimg_chw = img_hcw.swapaxes(1, 2).swapaxes(0, 1)\nprint(\"Image shape after HWC --\x3e CHW conversion: \", img_chw.shape)\n# we know this is going to go wrong, so...\ntry:\n    # Plot original\n    pyplot.figure()\n    pyplot.subplot(1, 2, 1)\n    pyplot.imshow(img_hcw)\n    pyplot.axis('on')\n    pyplot.title('hcw')\n    pyplot.subplot(1, 2, 2)\n    pyplot.imshow(img_chw)\n    pyplot.axis('on')\n    pyplot.title('chw')\nexcept:\n    print(\"Here come bad things!\")\n    # TypeError: Invalid dimensions for image data\n    raise \n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Introduction to Caffe2",src:n(26523).Z,width:"376",height:"189"})),(0,o.kt)("h3",{id:"rotation-and-mirroring"},"Rotation and Mirroring"),(0,o.kt)("p",null,"This topic is usually reserved for images that are coming from a smart phone. Phones, in general, take great pictures, but do a horrible job communicating how the image was taken and what orientation it should be in. Then there's the user who does everything under the sun with their phone's cameras, making them do things its designer never expected. Cameras - right, because there are often two cameras and these two cameras take different sized pictures in both pixel count and aspect ratio, and not only that, they sometimes take them mirrored, and they sometimes take them in portrait and landscape modes, and sometimes they don't bother to tell which mode they were in. "),(0,o.kt)("p",null,"In many ways this is the first thing you need to evaluate in your pipeline, then look at sizing (described below), then figure out the color situation. If you're developing for iOS, then you're in luck, it's going to be relatively easy. If you're a super-hacker wizard developer with lead-lined shorts and developing for Android, then at least you have lead-lined shorts. "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# Image came in sideways - it should be a portait image!\n# How you detect this depends on the platform\n# Could be a flag from the camera object\n# Could be in the EXIF data\n# ROTATED_IMAGE = \"https://upload.wikimedia.org/wikipedia/commons/8/87/Cell_Phone_Tower_in_Ladakh_India_with_Buddhist_Prayer_Flags.jpg\"\nROTATED_IMAGE = \"images/cell-tower.jpg\"\nimgRotated = skimage.img_as_float(skimage.io.imread(ROTATED_IMAGE)).astype(np.float32)\npyplot.figure()\npyplot.imshow(imgRotated)\npyplot.axis('on')\npyplot.title('Rotated image')\n\n# Image came in flipped or mirrored - text is backwards!\n# Again detection depends on the platform\n# This one is intended to be read by drivers in their rear-view mirror\n# MIRROR_IMAGE = \"https://upload.wikimedia.org/wikipedia/commons/2/27/Mirror_image_sign_to_be_read_by_drivers_who_are_backing_up_-b.JPG\"\nMIRROR_IMAGE = \"images/mirror-image.jpg\"\nimgMirror = skimage.img_as_float(skimage.io.imread(MIRROR_IMAGE)).astype(np.float32)\npyplot.figure()\npyplot.imshow(imgMirror)\npyplot.axis('on')\npyplot.title('Mirror image')\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Introduction to Caffe2",src:n(61756).Z,width:"352",height:"538"})),(0,o.kt)("p",null,"Let's transform these images into something Caffe2 and the standard detection models we have around can detect. Also, this little trick might save you if, say for example, you really had to detect the cell tower but there's no EXIF data to be found: then you'd cycle through every rotation, and every flip, spawning many derivatives of this photo and run them all through. When the percentage of confidence of detection is high enough, Bam!, you found the orientation you needed and that sneaky cell tower."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# Run me to flip the image back and forth\nimgMirror = np.fliplr(imgMirror)\npyplot.figure()\npyplot.imshow(imgMirror)\npyplot.axis('off')\npyplot.title('Mirror image')\n\n# Run me to rotate the image 90 degrees\nimgRotated = np.rot90(imgRotated, 3)\npyplot.figure()\npyplot.imshow(imgRotated)\npyplot.axis('off')\npyplot.title('Rotated image')\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Introduction to Caffe2",src:n(43147).Z,width:"310",height:"526"})),(0,o.kt)("h3",{id:"sizing"},"Sizing"),(0,o.kt)("p",null,"Part of preprocessing is resizing. For reasons we won't get into here, images in the Caffe2 pipeline should be square. Also, to help with performance, they should be resized to a standard height and width which is usually going to be smaller than your original source. In the example below we're resizing to ",(0,o.kt)("inlineCode",{parentName:"p"},"256 x 256")," pixels, however you might notice that the input_height and input_width is set to ",(0,o.kt)("inlineCode",{parentName:"p"},"224 x 224")," which is then used to specify the crop. This is what several image-based models are expecting. They were trained on images sized to ",(0,o.kt)("inlineCode",{parentName:"p"},"224 x 224")," and in order for the model to properly identify the suspect images you throw at it, these should also be ",(0,o.kt)("inlineCode",{parentName:"p"},"224 x 224"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# Model is expecting 224 x 224, so resize/crop needed.\n# First, let\'s resize the image to 256*256\norig_h, orig_w, _ = img.shape\nprint("Original image\'s shape is {}x{}".format(orig_h, orig_w))\ninput_height, input_width = 224, 224\nprint("Model\'s input shape is {}x{}".format(input_height, input_width))\nimg256 = skimage.transform.resize(img, (256, 256))\n\n# Plot original and resized images for comparison\nf, axarr = pyplot.subplots(1,2)\naxarr[0].imshow(img)\naxarr[0].set_title("Original Image (" + str(orig_h) + "x" + str(orig_w) + ")")\naxarr[0].axis(\'on\')\naxarr[1].imshow(img256)\naxarr[1].axis(\'on\')\naxarr[1].set_title(\'Resized image to 256x256\')\npyplot.tight_layout()\n\nprint("New image shape:" + str(img256.shape))\n')),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Original image's shape is 534x800"),(0,o.kt)("li",{parentName:"ul"},"Model's input shape is 224x224"),(0,o.kt)("li",{parentName:"ul"},"New image shape:(256, 256, 3)")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Introduction to Caffe2",src:n(63157).Z,width:"427",height:"212"})),(0,o.kt)("h3",{id:"rescaling"},"Rescaling"),(0,o.kt)("p",null,"If you imagine portait images versus landscape images you'll know that there are a lot of things that can get messed up by doing a slopping resize. Rescaling is assuming that you're locking down the aspect ratio to prevent distortion in the image. In this case, we'll scale down the image to the shortest side that matches with the model's input size."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Landscape"),": limit resize by the height"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Portrait"),": limit resize by the width")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'print("Original image shape:" + str(img.shape) + " and remember it should be in H, W, C!")\nprint("Model\'s input shape is {}x{}".format(input_height, input_width))\naspect = img.shape[1]/float(img.shape[0])\nprint("Orginal aspect ratio: " + str(aspect))\nif(aspect>1):\n    # landscape orientation - wide image\n    res = int(aspect * input_height)\n    imgScaled = skimage.transform.resize(img, (input_height, res))\nif(aspect<1):\n    # portrait orientation - tall image\n    res = int(input_width/aspect)\n    imgScaled = skimage.transform.resize(img, (res, input_width))\nif(aspect == 1):\n    imgScaled = skimage.transform.resize(img, (input_height, input_width))\npyplot.figure()\npyplot.imshow(imgScaled)\npyplot.axis(\'on\')\npyplot.title(\'Rescaled image\')\nprint("New image shape:" + str(imgScaled.shape) + " in HWC")\n')),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Original image shape:(534, 800, 3) and remember it should be in H, W, C!"),(0,o.kt)("li",{parentName:"ul"},"Model's input shape is 224x224"),(0,o.kt)("li",{parentName:"ul"},"Orginal aspect ratio: 1.49812734082"),(0,o.kt)("li",{parentName:"ul"},"New image shape:(224, 335, 3) in HWC")),(0,o.kt)("p",null,"At this point only one dimension is set to what the model's input requires. We still need to crop one side to make a square. "),(0,o.kt)("h3",{id:"cropping"},"Cropping"),(0,o.kt)("p",null,"There are a variety of strategies we could utilize. In fact, we could backpeddle and decide to do a center crop. So instead of scaling down to the smallest we could get on at least one side, we take a chunk out of the middle. If we had done that without scaling we would have ended up with just part of a flower pedal, so we still needed some resizing of the image."),(0,o.kt)("p",null,"Below we'll try a few strategies for cropping:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Just grab the exact dimensions you need from the middle!"),(0,o.kt)("li",{parentName:"ol"},"Resize to a square that's pretty close then grab from the middle."),(0,o.kt)("li",{parentName:"ol"},"Use the rescaled image and grab the middle.")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# Compare the images and cropping strategies\n# Try a center crop on the original for giggles\nprint(\"Original image shape:\" + str(img.shape) + \" and remember it should be in H, W, C!\")\ndef crop_center(img,cropx,cropy):\n    y,x,c = img.shape\n    startx = x//2-(cropx//2)\n    starty = y//2-(cropy//2)    \n    return img[starty:starty+cropy,startx:startx+cropx]\n# yes, the function above should match resize and take a tuple...\n\npyplot.figure()\n# Original image\nimgCenter = crop_center(img,224,224)\npyplot.subplot(1,3,1)\npyplot.imshow(imgCenter)\npyplot.axis('on')\npyplot.title('Original')\n\n# Now let's see what this does on the distorted image\nimg256Center = crop_center(img256,224,224)\npyplot.subplot(1,3,2)\npyplot.imshow(img256Center)\npyplot.axis('on')\npyplot.title('Squeezed')\n\n# Scaled image\nimgScaledCenter = crop_center(imgScaled,224,224)\npyplot.subplot(1,3,3)\npyplot.imshow(imgScaledCenter)\npyplot.axis('on')\npyplot.title('Scaled')\n\npyplot.tight_layout()\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Introduction to Caffe2",src:n(42256).Z,width:"429",height:"145"})),(0,o.kt)("p",null,"As you can see that didn't work out so well, except for maybe the last one. The middle one may be just fine too, but you won't know until you try on the model and test a lot of candidate images. At this point we can look at the difference we have, split it in half and remove some pixels from each side. This does have a drawback, however, as an off-center subject of interest would get clipped."),(0,o.kt)("h3",{id:"upscaling"},"Upscaling"),(0,o.kt)("p",null,'What do you do when the images you want to run are "tiny"? In our example we\'ve been prepping for Input Images with the spec of ',(0,o.kt)("inlineCode",{parentName:"p"},"224x224"),". Consider this ",(0,o.kt)("inlineCode",{parentName:"p"},"128x128")," image below. "),(0,o.kt)("p",null,"The most basic approach is going from a small square to a bigger square and using the defauls skimage provides for you. This resize method defaults the interpolation order parameter to 1 which happens to be bi-linear if you even cared, but it is worth mentioning because these might be the fine-tuning knobs you need later to fix problems, such as strange visual artifacts, that can be introduced in upscaling images."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"imgTiny = \"images/Cellsx128.png\"\nimgTiny = skimage.img_as_float(skimage.io.imread(imgTiny)).astype(np.float32)\nprint(\"Original image shape: \", imgTiny.shape)\nimgTiny224 = skimage.transform.resize(imgTiny, (224, 224))\nprint(\"Upscaled image shape: \", imgTiny224.shape)\n# Plot original\npyplot.figure()\npyplot.subplot(1, 2, 1)\npyplot.imshow(imgTiny)\npyplot.axis('on')\npyplot.title('128x128')\n# Plot upscaled\npyplot.subplot(1, 2, 2)\npyplot.imshow(imgTiny224)\npyplot.axis('on')\npyplot.title('224x224')\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Original image shape:  (128, 128, 4)"),(0,o.kt)("li",{parentName:"ul"},"Upscaled image shape:  (224, 224, 4)")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Introduction to Caffe2",src:n(89841).Z,width:"377",height:"195"})),(0,o.kt)("h3",{id:"batch-processing"},"Batch Processing"),(0,o.kt)("p",null,"In the last steps below we are going to switch the image's data order to BGR, stuff that into the Color column, then reorder the columns for GPU processing (",(0,o.kt)("inlineCode",{parentName:"p"},"HCW")," --\x3e ",(0,o.kt)("inlineCode",{parentName:"p"},"CHW"),") and then add a fourth dimension (N) to the image to track the number of images. In theory, you can just keep adding dimensions to your data, but this one is required for Caffe2 as it relays to Caffe how many images to expect in this batch. We set it to one (1) to indicate there's only one image going into Caffe in this batch. Note that in the final output when we check ",(0,o.kt)("inlineCode",{parentName:"p"},"img.shape")," the order is quite different. We've added N for number of images, and changed the order like so: ",(0,o.kt)("inlineCode",{parentName:"p"},"N"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"C"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"H"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"W"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# This next line helps with being able to rerun this section\n# if you want to try the outputs of the different crop strategies above\n# swap out imgScaled with img (original) or img256 (squeezed)\nimgCropped = crop_center(imgScaled,224,224)\nprint(\"Image shape before HWC --\x3e CHW conversion: \", imgCropped.shape)\n# (1) Since Caffe expects CHW order and the current image is HWC,\n#     we will need to change the order.\nimgCropped = imgCropped.swapaxes(1, 2).swapaxes(0, 1)\nprint(\"Image shape after HWC --\x3e CHW conversion: \", imgCropped.shape)\n\npyplot.figure()\nfor i in range(3):\n    # For some reason, pyplot subplot follows Matlab's indexing\n    # convention (starting with 1). Well, we'll just follow it...\n    pyplot.subplot(1, 3, i+1)\n    pyplot.imshow(imgCropped[i], cmap=pyplot.cm.gray)\n    pyplot.axis('off')\n    pyplot.title('RGB channel %d' % (i+1))\n\n# (2) Caffe uses a BGR order due to legacy OpenCV issues, so we\n#     will change RGB to BGR.\nimgCropped = imgCropped[(2, 1, 0), :, :]\nprint(\"Image shape after BGR conversion: \", imgCropped.shape)\n\n# for discussion later - not helpful at this point\n# (3) (Optional) We will subtract the mean image. Note that skimage loads\n#     image in the [0, 1] range so we multiply the pixel values\n#     first to get them into [0, 255].\n#mean_file = os.path.join(CAFFE_ROOT, 'python/caffe/imagenet/ilsvrc_2012_mean.npy')\n#mean = np.load(mean_file).mean(1).mean(1)\n#img = img * 255 - mean[:, np.newaxis, np.newaxis]\n\npyplot.figure()\nfor i in range(3):\n    # For some reason, pyplot subplot follows Matlab's indexing\n    # convention (starting with 1). Well, we'll just follow it...\n    pyplot.subplot(1, 3, i+1)\n    pyplot.imshow(imgCropped[i], cmap=pyplot.cm.gray)\n    pyplot.axis('off')\n    pyplot.title('BGR channel %d' % (i+1))\n# (4) Finally, since caffe2 expect the input to have a batch term\n#     so we can feed in multiple images, we will simply prepend a\n#     batch dimension of size 1. Also, we will make sure image is\n#     of type np.float32.\nimgCropped = imgCropped[np.newaxis, :, :, :].astype(np.float32)\nprint('Final input shape is:', imgCropped.shape)\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Image shape before HWC --\x3e CHW conversion:  (224, 224, 3)"),(0,o.kt)("li",{parentName:"ul"},"Image shape after HWC --\x3e CHW conversion:  (3, 224, 224)"),(0,o.kt)("li",{parentName:"ul"},"Image shape after BGR conversion:  (3, 224, 224)"),(0,o.kt)("li",{parentName:"ul"},"Final input shape is: (1, 3, 224, 224)")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Introduction to Caffe2",src:n(8279).Z,width:"352",height:"289"})))}c.isMDXComponent=!0},72690:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/introduction-to-pytorch-caffe_01-52365d331443f5232285f1dd260c1cd1.png"},46527:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/introduction-to-pytorch-caffe_02-a60006288066565368d6e220f7df93a4.png"},92001:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/introduction-to-pytorch-caffe_03-9d8db0f1a0330a8c16d2d9bc0db5f48f.png"},67071:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/introduction-to-pytorch-caffe_04-29e0161d91e68b2ef667be9a6ddaac32.png"},26059:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/introduction-to-pytorch-caffe_05-8408c66472fa2fe8c0c9b9180998fd24.png"},61756:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/introduction-to-pytorch-caffe_06-bc2a796863243b31aa64346fa4c90aa3.png"},43147:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/introduction-to-pytorch-caffe_07-0274f0916356f23ce9425024ada01aef.png"},63157:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/introduction-to-pytorch-caffe_08-50e9bb046f144402d8655f4bc8f195e4.png"},42256:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/introduction-to-pytorch-caffe_09-7dc84647ac6bed4679ad460f6e093213.png"},89841:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/introduction-to-pytorch-caffe_10-9c35f675b41de2c662336e9747e14c73.png"},26523:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/introduction-to-pytorch-caffe_11-4361a30172e8ad27d5c4beda1aae38ff.png"},8279:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/introduction-to-pytorch-caffe_12-674dbefa22615346cbb0becbda4e64ea.png"},76369:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-5a33ed1aeac871d5b7a7594cc7d702c8.jpg"}}]);