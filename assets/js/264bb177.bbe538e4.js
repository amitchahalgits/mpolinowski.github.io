"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[11886],{983969:(e,s,t)=>{t.r(s),t.d(s,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>a,metadata:()=>o,toc:()=>c});var n=t(785893),r=t(603905);const a={sidebar_position:8070,slug:"2021-03-25",title:"Log all the searches going through Elasticsearch",authors:"mpolinowski",tags:["LINUX","Elasticsearch"]},i=void 0,o={id:"DevOps/Elasticsearch/2021-03-25-elasticsearch7-activate-logging-of-search-queries/index",title:"Log all the searches going through Elasticsearch",description:"Bakhtapur, Nepal",source:"@site/docs/DevOps/Elasticsearch/2021-03-25-elasticsearch7-activate-logging-of-search-queries/index.md",sourceDirName:"DevOps/Elasticsearch/2021-03-25-elasticsearch7-activate-logging-of-search-queries",slug:"/DevOps/Elasticsearch/2021-03-25-elasticsearch7-activate-logging-of-search-queries/2021-03-25",permalink:"/docs/DevOps/Elasticsearch/2021-03-25-elasticsearch7-activate-logging-of-search-queries/2021-03-25",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/DevOps/Elasticsearch/2021-03-25-elasticsearch7-activate-logging-of-search-queries/index.md",tags:[{label:"LINUX",permalink:"/docs/tags/linux"},{label:"Elasticsearch",permalink:"/docs/tags/elasticsearch"}],version:"current",sidebarPosition:8070,frontMatter:{sidebar_position:8070,slug:"2021-03-25",title:"Log all the searches going through Elasticsearch",authors:"mpolinowski",tags:["LINUX","Elasticsearch"]},sidebar:"tutorialSidebar",previous:{title:"Elasticsearch 7 to log Linux System Events",permalink:"/docs/DevOps/Elasticsearch/2021-03-26-elasticsearch7-for-syslog-messages/2021-03-26"},next:{title:"Elasticsearch Cheat Sheet",permalink:"/docs/DevOps/Elasticsearch/2021-03-25-elasticsearch7-activate-logging-of-search-queries/elasticsearch-cheat-sheet"}},l={},c=[{value:"Enable logging of all searches",id:"enable-logging-of-all-searches",level:2},{value:"Inspecting the logs",id:"inspecting-the-logs",level:2},{value:"Disable logging of all searches",id:"disable-logging-of-all-searches",level:2},{value:"Logging Indexing Requests",id:"logging-indexing-requests",level:2},{value:"Working on the Live System",id:"working-on-the-live-system",level:2},{value:"Activate the search query logging",id:"activate-the-search-query-logging",level:3},{value:"Check that the logging is active",id:"check-that-the-logging-is-active",level:3}];function h(e){const s={a:"a",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.ah)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"Bakhtapur, Nepal",src:t(758444).Z+"",width:"1500",height:"542"})}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsx)(s.li,{children:(0,n.jsx)(s.a,{href:"#enable-logging-of-all-searches",children:"Enable logging of all searches"})}),"\n",(0,n.jsx)(s.li,{children:(0,n.jsx)(s.a,{href:"#inspecting-the-logs",children:"Inspecting the logs"})}),"\n",(0,n.jsx)(s.li,{children:(0,n.jsx)(s.a,{href:"#disable-logging-of-all-searches",children:"Disable logging of all searches"})}),"\n",(0,n.jsx)(s.li,{children:(0,n.jsx)(s.a,{href:"#logging-indexing-requests",children:"Logging Indexing Requests"})}),"\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.a,{href:"#working-on-the-live-system",children:"Working on the Live System"}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsx)(s.li,{children:(0,n.jsx)(s.a,{href:"#activate-the-search-query-logging",children:"Activate the search query logging"})}),"\n",(0,n.jsx)(s.li,{children:(0,n.jsx)(s.a,{href:"#check-that-the-logging-is-active",children:"Check that the logging is active"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(s.p,{children:'I always wondered why there is no one-button-solution in Kibana to log all searches and indexation requests sent to Elasticsearch in the engine log file. It would be very helpful to see what users are searching for to add new tags:[keywords] to articles that will position the "correct" article more prominent based on the search queries that are used.'}),"\n",(0,n.jsx)(s.h2,{id:"enable-logging-of-all-searches",children:"Enable logging of all searches"}),"\n",(0,n.jsxs)(s.p,{children:["We are going to leverage the ",(0,n.jsx)(s.strong,{children:"slowlog"})," functionality.  The ",(0,n.jsx)(s.strong,{children:"slowlog"})," functions logs everything that takes to long to be processed. Since search queries are usually fast they tend not to show up. But we can lower the threshold making sure that they will be recorded. I am working with an index ",(0,n.jsx)(s.code,{children:"apache-access-log"})," that was created earlier. To change the settings for this index run the following command in Kibana:"]}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-json",children:'PUT apache-access-log/_settings\r\n{\r\n  "index.search.slowlog.threshold.query.trace": "0s",\r\n  "index.search.slowlog.level": "trace"\r\n}\n'})}),"\n",(0,n.jsx)(s.p,{children:"I am running a search query in Kibana for the IP address `49.149.224.133``and get 17 hits:"}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"Elasticeasrch Log Search Queries",src:t(175936).Z+"",width:"1505",height:"909"})}),"\n",(0,n.jsx)(s.h2,{id:"inspecting-the-logs",children:"Inspecting the logs"}),"\n",(0,n.jsxs)(s.p,{children:["My Elasticsearch container is running under the name ",(0,n.jsx)(s.code,{children:"elasticsearch"}),". To check it's logs I need to run the following command:"]}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-bash",children:"docker logs elasticsearch\n"})}),"\n",(0,n.jsx)(s.p,{children:"And the log file shows a new entry at the bottom - this is our search query:"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-json",children:'{"type": "index_search_slowlog", "timestamp": "2021-08-04T05:37:26,667Z", "level": "TRACE", "component": "i.s.s.query", "cluster.name": "docker-cluster", "node.name": "debian11", "message": "[apache-access-log][0]", "took": "21.5ms", "took_millis": "21", "total_hits": "16 hits", "types": "[]", "stats": "[]", "search_type": "QUERY_THEN_FETCH", "total_shards": "1", "source": "{\\"size\\":500,\\"query\\":{\\"bool\\":{\\"filter\\":[{\\"multi_match\\":{\\"query\\":\\"49.149.224.133\\",\\"fields\\":[],\\"type\\":\\"best_fields\\",\\"operator\\":\\"OR\\",\\"slop\\":0,\\"prefix_length\\":0,\\"max_expansions\\":50,\\"lenient\\":true,\\"zero_terms_query\\":\\"NONE\\",\\"auto_generate_synonyms_phrase_query\\":true,\\"fuzzy_transpositions\\":true,\\"boost\\":1.0}}],\\"adjust_pure_negative\\":true,\\"boost\\":1.0}},\\"version\\":true,\\"_source\\":false,\\"stored_fields\\":\\"*\\",\\"fields\\":[{\\"field\\":\\"*\\",\\"include_unmapped\\":true},{\\"field\\":\\"@timestamp\\",\\"format\\":\\"strict_date_optional_time\\"},{\\"field\\":\\"read_timestamp\\",\\"format\\":\\"strict_date_optional_time\\"}],\\"script_fields\\":{},\\"sort\\":[{\\"_score\\":{\\"order\\":\\"desc\\"}}],\\"track_total_hits\\":2147483647,\\"highlight\\":{\\"pre_tags\\":[\\"@kibana-highlighted-field@\\"],\\"post_tags\\":[\\"@/kibana-highlighted-field@\\"],\\"fragment_size\\":2147483647,\\"fields\\":{\\"*\\":{}}}}", "id": "292795eb-82df-4bee-858d-254e4a1192d7", "cluster.uuid": "InACjldtRnu-vUy85P4txw", "node.id": "MsvdkSEdRBmBUftW7CciIw"  }\n'})}),"\n",(0,n.jsxs)(s.p,{children:["The part we want is inside the ",(0,n.jsx)(s.strong,{children:"source field"}),". As it is JSON inside a JSON it\u2019s escaped, we just need to replace ",(0,n.jsx)(s.code,{children:'\\"'})," by ",(0,n.jsx)(s.code,{children:'"'})," and we are good to go:"]}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-json",children:'"{"size":500,"query":{"bool":{"filter":[{"multi_match":{"query":"49.149.224.133","fields":[],"type":"best_fields","operator":"OR","slop":0,"prefix_length":0,"max_expansions":50,"lenient":true,"zero_terms_query":"NONE","auto_generate_synonyms_phrase_query":true,"fuzzy_transpositions":true,"boost":1.0}}],"adjust_pure_negative":true,"boost":1.0}},"version":true,"_source":false,"stored_fields":"*","fields":[{"field":"*","include_unmapped":true},{"field":"@timestamp","format":"strict_date_optional_time"},{"field":"read_timestamp","format":"strict_date_optional_time"}],"script_fields":{},"sort":[{"_score":{"order":"desc"}}],"track_total_hits":2147483647,"highlight":{"pre_tags":["@kibana-highlighted-field@"],"post_tags":["@/kibana-highlighted-field@"],"fragment_size":2147483647,"fields":{"*":{}}}}"\n'})}),"\n",(0,n.jsxs)(s.p,{children:["I copy the logfile to ",(0,n.jsx)(s.code,{children:"/opt/logstash/elasticsearch.log"})," and use the following Logstash configuration to ingest the log into Elasticsearch ",(0,n.jsx)(s.code,{children:"/opt/logstash/pipeline/logstash.conf"}),":"]}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-json",children:'input {\r\n  file {\r\n    path => "/usr/share/logstash/elasticsearch.log"\r\n    start_position => "beginning"\r\n    sincedb_path => "/dev/null"\r\n  }\r\n}\r\nfilter {\r\n  json {\r\n      source => "message"\r\n  }\r\n}\r\noutput {\r\n   elasticsearch {\r\n     hosts => "http://localhost:9200"\r\n     index => "elasticsearch-log"\r\n  }\r\n\r\nstdout {}\r\n\r\n}\n'})}),"\n",(0,n.jsx)(s.p,{children:"And run Logstash with the new configuration file:"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-bash",children:'docker run \\\r\n   --name logstash \\\r\n   --net=host \\\r\n   --rm -it \\\r\n   -v /opt/logstash/pipeline/logstash.conf:/usr/share/logstash/pipeline/logstash.conf \\\r\n   -v /opt/logstash/elasticsearch.log:/usr/share/logstash/elasticsearch.log \\\r\n   -e "ELASTIC_HOST=localhost:9200" \\\r\n   -e "XPACK_SECURITY_ENABLED=false" \\\r\n   -e "XPACK_REPORTING_ENABLED=false" \\\r\n   -e "XPACK_MONITORING_ENABLED=false" \\\r\n   -e "XPACK_MONITORING_ELASTICSEARCH_USERNAME=elastic" \\\r\n   -e "XPACK_MONITORING_ELASTICSEARCH_PASSWORD=changeme" \\\r\n   logstash:7.13.4\n'})}),"\n",(0,n.jsx)(s.p,{children:"This will give me the following result:"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-json",children:'{\r\n          "source" => "{\\"size\\":500,\\"query\\":{\\"bool\\":{\\"filter\\":[{\\"multi_match\\":{\\"query\\":\\"49.149.224.133\\",\\"fields\\":[],\\"type\\":\\"best_fields\\",\\"operator\\":\\"OR\\",\\"slop\\":0,\\"prefix_length\\":0,\\"max_expansions\\":50,\\"lenient\\":true,\\"zero_terms_query\\":\\"NONE\\",\\"auto_generate_synonyms_phrase_query\\":true,\\"fuzzy_transpositions\\":true,\\"boost\\":1.0}}],\\"adjust_pure_negative\\":true,\\"boost\\":1.0}},\\"version\\":true,\\"_source\\":false,\\"stored_fields\\":\\"*\\",\\"fields\\":[{\\"field\\":\\"*\\",\\"include_unmapped\\":true},{\\"field\\":\\"@timestamp\\",\\"format\\":\\"strict_date_optional_time\\"},{\\"field\\":\\"read_timestamp\\",\\"format\\":\\"strict_date_optional_time\\"}],\\"script_fields\\":{},\\"sort\\":[{\\"_score\\":{\\"order\\":\\"desc\\"}}],\\"track_total_hits\\":2147483647,\\"highlight\\":{\\"pre_tags\\":[\\"@kibana-highlighted-field@\\"],\\"post_tags\\":[\\"@/kibana-highlighted-field@\\"],\\"fragment_size\\":2147483647,\\"fields\\":{\\"*\\":{}}}}",\r\n           "level" => "TRACE",\r\n       "component" => "i.s.s.query",\r\n            "path" => "/usr/share/logstash/elasticsearch.log",\r\n            "type" => "index_search_slowlog",\r\n         "node.id" => "MsvdkSEdRBmBUftW7CciIw",\r\n            "took" => "21.5ms",\r\n      "@timestamp" => 2021-08-04T09:32:57.956Z,\r\n            "host" => "debian11",\r\n       "node.name" => "debian11",\r\n       "timestamp" => "2021-08-04T05:37:26,667Z",\r\n           "types" => "[]",\r\n      "total_hits" => "16 hits",\r\n              "id" => "292795eb-82df-4bee-858d-254e4a1192d7",\r\n         "message" => "[apache-access-log][0]",\r\n    "cluster.uuid" => "InACjldtRnu-vUy85P4txw",\r\n           "stats" => "[]",\r\n        "@version" => "1",\r\n    "total_shards" => "1",\r\n     "search_type" => "QUERY_THEN_FETCH",\r\n     "took_millis" => "21",\r\n    "cluster.name" => "docker-cluster"\r\n}\n'})}),"\n",(0,n.jsx)(s.p,{children:"Now instead of removing all the lines that I don't need explicitly:"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-json",children:'mutate {\r\n    remove_field => ["level", "@timestamp", "path", "host", "@version" ...]\r\n  }\n'})}),"\n",(0,n.jsxs)(s.p,{children:["I want to ",(0,n.jsx)(s.a,{href:"https://www.elastic.co/guide/en/logstash/current/plugins-filters-prune.html",children:"use a whitelist to prune everything"})," BUT the ",(0,n.jsx)(s.code,{children:"source"})," field:"]}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-json",children:'input {\r\n    file {\r\n      path => "/usr/share/logstash/elasticsearch.log"\r\n      start_position => "beginning"\r\n      sincedb_path => "/dev/null"\r\n    }\r\n  }\r\n  filter {\r\n    json {\r\n        source => "message"\r\n    }\r\n    prune {\r\n          interpolate => false\r\n          whitelist_names => ["source"]\r\n    }\r\n    ruby {\r\n      code => "\r\n        # Recursively remove `nil`s and empty objects from event\r\n        def remove_nils!(hash)\r\n          hash.each do |k, v|\r\n            if v.nil?\r\n              hash.delete(k)\r\n            elsif v.is_a?(Hash)\r\n              if v.empty?\r\n                hash.delete(k)\r\n              else\r\n                result = remove_nils!(v)\r\n                if result.empty?\r\n                  hash.delete(k)\r\n                else\r\n                  hash[k] = result\r\n                end\r\n              end\r\n            end\r\n          end\r\n        end\r\n        event_hash = event.to_hash\r\n        event.cancel\r\n        # This one could be improved, stay tuned!\r\n        new_event_block.call(\r\n          LogStash::Event.new(\r\n            remove_nils!(event_hash)\r\n          )\r\n        )\r\n        "\r\n    }\r\n  }\r\n  output {\r\n     elasticsearch {\r\n       hosts => "http://localhost:9200"\r\n       index => "elasticsearch-log"\r\n    }\r\n  \r\n  stdout {}\r\n  \r\n  }\n'})}),"\n",(0,n.jsx)(s.p,{children:"To delete the indexed data and re-run run Logstash with the new configuration:"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-bash",children:"curl -XDELETE -u elastic:changeme http://localhost:9200/elasticsearch-log\n"})}),"\n",(0,n.jsxs)(s.p,{children:['Note that "nasty bit" in the configuration named ruby I added because the pruning process left me with a lot of empty ',(0,n.jsx)(s.code,{children:"{}"})," outputs. I wanted to remove them... but it is not exactly working the way I want. I will have to get back to this later. But the important result is a cleaned up output:"]}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-json",children:'{\r\n      "@version" => "1",\r\n      "@timestamp" => 2021-08-04T10:13:52.032Z,\r\n      "source" => "{\\"size\\":500,\\"query\\":{\\"bool\\":{\\"filter\\":[{\\"multi_match\\":{\\"query\\":\\"49.149.224.133\\",\\"fields\\":[],\\"type\\":\\"best_fields\\",\\"operator\\":\\"OR\\",\\"slop\\":0,\\"prefix_length\\":0,\\"max_expansions\\":50,\\"lenient\\":true,\\"zero_terms_query\\":\\"NONE\\",\\"auto_generate_synonyms_phrase_query\\":true,\\"fuzzy_transpositions\\":true,\\"boost\\":1.0}}],\\"adjust_pure_negative\\":true,\\"boost\\":1.0}},\\"version\\":true,\\"_source\\":false,\\"stored_fields\\":\\"*\\",\\"fields\\":[{\\"field\\":\\"*\\",\\"include_unmapped\\":true},{\\"field\\":\\"@timestamp\\",\\"format\\":\\"strict_date_optional_time\\"},{\\"field\\":\\"read_timestamp\\",\\"format\\":\\"strict_date_optional_time\\"}],\\"script_fields\\":{},\\"sort\\":[{\\"_score\\":{\\"order\\":\\"desc\\"}}],\\"track_total_hits\\":2147483647,\\"highlight\\":{\\"pre_tags\\":[\\"@kibana-highlighted-field@\\"],\\"post_tags\\":[\\"@/kibana-highlighted-field@\\"],\\"fragment_size\\":2147483647,\\"fields\\":{\\"*\\":{}}}}"\r\n}\n'})}),"\n",(0,n.jsx)(s.h2,{id:"disable-logging-of-all-searches",children:"Disable logging of all searches"}),"\n",(0,n.jsx)(s.p,{children:"To go back to the default configuration:"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-json",children:'PUT foobar/_settings\r\n{\r\n  "index.search.slowlog.threshold.query.trace": "500ms",\r\n  "index.search.slowlog.level": "info"\r\n}\n'})}),"\n",(0,n.jsx)(s.h2,{id:"logging-indexing-requests",children:"Logging Indexing Requests"}),"\n",(0,n.jsx)(s.p,{children:"This is less common but we can do the same with the indexing options. Enabling slowlog with full source (warning this can be heavy):"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-json",children:'PUT foobar/_settings\r\n{\r\n  "index.indexing.slowlog.threshold.index.trace": "0s",\r\n  "index.indexing.slowlog.level": "trace",\r\n  "index.indexing.slowlog.source": true\r\n}\n'})}),"\n",(0,n.jsx)(s.p,{children:"Getting back to the defaults:"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-json",children:'PUT foobar/_settings\r\n{\r\n  "index.indexing.slowlog.threshold.index.trace": "500ms",\r\n  "index.indexing.slowlog.level": "info",\r\n  "index.indexing.slowlog.source": "1000"\r\n}\n'})}),"\n",(0,n.jsx)(s.h2,{id:"working-on-the-live-system",children:"Working on the Live System"}),"\n",(0,n.jsx)(s.h3,{id:"activate-the-search-query-logging",children:"Activate the search query logging"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-json",children:'PUT index_2021_06_07/_settings\r\n{\r\n  "index.search.slowlog.threshold.query.trace": "0s",\r\n  "index.search.slowlog.level": "trace"\r\n}\n'})}),"\n",(0,n.jsx)(s.h3,{id:"check-that-the-logging-is-active",children:"Check that the logging is active"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-bash",children:"tail -f `docker inspect --format='{{.LogPath}}' elasticsearch_container`\n"})}),"\n",(0,n.jsxs)(s.p,{children:["This command (replace the container name with the name of your Elasticsearch container) will print out log entries as they are written to the Elasticsearch logfile. Now run a search from your web frontend - it should show up like this:  ",(0,n.jsx)(s.code,{children:'{"multi_match":{"query": "My Search Query",'})]}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-bash",children:'{"log":"{\\"type\\": \\"index_search_slowlog\\", \\"timestamp\\": \\"2021-08-04T14:02:06,307Z\\", \\"level\\": \\"TRACE\\", \\"component\\": \\"i.s.s.query\\", \\"cluster.name\\": \\"docker-cluster\\", \\"node.name\\": \\"9954f7648983\\", \\"message\\": \\"[index_2021_06_07][0]\\", \\"took\\": \\"1.6ms\\", \\"took_millis\\": \\"1\\", \\"total_hits\\": \\"272 hits\\", \\"types\\": \\"[\\\\\\"_doc\\\\\\"]\\", \\"stats\\": \\"[]\\", \\"search_type\\": \\"QUERY_THEN_FETCH\\", \\"total_shards\\": \\"1\\", \\"source\\": \\"{\\\\\\"from\\\\\\":0,\\\\\\"size\\\\\\":5,\\\\\\"query\\\\\\":{\\\\\\"bool\\\\\\":{\\\\\\"must\\\\\\":[{\\\\\\"bool\\\\\\":{\\\\\\"must\\\\\\":[{\\\\\\"bool\\\\\\":{\\\\\\"should\\\\\\":[{\\\\\\"multi_match\\\\\\":{\\\\\\"query\\\\\\":\\\\\\"Port Weiterleitung AVM Fritzbox\\\\\\",\\\\\\"fields\\\\\\":[\\\\\\"abstract^5.0\\\\\\",\\\\\\"description^3.0\\\\\\",\\\\\\"tags^10.0\\\\\\",\\\\\\"title^8.0\\\\\\"],\\\\\\"type\\\\\\":\\\\\\"best_fields\\\\\\",\\\\\\"operator\\\\\\":\\\\\\"OR\\\\\\",\\\\\\"slop\\\\\\":0,\\\\\\"fuzziness\\\\\\":\\\\\\"1\\\\\\",\\\\\\"prefix_length\\\\\\":0,\\\\\\"max_expansions\\\\\\":50,\\\\\\"zero_terms_query\\\\\\":\\\\\\"NONE\\\\\\",\\\\\\"auto_generate_synonyms_phrase_query\\\\\\":true,\\\\\\"fuzzy_transpositions\\\\\\":true,\\\\\\"boost\\\\\\":1.0}},{\\\\\\"multi_match\\\\\\":{\\\\\\"query\\\\\\":\\\\\\"Port Weiterleitung AVM Fritzbox\\\\\\",\\\\\\"fields\\\\\\":[\\\\\\"abstract^5.0\\\\\\",\\\\\\"description^3.0\\\\\\",\\\\\\"tags^10.0\\\\\\",\\\\\\"title^8.0\\\\\\"],\\\\\\"type\\\\\\":\\\\\\"phrase\\\\\\",\\\\\\"operator\\\\\\":\\\\\\"OR\\\\\\",\\\\\\"slop\\\\\\":0,\\\\\\"prefix_length\\\\\\":0,\\\\\\"max_expansions\\\\\\":50,\\\\\\"zero_terms_query\\\\\\":\\\\\\"NONE\\\\\\",\\\\\\"auto_generate_synonyms_phrase_query\\\\\\":true,\\\\\\"fuzzy_transpositions\\\\\\":true,\\\\\\"boost\\\\\\":1.0}}],\\\\\\"adjust_pure_negative\\\\\\":true,\\\\\\"minimum_should_match\\\\\\":\\\\\\"1\\\\\\",\\\\\\"boost\\\\\\":1.0}}],\\\\\\"adjust_pure_negative\\\\\\":true,\\\\\\"boost\\\\\\":1.0}}],\\\\\\"adjust_pure_negative\\\\\\":true,\\\\\\"boost\\\\\\":1.0}},\\\\\\"_source\\\\\\":{\\\\\\"includes\\\\\\":[\\\\\\"*\\\\\\"],\\\\\\"excludes\\\\\\":[]}}\\", \\"cluster.uuid\\": \\"1F9RN9roSXqf1sa2Sam7yQ\\", \\"node.id\\": \\"bFc0o2JrTBqG0zf9Wydh3g\\"  }\\n","stream":"stdout","time":"2021-08-04T14:02:06.311539637Z"}\n'})}),"\n",(0,n.jsx)(s.p,{children:"Now I am going to check the file location of my ES container log:"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-bash",children:"docker inspect --format='{{.LogPath}}' elasticsearch_container\r\n\r\n/var/lib/docker/containers/9954f76489836bf3def6cc49605f533439431dbcb5e06914eb457055fad54ebf/9954f76489836bf3def6cc49605f533439431dbcb5e06914eb457055fad54ebf-json.log                                       \n"})}),"\n",(0,n.jsx)(s.p,{children:"I copied the file and cleaned it up - removed all those annoying back-slashes! Also removed a few things trying to make the Log valid JSON... but what a mess those logs are! Really not happy with that."}),"\n",(0,n.jsx)(s.p,{children:"Let's run Logstash with a regular Structured Data / JSON configuration and see what happens:"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-json",children:'input {\r\n  file {\r\n    path => "/usr/share/logstash/wiki_elasticseach_2021-08-05.json"\r\n    start_position => "beginning"\r\n    sincedb_path => "/dev/null"\r\n  }\r\n}\r\nfilter {\r\n  json {\r\n      source => "message"\r\n  }\r\n}\r\noutput {\r\n   elasticsearch {\r\n     hosts => "http://localhost:9200"\r\n     index => "elasticsearch-log-json"\r\n  }\r\n\r\nstdout {}\r\n\r\n}\n'})}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-bash",children:'docker run \\\r\n   --name logstash \\\r\n   --net=host \\\r\n   --rm -it \\\r\n   -v /opt/logstash/pipeline/logstash.conf:/usr/share/logstash/pipeline/logstash.conf \\\r\n   -v /opt/logstash/wiki_elasticseach_2021-08-05.json:/usr/share/logstash/wiki_elasticseach_2021-08-05.json \\\r\n   -e "ELASTIC_HOST=localhost:9200" \\\r\n   -e "XPACK_SECURITY_ENABLED=false" \\\r\n   -e "XPACK_REPORTING_ENABLED=false" \\\r\n   -e "XPACK_MONITORING_ENABLED=false" \\\r\n   -e "XPACK_MONITORING_ELASTICSEARCH_USERNAME=elastic" \\\r\n   -e "XPACK_MONITORING_ELASTICSEARCH_PASSWORD=changeme" \\\r\n   logstash:7.13.4\n'})}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"Elasticeasrch Log Search Queries",src:t(483495).Z+"",width:"1489",height:"567"})}),"\n",(0,n.jsx)(s.p,{children:"To delete the indexed data:"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-bash",children:"curl -XDELETE -u elastic:changeme http://localhost:9200/elasticsearch-log-json\n"})}),"\n",(0,n.jsx)(s.p,{children:"I found the Docker Log hard to work with. I might try to activate the Elasticsearch SlowLog directly. Currently the ES instance logs to STDOUT and Docker writes it to file. So I end up with two structures wrapped around each other. Also the ES does not separate to different log files this way - everything is just one big blob of broken JSON."})]})}function d(e={}){const{wrapper:s}={...(0,r.ah)(),...e.components};return s?(0,n.jsx)(s,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}},603905:(e,s,t)=>{t.d(s,{ah:()=>c});var n=t(667294);function r(e,s,t){return s in e?Object.defineProperty(e,s,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[s]=t,e}function a(e,s){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);s&&(n=n.filter((function(s){return Object.getOwnPropertyDescriptor(e,s).enumerable}))),t.push.apply(t,n)}return t}function i(e){for(var s=1;s<arguments.length;s++){var t=null!=arguments[s]?arguments[s]:{};s%2?a(Object(t),!0).forEach((function(s){r(e,s,t[s])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(s){Object.defineProperty(e,s,Object.getOwnPropertyDescriptor(t,s))}))}return e}function o(e,s){if(null==e)return{};var t,n,r=function(e,s){if(null==e)return{};var t,n,r={},a=Object.keys(e);for(n=0;n<a.length;n++)t=a[n],s.indexOf(t)>=0||(r[t]=e[t]);return r}(e,s);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)t=a[n],s.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=n.createContext({}),c=function(e){var s=n.useContext(l),t=s;return e&&(t="function"==typeof e?e(s):i(i({},s),e)),t},h={inlineCode:"code",wrapper:function(e){var s=e.children;return n.createElement(n.Fragment,{},s)}},d=n.forwardRef((function(e,s){var t=e.components,r=e.mdxType,a=e.originalType,l=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),g=c(t),u=r,p=g["".concat(l,".").concat(u)]||g[u]||h[u]||a;return t?n.createElement(p,i(i({ref:s},d),{},{components:t})):n.createElement(p,i({ref:s},d))}));d.displayName="MDXCreateElement"},175936:(e,s,t)=>{t.d(s,{Z:()=>n});const n=t.p+"assets/images/Elasticeasrch_Log_Search_Queries_01-35eb76128679470ab70e4dbfc53c4b6b.png"},483495:(e,s,t)=>{t.d(s,{Z:()=>n});const n=t.p+"assets/images/Elasticeasrch_Log_Search_Queries_02-5a30444fe043feb66fe4c5cbf41bdcec.png"},758444:(e,s,t)=>{t.d(s,{Z:()=>n});const n=t.p+"assets/images/photo-456tdsfggd_67gfh6dgdf4_d-ce46b6b3b5abb2f099879ac2721568d8.jpg"}}]);