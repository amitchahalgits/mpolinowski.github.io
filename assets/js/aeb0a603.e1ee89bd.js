"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[99637],{552309:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>r,default:()=>_,frontMatter:()=>s,metadata:()=>l,toc:()=>c});var t=a(785893),i=a(603905);const s={sidebar_position:4920,slug:"2022-12-16",title:"Tensorflow Image Classification",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"The CIFAR-10 is a labeled subset of the 80 million tiny images dataset that can be directly downloaded using Keras."},r=void 0,l={id:"IoT-and-Machine-Learning/ML/2022-12-16-tf-cifar/index",title:"Tensorflow Image Classification",description:"The CIFAR-10 is a labeled subset of the 80 million tiny images dataset that can be directly downloaded using Keras.",source:"@site/docs/IoT-and-Machine-Learning/ML/2022-12-16-tf-cifar/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2022-12-16-tf-cifar",slug:"/IoT-and-Machine-Learning/ML/2022-12-16-tf-cifar/2022-12-16",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-16-tf-cifar/2022-12-16",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2022-12-16-tf-cifar/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Tensorflow",permalink:"/docs/tags/tensorflow"}],version:"current",sidebarPosition:4920,frontMatter:{sidebar_position:4920,slug:"2022-12-16",title:"Tensorflow Image Classification",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"The CIFAR-10 is a labeled subset of the 80 million tiny images dataset that can be directly downloaded using Keras."},sidebar:"tutorialSidebar",previous:{title:"Tensorflow Transfer Learning",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-18-tf-transfer-learning/2022-12-18"},next:{title:"Breast Histopathology Image Segmentation Part 6",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6/2022-12-12"}},o={},c=[{value:"Datasets",id:"datasets",level:2},{value:"CIFAR-10",id:"cifar-10",level:3},{value:"Building the Model",id:"building-the-model",level:2},{value:"Training the Model",id:"training-the-model",level:2},{value:"Evaluating the Model",id:"evaluating-the-model",level:2},{value:"Run Predictions",id:"run-predictions",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.ah)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Guangzhou, China",src:a(526644).Z+"",width:"1500",height:"383"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#datasets",children:"Datasets"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#cifar-10",children:"CIFAR-10"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#building-the-model",children:"Building the Model"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#training-the-model",children:"Training the Model"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#evaluating-the-model",children:"Evaluating the Model"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#run-predictions",children:"Run Predictions"})}),"\n"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://github.com/mpolinowski/tf-cifar",children:"Github"})}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"datasets",children:"Datasets"}),"\n",(0,t.jsx)(n.h3,{id:"cifar-10",children:"CIFAR-10"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.a,{href:"https://www.cs.toronto.edu/~kriz/cifar.html",children:"CIFAR-10"})," dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images."]}),"\n",(0,t.jsxs)(n.p,{children:["The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. You can download the ",(0,t.jsx)(n.a,{href:"https://github.com/keras-team/keras/tree/master/keras/datasets",children:"Keras dataset"})," by:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"from tensorflow.keras import datasets\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n"})}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:["Keras models and datasets will be saved to ",(0,t.jsx)(n.code,{children:"/home/myuser/.keras"})," on Linux."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# print sample images from dataset\nplt.figure(figsize=(10,10))\n\n# pick 25 images\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    # choose images from training set\n    plt.imshow(train_images[i], cmap=plt.cm.binary)\n    # use train class name as labels\n    plt.xlabel(class_names[train_labels[i][0]])\n\nplt.show()\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Tensorflow Tiny Image Classification",src:a(825117).Z+"",width:"1000",height:"982"})}),"\n",(0,t.jsx)(n.h2,{id:"building-the-model",children:"Building the Model"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"model = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32,32, 3)))\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\n\nmodel.summary()\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(10))\n\nmodel.summary()\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'Model: "sequential"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 30, 30, 32)        896       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n 2D)                                                             \n                                                                 \n conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n                                                                 \n=================================================================\nTotal params: 56,320\nTrainable params: 56,320\nNon-trainable params: 0\n_________________________________________________________________\nModel: "sequential"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 30, 30, 32)        896       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n 2D)                                                             \n                                                                 \n conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n                                                                 \n flatten (Flatten)           (None, 1024)              0         \n                                                                 \n dense (Dense)               (None, 64)                65600     \n                                                                 \n dense_1 (Dense)             (None, 10)                650       \n                                                                 \n=================================================================\nTotal params: 122,570\nTrainable params: 122,570\nNon-trainable params: 0\n_________________________________________________________________\n'})}),"\n",(0,t.jsx)(n.h2,{id:"training-the-model",children:"Training the Model"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:'# compile the model\nmodel.compile(optimizer=\'adam\',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\'accuracy\'])\n\n# saving checkpoints when there are improvements\nMCName = os.path.sep.join(["./checkpoints", "weights-{epoch:03d}-{val_loss:.4f}.hdf5"])\ncheckpoint = ModelCheckpoint(MCName, monitor="val_loss", mode="min", save_best_only=True, verbose=1)\ncallbacks = [checkpoint]\n\n# fit the model\nhistory = model.fit(train_images,\n                    train_labels,\n                    epochs=25,\n                    callbacks=callbacks,\n                    validation_data=(test_images, test_labels))\n'})}),"\n",(0,t.jsx)(n.h2,{id:"evaluating-the-model",children:"Evaluating the Model"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.5, 1])\nplt.legend(loc='lower right')\nplt.show()\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Tensorflow Tiny Image Classification",src:a(8329).Z+"",width:"640",height:"480"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"313/313 - 1s - loss: 1.6254 - accuracy: 0.6483 - 507ms/epoch - 2ms/step\n"})}),"\n",(0,t.jsx)(n.h2,{id:"run-predictions",children:"Run Predictions"}),"\n",(0,t.jsx)(n.p,{children:"We can now use the test dataset to verify our training:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# load cifar10\n# to use images from the test dataset for prediction\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\nprint(type(x_test))\nprint(type(y_test[0]))\n\n# cifar10 category label name\ncifar10_labels = np.array([\n    'airplane',\n    'automobile',\n    'bird',\n    'cat',\n    'deer',\n    'dog',\n    'frog',\n    'horse',\n    'ship',\n    'truck'])\n"})}),"\n",(0,t.jsx)(n.p,{children:"Use Keras to load the latest training checkpoint:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# load latest checkpoint\nmodel = load_model('checkpoints/weights-008-1.0783.hdf5')\n"})}),"\n",(0,t.jsx)(n.p,{children:"Tensorflow does not like the shape the images are in:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'ValueError: Input 0 of layer "sequential" is incompatible with the layer: expected shape=(None, 32, 32, 3), found shape=(1, 32, 3)\n'})}),"\n",(0,t.jsx)(n.p,{children:"We need to take care of that by providing the following function:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# prepare test image    \ndef convertCIFER10Data(image):\n    img = image.astype('float32')\n    c = np.zeros(32*32*3).reshape((1,32,32,3))\n    c[0] = img\n    return c\n"})}),"\n",(0,t.jsx)(n.p,{children:"Now we can run a prediction loop over a batch of test images:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:'# run prediction\nfor i in range(75):\n    # Random test image\n    index = random.randint(0, x_test.shape[0])\n    image = x_test[index]\n    data = convertCIFER10Data(image)\n\n    plt.subplot(10, 10, i+1)\n    plt.tight_layout()\n    plt.imshow(image)\n    plt.axis(\'off\')\n\n    ret = model.predict(data, batch_size=1)\n\n    bestnum = 0.0\n    bestclass = 0\n    for n in [0,1,2,3,4,5,6,7,8,9]:\n        if bestnum < ret[0][n]:\n            bestnum = ret[0][n]\n            bestclass = n\n\n    if y_test[index] == bestclass:\n        plt.title(cifar10_labels[bestclass], fontsize=10)\n        right += 1\n    else:\n        plt.title(cifar10_labels[bestclass] + "!=" + cifar10_labels[y_test[index][0]], color=\'#ff0000\', fontsize=10)\n        mistake += 1\n                                                                   \nplt.show()\nprint("Correct predictions:", right)\nprint("False predictions:", mistake)\nprint("Rate:", right/(mistake + right)*100, \'%\')\n'})}),"\n",(0,t.jsxs)(n.p,{children:["On average I am seeing correct detection rates of around ",(0,t.jsx)(n.code,{children:"60%"})," which is not so bad for a training run of 25 epochs:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"Correct predictions: 46\nFalse predictions: 29\nRate: 61.33333333333333 %\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Tensorflow Tiny Image Classification",src:a(188564).Z+"",width:"1600",height:"839"})})]})}function _(e={}){const{wrapper:n}={...(0,i.ah)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},603905:(e,n,a)=>{a.d(n,{ah:()=>c});var t=a(667294);function i(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function s(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function r(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?s(Object(a),!0).forEach((function(n){i(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):s(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function l(e,n){if(null==e)return{};var a,t,i=function(e,n){if(null==e)return{};var a,t,i={},s=Object.keys(e);for(t=0;t<s.length;t++)a=s[t],n.indexOf(a)>=0||(i[a]=e[a]);return i}(e,n);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(t=0;t<s.length;t++)a=s[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var o=t.createContext({}),c=function(e){var n=t.useContext(o),a=n;return e&&(a="function"==typeof e?e(n):r(r({},n),e)),a},d={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},_=t.forwardRef((function(e,n){var a=e.components,i=e.mdxType,s=e.originalType,o=e.parentName,_=l(e,["components","mdxType","originalType","parentName"]),h=c(a),p=i,m=h["".concat(o,".").concat(p)]||h[p]||d[p]||s;return a?t.createElement(m,r(r({ref:n},_),{},{components:a})):t.createElement(m,r({ref:n},_))}));_.displayName="MDXCreateElement"},526644:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-918471126c0472aad97358a725e1a399.jpg"},825117:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/tf-cifar_01-c8b96b1c42d8bf64757bcc51b4d7f5a9.png"},8329:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/tf-cifar_02-ce05ae2ddcf207158ee13aaf550e3b2a.png"},188564:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/tf-cifar_03-cd6424fce60957bc48c333098a317f00.png"}}]);