"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[38175],{416051:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>s,default:()=>h,frontMatter:()=>i,metadata:()=>c,toc:()=>l});var t=r(785893),a=r(603905);const i={sidebar_position:6080,slug:"2021-12-05",title:"OpenCV Face Detection and Privacy",authors:"mpolinowski",tags:["Machine Learning","Python","OpenCV"]},s=void 0,c={id:"IoT-and-Machine-Learning/ML/2021-12-05--opencv-face-detection/index",title:"OpenCV Face Detection and Privacy",description:"Shenzhen, China",source:"@site/docs/IoT-and-Machine-Learning/ML/2021-12-05--opencv-face-detection/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2021-12-05--opencv-face-detection",slug:"/IoT-and-Machine-Learning/ML/2021-12-05--opencv-face-detection/2021-12-05",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-12-05--opencv-face-detection/2021-12-05",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2021-12-05--opencv-face-detection/index.md",tags:[{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Python",permalink:"/docs/tags/python"},{label:"OpenCV",permalink:"/docs/tags/open-cv"}],version:"current",sidebarPosition:6080,frontMatter:{sidebar_position:6080,slug:"2021-12-05",title:"OpenCV Face Detection and Privacy",authors:"mpolinowski",tags:["Machine Learning","Python","OpenCV"]},sidebar:"tutorialSidebar",previous:{title:"OpenCV Object Tracking",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-12-06--opencv-object-tracking/2021-12-06"},next:{title:"OpenCV Image Objects",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-12-04--opencv-image-objects/2021-12-04"}},o={},l=[{value:"Prepare OpenCV",id:"prepare-opencv",level:2},{value:"OpenCV Face Detection",id:"opencv-face-detection",level:2},{value:"Structure",id:"structure",level:3},{value:"Privacy Masks",id:"privacy-masks",level:3},{value:"Gaussian Blur",id:"gaussian-blur",level:4},{value:"Pixelation",id:"pixelation",level:4},{value:"Applying Privacy Masks to Images",id:"applying-privacy-masks-to-images",level:3},{value:"Command Line Arguments",id:"command-line-arguments",level:4},{value:"Detect Faces",id:"detect-faces",level:4},{value:"Handling RTSP Livestreams",id:"handling-rtsp-livestreams",level:3}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.ah)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Shenzhen, China",src:r(867316).Z+"",width:"2385",height:"919"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#prepare-opencv",children:"Prepare OpenCV"})}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#opencv-face-detection",children:"OpenCV Face Detection"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#structure",children:"Structure"})}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#privacy-masks",children:"Privacy Masks"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#gaussian-blur",children:"Gaussian Blur"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#pixelation",children:"Pixelation"})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#applying-privacy-masks-to-images",children:"Applying Privacy Masks to Images"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#command-line-arguments",children:"Command Line Arguments"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#detect-faces",children:"Detect Faces"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#handling-rtsp-livestreams",children:"Handling RTSP Livestreams"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://github.com/mpolinowski/opencv-face-detection",children:"Github Repo"})}),"\n",(0,t.jsx)(n.h2,{id:"prepare-opencv",children:"Prepare OpenCV"}),"\n",(0,t.jsx)(n.p,{children:"Create and activate a virtual work environment:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"python -m venv .env\r\nsource .env/bin/activate\r\npython -m pip install --upgrade pip\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Add a file ",(0,t.jsx)(n.code,{children:"dependencies.txt"})," with all project ",(0,t.jsx)(n.strong,{children:"pip dependencies"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"opencv-python\r\nnumpy\r\nimutils\n"})}),"\n",(0,t.jsx)(n.p,{children:"Install all dependencies with:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install -r dependencies.txt\n"})}),"\n",(0,t.jsx)(n.h2,{id:"opencv-face-detection",children:"OpenCV Face Detection"}),"\n",(0,t.jsxs)(n.p,{children:["Based on ",(0,t.jsx)(n.a,{href:"https://www.pyimagesearch.com/2020/04/06/blur-and-anonymize-faces-with-opencv-and-python/",children:"Blur and anonymize faces with OpenCV and Python"})," by Adrian Rosebrock. Detect faces in images and either add a gaussian blur or pixelate them."]}),"\n",(0,t.jsx)(n.h3,{id:"structure",children:"Structure"}),"\n",(0,t.jsxs)(n.p,{children:["This project uses the ",(0,t.jsx)(n.a,{href:"https://github.com/spmallick/learnopencv/blob/master/FaceDetectionComparison/models/res10_300x300_ssd_iter_140000_fp16.caffemodel",children:"res10_300x300_ssd_iter_140000_fp16.caffemodel"})," from ",(0,t.jsx)(n.a,{href:"https://github.com/spmallick/learnopencv/",children:"@spmallick | Learn OpenCV"}),". Download the model and ",(0,t.jsx)(n.a,{href:"https://raw.githubusercontent.com/spmallick/learnopencv/master/FaceDetectionComparison/models/deploy.prototxt",children:"deploy.prototxt"})," file from ",(0,t.jsx)(n.a,{href:"https://github.com/spmallick/learnopencv/tree/master/FaceDetectionComparison/models",children:"Github"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"\u251c\u2500\u2500 blur_face.py\r\n\u251c\u2500\u2500 blur_face_video.py\r\n\u251c\u2500\u2500 dependencies.txt\r\n\u251c\u2500\u2500 face_detector\r\n\u2502\xa0\xa0 \u251c\u2500\u2500 deploy.prototxt\r\n\u2502\xa0\xa0 \u2514\u2500\u2500 res10_300x300_ssd_iter_140000_fp16.caffemodel\r\n\u251c\u2500\u2500 processed\r\n\u2502\xa0\xa0 \u2514\u2500\u2500 face_blur.jpg\r\n\u251c\u2500\u2500 pyimagesearch\r\n\u2502\xa0\xa0 \u251c\u2500\u2500 face_blurring.py\r\n\u2514\u2500\u2500 resources\r\n    \u2514\u2500\u2500 metro.jpg\n"})}),"\n",(0,t.jsx)(n.h3,{id:"privacy-masks",children:"Privacy Masks"}),"\n",(0,t.jsxs)(n.p,{children:["The scripts ",(0,t.jsx)(n.code,{children:"blur_face.py"})," and ",(0,t.jsx)(n.code,{children:"blur_face_video.py"})," help us to detect faces and then perform face blurring in images and video streams. The ",(0,t.jsx)(n.code,{children:"pyimagesearch/face_blurring.py"})," script provides two helper functions:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"anonymize_face_simple"}),": Applies a simple Gaussian blur on the face ROI"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"anonymize_face_pixelate"}),": Creates a pixelated overlay on the ROI"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"gaussian-blur",children:"Gaussian Blur"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"anonymize_face_simple"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import numpy as np\r\nimport cv2\r\n\r\ndef anonymize_face_simple(image, factor=3.0):\r\n\t# automatically determine the size of the blurring kernel based\r\n\t# on the spatial dimensions of the input image\r\n\t(h, w) = image.shape[:2]\r\n\tkW = int(w / factor)\r\n\tkH = int(h / factor)\r\n\t# ensure the width of the kernel is odd\r\n\tif kW % 2 == 0:\r\n\t\tkW -= 1\r\n\t# ensure the height of the kernel is odd\r\n\tif kH % 2 == 0:\r\n\t\tkH -= 1\r\n\t# apply a Gaussian blur to the input image using our computed\r\n\t# kernel size\r\n\treturn cv2.GaussianBlur(image, (kW, kH), 0)\n"})}),"\n",(0,t.jsxs)(n.p,{children:["The helper function calculates the blurring kernel\u2019s width and height as a function of the input image dimensions. Gaussian Kernel Size: ",(0,t.jsx)(n.code,{children:"[height width]"})," - height and width should be odd. The number represents the number of neighboring pixels a central pixel is averaged with. If the pixel is white and surrounded by red pixels, it will take on a reddish tint after being averaged with them - the image looses it contours and will start to look blurry:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Larger kernel size -> pixels are averaged with more of their surrounding -> resulting in more blur"}),"\n",(0,t.jsx)(n.li,{children:"Smaller kernel size -> pixels are averaged with less of their surrounding -> resulting in less blur"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Increasing the factor"})," will therefore ",(0,t.jsx)(n.strong,{children:"increase the amount of blur"})," applied to the face."]}),"\n",(0,t.jsx)(n.p,{children:"The helper function increases the amount of blur applied the bigger the ROI becomes - making sure that the face remain unrecognizable even when working with huge image files."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"OpenCV Face Detection and Privacy",src:r(352908).Z+"",width:"1428",height:"548"})}),"\n",(0,t.jsx)(n.h4,{id:"pixelation",children:"Pixelation"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"anonymize_face_pixelate"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def anonymize_face_pixelate(image, blocks=3):\r\n\t# divide the input image into NxN blocks\r\n\t(h, w) = image.shape[:2]\r\n\txSteps = np.linspace(0, w, blocks + 1, dtype="int")\r\n\tySteps = np.linspace(0, h, blocks + 1, dtype="int")\r\n\t# loop over the blocks in both the x and y direction\r\n\tfor i in range(1, len(ySteps)):\r\n\t\tfor j in range(1, len(xSteps)):\r\n\t\t\t# compute the starting and ending (x, y)-coordinates\r\n\t\t\t# for the current block\r\n\t\t\tstartX = xSteps[j - 1]\r\n\t\t\tstartY = ySteps[i - 1]\r\n\t\t\tendX = xSteps[j]\r\n\t\t\tendY = ySteps[i]\r\n\t\t\t# extract the ROI using NumPy array slicing, compute the\r\n\t\t\t# mean of the ROI, and then draw a rectangle with the\r\n\t\t\t# mean RGB values over the ROI in the original image\r\n\t\t\troi = image[startY:endY, startX:endX]\r\n\t\t\t(B, G, R) = [int(x) for x in cv2.mean(roi)[:3]]\r\n\t\t\tcv2.rectangle(image, (startX, startY), (endX, endY),\r\n\t\t\t\t(B, G, R), -1)\r\n\t# return the pixelated blurred image\r\n\treturn image\n'})}),"\n",(0,t.jsx)(n.p,{children:"The helper script takes the input image and divides it into blocks and computes the mean RGB pixel intensities for the ROI."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"OpenCV Face Detection and Privacy",src:r(627145).Z+"",width:"1419",height:"539"})}),"\n",(0,t.jsx)(n.h3,{id:"applying-privacy-masks-to-images",children:"Applying Privacy Masks to Images"}),"\n",(0,t.jsx)(n.h4,{id:"command-line-arguments",children:"Command Line Arguments"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"blur_face.py"})}),"\n",(0,t.jsxs)(n.p,{children:["I want to be able to use command line flags to control the application - instead of hard-coding values in. To parse these flags I need to import the library ",(0,t.jsx)(n.code,{children:"argparse"})," and define the parameter I want to use:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Parse the arguments\r\nap = argparse.ArgumentParser()\r\nap.add_argument("-i", "--image", required=True, help="path to image")\r\nap.add_argument("-f", "--face", required=True, help="path to detector model")\r\nap.add_argument("-m", "--method", type=str, default="simple", choices=["simple", "pixelated"], help="face blurring method")\r\nap.add_argument("-b", "--blocks", type=int, default=12, help="number of pixel blocks for pixelate")\r\nap.add_argument("-c", "--confidence", type=float, default=0.5, help="minimum probability of positive detection")\r\nargs = vars(ap.parse_args())\n'})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"--image"}),": Relative path to the image I want to mask."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"--face"}),": Relative Path to your face detector model directory."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"--method"}),": Use gaussian blur or pixelate."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"--blocks"}),": Number of blocks used to pixelate. Smaller numbers make the ROI less recognizable."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"--confidence"}),": Lower confidence will catch more faces but might result in false positives."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Usage"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"python blur_face.py --image resources/metro.jpg --face face_detector --method simple"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"python blur_face.py --image resources/metro.jpg --face face_detector --method pixelated"})}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"detect-faces",children:"Detect Faces"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Load serialized face detector from disk\r\nprint("[INFO] loading face detector model")\r\nprototxtPath = os.path.sep.join([args["face"], "deploy.prototxt"])\r\nweightsPath = os.path.sep.join([args["face"], "res10_300x300_ssd_iter_140000_fp16.caffemodel"])\r\nnet = cv2.dnn.readNet(prototxtPath, weightsPath)\r\n\r\n# Copy resource image and get dimensions\r\nsource_image = cv2.imread(args["image"])\r\nimage = source_image.copy()\r\n(h, w) = image.shape[:2]\r\n\r\n# Get blob from image\r\nblob = cv2.dnn.blobFromImage(source_image, 1.0, (300, 300), (104.0, 177.0, 123.0))\r\n\r\n# Get face detections from blob\r\nprint("[INFO] computing face detections")\r\nnet.setInput(blob)\r\ndetections = net.forward()\n'})}),"\n",(0,t.jsx)(n.p,{children:"The neural network expects expects a blob from our input image:"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size, mean, swapRB=True)"})}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"A blob is just a image with the same spatial dimensions (i.e., width and height), same depth (number of channels), that have all be preprocessed in the same manner."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"image"})," : This is the input image we want to preprocess before passing it through our deep neural network for classification."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"scalefactor"})," : After we perform mean subtraction we can optionally scale our images by some factor. This value defaults to ",(0,t.jsx)(n.code,{children:"1.0"})," (i.e., no scaling)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"size"})," : Here we supply the spatial size that the ",(0,t.jsx)(n.strong,{children:"Convolutional Neural Network"})," expects. You usually get this value from the name of the model you are importing - above it is ",(0,t.jsx)(n.code,{children:"300x300"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"mean"})," : These are our mean subtraction values. They can be a 3-tuple of the RGB means or they can be a single value in which case the supplied value is subtracted from every channel of the image. If you\u2019re performing mean subtraction, ensure you supply the 3-tuple in ",(0,t.jsx)(n.code,{children:"(R, G, B)"})," order, especially when utilizing the default behavior of swapRB=True ."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"swapRB"})," : OpenCV assumes images are in BGR channel order; however, the ",(0,t.jsx)(n.code,{children:"mean"})," value assumes we are using RGB order. To resolve this discrepancy we can swap the R and B channels in image by setting this value to ",(0,t.jsx)(n.code,{children:"True"}),". By default OpenCV performs this channel swapping for us."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Loop over detections\r\nfor i in range(0, detections.shape[2]):\r\n    # extract confidence from detections\r\n    confidence = detections[0, 0, i, 2]\r\n    # filter by min confidence\r\n    if confidence > args["confidence"]:\r\n        # compute bounding box for\r\n        # passing detections\r\n        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\r\n        (startX, startY, endX, endY) = box.astype("int")\r\n        # extract face ROI\r\n        face = source_image[startY:endY, startX:endX]\r\n        # select blur method\r\n        if args["method"] == "simple":\r\n            face = anonymize_face_simple(face, factor=3.0)\r\n        else:\r\n            face = anonymize_face_pixelate(face, blocks=args["blocks"])\r\n\r\n        # store blurred face in output image\r\n        source_image[startY:endY, startX:endX] = face\n'})}),"\n",(0,t.jsx)(n.p,{children:"This part loops over all detected ROI's and - if they pass the confidence level - apply either the gaussian blur or pixelate them."}),"\n",(0,t.jsx)(n.h3,{id:"handling-rtsp-livestreams",children:"Handling RTSP Livestreams"}),"\n",(0,t.jsx)(n.p,{children:"To use RTSP streams instead of a static image I will need another CLI argument for the live stream URL:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Parse the arguments\r\nap = argparse.ArgumentParser()\r\nap.add_argument("-u", "--url", help="RTSP streaming URL", default="rtsp://admin:instar@192.168.2.19/livestream/13")\r\nap.add_argument("-f", "--face", required=True, help="path to detector model")\r\nap.add_argument("-m", "--method", type=str, default="simple", choices=["simple", "pixelated"], help="face blurring "\r\n                                                                                                    "method")\r\nap.add_argument("-b", "--blocks", type=int, default=20, help="number of pixel blocks for pixelate")\r\nap.add_argument("-c", "--confidence", type=float, default=0.5, help="minimum probability of positive detection")\r\nargs = vars(ap.parse_args())\n'})}),"\n",(0,t.jsx)(n.p,{children:"Load the detection model as before:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# load our serialized face detector model from disk\r\nprint("[INFO] loading face detector model...")\r\nprototxtPath = os.path.sep.join([args["face"], "deploy.prototxt"])\r\nweightsPath = os.path.sep.join([args["face"],\r\n                                "res10_300x300_ssd_iter_140000_fp16.caffemodel"])\r\nnet = cv2.dnn.readNet(prototxtPath, weightsPath)\n'})}),"\n",(0,t.jsxs)(n.p,{children:["And then use the ",(0,t.jsx)(n.code,{children:"imutils"})," library to grab the video stream:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Get video stream from IP camera\r\nprint("[INFO] starting video stream")\r\nvs = VideoStream(args["url"]).start()\n'})}),"\n",(0,t.jsx)(n.p,{children:"The detection part is pretty much the same. With one exception that I now need to read single frames out of the stream and loop the detection over it:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Loop over the frames from the video stream\r\nwhile True:\r\n    # grab the frame from the threaded video stream and resize it\r\n    # to have a maximum width of 1080 pixels\r\n    frame = vs.read()\r\n    frame = imutils.resize(frame, width=1080)\r\n    # grab the dimensions of the frame and then construct a blob\r\n    # from it\r\n    (h, w) = frame.shape[:2]\r\n    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), (104.0, 177.0, 123.0))\r\n    # pass the blob through the network and obtain the face detections\r\n    net.setInput(blob)\r\n    detections = net.forward()\r\n    # loop over the detections\r\n    for i in range(0, detections.shape[2]):\r\n        # extract the confidence (i.e., probability) associated with\r\n        # the detection\r\n        confidence = detections[0, 0, i, 2]\r\n        # filter out weak detections by ensuring the confidence is\r\n        # greater than the minimum confidence\r\n        if confidence > args["confidence"]:\r\n            # compute the (x, y)-coordinates of the bounding box for\r\n            # the object\r\n            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\r\n            (startX, startY, endX, endY) = box.astype("int")\r\n            # extract the face ROI\r\n            face = frame[startY:endY, startX:endX]\r\n            # check to see if we are applying the "simple" face\r\n            # blurring method\r\n            if args["method"] == "simple":\r\n                face = anonymize_face_simple(face, factor=3.0)\r\n            # otherwise, we must be applying the "pixelated" face\r\n            # anonymization method\r\n            else:\r\n                face = anonymize_face_pixelate(face,\r\n                                               blocks=args["blocks"])\r\n            # store the blurred face in the output image\r\n            frame[startY:endY, startX:endX] = face\r\n\r\n        # show the output frame\r\n        cv2.imshow("Frame", frame)\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"USAGE"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"python blur_face_video.py --url 'rtsp://admin:instar@192.168.2.19/livestream/12' --face face_detector --method simple"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"python blur_face_video.py --url 'rtsp://admin:instar@192.168.2.19/livestream/11' --face face_detector --method pixelated"})}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"OpenCV Face Detection and Privacy",src:r(857211).Z+"",width:"537",height:"340"})})]})}function h(e={}){const{wrapper:n}={...(0,a.ah)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},603905:(e,n,r)=>{r.d(n,{ah:()=>l});var t=r(667294);function a(e,n,r){return n in e?Object.defineProperty(e,n,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[n]=r,e}function i(e,n){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),r.push.apply(r,t)}return r}function s(e){for(var n=1;n<arguments.length;n++){var r=null!=arguments[n]?arguments[n]:{};n%2?i(Object(r),!0).forEach((function(n){a(e,n,r[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(r,n))}))}return e}function c(e,n){if(null==e)return{};var r,t,a=function(e,n){if(null==e)return{};var r,t,a={},i=Object.keys(e);for(t=0;t<i.length;t++)r=i[t],n.indexOf(r)>=0||(a[r]=e[r]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)r=i[t],n.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var o=t.createContext({}),l=function(e){var n=t.useContext(o),r=n;return e&&(r="function"==typeof e?e(n):s(s({},n),e)),r},d={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},h=t.forwardRef((function(e,n){var r=e.components,a=e.mdxType,i=e.originalType,o=e.parentName,h=c(e,["components","mdxType","originalType","parentName"]),p=l(r),m=a,u=p["".concat(o,".").concat(m)]||p[m]||d[m]||i;return r?t.createElement(u,s(s({ref:n},h),{},{components:r})):t.createElement(u,s({ref:n},h))}));h.displayName="MDXCreateElement"},352908:(e,n,r)=>{r.d(n,{Z:()=>t});const t=r.p+"assets/images/OpenCV_Face_Detection_01-6b45e78c2fdc12c2dd7ef4cdbdd78308.png"},627145:(e,n,r)=>{r.d(n,{Z:()=>t});const t=r.p+"assets/images/OpenCV_Face_Detection_02-4ff6f8a1d0aa0bf1e6f65a9ffbe25a10.png"},867316:(e,n,r)=>{r.d(n,{Z:()=>t});const t=r.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-5a0b68587d9242bbb46a1f1aaab44216.jpg"},857211:(e,n,r)=>{r.d(n,{Z:()=>t});const t=r.p+"assets/images/realtime_pixelation-13f81b7a0081f7eff2ad4ec7d0edc669.gif"}}]);