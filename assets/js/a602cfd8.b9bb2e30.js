"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[43939],{155374:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>l,toc:()=>c});var a=t(785893),i=t(603905);const s={sidebar_position:4980,slug:"2022-12-10",title:"Breast Histopathology Image Segmentation Part 1",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"Data Inspection and Pre-processing"},r=void 0,l={id:"IoT-and-Machine-Learning/ML/2022-12-10-tf-breast-cancer-classification-part1/index",title:"Breast Histopathology Image Segmentation Part 1",description:"Data Inspection and Pre-processing",source:"@site/docs/IoT-and-Machine-Learning/ML/2022-12-10-tf-breast-cancer-classification-part1/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2022-12-10-tf-breast-cancer-classification-part1",slug:"/IoT-and-Machine-Learning/ML/2022-12-10-tf-breast-cancer-classification-part1/2022-12-10",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-10-tf-breast-cancer-classification-part1/2022-12-10",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2022-12-10-tf-breast-cancer-classification-part1/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Tensorflow",permalink:"/docs/tags/tensorflow"}],version:"current",sidebarPosition:4980,frontMatter:{sidebar_position:4980,slug:"2022-12-10",title:"Breast Histopathology Image Segmentation Part 1",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"Data Inspection and Pre-processing"},sidebar:"tutorialSidebar",previous:{title:"Breast Histopathology Image Segmentation Part 2",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part2/2022-12-11"},next:{title:"Deep Docker on Arch",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-11-27-containerized-deep-learning/2022-11-27"}},o={},c=[{value:"Data Preprocessing",id:"data-preprocessing",level:2},{value:"Create the Training, Test and Validation Set",id:"create-the-training-test-and-validation-set",level:3},{value:"Custom Model Building and Evaluation",id:"custom-model-building-and-evaluation",level:2},{value:"Requirements",id:"requirements",level:3},{value:"Exploring the Dataset",id:"exploring-the-dataset",level:3},{value:"Distribution",id:"distribution",level:4},{value:"Sample Images",id:"sample-images",level:4}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.ah)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Guangzhou, China",src:t(661127).Z+"",width:"1500",height:"383"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-10-tf-breast-cancer-classification-part1/2022-12-10",children:"Part 1: Data Inspection and Pre-processing"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part2/2022-12-11",children:"Part 2: Weights, Data Augmentations and Generators"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part3/2022-12-11",children:"Part 3: Model creation based on a pre-trained and a custom model"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part4/2022-12-11",children:"Part 4: Train our model to fit the dataset"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part5/2022-12-12",children:"Part 5: Evaluate the performance of your trained model"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6/2022-12-12",children:"Part 6: Running Predictions"})}),"\n"]}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://github.com/mpolinowski/tf-bc-classification",children:"Github"})}),"\n"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"#data-preprocessing",children:"Data Preprocessing"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#create-the-training-test-and-validation-set",children:"Create the Training, Test and Validation Set"})}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"#custom-model-building-and-evaluation",children:"Custom Model Building and Evaluation"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#requirements",children:"Requirements"})}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"#exploring-the-dataset",children:"Exploring the Dataset"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#distribution",children:"Distribution"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#sample-images",children:"Sample Images"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:["Based on ",(0,a.jsx)(n.a,{href:"https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images",children:"Breast Histopathology Images"})," by ",(0,a.jsx)(n.a,{href:"https://www.kaggle.com/paultimothymooney",children:"Paul Mooney"}),".\n",(0,a.jsx)(n.code,{children:"Invasive Ductal Carcinoma (IDC) is the most common subtype of all breast cancers. To assign an aggressiveness grade to a whole mount sample, pathologists typically focus on the regions which contain the IDC. As a result, one of the common pre-processing steps for automatic aggressiveness grading is to delineate the exact regions of IDC inside of a whole mount slide."}),"\n",(0,a.jsx)(n.a,{href:"https://youtu.be/8XsiMQQ-4mM",children:"Can recurring breast cancer be spotted with AI tech? - BBC News"})]}),"\n"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Citation: ",(0,a.jsx)(n.a,{href:"https://pubmed.ncbi.nlm.nih.gov/27563488/",children:"Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases"})]}),"\n",(0,a.jsx)(n.li,{children:"Dataset: 198,738 IDC(negative) image patches; 78,786 IDC(positive) image patches"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"data-preprocessing",children:"Data Preprocessing"}),"\n",(0,a.jsxs)(n.p,{children:["After ",(0,a.jsx)(n.a,{href:"https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images",children:"downloading"})," the source photographs unzip them into a folder ",(0,a.jsx)(n.code,{children:"./dataset/raw"}),". Now we need to process this data and process it for the model training."]}),"\n",(0,a.jsx)(n.h3,{id:"create-the-training-test-and-validation-set",children:"Create the Training, Test and Validation Set"}),"\n",(0,a.jsx)(n.p,{children:"The configuration file defines the location where data will be stored as well as setting the parameter for the model training:"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.em,{children:"utils/config.py"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-py",children:'import os\n\n# Project location\nPROJECT_PATH = "./"\n# Input dataset directory\nRAW_INPUT_PATH = "./dataset/raw"\n# Result dataset directory\nSPLIT_INPUT_PATH = "./dataset/split"\n# Output directory\nOUTPUT_PATH = "./output"\n\n# Training testing, validation\nTRAIN_PATH = os.path.sep.join([SPLIT_INPUT_PATH, "training"])\nVAL_PATH = os.path.sep.join([SPLIT_INPUT_PATH, "validation"])\nTEST_PATH = os.path.sep.join([SPLIT_INPUT_PATH, "testing"])\n\n# Data splitting\nTRAIN_SPLIT = 0.8\nVAL_SPLIT = 0.1\n\n# Parameters\nCLASSES = ["benign","malignant"]\nBATCH_SIZE = 32\nINIT_LR = 1e-4\nEPOCHS = 20\n\n# Path to serialized model after training\nMODEL_PATH = os.path.sep.join([OUTPUT_PATH, "CarcinomaPrediction.model"])\n\n# Path to training history plots\nMODEL_PATH = os.path.sep.join([OUTPUT_PATH, "TrainingHistory.png"])\n'})}),"\n",(0,a.jsx)(n.p,{children:"First we need to split the raw dataset into training and validation set, according to the split ratio set above:"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.em,{children:"utils/create_dataset.py"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-py",children:'import config\nimport getPaths\nimport shutil\nimport random\nimport os\n\n\n# Get content of the original input directory and shuffle images\nallImagePaths = list(getPaths.listImages(config.RAW_INPUT_PATH))\nrandom.seed(42)\nrandom.shuffle(allImagePaths)\n\n# Only take 10% of the images to speed up the process\nprint(len(allImagePaths))\nimagePaths = allImagePaths[0:20000]\nprint(len(allImagePaths))\n\n# Split into training and testing data\ni = int(len(allImagePaths) * config.TRAIN_SPLIT)\ntrainPaths = allImagePaths[:i]\ntestPaths = allImagePaths[i:]\n\n# Separate validation split from training data\ni = int(len(trainPaths) * config.VAL_SPLIT)\nvalPaths = trainPaths[:i]\ntrainPaths = trainPaths[i:]\n\n# Defining the datasets which will be build in the result folder\ndatasets = [\n    ("training", config.TRAIN_PATH, trainPaths),\n    ("validation", config.VAL_PATH, valPaths),\n    ("testing", config.TEST_PATH, testPaths)\n]\n\n# Copy images from the initial into the result path\n# while splitting them into train, validation and test data\nfor (dSType, basePath, allImagePaths) in datasets:\n    print("Making \'{}\' split".format(dSType))\n    if not os.path.exists(basePath):\n        print("\'Creating {}\' directory".format(basePath))\n        os.makedirs(basePath)\n    # Looping over the image paths\n    for inputPath in allImagePaths:\n        # Extracting the filename of the input image\n        # Extracting class label ("0" for "Benign" and "1" for "Malignant")\n        filename = inputPath.split(os.path.sep)[-1]\n        label = filename[-5:-4]\n        # Making the path to form the label directory\n        labelPath = os.path.sep.join([basePath, label])\n        if not os.path.exists(labelPath):\n            print("\'creating {}\' directory".format(labelPath))\n            os.makedirs(labelPath)\n        # Creating the path to the destination image and then copying it\n        finalPath = os.path.sep.join([labelPath, filename])\n        shutil.copy2(inputPath, finalPath)\n'})}),"\n",(0,a.jsxs)(n.p,{children:["This file imports a helper function ",(0,a.jsx)(n.strong,{children:"getPaths"})," that helps us decide what files inside the dataset we actually want to use based on their file extension:"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.em,{children:"utils/getPaths.py"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'import os\n\n# Defining image types you want to allow\nimageTypes = (".jpg", ".jpeg", ".png", ".tif", ".tiff", ".bmp")\n\n\ndef listImages(basePath, contains=None):\n    return listFiles(basePath, validExts=imageTypes, contains=contains)\n\n\ndef listFiles(basePath, validExts=None, contains=None):\n    for (baseDir, dirNames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            # Get all files in filename / ignore empty directories\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # Extracting the file extension\n            fileExt = filename[filename.rfind("."):].lower()\n\n            # Only process files that are of imageTypes\n            if validExts is None or fileExt.endswith(validExts):\n                # Construct the path to the image and yield it\n                imagePath = os.path.join(baseDir, filename)\n                yield imagePath\n'})}),"\n",(0,a.jsx)(n.p,{children:"Running the script:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"python ./utils/create_dataset.py\nMaking 'training' split\n'Creating ./dataset/split/training' directory\n'creating ./dataset/split/training/0' directory\n'creating ./dataset/split/training/1' directory\nMaking 'validation' split\n'Creating ./dataset/split/validation' directory\n'creating ./dataset/split/validation/0' directory\n'creating ./dataset/split/validation/1' directory\nMaking 'testing' split\n'Creating ./dataset/split/testing' directory\n'creating ./dataset/split/testing/0' directory\n'creating ./dataset/split/testing/1' directory\n"})}),"\n",(0,a.jsxs)(n.p,{children:["will now go through the raw images in ",(0,a.jsx)(n.code,{children:"./dataset/raw"})," and split them into the ",(0,a.jsx)(n.code,{children:"./dataset/split"})," folders:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:".\n\u251c\u2500\u2500 raw\n\u251c\u2500\u2500 split\n\u2502\xa0\xa0 \u251c\u2500\u2500 testing\n|   \u2502\xa0\xa0 \u251c\u2500\u2500 0\n|   \u2502\xa0\xa0 \u2514\u2500\u2500 1\n\u2502\xa0\xa0 \u251c\u2500\u2500 training\n|   \u2502\xa0\xa0 \u251c\u2500\u2500 0\n|   \u2502\xa0\xa0 \u2514\u2500\u2500 1\n\u2502\xa0\xa0 \u2514\u2500\u2500 validation\n|   \u2502\xa0\xa0 \u251c\u2500\u2500 0\n|   \u2502\xa0\xa0 \u2514\u2500\u2500 1\n"})}),"\n",(0,a.jsxs)(n.p,{children:["We will get a ",(0,a.jsx)(n.code,{children:"80%"})," training, ",(0,a.jsx)(n.code,{children:"10%"})," testing and ",(0,a.jsx)(n.code,{children:"10%"})," validation split. And based on the file names the images will be separated into ",(0,a.jsx)(n.em,{children:"benign"})," ",(0,a.jsx)(n.code,{children:"0"})," and ",(0,a.jsx)(n.em,{children:"malignant"})," ",(0,a.jsx)(n.code,{children:"1"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"custom-model-building-and-evaluation",children:"Custom Model Building and Evaluation"}),"\n",(0,a.jsx)(n.h3,{id:"requirements",children:"Requirements"}),"\n",(0,a.jsxs)(n.p,{children:["Install the dependencies using ",(0,a.jsx)(n.code,{children:"pip"})," or ",(0,a.jsx)(n.a,{href:"/docs/Development/Python/2022-12-11-pipenv/2022-12-11",children:"pipenv"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pipenv install tensorflow-gpu scikit-learn matplotlib numpy pandas seaborn opencv-python\n"})}),"\n",(0,a.jsx)(n.p,{children:"or"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install -r requirements.txt --upgrade\n"})}),"\n",(0,a.jsx)(n.h3,{id:"exploring-the-dataset",children:"Exploring the Dataset"}),"\n",(0,a.jsx)(n.h4,{id:"distribution",children:"Distribution"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.em,{children:"utils/plotDistribution.py"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-py",children:"# Plotting the count of images within each segment in a directories\ndef plotData(dirPath):\n    # Get the path to the benign and malignant sub-directories\n    benign_cases_dir = dirPath+ '/0/'\n    malignant_cases_dir = dirPath + '/1/'\n\n    # Get the list of all the images from those paths\n    benign_cases = glob.glob(benign_cases_dir + '*.png')\n    malignant_cases = glob.glob(malignant_cases_dir + '*.png')\n\n    # An empty list\n    data1 = []\n\n    # Add all benign cases to list with label `0`\n    for img in benign_cases:\n        data1.append((img,0))\n\n    # Add all benign cases to list with label `1`\n    for img in malignant_cases:\n        data1.append((img, 1))\n\n    # data => pandas dataframe\n    data1 = pd.DataFrame(data1, columns=['image', 'label'],index=None)\n\n    # Shuffle the data \n    data1 = data1.sample(frac=1.).reset_index(drop=True)\n    \n    # Get the counts for each segment\n    cases_count = data1['label'].value_counts()\n    print(cases_count)\n\n    # Plot the results \n    plt.figure(figsize=(10,8))\n    sns.barplot(x=cases_count.index, y= cases_count.values)\n    plt.title('Number of cases', fontsize=14)\n    plt.xlabel('Case type', fontsize=12)\n    plt.ylabel('Count', fontsize=12)\n    plt.xticks(range(len(cases_count.index)), ['benign(0)', 'malignant(1)'])\n    plt.show()\n\ntry:\n    function = sys.argv[1]\n    globals()[function]()\nexcept IndexError:\n    raise Exception(\"Please provide function name\")\nexcept KeyError:\n    raise Exception(\"Function {} hasn't been found\".format(function))\n"})}),"\n",(0,a.jsxs)(n.p,{children:["By importing the configuration file ",(0,a.jsx)(n.code,{children:"from utils import config"})," we can now print out the distribution of benign and malignant cases in all three folders by running the following commands after adding a little helper function:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-py",children:'def exploreData():\n    plotData(config.TRAIN_PATH,"Training Path")\n    # plotData(config.VAL_PATH,"Validation Path")\n    # plotData(config.TEST_PATH,"Test Path")\n'})}),"\n",(0,a.jsx)(n.p,{children:"The function can be executed directly:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pipenv run python ./utils/plotDistribution.py exploreData\n"})}),"\n",(0,a.jsx)(n.p,{children:"We can see that we are working with a skewed dataset that is heavy on benign cases. But the splitting of images into the three paths was successful - we did not change the distribution:"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Breast Histopathology Image Segmentation",src:t(423273).Z+"",width:"2389",height:"635"})}),"\n",(0,a.jsx)(n.h4,{id:"sample-images",children:"Sample Images"}),"\n",(0,a.jsx)(n.p,{children:"Now we can inspect our data by printing a sample of each class:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-py",children:"# Get the path to the benign and malignant sub-directories\nbenign_cases_dir = config.TRAIN_PATH + '/0/'\nmalignant_cases_dir = config.TRAIN_PATH + '/1/'\n\n# Get the list of all the images from those paths\nbenign_cases = glob.glob(benign_cases_dir + '*.png')\nmalignant_cases = glob.glob(malignant_cases_dir + '*.png')\n\n# An empty list\ntrain_data1 = []\n\n## Add all benign cases to list with label `0`\nfor img in benign_cases:\n    train_data1.append((img,0))\n\n# Go through all the malignant cases. The label for these cases will be 1\nfor img in malignant_cases:\n    train_data1.append((img, 1))\n\n# Add all benign cases to list with label `1`\ntrain_data1 = pd.DataFrame(train_data1, columns=['image', 'label'],index=None)\n\n# Get first 5 images for both classes\nmalignant_samples = (train_data1[train_data1['label']==1]['image'].iloc[:5]).tolist()\nbenign_samples = (train_data1[train_data1['label']==0]['image'].iloc[:5]).tolist()\n\n# Concat the data in a single list and del the above two list\nsamples = malignant_samples + benign_samples\ndel malignant_samples, benign_samples\n\n# Plot the data \nf, ax = plt.subplots(2,5, figsize=(30,10))\nfor i in range(10):\n    img = cv2.imread(samples[i])\n    ax[i//5, i%5].imshow(img, cmap='gray')\n    if i<5:\n        ax[i//5, i%5].set_title(\"benign\")\n    else:\n        ax[i//5, i%5].set_title(\"malignant\")\n    ax[i//5, i%5].axis('off')\n    ax[i//5, i%5].set_aspect('auto')\nplt.show()\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pipenv run python ./utils/sampleSet.py\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Breast Histopathology Image Segmentation",src:t(789548).Z+"",width:"2010",height:"792"})})]})}function h(e={}){const{wrapper:n}={...(0,i.ah)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},603905:(e,n,t)=>{t.d(n,{ah:()=>c});var a=t(667294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function s(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?s(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):s(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},s=Object.keys(e);for(a=0;a<s.length;a++)t=s[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(a=0;a<s.length;a++)t=s[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var o=a.createContext({}),c=function(e){var n=a.useContext(o),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},d={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},h=a.forwardRef((function(e,n){var t=e.components,i=e.mdxType,s=e.originalType,o=e.parentName,h=l(e,["components","mdxType","originalType","parentName"]),g=c(t),p=i,m=g["".concat(o,".").concat(p)]||g[p]||d[p]||s;return t?a.createElement(m,r(r({ref:n},h),{},{components:t})):a.createElement(m,r({ref:n},h))}));h.displayName="MDXCreateElement"},423273:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/Breast_Histopathology_Image_Segmentation_01-6d3a714b0e24b2c043ff5ada7ed4e067.png"},789548:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/Breast_Histopathology_Image_Segmentation_02-d48e4d9da9baae2434aa5554f9e2dee0.png"},661127:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-918471126c0472aad97358a725e1a399.jpg"}}]);