"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[94031],{380489:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>c});var t=i(474848),s=i(28453);const o={sidebar_position:4820,slug:"2023-01-08",title:"MiDaS Depth Vision",authors:"mpolinowski",tags:["Python","Machine Learning","Torch"],description:"MiDaS computes relative inverse depth from a single image."},r=void 0,a={id:"IoT-and-Machine-Learning/ML/2023-01-08-depth-vision-midas/index",title:"MiDaS Depth Vision",description:"MiDaS computes relative inverse depth from a single image.",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-01-08-depth-vision-midas/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-01-08-depth-vision-midas",slug:"/IoT-and-Machine-Learning/ML/2023-01-08-depth-vision-midas/2023-01-08",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-01-08-depth-vision-midas/2023-01-08",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-01-08-depth-vision-midas/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Torch",permalink:"/docs/tags/torch"}],version:"current",sidebarPosition:4820,frontMatter:{sidebar_position:4820,slug:"2023-01-08",title:"MiDaS Depth Vision",authors:"mpolinowski",tags:["Python","Machine Learning","Torch"],description:"MiDaS computes relative inverse depth from a single image."},sidebar:"tutorialSidebar",previous:{title:"YOLOv7 Training with Custom Data",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-01-10-yolov7_custom_data/2023-01-10"},next:{title:"YOLOv7 Introduction",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-01-05-yolov7/2023-01-05"}},d={},c=[{value:"Working with Image Files",id:"working-with-image-files",level:2},{value:"Choosing the Right Model",id:"choosing-the-right-model",level:3},{value:"GPU or CPU",id:"gpu-or-cpu",level:3},{value:"Transformations",id:"transformations",level:3},{value:"Prediction",id:"prediction",level:3},{value:"Working with Video Streams",id:"working-with-video-streams",level:2}];function l(e){const n={a:"a",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Guangzhou, China",src:i(397479).A+"",width:"1500",height:"652"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#working-with-image-files",children:"Working with Image Files"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#choosing-the-right-model",children:"Choosing the Right Model"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#gpu-or-cpu",children:"GPU or CPU"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#transformations",children:"Transformations"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#prediction",children:"Prediction"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#working-with-video-streams",children:"Working with Video Streams"})}),"\n"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:["MiDaS - see ",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/1907.01341",children:"Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer"})," by Ren\xe9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, Vladlen Koltun - computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://github.com/mpolinowski/torch-depth",children:"Github Repository"})}),"\n",(0,t.jsxs)(n.p,{children:["MiDaS depends on ",(0,t.jsx)(n.a,{href:"https://huggingface.co/docs/timm/index",children:"timm"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install timm\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"import cv2\nimport os\nimport torch\nimport urllib.request\n\nimport matplotlib.pyplot as plt\n"})}),"\n",(0,t.jsx)(n.h2,{id:"working-with-image-files",children:"Working with Image Files"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:'url, filename = ("https://cdn.wallpapersafari.com/45/74/Ye9R0H.jpg", "bridge.jpg")\nurllib.request.urlretrieve(url, filename)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"choosing-the-right-model",children:"Choosing the Right Model"}),"\n",(0,t.jsxs)(n.p,{children:["Depending on your Hardware you can choose one of three models with different ",(0,t.jsx)(n.a,{href:"https://github.com/isl-org/MiDaS#Accuracy",children:"accuracy and speed"}),":"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"MiDaS Depth Vision",src:i(281544).A+"",width:"832",height:"698"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:'# Load a model\n#model_type = "DPT_Large"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)\n#model_type = "DPT_Hybrid"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)\nmodel_type = "MiDaS_small"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)\n\nmidas = torch.hub.load("intel-isl/MiDaS", model_type)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"gpu-or-cpu",children:"GPU or CPU"}),"\n",(0,t.jsx)(n.p,{children:"You can run the prediction either on CUDA/Nvidia or CPU:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:'# Use GPU if available\ndevice = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")\nmidas.to(device)\nmidas.eval()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"transformations",children:"Transformations"}),"\n",(0,t.jsxs)(n.p,{children:["Inputs need to be transformed to match the dataset the model was trained with. These are available on ",(0,t.jsx)(n.strong,{children:"Torch Hub"})," and can be downloaded matching the model you choose earlier:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:'# Use transforms to resize and normalize the image\nmidas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")\n\nif model_type == "DPT_Large" or model_type == "DPT_Hybrid":\n    transform = midas_transforms.dpt_transform\nelse:\n    transform = midas_transforms.small_transform\n'})}),"\n",(0,t.jsx)(n.p,{children:"Once downloaded you can apply them to your input image:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# Apply transforms\nimg = cv2.imread(filename)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\ninput_batch = transform(img).to(device)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"prediction",children:"Prediction"}),"\n",(0,t.jsx)(n.p,{children:"Now we can run the prediction:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:'# Predict and resize to original resolution\nwith torch.no_grad():\n    prediction = midas(input_batch)\n\n    prediction = torch.nn.functional.interpolate(\n        prediction.unsqueeze(1),\n        size=img.shape[:2],\n        mode="bicubic",\n        align_corners=False,\n    ).squeeze()\n\noutput = prediction.cpu().numpy()\n'})}),"\n",(0,t.jsx)(n.p,{children:"And show the result depth map:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# Show depth map\nplt.imshow(output)\nplt.show()\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"MiDaS Depth Vision",src:i(134755).A+"",width:"1275",height:"473"})}),"\n",(0,t.jsx)(n.h2,{id:"working-with-video-streams",children:"Working with Video Streams"}),"\n",(0,t.jsxs)(n.p,{children:["Now that we know that the model is working I now want to see if I can feed it the ",(0,t.jsx)(n.a,{href:"https://github.com/mpolinowski/opencv-rtsp",children:"RTSP Stream of my INSTAR IP Camera"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"# Get the video stream\nRTSP_URL = 'rtsp://admin:instar@192.168.2.120/livestream/12'\n\nos.environ['OPENCV_FFMPEG_CAPTURE_OPTIONS'] = 'rtsp_transport;udp'\n\ncap = cv2.VideoCapture(RTSP_URL, cv2.CAP_FFMPEG)\n\nif not cap.isOpened():\n    print('Cannot open RTSP stream')\n    exit(-1)\n\nwhile True:\n    success, img = cap.read()\n    cv2.imshow('RTSP stream', img)\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):  # Keep running until you press `q`\n        break\n"})}),"\n",(0,t.jsx)(n.p,{children:"This code will output the original video from our camera. So we now need to add the prediction code into the while loop like so:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:"while True:\n    success, frame = cap.read()\n\n    # Apply transforms\n    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    input_batch = transform(img).to(device)\n\n    # Predict and resize to original resolution\n    with torch.no_grad():\n        prediction = midas(input_batch)\n\n        prediction = torch.nn.functional.interpolate(\n            prediction.unsqueeze(1),\n            size=img.shape[:2],\n            mode=\"bicubic\",\n            align_corners=False,\n        ).squeeze()\n\n    output = prediction.cpu().numpy()\n    \n    plt.imshow(output)\n    plt.pause(0.00001)\n    \n    cv2.imshow('RTSP stream', img)\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):  # Keep running until you press `q`\n        break\n\nplt.show()\n"})}),"\n",(0,t.jsx)(n.p,{children:"This now outputs the RTSP source through OpenCV and the corresponding prediction with Matplotlib:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"MiDaS Depth Vision",src:i(913178).A+"",width:"1271",height:"432"})})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},281544:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/Torch_Depth_Vision_01-a5276b5d2a1b2a212599ceb94882c296.png"},134755:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/Torch_Depth_Vision_02-f9d6b125f9fd2d03ddf56c64c6fd6714.png"},913178:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/Torch_Depth_Vision_03-ee193cea9d2c45ca0c7dcaffeeb8c90d.png"},397479:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-ba3b23aa3d5392c02b451d1b2b911721.jpg"},28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(296540);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);