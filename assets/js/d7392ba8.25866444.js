"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[7053],{3905:(e,a,n)=>{n.d(a,{Zo:()=>c,kt:()=>u});var t=n(67294);function r(e,a,n){return a in e?Object.defineProperty(e,a,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[a]=n,e}function i(e,a){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),n.push.apply(n,t)}return n}function o(e){for(var a=1;a<arguments.length;a++){var n=null!=arguments[a]?arguments[a]:{};a%2?i(Object(n),!0).forEach((function(a){r(e,a,n[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(n,a))}))}return e}function l(e,a){if(null==e)return{};var n,t,r=function(e,a){if(null==e)return{};var n,t,r={},i=Object.keys(e);for(t=0;t<i.length;t++)n=i[t],a.indexOf(n)>=0||(r[n]=e[n]);return r}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)n=i[t],a.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=t.createContext({}),p=function(e){var a=t.useContext(s),n=a;return e&&(n="function"==typeof e?e(a):o(o({},a),e)),n},c=function(e){var a=p(e.components);return t.createElement(s.Provider,{value:a},e.children)},d={inlineCode:"code",wrapper:function(e){var a=e.children;return t.createElement(t.Fragment,{},a)}},m=t.forwardRef((function(e,a){var n=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),m=p(n),u=r,f=m["".concat(s,".").concat(u)]||m[u]||d[u]||i;return n?t.createElement(f,o(o({ref:a},c),{},{components:n})):t.createElement(f,o({ref:a},c))}));function u(e,a){var n=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=m;var l={};for(var s in a)hasOwnProperty.call(a,s)&&(l[s]=a[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,o[1]=l;for(var p=2;p<i;p++)o[p]=n[p];return t.createElement.apply(null,o)}return t.createElement.apply(null,n)}m.displayName="MDXCreateElement"},2732:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>s,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>l,toc:()=>p});var t=n(87462),r=(n(67294),n(3905));const i={sidebar_position:4680,slug:"2023-02-06",title:"Apache Airflow Dynamic DAGs",authors:"mpolinowski",tags:["Python","Machine Learning","Airflow"],description:"Airflow is a platform to author, schedule and monitor workflows."},o=void 0,l={unversionedId:"IoT-and-Machine-Learning/AIOps/2023-02-06-apache-airflow-dynamic-dags/index",id:"IoT-and-Machine-Learning/AIOps/2023-02-06-apache-airflow-dynamic-dags/index",title:"Apache Airflow Dynamic DAGs",description:"Airflow is a platform to author, schedule and monitor workflows.",source:"@site/docs/IoT-and-Machine-Learning/AIOps/2023-02-06-apache-airflow-dynamic-dags/index.md",sourceDirName:"IoT-and-Machine-Learning/AIOps/2023-02-06-apache-airflow-dynamic-dags",slug:"/IoT-and-Machine-Learning/AIOps/2023-02-06-apache-airflow-dynamic-dags/2023-02-06",permalink:"/docs/IoT-and-Machine-Learning/AIOps/2023-02-06-apache-airflow-dynamic-dags/2023-02-06",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/AIOps/2023-02-06-apache-airflow-dynamic-dags/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Airflow",permalink:"/docs/tags/airflow"}],version:"current",sidebarPosition:4680,frontMatter:{sidebar_position:4680,slug:"2023-02-06",title:"Apache Airflow Dynamic DAGs",authors:"mpolinowski",tags:["Python","Machine Learning","Airflow"],description:"Airflow is a platform to author, schedule and monitor workflows."},sidebar:"tutorialSidebar",previous:{title:"MLflow 2.1 Introduction",permalink:"/docs/IoT-and-Machine-Learning/AIOps/2023-02-09-mlflow-introduction/2023-02-09"},next:{title:"Apache Airflow DAG Scheduling",permalink:"/docs/IoT-and-Machine-Learning/AIOps/2023-02-05-apache-airflow-scheduler/2023-02-05"}},s={},p=[{value:"Docker Compose",id:"docker-compose",level:2},{value:"Preparation",id:"preparation",level:3},{value:"Initialization",id:"initialization",level:3},{value:"Running Airflow",id:"running-airflow",level:3},{value:"DAG Templating",id:"dag-templating",level:2}],c={toc:p};function d(e){let{components:a,...i}=e;return(0,r.kt)("wrapper",(0,t.Z)({},c,i,{components:a,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Guangzhou, China",src:n(69756).Z,width:"1061",height:"405"})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#docker-compose"},"Docker Compose"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#preparation"},"Preparation")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#initialization"},"Initialization")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#running-airflow"},"Running Airflow")))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#dag-templating"},"DAG Templating"))),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/mpolinowski/apache-airflow-intro"},"Github Repository"))),(0,r.kt)("h2",{id:"docker-compose"},"Docker Compose"),(0,r.kt)("h3",{id:"preparation"},"Preparation"),(0,r.kt)("p",null,"To deploy ",(0,r.kt)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html"},"Airflow with Docker Compose"),", you should fetch ",(0,r.kt)("inlineCode",{parentName:"p"},"docker-compose.yaml"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.5.1/docker-compose.yaml'\n")),(0,r.kt)("p",null,"This file contains several service definitions:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"airflow-scheduler"),": The scheduler monitors all tasks and DAGs, then triggers the task instances once their dependencies are complete."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"airflow-webserver"),": The webserver is available at ",(0,r.kt)("inlineCode",{parentName:"li"},"http://localhost:8080"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"airflow-worker"),": The worker that executes the tasks given by the scheduler."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"airflow-init"),": The initialization service."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"postgres"),": The database."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"redis"),": The redis - broker that forwards messages from scheduler to worker.")),(0,r.kt)("p",null,"Some directories in the container are mounted, which means that their contents are synchronized between your computer and the container:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"dags"),": you can put your DAG files here."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"logs"),": contains logs from task execution and scheduler."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"plugins"),": you can put your custom plugins here.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'mkdir -p ./dags ./logs ./plugins\necho -e "AIRFLOW_UID=$(id -u)" > .env\n')),(0,r.kt)("h3",{id:"initialization"},"Initialization"),(0,r.kt)("p",null,"We first need to bring up the Postgres container, run database migrations and create the first user account. The following command also downloads all the other needed container images for us to be ready to go:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"docker-compose up airflow-init\n")),(0,r.kt)("p",null,"After initialization is complete, you should see a message like this:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'airflow-init_1       | INFO - Added user airflow\nairflow-init_1       | User "airflow" created with role "Admin"\nairflow-init_1       | 2.5.1\ndocker-compose_airflow-init_1 exited with code 0\n')),(0,r.kt)("p",null,"The account created has the login airflow and the password airflow."),(0,r.kt)("h3",{id:"running-airflow"},"Running Airflow"),(0,r.kt)("p",null,"Now we can start all services with:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"docker-compose up\n\n|   ____________       _____________\n|  ____    |__( )_________  __/__  /________      __\n| ____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\n| ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\n|  _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"docker ps\n\nIMAGE                  PORTS                    NAMES\napache/airflow:2.5.1   8080/tcp                 docker-compose_airflow-triggerer_1\napache/airflow:2.5.1   0.0.0.0:8080->8080/tcp   docker-compose_airflow-webserver_1\napache/airflow:2.5.1   8080/tcp                 docker-compose_airflow-worker_1\napache/airflow:2.5.1   8080/tcp                 docker-compose_airflow-scheduler_1\npostgres:13            5432/tcp                 docker-compose_postgres_1\nredis:latest           6379/tcp                 docker-compose_redis_1\n")),(0,r.kt)("p",null,"The webserver is available at: ",(0,r.kt)("inlineCode",{parentName:"p"},"http://localhost:8080"),". The default account has the login ",(0,r.kt)("inlineCode",{parentName:"p"},"airflow")," and the password ",(0,r.kt)("inlineCode",{parentName:"p"},"airflow"),"."),(0,r.kt)("p",null,"To bring everything down again afterwards run:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"docker-compose down --volumes --remove-orphans\n")),(0,r.kt)("p",null,"To get rid of all downloaded images:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"docker compose down --volumes --rmi all\n")),(0,r.kt)("h2",{id:"dag-templating"},"DAG Templating"),(0,r.kt)("p",null,"Generating DAGs On-Demand using ",(0,r.kt)("strong",{parentName:"p"},"Jinja templating")," and ",(0,r.kt)("strong",{parentName:"p"},"YAML")," (",(0,r.kt)("inlineCode",{parentName:"p"},"pip install jinja2 pyyaml"),"). We can start by writing a DAG template that can be used to generate DAGs for us that only differ in small configuration changes:"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"dags/dynamic","_","dags/template","_","dag.jinja2")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"from airflow import DAG \nfrom airflow.decorators import task\n\nfrom datetime import datetime\n\nstart_date = datetime(2023,2,6)\ntags=['generated']\n\nwith DAG(\n    dag_id = 'get_price_{{ dag_id }}',\n    start_date = start_date,\n    shedule_interval = '{{ shedule_interval }}',\n    catchup = {{ catchup or False }},\n    tags = tags\n    ):\n\n    @task\n    def extract(symbol):\n        return symbol\n\n    @task\n    def process(symbol):\n        return symbol\n\n    @task\n    def store(symbol):\n        return symbol\n\n    store(process(extract({{ input }})))\n")),(0,r.kt)("p",null,"And example configuration YAML file for this template looks like:"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"dags/dynamic","_","dags/config","_","fb.yml")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yml"},"dag_id: 'FB'\nschedule_interval: '@weekly'\ncatchup: False\ninput: 123\n")),(0,r.kt)("p",null,"Create all your configuration files and write a generator script that will build your DAGs for you:"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"dags/dynamic","_","dags/generator.py")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"from jinja2 import Environment, FileSystemLoader\nimport yaml\nimport os\n\nfile_dir = os.path.dirname(os.path.abspath(__file__))\nenv = Environment(loader=FileSystemLoader(file_dir))\ntemplate = env.get_template('template_dag.jinja2')\n\nfor filename in os.listdir(file_dir):\n    # loop over configuration yaml\n    if filename.endswith('.yml'):\n        # read configuration\n        with open(f'{file_dir}/{filename}', 'r') as configfile:\n            config = yaml.safe_load(configfile)\n            # generate dags based on config and template\n            with open(f'dags/get_price_{config[\"dag_id\"]}.py', 'w') as file:\n                file.write(template.render(config))\n")),(0,r.kt)("p",null,"You can now run the generator to output DAGs for every YAML configuration file you created:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"python dags/dynamic_dags/generator.py\n")),(0,r.kt)("p",null,"And the following DAGs will be created for you:"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Apache Airflow Dynamic DAGs",src:n(33333).Z,width:"1248",height:"409"})),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Apache Airflow Dynamic DAGs",src:n(54466).Z,width:"1235",height:"781"})))}d.isMDXComponent=!0},33333:(e,a,n)=>{n.d(a,{Z:()=>t});const t=n.p+"assets/images/Apache_Airflow_DynDAGs_01-123fc6f3b30f7f924de82e8b2657ffb2.png"},54466:(e,a,n)=>{n.d(a,{Z:()=>t});const t=n.p+"assets/images/Apache_Airflow_DynDAGs_02-85158fd0de54924374366d1ebdcd5005.png"},69756:(e,a,n)=>{n.d(a,{Z:()=>t});const t=n.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-fe9bbb57ea8da08fea2f3fef2bf2515b.jpg"}}]);