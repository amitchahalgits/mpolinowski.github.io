"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[71989],{80444:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>l});var i=a(474848),t=a(28453);const s={sidebar_position:4930,slug:"2022-12-12",title:"Breast Histopathology Image Segmentation Part 6",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"Running Predictions"},o=void 0,r={id:"IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6/index",title:"Breast Histopathology Image Segmentation Part 6",description:"Running Predictions",source:"@site/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6",slug:"/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6/2022-12-12",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6/2022-12-12",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Tensorflow",permalink:"/docs/tags/tensorflow"}],version:"current",sidebarPosition:4930,frontMatter:{sidebar_position:4930,slug:"2022-12-12",title:"Breast Histopathology Image Segmentation Part 6",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"Running Predictions"},sidebar:"tutorialSidebar",previous:{title:"Tensorflow Image Classification",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-16-tf-cifar/2022-12-16"},next:{title:"Breast Histopathology Image Segmentation Part 5",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part5/2022-12-12"}},c={},l=[{value:"Loading the Model",id:"loading-the-model",level:2},{value:"Preparing the Test Image File",id:"preparing-the-test-image-file",level:2},{value:"Make Predictions",id:"make-predictions",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h2:"h2",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Guangzhou, China",src:a(104514).A+"",width:"1500",height:"383"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-10-tf-breast-cancer-classification-part1/2022-12-10",children:"Part 1: Data Inspection and Pre-processing"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part2/2022-12-11",children:"Part 2: Weights, Data Augmentations and Generators"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part3/2022-12-11",children:"Part 3: Model creation based on a pre-trained and a custom model"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part4/2022-12-11",children:"Part 4: Train our model to fit the dataset"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part5/2022-12-12",children:"Part 5: Evaluate the performance of your trained model"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6/2022-12-12",children:"Part 6: Running Predictions"})}),"\n"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://github.com/mpolinowski/tf-bc-classification",children:"Github"})}),"\n"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#loading-the-model",children:"Loading the Model"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#preparing-the-test-image-file",children:"Preparing the Test Image File"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#make-predictions",children:"Make Predictions"})}),"\n"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["Based on ",(0,i.jsx)(n.a,{href:"https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images",children:"Breast Histopathology Images"})," by ",(0,i.jsx)(n.a,{href:"https://www.kaggle.com/paultimothymooney",children:"Paul Mooney"}),".\n",(0,i.jsx)(n.code,{children:"Invasive Ductal Carcinoma (IDC) is the most common subtype of all breast cancers. To assign an aggressiveness grade to a whole mount sample, pathologists typically focus on the regions which contain the IDC. As a result, one of the common pre-processing steps for automatic aggressiveness grading is to delineate the exact regions of IDC inside of a whole mount slide."}),"\n",(0,i.jsx)(n.a,{href:"https://youtu.be/8XsiMQQ-4mM",children:"Can recurring breast cancer be spotted with AI tech? - BBC News"})]}),"\n"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Citation: ",(0,i.jsx)(n.a,{href:"https://pubmed.ncbi.nlm.nih.gov/27563488/",children:"Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases"})]}),"\n",(0,i.jsx)(n.li,{children:"Dataset: 198,738 IDC(negative) image patches; 78,786 IDC(positive) image patches"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"loading-the-model",children:"Loading the Model"}),"\n",(0,i.jsxs)(n.p,{children:["After fitting the model to classify our source images into ",(0,i.jsx)(n.code,{children:"malignant"})," and ",(0,i.jsx)(n.code,{children:"benign"})," we can now use this model to make predictions about un-classified images. So load the model we want to use:"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"./RunPrediction.py"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-py",children:"# Model you want to use\nmodelName = 'resnet50_weights.hdf5'\n# modelName = 'custom_weights.hdf5'\nmodelPath = config.OUTPUT_PATH + '/' + modelName\nprint(\"Loading Breast Cancer detector model...\")\nmodel = load_model(modelPath)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"preparing-the-test-image-file",children:"Preparing the Test Image File"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-py",children:'# Test image\n# imagePath ="./sample_pictures/malignant.png"\nimagePath =  "./sample_pictures/benign.png"\n\n# Loading the input image using openCV\nimage = cv2.imread(imagePath)\n\n# Convert it from BGR to RGB and then resize it to 48x48, \n# the same parameter we used for training\nimage1 = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nimage1 = cv2.resize(image1, (48, 48))\nimage1 = img_to_array(image1)\n\n# Only for ResNet50\nimage1 = preprocess_input(image1)\n\n# The image is now represented by a NumPy array of shape (48, 48, 3), however \n# we need the dimensions to be (1, 3, 48, 48) so we can pass it\n# through the network and then we\'ll also preprocess the image by subtracting the \n# mean RGB pixel intensity from the ImageNet dataset\nimage1 /=  255.0\n\nimage1 = np.expand_dims(image1, axis=0)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"make-predictions",children:"Make Predictions"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-py",children:'# Pass the image through the model to determine if the person has malignant\n(benign, malignant) = model.predict(image1)[0]\n\n\n# Adding the probability in the label\nlabel = "benign" if benign > malignant else "malignant"\nlabel = "{}: {:.2f}%".format(label, max(benign, malignant) * 100)\n        \n# Showing the output image\nprint("RESULT :" +label)\n\ncv2.imshow("IDC", image)\nif cv2.waitKey(5000) & 0xFF == ord(\'q\'):\n        cv2.destroyAllWindows()\n'})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},104514:(e,n,a)=>{a.d(n,{A:()=>i});const i=a.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-918471126c0472aad97358a725e1a399.jpg"},28453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>r});var i=a(296540);const t={},s=i.createContext(t);function o(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);