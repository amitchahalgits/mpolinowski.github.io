"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[67398],{515491:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var t=a(785893),i=a(603905);const s={sidebar_position:4140,slug:"2023-08-27",title:"Image Segmentation with PyTorch",authors:"mpolinowski",tags:["Python","Machine Learning","PyTorch"],description:"Food item segmentation from images of the Tray Food Segmentation dataset"},r="Image Segmentation with PyTorch",o={id:"IoT-and-Machine-Learning/ML/2023-08-27--image-segmentation-with-pytorch/index",title:"Image Segmentation with PyTorch",description:"Food item segmentation from images of the Tray Food Segmentation dataset",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-08-27--image-segmentation-with-pytorch/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-08-27--image-segmentation-with-pytorch",slug:"/IoT-and-Machine-Learning/ML/2023-08-27--image-segmentation-with-pytorch/2023-08-27",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-08-27--image-segmentation-with-pytorch/2023-08-27",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-08-27--image-segmentation-with-pytorch/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"PyTorch",permalink:"/docs/tags/py-torch"}],version:"current",sidebarPosition:4140,frontMatter:{sidebar_position:4140,slug:"2023-08-27",title:"Image Segmentation with PyTorch",authors:"mpolinowski",tags:["Python","Machine Learning","PyTorch"],description:"Food item segmentation from images of the Tray Food Segmentation dataset"},sidebar:"tutorialSidebar",previous:{title:"Image Segmentation with PyTorch (RCNN)",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-08-28--semantic-segmentation-detectron2-model-zoo/2023-08-28"},next:{title:"Containerized PyTorch Dev Workflow",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-08-21--pytorch-development-in-docker/2023-08-21"}},l={},c=[{value:"Creating Label Annotations",id:"creating-label-annotations",level:2},{value:"Dataset",id:"dataset",level:2},{value:"Preparing the Dataset",id:"preparing-the-dataset",level:3},{value:"Dataset Visualization",id:"dataset-visualization",level:4},{value:"Data Augmentation",id:"data-augmentation",level:4},{value:"Visualize Augmented Data",id:"visualize-augmented-data",level:5},{value:"Data Loading",id:"data-loading",level:3},{value:"Performance Metrics",id:"performance-metrics",level:2},{value:"Building the Segmentation Model",id:"building-the-segmentation-model",level:2},{value:"Data Loader",id:"data-loader",level:3},{value:"Model Training",id:"model-training",level:3},{value:"Model Evaluation",id:"model-evaluation",level:2},{value:"Visualize Segmentation",id:"visualize-segmentation",level:3},{value:"Predicted Segmentation Map",id:"predicted-segmentation-map",level:4}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.ah)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"TST, Hong Kong",src:a(332995).Z+"",width:"1500",height:"811"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#image-segmentation-with-pytorch",children:"Image Segmentation with PyTorch"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#creating-label-annotations",children:"Creating Label Annotations"})}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#dataset",children:"Dataset"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#preparing-the-dataset",children:"Preparing the Dataset"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#dataset-visualization",children:"Dataset Visualization"})}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#data-augmentation",children:"Data Augmentation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#visualize-augmented-data",children:"Visualize Augmented Data"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#data-loading",children:"Data Loading"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#performance-metrics",children:"Performance Metrics"})}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#building-the-segmentation-model",children:"Building the Segmentation Model"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#data-loader",children:"Data Loader"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#model-training",children:"Model Training"})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#model-evaluation",children:"Model Evaluation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#visualize-segmentation",children:"Visualize Segmentation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#predicted-segmentation-map",children:"Predicted Segmentation Map"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/mpolinowski/pt-seg-i-see-you",children:"Github Repository"})}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Related"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Image Segmentation with PyTorch"}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-08-28--semantic-segmentation-detectron2-model-zoo/2023-08-28",children:"Semantic Segmentation Detectron2 Model Zoo"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-08-29--semantic-segmentation-detectron2-model-zoo-faster-rcnn/2023-08-29",children:"Semantic Segmentation Detectron2 Model Zoo: Faster RCNN"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-08-30--instance_segmentation_detectron2_model_zoo_mask_rcnn/2023-08-30",children:"Semantic Segmentation Detectron2 Model Zoo: Mask RCNN"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/IoT-and-Machine-Learning/ML/2023-08-31--semantic_segmentation_detectron2_openimages_dataset/2023-08-31",children:"Detectron Object Detection with OpenImages Dataset (WIP)"})}),"\n"]}),"\n",(0,t.jsx)(n.h1,{id:"image-segmentation-with-pytorch",children:"Image Segmentation with PyTorch"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://www.kaggle.com/datasets/thezaza102/tray-food-segmentation",children:"Tray Food Segmentation"}),":\n",(0,t.jsx)(n.em,{children:"Food item segmentation from images of trays"})]}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/thezaza101/Meal-Compliance-Project",children:"Meal-Compliance-Project"})}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"creating-label-annotations",children:"Creating Label Annotations"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://pypi.org/project/labelme/",children:"labelme "})}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"!pip install matplotlib opencv-python albumentations tqdm\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"!pip install git+https://github.com/qubvel/segmentation_models.pytorch\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import lr_scheduler\n\nimport segmentation_models_pytorch as smp\n\nimport torchvision\nfrom torchvision import datasets, models, transforms\n\nimport albumentations as albu\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from helper.metrics import IoU, Accuracy, Fscore, Recall, Precision, DiceLoss\nfrom helper.train import BaseDataset, VisualizeDataset, VisualizeResult, TestEpoch, TrainEpoch\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"LR = 0.0001\nDLR_STEPS = 7\nDLR_GAMMA = 0.1\nEPOCHS = 20\nMODEL_PATH = '../../saved_models/MobileNetV3Encoder_UnetPlusPlus.pth'\nDATA_DIR = '../../dataset/TrayDataset/'\nBATCH = 8\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"})}),"\n",(0,t.jsx)(n.h2,{id:"dataset",children:"Dataset"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"CLASS_LABELS = ['background','tray','cutlery','form','straw','meatball','beef','roastlamb','beeftomatocasserole','ham','bean','cucumber','leaf','tomato','boiledrice','beefmexicanmeatballs','spinachandpumpkinrisotto','bakedfish','gravy','zucchini','carrot','broccoli','pumpkin','celery','sandwich','sidesalad','tartaresauce','jacketpotato','creamedpotato','bread','margarine','soup','apple','cannedfruit','milk','vanillayogurt','jelly','custard','lemonsponge','juice','applejuice','orangejuice','water']\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"len(CLASS_LABELS)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"CLASS_COLOUR_MAP = np.array([\n    (0, 0, 0), (100, 127, 150),(50, 0, 0),(0, 0, 255),(100, 0, 0),(0, 100, 0),\n    (0, 100, 0),(0, 50, 50),(50, 100, 0),(0, 250, 0),(180, 0, 0),\n    (100, 100, 0),(128, 0, 100),(100, 128, 0),(0, 100, 128),(100, 0, 100),\n    (150, 100, 0),(0, 100, 200), (100, 50, 50),(50, 100, 250),(100, 250, 50),(180, 100, 0),\n    (100, 50, 218),(200, 128, 100),(100, 0, 128),(10, 100, 128),(100, 150, 75),\n    (175, 100, 90),(30, 100, 128),(100, 250, 125),(50, 10, 50), (175, 10, 175),(25, 225, 50),\n    (100, 128, 218),(128, 0, 100),(128, 128, 0),(90, 100, 0),(100, 200, 0),(175, 100, 150),\n    (200, 100, 200),(200, 50, 50),(250, 100, 50),(100, 25, 50),(150, 100, 100)\n])\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"x_train_dir = os.path.join(DATA_DIR, 'XTrain')\ny_train_dir = os.path.join(DATA_DIR, 'yTrain')\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"x_test_dir = os.path.join(DATA_DIR, 'XTest')\ny_test_dir = os.path.join(DATA_DIR, 'yTest')\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"path, dirs, files = next(os.walk(x_train_dir))\nfile_count = len(files)\n\nprint('Test Dataset: ',file_count)\n"})}),"\n",(0,t.jsx)(n.p,{children:"Test Dataset:  1241"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"path, dirs, files = next(os.walk(x_test_dir))\nfile_count = len(files)\n\nprint('Training Dataset: ',file_count)\n"})}),"\n",(0,t.jsx)(n.p,{children:"Training Dataset:  8"}),"\n",(0,t.jsx)(n.h3,{id:"preparing-the-dataset",children:"Preparing the Dataset"}),"\n",(0,t.jsx)(n.h4,{id:"dataset-visualization",children:"Dataset Visualization"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"for label  in CLASS_LABELS:\n    dataset = BaseDataset(x_test_dir, y_test_dir, classes=[label])\n    image, mask = dataset[2]\n    VisualizeDataset(\n        image = image, mask=mask.squeeze(),\n        label = label\n    )\n"})}),"\n",(0,t.jsxs)(n.p,{children:["/opt/app/notebook/MobileNetV3Encoder_UnetPlusPlus/helper/train.py:20: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (",(0,t.jsx)(n.code,{children:"matplotlib.pyplot.figure"}),") are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam ",(0,t.jsx)(n.code,{children:"figure.max_open_warning"}),"). Consider using ",(0,t.jsx)(n.code,{children:"matplotlib.pyplot.close()"}),".\nplt.figure(figsize=(14, 20))"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Image Segmentation with PyTorch",src:a(364456).Z+"",width:"3740",height:"3693"})}),"\n",(0,t.jsx)(n.h4,{id:"data-augmentation",children:"Data Augmentation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def apply_train_aug():\n    train_transform = [\n        # apply to all\n        albu.Resize(256, 416, p=1),\n        # apply to 50% of images\n        albu.HorizontalFlip(p=0.5),\n        # apply one randomly to 90% of images\n        albu.OneOf([\n            albu.RandomBrightnessContrast(\n                  brightness_limit=0.4, contrast_limit=0.4, p=1),\n            albu.CLAHE(p=1),\n            albu.HueSaturationValue(p=1)\n            ], p=0.9,),\n        # add noise to 20% of images\n        albu.GaussNoise(var_limit=(10.0, 50.0), mean=0, per_channel=True, always_apply=False, p=0.2),\n    ]\n    \n    return albu.Compose(train_transform)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def apply_test_aug():\n    """Add paddings to make image shape divisible by 32"""\n    test_transform = [\n        albu.PadIfNeeded(256, 416)\n    ]\n\n    return albu.Compose(test_transform)\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def apply_preprocessing(preprocessing_fn):\n    _transform = [\n        albu.Lambda(image=preprocessing_fn),\n        albu.Lambda(image=to_tensor, mask=to_tensor),\n    ]\n\n    return albu.Compose(_transform)\n"})}),"\n",(0,t.jsx)(n.h5,{id:"visualize-augmented-data",children:"Visualize Augmented Data"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"train_dataset = BaseDataset(\n    x_train_dir,\n    y_train_dir,\n    augmentation=apply_train_aug(),\n    classes=['tray'],\n)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"test_dataset = BaseDataset(\n    x_test_dir,\n    y_test_dir,\n    augmentation=apply_test_aug(),\n    classes=['tray'],\n)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# show image with 5 different augmentations\nfor i in range(5):\n    image, mask = train_dataset[8]\n    VisualizeResult(image=image, mask=mask.squeeze(-1))\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Image Segmentation with PyTorch",src:a(375427).Z+"",width:"536",height:"944"})}),"\n",(0,t.jsx)(n.h3,{id:"data-loading",children:"Data Loading"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"train_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH,\n    shuffle=True,\n    num_workers=0)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"test_loader = DataLoader(\n    test_dataset,\n    batch_size=1,\n    shuffle=False,\n    num_workers=0)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.em,{children:"Intersection-over-Union"})," (",(0,t.jsx)(n.strong,{children:"IOU"}),"): Calculated from the overlap of the ground truth and predicted area divided by the overall area of both: ",(0,t.jsx)(n.code,{children:"(Area of Overlap) / (Area of Union)"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.em,{children:"Mean-Intersection-over-Union"})," (",(0,t.jsx)(n.strong,{children:"mIOU"}),"): Mean ",(0,t.jsx)(n.strong,{children:"IOU"})," over all classes."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"metrics = [\n    IoU(threshold=0.5),\n    Accuracy(threshold=0.5),\n    Fscore(threshold=0.5),\n    Recall(threshold=0.5),\n    Precision(threshold=0.5),\n]\n"})}),"\n",(0,t.jsx)(n.h2,{id:"building-the-segmentation-model",children:"Building the Segmentation Model"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# https://github.com/qubvel/segmentation_models.pytorch\n# resnet50, resnext50_32x4d, resnext101_32x8d, xception, timm-gernet_s, mobileone_s0, timm-efficientnet-b0, timm-mobilenetv3_small_100, resnet152, vgg13\nENCODER = 'timm-mobilenetv3_small_100'\nENCODER_WEIGHTS = 'imagenet'\nACTIVATION = 'softmax'\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# https://github.com/qubvel/segmentation_models.pytorch/tree/master/segmentation_models_pytorch/decoders\n# FPN, PAN, PSPNet, MAnet, Linknet, FPN, DeepLabV3, DeepLabV3Plus, Unet\nmodel = smp.UnetPlusPlus(\n    encoder_name=ENCODER,\n    encoder_weights=ENCODER_WEIGHTS,\n    classes=len(CLASS_LABELS),\n    activation=ACTIVATION,\n)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#Normalize your data the same way as during encoder weight pre-training\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"print(model)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Define Optimization algorithm with Learning rate\noptimizer = torch.optim.Adam([\n    dict(params=model.parameters(), lr=LR),\n])\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Define Loss Function\nloss = DiceLoss()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"data-loader",children:"Data Loader"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"train_dataset = BaseDataset(\n    x_train_dir,\n    y_train_dir,\n    augmentation=apply_train_aug(),\n    preprocessing=apply_preprocessing(preprocessing_fn),\n    classes=CLASS_LABELS,\n)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"test_dataset = BaseDataset(\n    x_test_dir,\n    y_test_dir,\n    augmentation=apply_test_aug(),\n    preprocessing=apply_preprocessing(preprocessing_fn),\n    classes=CLASS_LABELS,\n)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"train_loader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True, num_workers=0)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"model-training",children:"Model Training"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"train_epoch = TrainEpoch(\n    model,\n    loss=loss,\n    metrics=metrics,\n    optimizer=optimizer,\n    device=DEVICE,\n    verbose=True,\n)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"test_epoch = TestEpoch(\n    model,\n    loss=loss,\n    metrics=metrics,\n    device=DEVICE,\n    verbose=True,\n)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"max_score = 0\n\nfor i in range(0, 40):\n\n    print('\\nEpoch: {}'.format(i))\n    train_logs = train_epoch.run(train_loader)\n    test_logs = test_epoch.run(test_loader)\n    \n    # Save the model with best iou score\n    if max_score < test_logs['iou_score']:\n        max_score = test_logs['iou_score']\n        torch.save(model, MODEL_PATH)\n        print('Model saved!')\n\n    if i == 50:\n        optimizer.param_groups[0]['lr'] = 1e-5\n        print('Decrease decoder learning rate to 1e-5!')\n"})}),"\n",(0,t.jsx)(n.p,{children:"Epoch: 39\ntrain: 100%|\u2588| 156/156 [06:10,  2.37s/it, dice_loss - 0.03158, iou_score - 0.9396, accuracy - 0.9985, fscore - 0.9688, recall - 0.9686, precision\ntest: 100%|\u2588| 8/8 [00:01,  4.94it/s, dice_loss - 0.04502, iou_score - 0.9164, accuracy - 0.9979, fscore - 0.9555, recall - 0.9552, precision - 0."}),"\n",(0,t.jsx)(n.h2,{id:"model-evaluation",children:"Model Evaluation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"trained_model = torch.load(MODEL_PATH)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"logs = test_epoch.run(test_loader)\n"})}),"\n",(0,t.jsx)(n.p,{children:"test:  12%|\u258f| 1/8 [00:0215,  2.16s/it, dice_loss - 0.03128, iou_score - 0.9401, accuracy - 0.9986, fscore - 0.9691, recall - 0.9689, precision - 0."}),"\n",(0,t.jsx)(n.p,{children:"/opt/conda/lib/python3.10/site-packages/segmentation_models_pytorch/base/modules.py:116: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\nreturn self.activation(x)"}),"\n",(0,t.jsx)(n.p,{children:"test: 100%|\u2588| 8/8 [00:0300,  2.27it/s, dice_loss - 0.04451, iou_score - 0.9178, accuracy - 0.998, fscore - 0.9563, recall - 0.9558, precision - 0.9"}),"\n",(0,t.jsx)(n.p,{children:"test: 100%|\u2588| 8/8 [00:0100,  4.21it/s, dice_loss - 0.2597, iou_score - 0.8069, accuracy - 0.9953, fscore - 0.8928, recall - 0.839, precision - 0.95"}),"\n",(0,t.jsx)(n.h3,{id:"visualize-segmentation",children:"Visualize Segmentation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#Get orignial image and mask from test dataset\nimage, gt_mask = test_dataset[3]\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\npredicted_mask = trained_model.predict(x_tensor)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"pr_mask = (predicted_mask.squeeze().cpu().numpy().round())\npr_mask = pr_mask[1,:,:]\ngt_mask = gt_mask[1,:,:]\nimage_t = image.transpose(1, 2, 0)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"VisualizeResult(\n    image=image_t,\n    ground_truth_mask=gt_mask,\n    predicted_mask=pr_mask\n)\n"})}),"\n",(0,t.jsx)(n.p,{children:"Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers)."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"png",src:a(779820).Z+"",width:"1260",height:"266"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#Convert the predicted mask to numpy and get the predicted class indices\npredicted_output = torch.argmax(predicted_mask.squeeze(), dim=0).detach().cpu().numpy()\nindices = np.unique(predicted_output)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"for i in indices:\n  print(CLASS_LABELS[i])\n"})}),"\n",(0,t.jsx)(n.p,{children:"background\ntray\ncutlery\nboiledrice\nbeefmexicanmeatballs\nzucchini\npumpkin\nwater"}),"\n",(0,t.jsx)(n.h4,{id:"predicted-segmentation-map",children:"Predicted Segmentation Map"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Define function to convert 2D segmentation to RGB Image\n\ndef decode_segmentation_map(image, classesLength=43):\n\n  r = np.zeros_like(image).astype(np.uint8)\n  g = np.zeros_like(image).astype(np.uint8)\n  b = np.zeros_like(image).astype(np.uint8)\n\n  for l in range(0, classesLength):\n    idx = image == l\n    r[idx] = CLASS_COLOUR_MAP[l, 0]\n    g[idx] = CLASS_COLOUR_MAP[l, 1]\n    b[idx] = CLASS_COLOUR_MAP[l, 2]\n\n  rgb = np.stack([r, g, b], axis=2)\n\n  return rgb\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"rgb_map = decode_segmentation_map(predicted_output,43)\nplt.imshow(rgb_map);\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"png",src:a(966689).Z+"",width:"552",height:"353"})})]})}function h(e={}){const{wrapper:n}={...(0,i.ah)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},603905:(e,n,a)=>{a.d(n,{ah:()=>c});var t=a(667294);function i(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function s(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function r(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?s(Object(a),!0).forEach((function(n){i(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):s(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function o(e,n){if(null==e)return{};var a,t,i=function(e,n){if(null==e)return{};var a,t,i={},s=Object.keys(e);for(t=0;t<s.length;t++)a=s[t],n.indexOf(a)>=0||(i[a]=e[a]);return i}(e,n);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(t=0;t<s.length;t++)a=s[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var l=t.createContext({}),c=function(e){var n=t.useContext(l),a=n;return e&&(a="function"==typeof e?e(n):r(r({},n),e)),a},d={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},h=t.forwardRef((function(e,n){var a=e.components,i=e.mdxType,s=e.originalType,l=e.parentName,h=o(e,["components","mdxType","originalType","parentName"]),p=c(a),m=i,g=p["".concat(l,".").concat(m)]||p[m]||d[m]||s;return a?t.createElement(g,r(r({ref:n},h),{},{components:a})):t.createElement(g,r({ref:n},h))}));h.displayName="MDXCreateElement"},364456:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/Model_Eval_01-b566a283bce965027cf79e1e81b83298.webp"},375427:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/Model_Eval_02-0cb1ef77ae4a34912b4961a50b507619.webp"},779820:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/output_55_1-6aeaad3a8be76479038d3053a3b8604f.png"},966689:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/output_60_0-47d9520cd54c9e9c918a7b288c7ea8f1.png"},332995:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-f940fa4541ff8a00764cf3f41cd6b985.jpg"}}]);