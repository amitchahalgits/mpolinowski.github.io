<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Development/Python/2023-05-20-python-sklearn-cheat-sheet/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.1">
<title data-rh="true">SciKit-Learn Cheat Sheet | Mike Polinowski</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://mpolinowski.github.io/docs/Development/Python/2023-05-20-python-sklearn-cheat-sheet/2023-05-20"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="SciKit-Learn Cheat Sheet | Mike Polinowski"><meta data-rh="true" name="description" content="List of common SkLearn functions for Data Scientists"><meta data-rh="true" property="og:description" content="List of common SkLearn functions for Data Scientists"><link data-rh="true" rel="icon" href="/img/icons/favicon-32x32.png"><link data-rh="true" rel="canonical" href="https://mpolinowski.github.io/docs/Development/Python/2023-05-20-python-sklearn-cheat-sheet/2023-05-20"><link data-rh="true" rel="alternate" href="https://mpolinowski.github.io/docs/Development/Python/2023-05-20-python-sklearn-cheat-sheet/2023-05-20" hreflang="en"><link data-rh="true" rel="alternate" href="https://mpolinowski.github.io/docs/Development/Python/2023-05-20-python-sklearn-cheat-sheet/2023-05-20" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Mike Polinowski RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Mike Polinowski Atom Feed">




<link rel="icon" href="/img/angular_momentum.png">
<link rel="manifest" href="/manifest.json">
<meta name="theme-color" content="rgb(37,194,160)">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#000">
<link rel="apple-touch-icon" href="/img/angular_momentum.png">
<link rel="mask-icon" href="/img/angular_momentum.png" color="rgb(33,33,33)">
<meta name="msapplication-TileImage" content="/img/angular_momentum.png">
<meta name="msapplication-TileColor" content="#000">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-P74BDWF0C6"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-P74BDWF0C6",{})</script><link rel="stylesheet" href="/assets/css/styles.08c8c484.css">
<script src="/assets/js/runtime~main.ab71403f.js" defer="defer"></script>
<script src="/assets/js/main.88c0252d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"dark")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/angular_momentum.png" alt="Mike Polinowski :: Dev Notebook" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/angular_momentum.png" alt="Mike Polinowski :: Dev Notebook" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Mike Polinowski</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/docs/tags">Tags</a><a class="navbar__item navbar__link" href="/Search">Search</a><a href="https://mpolinowski.github.io/Personal" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">About<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/mpolinowski" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently dark mode)" aria-label="Switch between dark and light mode (currently dark mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/docs/category/development">Development</a><button aria-label="Collapse sidebar category &#x27;Development&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/category/javascript">Javascript</a><button aria-label="Expand sidebar category &#x27;Javascript&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" tabindex="0" href="/docs/category/python">Python</a><button aria-label="Collapse sidebar category &#x27;Python&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2023-08-14-python-scikit-image-opencv/2023-08-14">OpenCV &amp; SciPy and Scikit Image Cheat Sheet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2023-07-09-python-scikit-image-intro/2023-07-09">Introduction to Scikit-Image</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2023-05-28-telco-churn-cohort-study/2023-05-28">Supervised Learning with Scikit-Learn</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Development/Python/2023-05-20-python-sklearn-cheat-sheet/2023-05-20">SciKit-Learn Cheat Sheet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2023-05-18-python-asserts/2023-05-18">Python Asserts in Data Science Cheat Sheet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2023-05-17-fandango-ratings-controversy/2023-05-17">FiveThirtyEight Fandango Dataset</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2023-05-16-seaborn-cc-churn-vis/2023-05-16">Seaborn to Explore the CC Churn Dataset</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2023-05-14-visualizing-text-datasets/2023-05-14">Plotly &amp; Seaborn to Explore Text Dataset</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2023-05-12-matplotlib-seaborn-titanic-dataset/2023-05-12">Seaborn Titanic Dataset Exploration</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2023-05-10-plotly-COVID19-dataset/2023-05-10">Plotly COVID19 Dataset Exploration</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2023-05-07-python-seaborn-cheat-sheet/2023-05-07">Seaborn Cheat Sheet 2023</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2023-05-07-python-scipy-introduction/2023-05-07">A little bit of SciPy...</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2023-05-03-python-matplotlib-cheat-sheet/2023-05-03">Matplotlib Pyplot Cheat Sheet 2023</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2023-04-24-python-pandas-cheat-sheet/2023-04-24">Pandas Cheat Sheet 2023</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2023-01-16-python-3-11-features/2023-01-16">Python 3.11 New Features</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2022-12-11-pipenv/2022-12-11">Pipenv - Welcome NPM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2022-10-22-python-nlp/2022-10-22">Python - Natural Language Processing</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2022-10-15-python-rest-server/2022-10-15">Python - Build a REST API</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2022-10-14-python-rest-elastic/2022-10-14">Python - Working with the Elasticsearch REST API</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2022-10-13-python-rest-api/2022-10-13">Python - Working with REST API Requests</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2022-10-12-python-regular-expressions/2022-10-12">Python - RE</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2022-10-10-python-filesystem/2022-10-10">Python - The Filesystem</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2022-10-09-python-flask-elasticsearch/2022-10-09">Python - Flask Frontend to generate Elasticsearch Docs from Sitemaps</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2022-10-08-python-desktop-app/2022-10-08">Python - PyQt Desktop App</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2022-10-08-python-flask-app/2022-10-08">Python - Deploying a Web App with Flask</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2022-10-07-python-docusaurus-elasticsearch/2022-10-07">Python - Build an Elasticsearch Index for your Docusaurus Blog</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2022-10-06-python-minify-text/2022-10-06">Python - Minify Text for Elasticsearch</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2022-10-05-python-text-processing/2022-10-05">Python - Text Processing with</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2022-09-17-python-video-processing/2022-09-17">Python - Video Processing with OpenCV</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2022-06-27-python-web-scraping/2022-06-27">Web Scraping Essentials with Python</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2022-06-01-python-pyscript/2022-06-01">Introduction to PyScript</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2017-12-13--getting-started-with-python-part-ii/2017-12-13">Getting started with Python Part II</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2017-12-11--getting-started-with-python/2017-12-11">Getting started with Python</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Development/Python/2017-12-17--python-ssh-logger/2017-12-17">Python Network Logger</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/category/go">Go</a><button aria-label="Expand sidebar category &#x27;Go&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/category/graphs">Graphs</a><button aria-label="Expand sidebar category &#x27;Graphs&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/category/react-native">React Native</a><button aria-label="Expand sidebar category &#x27;React Native&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/category/magento">Magento</a><button aria-label="Expand sidebar category &#x27;Magento&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/category/misc">Misc</a><button aria-label="Expand sidebar category &#x27;Misc&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/devops">DevOps</a><button aria-label="Expand sidebar category &#x27;DevOps&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/machine-learning-ai-and-computer-vision">Machine Learning, AI and Computer Vision</a><button aria-label="Expand sidebar category &#x27;Machine Learning, AI and Computer Vision&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/automation-deep-vision-and-robotics">Automation, Deep Vision and Robotics</a><button aria-label="Expand sidebar category &#x27;Automation, Deep Vision and Robotics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/category/development"><span itemprop="name">Development</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/category/python"><span itemprop="name">Python</span></a><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">SciKit-Learn Cheat Sheet</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><p><img alt="Sham Sui Po, Hong Kong" src="/assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-5f44d483789c3ce79f05418f930f5cd2.jpg" width="1500" height="548"></p>
<h1 id="python-scikit-learn-cheat-sheet">Python SciKit-Learn Cheat Sheet</h1>
<ul>
<li>Simple and efficient tools for predictive data analysis</li>
<li>Accessible to everybody, and reusable in various contexts</li>
<li>Built on NumPy, SciPy, and matplotlib</li>
<li>Open source, commercially usable - BSD license</li>
</ul>
<p><img alt="SciKit-Learn Cheat Sheet" src="/assets/images/scikit_learn_model_selection-8d00a167fd915509325996a94149e242.png" width="1600" height="888"></p>
<p><a href="https://scikit-learn.org/stable/user_guide.html">Image Source: SciKit Learn User Guide</a></p>
<blockquote>
<p>Regressions ++ Classifications ++ Clustering ++ Dimensionality Reduction ++ Model Selection ++ Pre-processing</p>
</blockquote>
<p><a href="https://github.com/mpolinowski/python-scikitlearn-cheatsheet">Github Repository</a></p>
<ul>
<li><a href="#python-scikit-learn-cheat-sheet">Python SciKit-Learn Cheat Sheet</a>
<ul>
<li><a href="#working-with-missing-values">Working with Missing Values</a>
<ul>
<li><a href="#missing-indicator">Missing Indicator</a></li>
<li><a href="#simple-imputer">Simple Imputer</a></li>
<li><a href="#drop-missing-data">Drop Missing Data</a></li>
</ul>
</li>
<li><a href="#categorical-data-preprocessing">Categorical Data Preprocessing</a>
<ul>
<li><a href="#ordinal-encoder">Ordinal Encoder</a></li>
<li><a href="#label-encoder">Label Encoder</a></li>
<li><a href="#onehot--encoder">OneHot  Encoder</a></li>
</ul>
</li>
<li><a href="#loading-sk-datasets">Loading SK Datasets</a>
<ul>
<li><a href="#toy-datasets">Toy Datasets</a></li>
<li><a href="#real-world-datasets">Real World Datasets</a></li>
<li><a href="#openml-datasets">OpenML Datasets</a></li>
</ul>
</li>
<li><a href="#supervised-learning---regression-models">Supervised Learning - Regression Models</a>
<ul>
<li><a href="#simple-linear-regression">Simple Linear Regression</a>
<ul>
<li><a href="#data-pre-processing">Data Pre-processing</a></li>
<li><a href="#model-training">Model Training</a></li>
<li><a href="#predictions">Predictions</a></li>
<li><a href="#model-evaluation">Model Evaluation</a></li>
</ul>
</li>
<li><a href="#elasticnet-regression">ElasticNet Regression</a>
<ul>
<li><a href="#dataset">Dataset</a></li>
<li><a href="#preprocessing">Preprocessing</a></li>
<li><a href="#grid-search-for-hyperparameters">Grid Search for Hyperparameters</a></li>
<li><a href="#model-evaluation-1">Model Evaluation</a></li>
</ul>
</li>
<li><a href="#multiple-linear-regression">Multiple Linear Regression</a></li>
</ul>
</li>
<li><a href="#supervised-learning---logistic-regression-model">Supervised Learning - Logistic Regression Model</a>
<ul>
<li><a href="#binary-logistic-regression">Binary Logistic Regression</a>
<ul>
<li><a href="#dataset-1">Dataset</a></li>
<li><a href="#model-fitting">Model Fitting</a></li>
<li><a href="#model-predictions">Model Predictions</a></li>
<li><a href="#model-evaluation-2">Model Evaluation</a></li>
</ul>
</li>
<li><a href="#logistic-regression-pipelines">Logistic Regression Pipelines</a>
<ul>
<li><a href="#dataset-preprocessing">Dataset Preprocessing</a></li>
</ul>
</li>
<li><a href="#pipeline">Pipeline</a>
<ul>
<li><a href="#cross-validation">Cross Validation</a>
<ul>
<li><a href="#train--test-split">Train | Test Split</a></li>
<li><a href="#model-fitting-1">Model Fitting</a></li>
<li><a href="#model-evaluation-3">Model Evaluation</a></li>
<li><a href="#adjusting-hyper-parameter">Adjusting Hyper Parameter</a></li>
</ul>
</li>
<li><a href="#train--validation--test-split">Train | Validation | Test Split</a>
<ul>
<li><a href="#model-fitting-and-evaluation">Model Fitting and Evaluation</a></li>
<li><a href="#adjusting-hyper-parameter-1">Adjusting Hyper Parameter</a></li>
</ul>
</li>
<li><a href="#k-fold-cross-validation">k-fold Cross Validation</a>
<ul>
<li><a href="#train-test-split">Train-Test Split</a></li>
<li><a href="#model-scoring">Model Scoring</a></li>
<li><a href="#adjusting-hyper-parameter-2">Adjusting Hyper Parameter</a></li>
<li><a href="#model-fitting-and-final-evaluation">Model Fitting and Final Evaluation</a></li>
</ul>
</li>
<li><a href="#cross-validate">Cross Validate</a>
<ul>
<li><a href="#dataset-re-import">Dataset (re-import)</a></li>
<li><a href="#model-scoring-1">Model Scoring</a></li>
<li><a href="#adjusting-hyper-parameter-3">Adjusting Hyper Parameter</a></li>
<li><a href="#model-fitting-and-final-evaluation-1">Model Fitting and Final Evaluation</a></li>
</ul>
</li>
<li><a href="#grid-search">Grid Search</a>
<ul>
<li><a href="#hyperparameter-search">Hyperparameter Search</a></li>
<li><a href="#model-evaluation-4">Model Evaluation</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#supervised-learning---knn-algorithm">Supervised Learning - KNN Algorithm</a>
<ul>
<li><a href="#dataset-2">Dataset</a></li>
<li><a href="#data-pre-processing-1">Data Pre-processing</a></li>
<li><a href="#model-fitting-2">Model Fitting</a></li>
</ul>
</li>
<li><a href="#supervised-learning---decision-tree-classifier">Supervised Learning - Decision Tree Classifier</a>
<ul>
<li><a href="#dataset-3">Dataset</a></li>
<li><a href="#preprocessing-1">Preprocessing</a></li>
<li><a href="#model-fitting-3">Model Fitting</a></li>
<li><a href="#evaluation">Evaluation</a></li>
</ul>
</li>
<li><a href="#supervised-learning---random-forest-classifier">Supervised Learning - Random Forest Classifier</a>
<ul>
<li><a href="#dataset-4">Dataset</a></li>
<li><a href="#preprocessing-2">Preprocessing</a></li>
<li><a href="#model-fitting-4">Model Fitting</a></li>
<li><a href="#evaluation-1">Evaluation</a></li>
<li><a href="#random-forest-hyperparameter-tuning">Random Forest Hyperparameter Tuning</a>
<ul>
<li><a href="#testing-hyperparameters">Testing Hyperparameters</a></li>
<li><a href="#grid-search-cross-validation">Grid-Search Cross-Validation</a></li>
</ul>
</li>
<li><a href="#random-forest-classifier-1---penguins">Random Forest Classifier 1 - Penguins</a>
<ul>
<li><a href="#feature-importance">Feature Importance</a></li>
<li><a href="#model-evaluation-5">Model Evaluation</a></li>
</ul>
</li>
<li><a href="#random-forest-classifier---banknote-authentication">Random Forest Classifier - Banknote Authentication</a>
<ul>
<li><a href="#grid-search-for-hyperparameters-1">Grid Search for Hyperparameters</a></li>
<li><a href="#model-training-and-evaluation">Model Training and Evaluation</a></li>
<li><a href="#optimizations">Optimizations</a></li>
</ul>
</li>
<li><a href="#random-forest-regressor">Random Forest Regressor</a>
<ul>
<li><a href="#vs-linear-regression">vs Linear Regression</a></li>
<li><a href="#vs-polynomial-regression">vs Polynomial Regression</a></li>
<li><a href="#vs-kneighbors-regression">vs KNeighbors Regression</a></li>
<li><a href="#vs-decision-tree-regression">vs Decision Tree Regression</a></li>
<li><a href="#vs-support-vector-regression">vs Support Vector Regression</a></li>
<li><a href="#vs-gradient-boosting-regression">vs Gradient Boosting Regression</a></li>
<li><a href="#vs-ada-boosting-regression">vs Ada Boosting Regression</a></li>
<li><a href="#finally-random-forrest-regression">Finally, Random Forrest Regression</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#supervised-learning---svc-model">Supervised Learning - SVC Model</a>
<ul>
<li><a href="#dataset-5">Dataset</a>
<ul>
<li><a href="#preprocessing-3">Preprocessing</a></li>
<li><a href="#model-training-1">Model Training</a></li>
<li><a href="#model-evaluation-6">Model Evaluation</a></li>
</ul>
</li>
<li><a href="#margin-plots-for-support-vector-classifier">Margin Plots for Support Vector Classifier</a>
<ul>
<li><a href="#svc-with-a-linear-kernel">SVC with a Linear Kernel</a></li>
<li><a href="#svc-with-a-radial-basis-function-kernel">SVC with a Radial Basis Function Kernel</a></li>
<li><a href="#svc-with-a-sigmoid-kernel">SVC with a Sigmoid Kernel</a></li>
<li><a href="#svc-with-a-polynomial-kernel">SVC with a Polynomial Kernel</a></li>
</ul>
</li>
<li><a href="#grid-search-for-support-vector-classifier">Grid Search for Support Vector Classifier</a></li>
<li><a href="#support-vector-regression">Support Vector Regression</a>
<ul>
<li><a href="#base-model-run">Base Model Run</a></li>
<li><a href="#grid-search-for-better-hyperparameter">Grid Search for better Hyperparameter</a></li>
</ul>
</li>
<li><a href="#example-task---wine-fraud">Example Task - Wine Fraud</a>
<ul>
<li><a href="#data-exploration">Data Exploration</a></li>
<li><a href="#regression-model">Regression Model</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#supervised-learning---boosting-methods">Supervised Learning - Boosting Methods</a>
<ul>
<li><a href="#dataset-exploration">Dataset Exploration</a></li>
<li><a href="#adaptive-boosting">Adaptive Boosting</a>
<ul>
<li><a href="#feature-exploration">Feature Exploration</a></li>
<li><a href="#optimizing-hyperparameters">Optimizing Hyperparameters</a></li>
</ul>
</li>
<li><a href="#gradient-boosting">Gradient Boosting</a>
<ul>
<li><a href="#gridsearch-for-best-hyperparameter">Gridsearch for best Hyperparameter</a></li>
<li><a href="#feature-importance-1">Feature Importance</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#supervised-learning---naive-bayes-nlp">Supervised Learning - Naive Bayes NLP</a>
<ul>
<li><a href="#feature-extraction">Feature Extraction</a>
<ul>
<li><a href="#countvectorizer--tfidftransformer">CountVectorizer &amp; TfidfTransformer</a></li>
<li><a href="#tfidfvectorizer">TfidfVectorizer</a></li>
<li><a href="#dataset-exploration-1">Dataset Exploration</a></li>
<li><a href="#data-preprocessing">Data Preprocessing</a></li>
<li><a href="#tfidf-vectorizer">TFIDF Vectorizer</a></li>
<li><a href="#model-comparison">Model Comparison</a></li>
<li><a href="#model-deployment">Model Deployment</a></li>
</ul>
</li>
<li><a href="#text-classification">Text Classification</a>
<ul>
<li><a href="#data-exploration-1">Data Exploration</a></li>
<li><a href="#top-30-features-by-label">Top 30 Features by Label</a></li>
<li><a href="#data-preprocessing-1">Data Preprocessing</a></li>
<li><a href="#model-training-2">Model Training</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#unsupervised-learning---kmeans-clustering">Unsupervised Learning - KMeans Clustering</a>
<ul>
<li><a href="#dataset-exploration-2">Dataset Exploration</a></li>
<li><a href="#dataset-preprocessing-1">Dataset Preprocessing</a></li>
<li><a href="#model-training-3">Model Training</a></li>
<li><a href="#choosing-a-k-value">Choosing a K Value</a>
<ul>
<li><a href="#re-fitting-the-model">Re-fitting the Model</a></li>
</ul>
</li>
<li><a href="#example-1--color-quantization">Example 1 : Color Quantization</a></li>
<li><a href="#example-2--country-clustering">Example 2 : Country Clustering</a>
<ul>
<li><a href="#dataset-exploration-3">Dataset Exploration</a></li>
<li><a href="#dataset-preprocessing-2">Dataset Preprocessing</a></li>
<li><a href="#model-training-4">Model Training</a></li>
<li><a href="#model-evaluation-7">Model Evaluation</a></li>
<li><a href="#plotly-choropleth-map">Plotly Choropleth Map</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#unsupervised-learning---agglomerative-clustering">Unsupervised Learning - Agglomerative Clustering</a>
<ul>
<li><a href="#dataset-preprocessing-3">Dataset Preprocessing</a></li>
<li><a href="#assigning-cluster-labels">Assigning Cluster Labels</a>
<ul>
<li><a href="#known-number-of-clusters">Known Number of Clusters</a></li>
<li><a href="#unknown-number-of-clusters">Unknown Number of Clusters</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#unsupervised-learning---density-based-spatial-clustering-dbscan">Unsupervised Learning - Density-based Spatial Clustering (DBSCAN)</a>
<ul>
<li><a href="#dbscan-vs-kmeans">DBSCAN vs KMeans</a></li>
<li><a href="#dbscan-hyperparameter-tuning">DBSCAN Hyperparameter Tuning</a>
<ul>
<li><a href="#elbow-plot">Elbow Plot</a></li>
</ul>
</li>
<li><a href="#realworld-dataset">Realworld Dataset</a>
<ul>
<li><a href="#dataset-exploration-4">Dataset Exploration</a></li>
<li><a href="#data-preprocessing-2">Data Preprocessing</a></li>
<li><a href="#model-hyperparameter-tuning">Model Hyperparameter Tuning</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#dimensionality-reduction---principal-component-analysis-pca">Dimensionality Reduction - Principal Component Analysis (PCA)</a>
<ul>
<li><a href="#dataset-preprocessing-4">Dataset Preprocessing</a></li>
<li><a href="#model-fitting-5">Model Fitting</a></li>
<li><a href="#dataset-2">Dataset 2</a>
<ul>
<li><a href="#dataset-2-preprocessing">Dataset 2 Preprocessing</a></li>
<li><a href="#model-fitting-6">Model Fitting</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code class="language-python">import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from mpl_toolkits import mplot3d
import numpy as np
import pandas as pd
import plotly.express as px
from scipy.cluster import hierarchy
import seaborn as sns
from sklearn import svm
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.datasets import load_iris, load_wine, fetch_20newsgroups, fetch_openml
from sklearn.impute import MissingIndicator, SimpleImputer
from sklearn.decomposition import PCA
from sklearn.ensemble import (
    RandomForestClassifier,
    RandomForestRegressor,
    GradientBoostingRegressor,
    AdaBoostRegressor,
    GradientBoostingClassifier,
    AdaBoostClassifier
)
from sklearn.feature_extraction.text import (
    CountVectorizer,
    TfidfTransformer,
    TfidfVectorizer
)
from sklearn.linear_model import (
    LinearRegression,
    LogisticRegression,
    Ridge,
    ElasticNet
)
from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
    accuracy_score
)
from sklearn.model_selection import (
    train_test_split,
    GridSearchCV,
    cross_val_score,
    cross_validate
)
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import (
    MinMaxScaler,
    StandardScaler,
    OrdinalEncoder,
    LabelEncoder,
    OneHotEncoder,
    PolynomialFeatures
)
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
</code></pre>
<h2 id="working-with-missing-values">Working with Missing Values</h2>
<pre><code class="language-python">X_missing = pd.DataFrame(
    np.array([5,2,3,np.NaN,np.NaN,4,-3,2,1,8,np.NaN,4,10,np.NaN,5]).reshape(5,3)
)
X_missing.columns = [&#x27;f1&#x27;,&#x27;f2&#x27;,&#x27;f3&#x27;]

X_missing
</code></pre>
<table><thead><tr><th></th><th>f1</th><th>f2</th><th>f3</th></tr></thead><tbody><tr><td>0</td><td>5.0</td><td>2.0</td><td>3.0</td></tr><tr><td>1</td><td>NaN</td><td>NaN</td><td>4.0</td></tr><tr><td>2</td><td>-3.0</td><td>2.0</td><td>1.0</td></tr><tr><td>3</td><td>8.0</td><td>NaN</td><td>4.0</td></tr><tr><td>4</td><td>10.0</td><td>NaN</td><td>5.0</td></tr></tbody></table>
<pre><code class="language-python">X_missing.isnull().sum()

# f1    1
# f2    3
# f3    0
# dtype: int64
</code></pre>
<h3 id="missing-indicator">Missing Indicator</h3>
<pre><code class="language-python">indicator = MissingIndicator(missing_values=np.NaN)
indicator = indicator.fit_transform(X_missing)
indicator = pd.DataFrame(indicator, columns=[&#x27;a1&#x27;, &#x27;a2&#x27;])
indicator
</code></pre>
<table><thead><tr><th></th><th>a1</th><th>a2</th></tr></thead><tbody><tr><td>0</td><td>False</td><td>False</td></tr><tr><td>1</td><td>True</td><td>True</td></tr><tr><td>2</td><td>False</td><td>False</td></tr><tr><td>3</td><td>False</td><td>True</td></tr><tr><td>4</td><td>False</td><td>True</td></tr></tbody></table>
<h3 id="simple-imputer">Simple Imputer</h3>
<pre><code class="language-python">imputer_mean = SimpleImputer(missing_values=np.NaN, strategy=&#x27;mean&#x27;)
X_filled_mean = pd.DataFrame(imputer_mean.fit_transform(X_missing))
X_filled_mean.columns = [&#x27;f1&#x27;,&#x27;f2&#x27;,&#x27;f3&#x27;]
X_filled_mean
</code></pre>
<table><thead><tr><th></th><th>f1</th><th>f2</th><th>f3</th></tr></thead><tbody><tr><td>0</td><td>5.0</td><td>2.0</td><td>3.0</td></tr><tr><td>1</td><td>5.0</td><td>2.0</td><td>4.0</td></tr><tr><td>2</td><td>-3.0</td><td>2.0</td><td>1.0</td></tr><tr><td>3</td><td>8.0</td><td>2.0</td><td>4.0</td></tr><tr><td>4</td><td>10.0</td><td>2.0</td><td>5.0</td></tr></tbody></table>
<pre><code class="language-python">imputer_median = SimpleImputer(missing_values=np.NaN, strategy=&#x27;median&#x27;)
X_filled_median = pd.DataFrame(imputer_median.fit_transform(X_missing))
X_filled_median.columns = [&#x27;f1&#x27;,&#x27;f2&#x27;,&#x27;f3&#x27;]
X_filled_median
</code></pre>
<table><thead><tr><th></th><th>f1</th><th>f2</th><th>f3</th></tr></thead><tbody><tr><td>0</td><td>5.0</td><td>2.0</td><td>3.0</td></tr><tr><td>1</td><td>6.5</td><td>2.0</td><td>4.0</td></tr><tr><td>2</td><td>-3.0</td><td>2.0</td><td>1.0</td></tr><tr><td>3</td><td>8.0</td><td>2.0</td><td>4.0</td></tr><tr><td>4</td><td>10.0</td><td>2.0</td><td>5.0</td></tr></tbody></table>
<pre><code class="language-python">imputer_median = SimpleImputer(missing_values=np.NaN, strategy=&#x27;most_frequent&#x27;)
X_filled_median = pd.DataFrame(imputer_median.fit_transform(X_missing))
X_filled_median.columns = [&#x27;f1&#x27;,&#x27;f2&#x27;,&#x27;f3&#x27;]
X_filled_median
</code></pre>
<table><thead><tr><th></th><th>f1</th><th>f2</th><th>f3</th></tr></thead><tbody><tr><td>0</td><td>5.0</td><td>2.0</td><td>3.0</td></tr><tr><td>1</td><td>-3.0</td><td>2.0</td><td>4.0</td></tr><tr><td>2</td><td>-3.0</td><td>2.0</td><td>1.0</td></tr><tr><td>3</td><td>8.0</td><td>2.0</td><td>4.0</td></tr><tr><td>4</td><td>10.0</td><td>2.0</td><td>5.0</td></tr></tbody></table>
<h3 id="drop-missing-data">Drop Missing Data</h3>
<pre><code class="language-python">X_missing_dropped = X_missing.dropna(axis=1)
X_missing_dropped
</code></pre>
<table><thead><tr><th></th><th>f3</th></tr></thead><tbody><tr><td>0</td><td>3.0</td></tr><tr><td>1</td><td>4.0</td></tr><tr><td>2</td><td>1.0</td></tr><tr><td>3</td><td>4.0</td></tr><tr><td>4</td><td>5.0</td></tr></tbody></table>
<pre><code class="language-python">X_missing_dropped = X_missing.dropna(axis=0).reset_index()
X_missing_dropped
</code></pre>
<table><thead><tr><th></th><th>f1</th><th>f2</th><th>f3</th></tr></thead><tbody><tr><td>0</td><td>5.0</td><td>2.0</td><td>3.0</td></tr><tr><td>1</td><td>-3.0</td><td>2.0</td><td>1.0</td></tr></tbody></table>
<h2 id="categorical-data-preprocessing">Categorical Data Preprocessing</h2>
<pre><code class="language-python">X_cat_df = pd.DataFrame(
    np.array([
        [&#x27;M&#x27;, &#x27;O-&#x27;, &#x27;medium&#x27;],
        [&#x27;M&#x27;, &#x27;O-&#x27;, &#x27;high&#x27;],
        [&#x27;F&#x27;, &#x27;O+&#x27;, &#x27;high&#x27;],
        [&#x27;F&#x27;, &#x27;AB&#x27;, &#x27;low&#x27;],
        [&#x27;F&#x27;, &#x27;B+&#x27;, &#x27;medium&#x27;]
    ])
)

X_cat_df.columns = [&#x27;f1&#x27;,&#x27;f2&#x27;,&#x27;f3&#x27;]

X_cat_df
</code></pre>
<table><thead><tr><th></th><th>f1</th><th>f2</th><th>f3</th></tr></thead><tbody><tr><td>0</td><td>M</td><td>O-</td><td>medium</td></tr><tr><td>1</td><td>M</td><td>O-</td><td>high</td></tr><tr><td>2</td><td>F</td><td>O+</td><td>high</td></tr><tr><td>3</td><td>F</td><td>AB</td><td>low</td></tr><tr><td>4</td><td>F</td><td>B+</td><td>medium</td></tr></tbody></table>
<h3 id="ordinal-encoder">Ordinal Encoder</h3>
<pre><code class="language-python">encoder_ord = OrdinalEncoder(dtype=&#x27;int&#x27;)

X_cat_df.f3 = encoder_ord.fit_transform(X_cat_df.f3.values.reshape(-1, 1))
X_cat_df
</code></pre>
<table><thead><tr><th></th><th>f1</th><th>f2</th><th>f3</th></tr></thead><tbody><tr><td>0</td><td>M</td><td>O-</td><td>2</td></tr><tr><td>1</td><td>M</td><td>O-</td><td>0</td></tr><tr><td>2</td><td>F</td><td>O+</td><td>0</td></tr><tr><td>3</td><td>F</td><td>AB</td><td>1</td></tr><tr><td>4</td><td>F</td><td>B+</td><td>2</td></tr></tbody></table>
<h3 id="label-encoder">Label Encoder</h3>
<pre><code class="language-python">encoder_lab = LabelEncoder()
X_cat_df[&#x27;f2&#x27;] = encoder_lab.fit_transform(X_cat_df[&#x27;f2&#x27;])
X_cat_df
</code></pre>
<table><thead><tr><th></th><th>f1</th><th>f2</th><th>f3</th></tr></thead><tbody><tr><td>0</td><td>M</td><td>3</td><td>2</td></tr><tr><td>1</td><td>M</td><td>3</td><td>0</td></tr><tr><td>2</td><td>F</td><td>2</td><td>0</td></tr><tr><td>3</td><td>F</td><td>0</td><td>1</td></tr><tr><td>4</td><td>F</td><td>1</td><td>2</td></tr></tbody></table>
<h3 id="onehot--encoder">OneHot  Encoder</h3>
<pre><code class="language-python">encoder_oh = OneHotEncoder(dtype=&#x27;int&#x27;)

onehot_df = pd.DataFrame(
    encoder_oh.fit_transform(X_cat_df[[&#x27;f1&#x27;]])
    .toarray(),
    columns=[&#x27;F&#x27;, &#x27;M&#x27;]
)

onehot_df[&#x27;f2&#x27;] = X_cat_df.f2
onehot_df[&#x27;f3&#x27;] = X_cat_df.f3
onehot_df
</code></pre>
<table><thead><tr><th></th><th>F</th><th>M</th><th>f2</th><th>f3</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>1</td><td>3</td><td>2</td></tr><tr><td>1</td><td>0</td><td>1</td><td>3</td><td>0</td></tr><tr><td>2</td><td>1</td><td>0</td><td>2</td><td>0</td></tr><tr><td>3</td><td>1</td><td>0</td><td>0</td><td>1</td></tr><tr><td>4</td><td>1</td><td>0</td><td>1</td><td>2</td></tr></tbody></table>
<h2 id="loading-sk-datasets">Loading SK Datasets</h2>
<h3 id="toy-datasets">Toy Datasets</h3>
<table><thead><tr><th></th><th></th><th></th></tr></thead><tbody><tr><td>load_iris(*[, return_X_y, as_frame])</td><td>classification</td><td>Load and return the iris dataset.</td></tr><tr><td>load_diabetes(*[, return_X_y, as_frame, scaled])</td><td>regression</td><td>Load and return the diabetes dataset.</td></tr><tr><td>load_digits(*[, n_class, return_X_y, as_frame])</td><td>classification</td><td>Load and return the digits dataset.</td></tr><tr><td>load_linnerud(*[, return_X_y, as_frame])</td><td>multi-output regression</td><td>Load and return the physical exercise Linnerud dataset.</td></tr><tr><td>load_wine(*[, return_X_y, as_frame])</td><td>classification</td><td>Load and return the wine dataset.</td></tr><tr><td>load_breast_cancer(*[, return_X_y, as_frame])</td><td>classification</td><td>Load and return the breast cancer wisconsin dataset.</td></tr></tbody></table>
<pre><code class="language-python">iris_ds = load_iris()
iris_data = iris_ds.data
col_names = iris_ds.feature_names
target_names = iris_ds.target_names

print(
    &#x27;Iris Dataset&#x27;,
    &#x27;\n * Data array: &#x27;,
    iris_data.shape,
    &#x27;\n * Column names: &#x27;,
    col_names,
    &#x27;\n * Target names: &#x27;,
    target_names
)

# Iris Dataset 
#  * Data array:  (150, 4) 
#  * Column names:  [&#x27;sepal length (cm)&#x27;, &#x27;sepal width (cm)&#x27;, &#x27;petal length (cm)&#x27;, &#x27;petal width (cm)&#x27;] 
#  * Target names:  [&#x27;setosa&#x27; &#x27;versicolor&#x27; &#x27;virginica&#x27;]
</code></pre>
<pre><code class="language-python">iris_df = pd.DataFrame(data=iris_data, columns=col_names)

iris_df.head()
</code></pre>
<table><thead><tr><th></th><th>sepal length (cm)</th><th>sepal width (cm)</th><th>petal length (cm)</th><th>petal width (cm)</th></tr></thead><tbody><tr><td>0</td><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td></tr><tr><td>1</td><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td></tr><tr><td>2</td><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td></tr><tr><td>3</td><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td></tr><tr><td>4</td><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td></tr></tbody></table>
<h3 id="real-world-datasets">Real World Datasets</h3>
<table><thead><tr><th></th><th></th><th></th></tr></thead><tbody><tr><td>fetch_olivetti_faces(*[, data_home, ...])</td><td>classification</td><td>Load the Olivetti faces data-set from AT&amp;T.</td></tr><tr><td>fetch_20newsgroups(*[, data_home, subset, ...])</td><td>classification</td><td>Load the filenames and data from the 20 newsgroups dataset.</td></tr><tr><td>fetch_20newsgroups_vectorized(*[, subset, ...])</td><td>classification</td><td>Load and vectorize the 20 newsgroups dataset.</td></tr><tr><td>fetch_lfw_people(*[, data_home, funneled, ...])</td><td>classification</td><td>Load the Labeled Faces in the Wild (LFW) people dataset.</td></tr><tr><td>fetch_lfw_pairs(*[, subset, data_home, ...])</td><td>classification</td><td>Load the Labeled Faces in the Wild (LFW) pairs dataset.</td></tr><tr><td>fetch_covtype(*[, data_home, ...])</td><td>classification</td><td>Load the covertype dataset.</td></tr><tr><td>fetch_rcv1(*[, data_home, subset, ...])</td><td>classification</td><td>Load the RCV1 multilabel dataset.</td></tr><tr><td>fetch_kddcup99(*[, subset, data_home, ...])</td><td>classification</td><td>Load the kddcup99 dataset.</td></tr><tr><td>fetch_california_housing(*[, data_home, ...])</td><td>regression</td><td>Load the California housing dataset.</td></tr></tbody></table>
<pre><code class="language-python">newsgroups_train = fetch_20newsgroups(subset=&#x27;train&#x27;)
train_data = newsgroups_train.data
col_names = newsgroups_train.filenames.shape
target_names = newsgroups_train.target.shape

print(
    &#x27;Newsgroup - Train Subset&#x27;,
    &#x27;\n * Data array: &#x27;,
    len(train_data),
    &#x27;\n * Column names: &#x27;,
    col_names,
    &#x27;\n * Target names: &#x27;,
    target_names
)

# Newsgroup - Train Subset 
#  * Data array:  11314 
#  * Column names:  (11314,) 
#  * Target names:  (11314,)
</code></pre>
<pre><code class="language-python">print(&#x27;Target Names: &#x27;, newsgroups_train.target_names)

# Target Names:  [&#x27;alt.atheism&#x27;, &#x27;comp.graphics&#x27;, &#x27;comp.os.ms-windows.misc&#x27;, &#x27;comp.sys.ibm.pc.hardware&#x27;, &#x27;comp.sys.mac.hardware&#x27;, &#x27;comp.windows.x&#x27;, &#x27;misc.forsale&#x27;, &#x27;rec.autos&#x27;, &#x27;rec.motorcycles&#x27;, &#x27;rec.sport.baseball&#x27;, &#x27;rec.sport.hockey&#x27;, &#x27;sci.crypt&#x27;, &#x27;sci.electronics&#x27;, &#x27;sci.med&#x27;, &#x27;sci.space&#x27;, &#x27;soc.religion.christian&#x27;, &#x27;talk.politics.guns&#x27;, &#x27;talk.politics.mideast&#x27;, &#x27;talk.politics.misc&#x27;, &#x27;talk.religion.misc&#x27;]
</code></pre>
<h3 id="openml-datasets">OpenML Datasets</h3>
<ul>
<li><a href="https://openml.org/search?type=data&amp;sort=runs&amp;status=active">openml.org</a></li>
<li><a href="https://openml.org/search?type=data&amp;status=active&amp;id=40966">Mice Protein Dataset</a></li>
</ul>
<pre><code class="language-python">mice_ds = fetch_openml(name=&#x27;miceprotein&#x27;, version=4, parser=&quot;auto&quot;)
</code></pre>
<pre><code class="language-python">print(
    &#x27;Mice Protein Dataset&#x27;,
    &#x27;\n * Data Shape: &#x27;,
    mice_ds.data.shape,
    &#x27;\n * Target Shape: &#x27;,
    mice_ds.target.shape,
    &#x27;\n * Target Names: &#x27;,
    np.unique(mice_ds.target)
)

# Mice Protein Dataset 
#  * Data Shape:  (1080, 77) 
#  * Target Shape:  (1080,) 
#  * Target Names:  [&#x27;c-CS-m&#x27; &#x27;c-CS-s&#x27; &#x27;c-SC-m&#x27; &#x27;c-SC-s&#x27; &#x27;t-CS-m&#x27; &#x27;t-CS-s&#x27; &#x27;t-SC-m&#x27; &#x27;t-SC-s&#x27;]

</code></pre>
<pre><code class="language-python">print(mice_ds.DESCR)
</code></pre>
<h2 id="supervised-learning---regression-models">Supervised Learning - Regression Models</h2>
<h3 id="simple-linear-regression">Simple Linear Regression</h3>
<pre><code class="language-python">iris_df.plot(
    figsize=(12,5),
    kind=&#x27;scatter&#x27;,
    x=&#x27;sepal length (cm)&#x27;,
    y=&#x27;sepal width (cm)&#x27;,
    title=&#x27;Iris Dataset :: Sepal Width&amp;Height&#x27;
)

print(iris_df.corr())
</code></pre>
<blockquote>
<p>The <strong>Sepal Width</strong> has very little correlation to all other metrics but itself. While the other three correlate nicely:</p>
</blockquote>
<table><thead><tr><th></th><th>sepal length (cm)</th><th>sepal width (cm)</th><th>petal length (cm)</th><th>petal width (cm)</th></tr></thead><tbody><tr><td>sepal length (cm)</td><td>1.000000</td><td>-0.117570</td><td>0.871754</td><td>0.817941</td></tr><tr><td>sepal width (cm)</td><td>-0.117570</td><td>1.000000</td><td>-0.428440</td><td>-0.366126</td></tr><tr><td>petal length (cm)</td><td>0.871754</td><td>-0.428440</td><td>1.000000</td><td>0.962865</td></tr><tr><td>petal width (cm)</td><td>0.817941</td><td>-0.366126</td><td>0.962865</td><td>1.000000</td></tr></tbody></table>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_01-84d3cb1be83a80102336af106a05191f.webp" width="1001" height="470"></p>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_02-4e4bb2ff890901e98b3170eaa491f2c9.webp" width="1001" height="470"></p>
<h4 id="data-pre-processing">Data Pre-processing</h4>
<pre><code class="language-python">iris_df[&#x27;petal length (cm)&#x27;][:1]
# 0    1.4
# Name: petal length (cm), dtype: float64
</code></pre>
<pre><code class="language-python">iris_df[&#x27;petal length (cm)&#x27;].values.reshape(-1,1)[:1]
# array([[1.4]])
</code></pre>
<pre><code class="language-python"># scikit expects a 2s imput =&gt; remove index
X = iris_df[&#x27;petal length (cm)&#x27;].values.reshape(-1,1)
y = iris_df[&#x27;petal width (cm)&#x27;].values.reshape(-1,1)
</code></pre>
<pre><code class="language-python"># train/test split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)
print(X_train.shape, X_test.shape)
# (120, 1) (30, 1) 80:20 split
</code></pre>
<h4 id="model-training">Model Training</h4>
<pre><code class="language-python">regressor = LinearRegression()
regressor.fit(X_train,y_train)

intercept = regressor.intercept_
slope = regressor.coef_

print(&#x27; Intercept: &#x27;, intercept, &#x27;\n Slope: &#x27;, slope)
#  Intercept:  [-0.35135666] 
#  Correlation Coeficient:  [[0.41310505]]
</code></pre>
<h4 id="predictions">Predictions</h4>
<pre><code class="language-python">y_pred = regressor.predict([X_test[0]])
print(&#x27; Prediction: &#x27;, y_pred, &#x27;\n True Value: &#x27;, y_test[0])
#  Prediction:  [[0.22699041]] 
#  True Value:  [0.2]
</code></pre>
<pre><code class="language-python">def predict(value):
    return (slope*value + intercept)[0][0]
</code></pre>
<pre><code class="language-python">print(&#x27;Prediction: &#x27;, predict(X_test[0]))
# Prediction:  [[0.22699041]]
</code></pre>
<pre><code class="language-python">iris_df[&#x27;petal width (cm) prediction&#x27;] = iris_df[&#x27;petal length (cm)&#x27;].apply(predict)
print(&#x27; Prediction: &#x27;, iris_df[&#x27;petal width (cm) prediction&#x27;][0], &#x27;\n True Value: &#x27;, iris_df[&#x27;petal width (cm)&#x27;][0])
#  Prediction:  0.22699041280334376 
#  True Value:  0.2
</code></pre>
<pre><code class="language-python">iris_df.head(10)
</code></pre>
<table><thead><tr><th></th><th>sepal length (cm)</th><th>sepal width (cm)</th><th>petal length (cm)</th><th>petal width (cm)</th><th>petal width (cm) prediction</th></tr></thead><tbody><tr><td>0</td><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>0.226990</td></tr><tr><td>1</td><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>0.226990</td></tr><tr><td>2</td><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>0.185680</td></tr><tr><td>3</td><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>0.268301</td></tr><tr><td>4</td><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>0.226990</td></tr><tr><td>5</td><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>0.350922</td></tr><tr><td>6</td><td>4.6</td><td>3.4</td><td>1.4</td><td>0.3</td><td>0.226990</td></tr><tr><td>7</td><td>5.0</td><td>3.4</td><td>1.5</td><td>0.2</td><td>0.268301</td></tr><tr><td>8</td><td>4.4</td><td>2.9</td><td>1.4</td><td>0.2</td><td>0.226990</td></tr><tr><td>9</td><td>4.9</td><td>3.1</td><td>1.5</td><td>0.1</td><td>0.268301</td></tr></tbody></table>
<pre><code class="language-python">iris_df.plot(
    figsize=(12,5),
    kind=&#x27;scatter&#x27;,
    x=&#x27;petal width (cm)&#x27;,
    y=&#x27;petal width (cm) prediction&#x27;,
    # no value in colorizing..just looks pretty
    c=&#x27;petal width (cm) prediction&#x27;,
    colormap=&#x27;summer&#x27;,
    title=&#x27;Iris Dataset - Sepal Width True vs Prediction&#x27;
)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_03-ae1d61b67118f984fd72dee177007a8c.webp" width="932" height="470"></p>
<h4 id="model-evaluation">Model Evaluation</h4>
<pre><code class="language-python">mae = mean_absolute_error(
    iris_df[&#x27;petal width (cm)&#x27;],
    iris_df[&#x27;petal width (cm) prediction&#x27;]
)

mse = mean_squared_error(
    iris_df[&#x27;petal width (cm)&#x27;],
    iris_df[&#x27;petal width (cm) prediction&#x27;]
)

rmse = np.sqrt(mse)

print(&#x27; MAE: &#x27;, mae, &#x27;\n MSE: &#x27;, mse, &#x27;\n RMSE: &#x27;, rmse)

#  MAE:  0.1569441318761155 
#  MSE:  0.04209214667485277 
#  RMSE:  0.2051637070118708
</code></pre>
<h3 id="elasticnet-regression">ElasticNet Regression</h3>
<h4 id="dataset">Dataset</h4>
<pre><code class="language-python">!wget https://raw.githubusercontent.com/Satish-Vennapu/DataScience/main/AMES_Final_DF.csv -P datasets
</code></pre>
<pre><code class="language-python">ames_df = pd.read_csv(&#x27;datasets/AMES_Final_DF.csv&#x27;)
ames_df.head(5).transpose()
</code></pre>
<table><thead><tr><th></th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>Lot Frontage</td><td>141.0</td><td>80.0</td><td>81.0</td><td>93.0</td><td>74.0</td></tr><tr><td>Lot Area</td><td>31770.0</td><td>11622.0</td><td>14267.0</td><td>11160.0</td><td>13830.0</td></tr><tr><td>Overall Qual</td><td>6.0</td><td>5.0</td><td>6.0</td><td>7.0</td><td>5.0</td></tr><tr><td>Overall Cond</td><td>5.0</td><td>6.0</td><td>6.0</td><td>5.0</td><td>5.0</td></tr><tr><td>Year Built</td><td>1960.0</td><td>1961.0</td><td>1958.0</td><td>1968.0</td><td>1997.0</td></tr><tr><td>...</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Sale Condition_AdjLand</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>Sale Condition_Alloca</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>Sale Condition_Family</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>Sale Condition_Normal</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>Sale Condition_Partial</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td><em>274 rows × 5 columns</em></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table>
<pre><code class="language-python"># the target value is:
ames_df[&#x27;SalePrice&#x27;]
</code></pre>
<table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>0</td><td>215000</td></tr><tr><td>1</td><td>105000</td></tr><tr><td>2</td><td>172000</td></tr><tr><td>3</td><td>244000</td></tr><tr><td>4</td><td>189900</td></tr><tr><td>...</td><td></td></tr><tr><td>2920</td><td>142500</td></tr><tr><td>2921</td><td>131000</td></tr><tr><td>2922</td><td>132000</td></tr><tr><td>2923</td><td>170000</td></tr><tr><td>2924</td><td>188000</td></tr><tr><td><em>Name: SalePrice, Length: 2925, dtype: int64</em></td><td></td></tr></tbody></table>
<h4 id="preprocessing">Preprocessing</h4>
<pre><code class="language-python"># remove target column from training dataset
X_ames = ames_df.drop(&#x27;SalePrice&#x27;, axis=1)
y_ames = ames_df[&#x27;SalePrice&#x27;]

print(X_ames.shape, y_ames.shape)
# (2925, 273) (2925,)
</code></pre>
<pre><code class="language-python"># train/test split
X_ames_train, X_ames_test, y_ames_train, y_ames_test = train_test_split(
    X_ames,
    y_ames,
    test_size=0.1,
    random_state=101
)

print(X_ames_train.shape, X_ames_test.shape)
# (2632, 273) (293, 273)
</code></pre>
<pre><code class="language-python"># normalize feature set
scaler = StandardScaler()
X_ames_train_scaled = scaler.fit_transform(X_ames_train)

X_ames_test_scaled = scaler.transform(X_ames_test)
</code></pre>
<h4 id="grid-search-for-hyperparameters">Grid Search for Hyperparameters</h4>
<pre><code class="language-python">base_ames_elastic_net_model = ElasticNet(max_iter=int(1e4))
</code></pre>
<pre><code class="language-python">param_grid = \{
    &#x27;alpha&#x27;: [50, 75, 100, 125, 150],
    &#x27;l1_ratio&#x27;:[0.2, 0.4, 0.6, 0.8, 1.0]
\}
</code></pre>
<pre><code class="language-python">grid_ames_model = GridSearchCV(
    estimator=base_ames_elastic_net_model,
    param_grid=param_grid,
    scoring=&#x27;neg_mean_squared_error&#x27;,
    cv=5, verbose=1
)

grid_ames_model.fit(X_ames_train_scaled, y_ames_train)

print(
    &#x27;Results:\nBest Estimator: &#x27;,
    grid_ames_model.best_estimator_,
    &#x27;\nBest Hyperparameter: &#x27;,
    grid_ames_model.best_params_
)
</code></pre>
<p><strong>Results</strong>:</p>
<ul>
<li>Best Estimator:  <code>ElasticNet(alpha=125, l1_ratio=1.0, max_iter=10000)</code></li>
<li>Best Hyperparameter:  <code>\{&#x27;alpha&#x27;: 125, &#x27;l1_ratio&#x27;: 1.0\}</code></li>
</ul>
<h4 id="model-evaluation-1">Model Evaluation</h4>
<pre><code class="language-python">y_ames_pred = grid_ames_model.predict(X_ames_test_scaled)

print(
    &#x27;MAE: &#x27;,
    mean_absolute_error(y_ames_test, y_ames_pred),
    &#x27;MSE: &#x27;,
    mean_squared_error(y_ames_test, y_ames_pred),
    &#x27;RMSE: &#x27;,
    np.sqrt(mean_squared_error(y_ames_test, y_ames_pred))
)

# MAE:  14185.506207185055 MSE:  422714457.5190704 RMSE:  20560.020854052418
</code></pre>
<pre><code class="language-python"># average SalePrize
np.mean(ames_df[&#x27;SalePrice&#x27;])
# 180815.53743589742

rel_error_avg = mean_absolute_error(y_ames_test, y_ames_pred) * 100 / np.mean(ames_df[&#x27;SalePrice&#x27;])
print(&#x27;Pridictions are on average off by: &#x27;, rel_error_avg.round(2), &#x27;%&#x27;)
# Pridictions are on average off by:  7.85 %
</code></pre>
<pre><code class="language-python">plt.figure(figsize=(10,4))

plt.scatter(y_ames_test,y_ames_pred, c=&#x27;mediumspringgreen&#x27;, s=3)
plt.axline((0, 0), slope=1, color=&#x27;dodgerblue&#x27;, linestyle=(&#x27;:&#x27;))

plt.title(&#x27;Prediction Accuracy :: MAE:&#x27;+ str(mean_absolute_error(y_ames_test, y_ames_pred).round(2)) + &#x27;US$&#x27;)
plt.xlabel(&#x27;True Sales Price&#x27;)
plt.ylabel(&#x27;Predicted Sales Price&#x27;)
plt.savefig(&#x27;assets/Scikit_Learn_11.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_11-0a80f239d95b1f8282ec3905d351df3c.webp" width="876" height="393"></p>
<h3 id="multiple-linear-regression">Multiple Linear Regression</h3>
<p>Above I used the <code>petal width</code> and <code>length</code> to create a linear regression model. But as explored earlier we can also use the <code>sepal length</code> (only the <code>sepal width</code> does not show a linear correlation):</p>
<pre><code class="language-python">print(iris_df.corr())
</code></pre>
<table><thead><tr><th></th><th>sepal length (cm)</th><th>sepal width (cm)</th><th>petal length (cm)</th><th>petal width (cm)</th></tr></thead><tbody><tr><td>sepal length (cm)</td><td>1.000000</td><td>-0.117570</td><td>0.871754</td><td>0.817941</td></tr><tr><td>sepal width (cm)</td><td>-0.117570</td><td>1.000000</td><td>-0.428440</td><td>-0.366126</td></tr><tr><td>petal length (cm)</td><td>0.871754</td><td>-0.428440</td><td>1.000000</td><td>0.962865</td></tr><tr><td>petal width (cm)</td><td>0.817941</td><td>-0.366126</td><td>0.962865</td><td>1.000000</td></tr></tbody></table>
<pre><code class="language-python">X_multi = iris_df[[&#x27;petal length (cm)&#x27;, &#x27;sepal length (cm)&#x27;]]
y = iris_df[&#x27;petal width (cm)&#x27;]
</code></pre>
<pre><code class="language-python">regressor_multi = LinearRegression()
regressor_multi.fit(X_multi, y)

intercept_multi = regressor_multi.intercept_
slope_multi = regressor_multi.coef_

print(&#x27; Intercept: &#x27;, intercept_multi, &#x27;\n Slope: &#x27;, slope_multi)

#  Intercept:  -0.00899597269816943 
#  Slope:  [ 0.44937611 -0.08221782]
</code></pre>
<pre><code class="language-python">def predict_multi(petal_length, sepal_length):
    return (slope_multi[0]*petal_length + slope_multi[1]*sepal_length + intercept_multi)
</code></pre>
<pre><code class="language-python">y_pred = predict_multi(
    iris_df[&#x27;petal length (cm)&#x27;][0],
    iris_df[&#x27;sepal length (cm)&#x27;][0]
)

print(&#x27; Prediction: &#x27;, y_pred, &#x27;\n True value: &#x27;, iris_df[&#x27;petal width (cm)&#x27;][0])
#  Prediction:  0.20081970121763193 
#  True value:  0.2
</code></pre>
<pre><code class="language-python">iris_df[&#x27;petal width (cm) prediction (multi)&#x27;] = (
    (
        slope_multi[0] * iris_df[&#x27;petal length (cm)&#x27;]
    ) + (
        slope_multi[1] * iris_df[&#x27;sepal length (cm)&#x27;]
    ) + (
        intercept_multi
    ) 
)
</code></pre>
<pre><code class="language-python">iris_df.head(10)
</code></pre>
<table><thead><tr><th></th><th>sepal length (cm)</th><th>sepal width (cm)</th><th>petal length (cm)</th><th>petal width (cm)</th><th>petal width (cm) prediction</th><th>petal width (cm) prediction (multi)</th></tr></thead><tbody><tr><td>0</td><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>0.226990</td><td>0.200820</td></tr><tr><td>1</td><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>0.226990</td><td>0.217263</td></tr><tr><td>2</td><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>0.185680</td><td>0.188769</td></tr><tr><td>3</td><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>0.268301</td><td>0.286866</td></tr><tr><td>4</td><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>0.226990</td><td>0.209041</td></tr><tr><td>5</td><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>0.350922</td><td>0.310967</td></tr><tr><td>6</td><td>4.6</td><td>3.4</td><td>1.4</td><td>0.3</td><td>0.226990</td><td>0.241929</td></tr><tr><td>7</td><td>5.0</td><td>3.4</td><td>1.5</td><td>0.2</td><td>0.268301</td><td>0.253979</td></tr><tr><td>8</td><td>4.4</td><td>2.9</td><td>1.4</td><td>0.2</td><td>0.226990</td><td>0.258372</td></tr><tr><td>9</td><td>4.9</td><td>3.1</td><td>1.5</td><td>0.1</td><td>0.268301</td><td>0.262201</td></tr></tbody></table>
<pre><code class="language-python">iris_df.plot(
    figsize=(12,5),
    kind=&#x27;scatter&#x27;,
    x=&#x27;petal width (cm)&#x27;,
    y=&#x27;petal width (cm) prediction (multi)&#x27;,
    c=&#x27;petal width (cm) prediction&#x27;,
    colormap=&#x27;summer&#x27;,
    title=&#x27;Iris Dataset - Sepal Width True vs Prediction (multi)&#x27;
)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_04-598be509fe770526f7d831203daee448.webp" width="932" height="470"></p>
<pre><code class="language-python">mae_multi = mean_absolute_error(
    iris_df[&#x27;petal width (cm)&#x27;],
    iris_df[&#x27;petal width (cm) prediction (multi)&#x27;]
)

mse_multi = mean_squared_error(
    iris_df[&#x27;petal width (cm)&#x27;],
    iris_df[&#x27;petal width (cm) prediction (multi)&#x27;]
)

rmse_multi = np.sqrt(mse_multi)

print(&#x27; MAE_Multi: &#x27;, mae_multi,&#x27; MAE: &#x27;, mae, &#x27;\n MSE_Multi: &#x27;, mse_multi, &#x27; MSE: &#x27;, mse, &#x27;\n RMSE_Multi: &#x27;, rmse_multi, &#x27; RMSE: &#x27;, rmse)
</code></pre>
<p>The accuracy of the model was improved by adding an additional, correlating value:</p>
<table><thead><tr><th></th><th>Multi Regression</th><th>Single Regression</th></tr></thead><tbody><tr><td>Mean Absolute Error</td><td>0.15562108079300102</td><td>0.1569441318761155</td></tr><tr><td>Mean Squared Error</td><td>0.04096208526408982</td><td>0.04209214667485277</td></tr><tr><td>Root Mean Squared Error</td><td>0.20239092189149646</td><td>0.2051637070118708</td></tr></tbody></table>
<h2 id="supervised-learning---logistic-regression-model">Supervised Learning - Logistic Regression Model</h2>
<h3 id="binary-logistic-regression">Binary Logistic Regression</h3>
<h4 id="dataset-1">Dataset</h4>
<pre><code class="language-python">np.random.seed(666)

# generate 10 index values between 0-10
x_data_logistic_binary = np.random.randint(10, size=(10)).reshape(-1, 1)
# generate binary category for values above
y_data_logistic_binary = np.random.randint(2, size=10)
</code></pre>
<h4 id="model-fitting">Model Fitting</h4>
<pre><code class="language-python">logistic_binary_model = LogisticRegression(
    solver=&#x27;liblinear&#x27;,
    C=10.0,
    random_state=0
)

logistic_binary_model.fit(x_data_logistic_binary, y_data_logistic_binary)

intercept_logistic_binary = logistic_binary_model.intercept_
slope_logistic_binary = logistic_binary_model.coef_

print(&#x27; Intercept: &#x27;, intercept_logistic_binary, &#x27;\n Slope: &#x27;, slope_logistic_binary)

#  Intercept:  [-0.4832956] 
#  Slope:  [[0.11180522]]
</code></pre>
<h4 id="model-predictions">Model Predictions</h4>
<pre><code class="language-python">prob_pred_logistic_binary = logistic_binary_model.predict_proba(x_data_logistic_binary)
y_pred_logistic_binary = logistic_binary_model.predict(x_data_logistic_binary)


print(&#x27;Prediction Probabilities: &#x27;, prob_pred[:1])

unique, counts = np.unique(y_pred_logistic_binary, return_counts=True)
print(&#x27;Classes: &#x27;, unique, &#x27;| Number of Class Instances: &#x27;, counts)

# probabilities e.g. below -&gt; 58% certainty that the first element is class 0

# Prediction Probabilities:  [[0.58097284 0.41902716]]
# Classes:  [0 1] | Number of Class Instances:  [5 5]
</code></pre>
<h4 id="model-evaluation-2">Model Evaluation</h4>
<pre><code class="language-python">conf_mtx = confusion_matrix(y_data_logistic_binary, y_pred_logistic_binary)
conf_mtx

# [2, 3] [TP, FP]
# [3, 2] [FN, TN]
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/confusion-matrix-2123c58a149ecab802adaac75f2fd15a.webp" width="816" height="459"></p>
<pre><code class="language-python">report = classification_report(y_data_logistic_binary, y_pred_logistic_binary)
print(report)
</code></pre>
<table><thead><tr><th></th><th>precision</th><th>recall</th><th>f1-score</th><th>support</th></tr></thead><tbody><tr><td>0</td><td>0.40</td><td>0.40</td><td>0.40</td><td>5</td></tr><tr><td>1</td><td>0.40</td><td>0.40</td><td>0.40</td><td>5</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.40</td><td>10</td></tr><tr><td>macro avg</td><td>0.40</td><td>0.40</td><td>0.40</td><td>10</td></tr><tr><td>weighted avg</td><td>0.40</td><td>0.40</td><td>0.40</td><td>10</td></tr></tbody></table>
<h3 id="logistic-regression-pipelines">Logistic Regression Pipelines</h3>
<h4 id="dataset-preprocessing">Dataset Preprocessing</h4>
<pre><code class="language-python">iris_ds = load_iris()

# train/test split
X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(
    iris_ds.data,
    iris_ds.target,
    test_size=0.2,
    random_state=42
)
print(X_train_iris.shape, X_test_iris.shape)
# (120, 4) (30, 4)
</code></pre>
<h3 id="pipeline">Pipeline</h3>
<pre><code class="language-python">pipe_iris = Pipeline([
    (&#x27;minmax&#x27;, MinMaxScaler()),
    (&#x27;log_reg&#x27;, LogisticRegression()),
])

pipe_iris.fit(X_train_iris, y_train_iris)
</code></pre>
<pre><code class="language-python">iris_score = pipe_iris.score(X_test_iris, y_test_iris)
print(&#x27;Prediction Accuracy: &#x27;, iris_score.round(4)*100, &#x27;%&#x27;)
# Prediction Accuracy:  96.67 %
</code></pre>
<h4 id="cross-validation">Cross Validation</h4>
<h5 id="train--test-split">Train | Test Split</h5>
<pre><code class="language-python">!wget https://raw.githubusercontent.com/reisanar/datasets/master/Advertising.csv -P datasets
</code></pre>
<pre><code class="language-python">adv_df = pd.read_csv(&#x27;datasets/Advertising.csv&#x27;)
adv_df.head(5)
</code></pre>
<table><thead><tr><th></th><th>TV</th><th>Radio</th><th>Newspaper</th><th>Sales</th></tr></thead><tbody><tr><td>0</td><td>230.1</td><td>37.8</td><td>69.2</td><td>22.1</td></tr><tr><td>1</td><td>44.5</td><td>39.3</td><td>45.1</td><td>10.4</td></tr><tr><td>2</td><td>17.2</td><td>45.9</td><td>69.3</td><td>9.3</td></tr><tr><td>3</td><td>151.5</td><td>41.3</td><td>58.5</td><td>18.5</td></tr><tr><td>4</td><td>180.8</td><td>10.8</td><td>58.4</td><td>12.9</td></tr></tbody></table>
<pre><code class="language-python"># Split ds into features and targets
X_adv = adv_df.drop(&#x27;Sales&#x27;, axis=1)
y_adv = adv_df[&#x27;Sales&#x27;]
</code></pre>
<pre><code class="language-python"># 70:30 train/test split
X_adv_train, X_adv_test, y_adv_train, y_adv_test = train_test_split(
    X_adv, y_adv, test_size=0.3, random_state=666
)

print(X_adv_train.shape, y_adv_train.shape)
# (140, 3) (140,)
</code></pre>
<pre><code class="language-python"># normalize features
scaler_adv = StandardScaler()
scaler_adv.fit(X_adv_train)

X_adv_train = scaler_adv.transform(X_adv_train)
X_adv_test = scaler_adv.transform(X_adv_test)
</code></pre>
<h5 id="model-fitting-1">Model Fitting</h5>
<pre><code class="language-python">model_adv1 = Ridge(
    alpha=100.0
)

model_adv1.fit(X_adv_train, y_adv_train)
</code></pre>
<h5 id="model-evaluation-3">Model Evaluation</h5>
<pre><code class="language-python">y_adv_pred = model_adv1.predict(X_adv_test)

mean_squared_error(y_adv_test, y_adv_pred)
# 6.528575771818745
</code></pre>
<h5 id="adjusting-hyper-parameter">Adjusting Hyper Parameter</h5>
<pre><code class="language-python">model_adv2 = Ridge(
    alpha=1.0
)

model_adv2.fit(X_adv_train, y_adv_train)
</code></pre>
<pre><code class="language-python">y_adv_pred2 = model_adv2.predict(X_adv_test)
mean_squared_error(y_adv_test, y_adv_pred2)
# 2.3319016551123535
</code></pre>
<h4 id="train--validation--test-split">Train | Validation | Test Split</h4>
<pre><code class="language-python"># 70:30 train/temp split
X_adv_train, X_adv_temp, y_adv_train, y_adv_temp = train_test_split(
    X_adv, y_adv, test_size=0.3, random_state=666
)

# 50:50 test/val split
X_adv_test, X_adv_val, y_adv_test, y_adv_val = train_test_split(
    X_adv_temp, y_adv_temp, test_size=0.5, random_state=666
)

print(X_adv_train.shape, X_adv_test.shape, X_adv_val.shape)
# (140, 3) (30, 3) (30, 3)
</code></pre>
<pre><code class="language-python"># normalize features
scaler_adv = StandardScaler()
scaler_adv.fit(X_adv_train)

X_adv_train = scaler_adv.transform(X_adv_train)
X_adv_test = scaler_adv.transform(X_adv_test)
X_adv_val = scaler_adv.transform(X_adv_val)
</code></pre>
<h5 id="model-fitting-and-evaluation">Model Fitting and Evaluation</h5>
<pre><code class="language-python">model_adv3 = Ridge(
    alpha=100.0
)

model_adv3.fit(X_adv_train, y_adv_train)
</code></pre>
<pre><code class="language-python"># do evaluation with the validation set
y_adv_pred3 = model_adv3.predict(X_adv_val)
mean_squared_error(y_adv_val, y_adv_pred3)
# 7.136230975501291
</code></pre>
<h5 id="adjusting-hyper-parameter-1">Adjusting Hyper Parameter</h5>
<pre><code class="language-python">model_adv4 = Ridge(
    alpha=1.0
)

model_adv4.fit(X_adv_train, y_adv_train)

y_adv_pred4 = model_adv4.predict(X_adv_val)
mean_squared_error(y_adv_val, y_adv_pred4)
# 2.6393803874124435
</code></pre>
<pre><code class="language-python"># only once you are certain that you have the best performance
# do a final evaluation with the test set
y_adv4_final_pred = model_adv4.predict(X_adv_test)
mean_squared_error(y_adv_test, y_adv4_final_pred)
# 2.024422922812264
</code></pre>
<h4 id="k-fold-cross-validation">k-fold Cross Validation</h4>
<p>Do a train/test split and segment the training set by k-folds (e.g. 5-10) and use each of those segments once to validate a training step. The resulting error is the average of all k errors.</p>
<h5 id="train-test-split">Train-Test Split</h5>
<pre><code class="language-python"># 70:30 train/temp split
X_adv_train, X_adv_test, y_adv_train, y_adv_test = train_test_split(
    X_adv, y_adv, test_size=0.3, random_state=666
)
</code></pre>
<pre><code class="language-python"># normalize features
scaler_adv = StandardScaler()
scaler_adv.fit(X_adv_train)

X_adv_train = scaler_adv.transform(X_adv_train)
X_adv_test = scaler_adv.transform(X_adv_test)
</code></pre>
<h5 id="model-scoring">Model Scoring</h5>
<pre><code class="language-python">model_adv5 = Ridge(
    alpha=100.0
)
</code></pre>
<pre><code class="language-python"># do a 5-fold cross-eval
scores = cross_val_score(
    estimator=model_adv5,
    X=X_adv_train,
    y=y_adv_train,
    scoring=&#x27;neg_mean_squared_error&#x27;,
    cv=5
)

# take the mean of all five neg. error values
abs(scores.mean())
# 8.688107513529168
</code></pre>
<h5 id="adjusting-hyper-parameter-2">Adjusting Hyper Parameter</h5>
<pre><code class="language-python">model_adv6 = Ridge(
    alpha=1.0
)
</code></pre>
<pre><code class="language-python"># do a 5-fold cross-eval
scores = cross_val_score(
    estimator=model_adv6,
    X=X_adv_train,
    y=y_adv_train,
    scoring=&#x27;neg_mean_squared_error&#x27;,
    cv=5
)

# take the mean of all five neg. error values
abs(scores.mean())
# 3.3419582340688576
</code></pre>
<h5 id="model-fitting-and-final-evaluation">Model Fitting and Final Evaluation</h5>
<pre><code class="language-python">model_adv6.fit(X_adv_train, y_adv_train)

y_adv6_final_pred = model_adv6.predict(X_adv_test)
mean_squared_error(y_adv_test, y_adv6_final_pred)
# 2.3319016551123535
</code></pre>
<h4 id="cross-validate">Cross Validate</h4>
<h5 id="dataset-re-import">Dataset (re-import)</h5>
<pre><code class="language-python">adv_df = pd.read_csv(&#x27;datasets/Advertising.csv&#x27;)
X_adv = adv_df.drop(&#x27;Sales&#x27;, axis=1)
y_adv = adv_df[&#x27;Sales&#x27;]
</code></pre>
<pre><code class="language-python"># 70:30 train/test split
X_adv_train, X_adv_test, y_adv_train, y_adv_test = train_test_split(
    X_adv, y_adv, test_size=0.3, random_state=666
)
</code></pre>
<pre><code class="language-python"># normalize features
scaler_adv = StandardScaler()
scaler_adv.fit(X_adv_train)

X_adv_train = scaler_adv.transform(X_adv_train)
X_adv_test = scaler_adv.transform(X_adv_test)
</code></pre>
<h5 id="model-scoring-1">Model Scoring</h5>
<pre><code class="language-python">model_adv7 = Ridge(
    alpha=100.0
)
</code></pre>
<pre><code class="language-python">scores = cross_validate(
    model_adv7,
    X_adv_train,
    y_adv_train,
    scoring=[
        &#x27;neg_mean_squared_error&#x27;,
        &#x27;neg_mean_absolute_error&#x27;
    ],
    cv=10
)
</code></pre>
<pre><code class="language-python">scores_df = pd.DataFrame(scores)
scores_df
</code></pre>
<table><thead><tr><th></th><th>fit_time</th><th>score_time</th><th>test_neg_mean_squared_error</th><th>test_neg_mean_absolute_error</th></tr></thead><tbody><tr><td>0</td><td>0.016399</td><td>0.000749</td><td>-12.539147</td><td>-2.851864</td></tr><tr><td>1</td><td>0.000684</td><td>0.000452</td><td>-2.806466</td><td>-1.423516</td></tr><tr><td>2</td><td>0.000937</td><td>0.000782</td><td>-11.142227</td><td>-2.740332</td></tr><tr><td>3</td><td>0.001060</td><td>0.000633</td><td>-7.237347</td><td>-2.196963</td></tr><tr><td>4</td><td>0.001045</td><td>0.000738</td><td>-11.313985</td><td>-2.690813</td></tr><tr><td>5</td><td>0.000650</td><td>0.000510</td><td>-3.169169</td><td>-1.526568</td></tr><tr><td>6</td><td>0.000698</td><td>0.000429</td><td>-6.578249</td><td>-1.727616</td></tr><tr><td>7</td><td>0.000600</td><td>0.000423</td><td>-5.740245</td><td>-1.640964</td></tr><tr><td>8</td><td>0.000565</td><td>0.000463</td><td>-10.268075</td><td>-2.415688</td></tr><tr><td>9</td><td>0.000562</td><td>0.000487</td><td>-10.641669</td><td>-1.974407</td></tr></tbody></table>
<pre><code class="language-python">abs(scores_df.mean())
</code></pre>
<table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>fit_time</td><td>0.002320</td></tr><tr><td>score_time</td><td>0.000566</td></tr><tr><td>test_neg_mean_squared_error</td><td>8.143658</td></tr><tr><td>test_neg_mean_absolute_error</td><td>2.118873</td></tr><tr><td><em>dtype: float64</em></td><td></td></tr></tbody></table>
<h5 id="adjusting-hyper-parameter-3">Adjusting Hyper Parameter</h5>
<pre><code class="language-python">model_adv8 = Ridge(
    alpha=1.0
)
</code></pre>
<pre><code class="language-python">scores = cross_validate(
    model_adv8,
    X_adv_train,
    y_adv_train,
    scoring=[
        &#x27;neg_mean_squared_error&#x27;,
        &#x27;neg_mean_absolute_error&#x27;
    ],
    cv=10
)

abs(pd.DataFrame(scores).mean())
</code></pre>
<table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>fit_time</td><td>0.001141</td></tr><tr><td>score_time</td><td>0.000777</td></tr><tr><td>test_neg_mean_squared_error</td><td>3.272673</td></tr><tr><td>test_neg_mean_absolute_error</td><td>1.345709</td></tr><tr><td><em>dtype: float64</em></td><td></td></tr></tbody></table>
<h5 id="model-fitting-and-final-evaluation-1">Model Fitting and Final Evaluation</h5>
<pre><code class="language-python">model_adv8.fit(X_adv_train, y_adv_train)

y_adv8_final_pred = model_adv8.predict(X_adv_test)
mean_squared_error(y_adv_test, y_adv8_final_pred)
# 2.3319016551123535
</code></pre>
<h4 id="grid-search">Grid Search</h4>
<p>Loop through a set of hyperparameters to find an optimum.</p>
<h5 id="hyperparameter-search">Hyperparameter Search</h5>
<pre><code class="language-python">base_elastic_net_model = ElasticNet()
</code></pre>
<pre><code class="language-python">param_grid = \{
    &#x27;alpha&#x27;: [0.1, 1, 5, 10, 50, 100],
    &#x27;l1_ratio&#x27;:[0.1, 0.3, 0.5, 0.7, 0.9, 1.0]
\}
</code></pre>
<pre><code class="language-python">grid_model = GridSearchCV(
    estimator=base_elastic_net_model,
    param_grid=param_grid,
    scoring=&#x27;neg_mean_squared_error&#x27;,
    cv=5, verbose=2
)

grid_model.fit(X_adv_train, y_adv_train)

print(
    &#x27;Results:\nBest Estimator: &#x27;,
    grid_model.best_estimator_,
    &#x27;\nBest Hyperparameter: &#x27;,
    grid_model.best_params_
)
</code></pre>
<p><strong>Results</strong>:</p>
<ul>
<li>Best Estimator:  <code>ElasticNet(alpha=0.1, l1_ratio=1.0)</code></li>
<li>Best Hyperparameter:  <code>\{&#x27;alpha&#x27;: 0.1, &#x27;l1_ratio&#x27;: 1.0\}</code></li>
</ul>
<pre><code class="language-python">gridcv_results = pd.DataFrame(grid_model.cv_results_)
</code></pre>
<table><thead><tr><th></th><th>mean_fit_time</th><th>std_fit_time</th><th>mean_score_time</th><th>std_score_time</th><th>param_alpha</th><th>param_l1_ratio</th><th>params</th><th>split0_test_score</th><th>split1_test_score</th><th>split2_test_score</th><th>split3_test_score</th><th>split4_test_score</th><th>mean_test_score</th><th>std_test_score</th><th>rank_test_score</th></tr></thead><tbody><tr><td>0</td><td>0.001156</td><td>0.000160</td><td>0.000449</td><td>0.000038</td><td>0.1</td><td>0.1</td><td>{&#x27;alpha&#x27;: 0.1, &#x27;l1_ratio&#x27;: 0.1}</td><td>-1.924119</td><td>-3.384152</td><td>-3.588444</td><td>-3.703040</td><td>-5.091974</td><td>-3.538346</td><td>1.007264</td><td>6</td></tr><tr><td>1</td><td>0.001144</td><td>0.000181</td><td>0.000407</td><td>0.000091</td><td>0.1</td><td>0.3</td><td>{&#x27;alpha&#x27;: 0.1, &#x27;l1_ratio&#x27;: 0.3}</td><td>-1.867117</td><td>-3.304382</td><td>-3.561106</td><td>-3.623188</td><td>-5.061781</td><td>-3.483515</td><td>1.016000</td><td>5</td></tr><tr><td>2</td><td>0.000623</td><td>0.000026</td><td>0.000272</td><td>0.000052</td><td>0.1</td><td>0.5</td><td>{&#x27;alpha&#x27;: 0.1, &#x27;l1_ratio&#x27;: 0.5}</td><td>-1.812633</td><td>-3.220727</td><td>-3.539711</td><td>-3.547572</td><td>-5.043259</td><td>-3.432780</td><td>1.028406</td><td>4</td></tr><tr><td>3</td><td>0.000932</td><td>0.000165</td><td>0.000321</td><td>0.000060</td><td>0.1</td><td>0.7</td><td>{&#x27;alpha&#x27;: 0.1, &#x27;l1_ratio&#x27;: 0.7}</td><td>-1.750153</td><td>-3.144120</td><td>-3.525226</td><td>-3.477228</td><td>-5.034008</td><td>-3.386147</td><td>1.046722</td><td>3</td></tr><tr><td>4</td><td>0.000725</td><td>0.000106</td><td>0.000259</td><td>0.000024</td><td>0.1</td><td>0.9</td><td>{&#x27;alpha&#x27;: 0.1, &#x27;l1_ratio&#x27;: 0.9}</td><td>-1.693440</td><td>-3.075686</td><td>-3.518777</td><td>-3.413393</td><td>-5.029683</td><td>-3.346196</td><td>1.065195</td><td>2</td></tr><tr><td>5</td><td>0.000654</td><td>0.000053</td><td>0.000274</td><td>0.000026</td><td>0.1</td><td>1.0</td><td>{&#x27;alpha&#x27;: 0.1, &#x27;l1_ratio&#x27;: 1.0}</td><td>-1.667506</td><td>-3.044928</td><td>-3.518866</td><td>-3.384363</td><td>-5.031297</td><td>-3.329392</td><td>1.075006</td><td>1</td></tr><tr><td>6</td><td>0.000595</td><td>0.000016</td><td>0.000244</td><td>0.000002</td><td>1</td><td>0.1</td><td>{&#x27;alpha&#x27;: 1, &#x27;l1_ratio&#x27;: 0.1}</td><td>-8.575470</td><td>-11.021534</td><td>-8.212152</td><td>-6.808719</td><td>-10.792072</td><td>-9.081990</td><td>1.604192</td><td>12</td></tr><tr><td>7</td><td>0.000591</td><td>0.000018</td><td>0.000244</td><td>0.000002</td><td>1</td><td>0.3</td><td>{&#x27;alpha&#x27;: 1, &#x27;l1_ratio&#x27;: 0.3}</td><td>-8.131855</td><td>-10.448423</td><td>-7.774620</td><td>-6.179358</td><td>-10.071728</td><td>-8.521197</td><td>1.569173</td><td>11</td></tr><tr><td>8</td><td>0.000628</td><td>0.000049</td><td>0.000266</td><td>0.000023</td><td>1</td><td>0.5</td><td>{&#x27;alpha&#x27;: 1, &#x27;l1_ratio&#x27;: 0.5}</td><td>-7.519809</td><td>-9.562473</td><td>-7.261824</td><td>-5.453399</td><td>-9.213320</td><td>-7.802165</td><td>1.481785</td><td>10</td></tr><tr><td>9</td><td>0.000594</td><td>0.000015</td><td>0.000243</td><td>0.000002</td><td>1</td><td>0.7</td><td>{&#x27;alpha&#x27;: 1, &#x27;l1_ratio&#x27;: 0.7}</td><td>-6.614835</td><td>-8.351711</td><td>-6.702104</td><td>-4.698977</td><td>-8.230616</td><td>-6.919649</td><td>1.329741</td><td>9</td></tr><tr><td>10</td><td>0.000714</td><td>0.000108</td><td>0.000268</td><td>0.000033</td><td>1</td><td>0.9</td><td>{&#x27;alpha&#x27;: 1, &#x27;l1_ratio&#x27;: 0.9}</td><td>-5.537250</td><td>-6.887828</td><td>-6.148400</td><td>-4.106124</td><td>-7.101573</td><td>-5.956235</td><td>1.078430</td><td>8</td></tr><tr><td>11</td><td>0.000649</td><td>0.000067</td><td>0.000263</td><td>0.000028</td><td>1</td><td>1.0</td><td>{&#x27;alpha&#x27;: 1, &#x27;l1_ratio&#x27;: 1.0}</td><td>-4.932027</td><td>-6.058207</td><td>-5.892529</td><td>-3.798441</td><td>-6.472871</td><td>-5.430815</td><td>0.959804</td><td>7</td></tr><tr><td>12</td><td>0.000645</td><td>0.000042</td><td>0.000264</td><td>0.000040</td><td>5</td><td>0.1</td><td>{&#x27;alpha&#x27;: 5, &#x27;l1_ratio&#x27;: 0.1}</td><td>-21.863798</td><td>-25.767488</td><td>-18.768865</td><td>-12.608680</td><td>-23.207907</td><td>-20.443347</td><td>4.520904</td><td>13</td></tr><tr><td>13</td><td>0.000617</td><td>0.000030</td><td>0.000281</td><td>0.000038</td><td>5</td><td>0.3</td><td>{&#x27;alpha&#x27;: 5, &#x27;l1_ratio&#x27;: 0.3}</td><td>-23.626694</td><td>-27.439028</td><td>-20.266203</td><td>-12.788078</td><td>-24.609195</td><td>-21.745840</td><td>5.031493</td><td>14</td></tr><tr><td>14</td><td>0.000599</td><td>0.000011</td><td>0.000249</td><td>0.000013</td><td>5</td><td>0.5</td><td>{&#x27;alpha&#x27;: 5, &#x27;l1_ratio&#x27;: 0.5}</td><td>-26.202964</td><td>-29.867138</td><td>-22.527913</td><td>-13.423857</td><td>-26.835934</td><td>-23.771561</td><td>5.675911</td><td>15</td></tr><tr><td>15</td><td>0.000588</td><td>0.000013</td><td>0.000276</td><td>0.000035</td><td>5</td><td>0.7</td><td>{&#x27;alpha&#x27;: 5, &#x27;l1_ratio&#x27;: 0.7}</td><td>-27.768946</td><td>-33.428462</td><td>-23.506474</td><td>-14.599984</td><td>-29.112276</td><td>-25.683228</td><td>6.382379</td><td>17</td></tr><tr><td>16</td><td>0.000580</td><td>0.000003</td><td>0.000271</td><td>0.000001</td><td>5</td><td>0.9</td><td>{&#x27;alpha&#x27;: 5, &#x27;l1_ratio&#x27;: 0.9}</td><td>-29.868949</td><td>-34.423737</td><td>-25.623955</td><td>-16.750237</td><td>-31.056181</td><td>-27.544612</td><td>6.087093</td><td>19</td></tr><tr><td>17</td><td>0.000591</td><td>0.000011</td><td>0.000259</td><td>0.000021</td><td>5</td><td>1.0</td><td>{&#x27;alpha&#x27;: 5, &#x27;l1_ratio&#x27;: 1.0}</td><td>-29.868949</td><td>-34.423737</td><td>-25.623955</td><td>-16.750237</td><td>-31.056181</td><td>-27.544612</td><td>6.087093</td><td>19</td></tr><tr><td>18</td><td>0.000632</td><td>0.000028</td><td>0.000250</td><td>0.000012</td><td>10</td><td>0.1</td><td>{&#x27;alpha&#x27;: 10, &#x27;l1_ratio&#x27;: 0.1}</td><td>-26.179546</td><td>-30.396420</td><td>-22.386698</td><td>-14.596498</td><td>-27.292337</td><td>-24.170300</td><td>5.429322</td><td>16</td></tr><tr><td>19</td><td>0.000593</td><td>0.000020</td><td>0.000239</td><td>0.000001</td><td>10</td><td>0.3</td><td>{&#x27;alpha&#x27;: 10, &#x27;l1_ratio&#x27;: 0.3}</td><td>-28.704426</td><td>-33.379967</td><td>-24.561645</td><td>-15.634153</td><td>-29.883725</td><td>-26.432783</td><td>6.090062</td><td>18</td></tr><tr><td>20</td><td>0.000595</td><td>0.000036</td><td>0.000245</td><td>0.000013</td><td>10</td><td>0.5</td><td>{&#x27;alpha&#x27;: 10, &#x27;l1_ratio&#x27;: 0.5}</td><td>-29.868949</td><td>-34.423737</td><td>-25.623955</td><td>-16.750237</td><td>-31.056181</td><td>-27.544612</td><td>6.087093</td><td>19</td></tr><tr><td>21</td><td>0.000610</td><td>0.000053</td><td>0.000258</td><td>0.000015</td><td>10</td><td>0.7</td><td>{&#x27;alpha&#x27;: 10, &#x27;l1_ratio&#x27;: 0.7}</td><td>-29.868949</td><td>-34.423737</td><td>-25.623955</td><td>-16.750237</td><td>-31.056181</td><td>-27.544612</td><td>6.087093</td><td>19</td></tr><tr><td>22</td><td>0.000597</td><td>0.000022</td><td>0.000248</td><td>0.000015</td><td>10</td><td>0.9</td><td>{&#x27;alpha&#x27;: 10, &#x27;l1_ratio&#x27;: 0.9}</td><td>-29.868949</td><td>-34.423737</td><td>-25.623955</td><td>-16.750237</td><td>-31.056181</td><td>-27.544612</td><td>6.087093</td><td>19</td></tr><tr><td>23</td><td>0.000623</td><td>0.000057</td><td>0.000305</td><td>0.000076</td><td>10</td><td>1.0</td><td>{&#x27;alpha&#x27;: 10, &#x27;l1_ratio&#x27;: 1.0}</td><td>-29.868949</td><td>-34.423737</td><td>-25.623955</td><td>-16.750237</td><td>-31.056181</td><td>-27.544612</td><td>6.087093</td><td>19</td></tr><tr><td>24</td><td>0.000602</td><td>0.000016</td><td>0.000252</td><td>0.000013</td><td>50</td><td>0.1</td><td>{&#x27;alpha&#x27;: 50, &#x27;l1_ratio&#x27;: 0.1}</td><td>-29.868949</td><td>-34.423737</td><td>-25.623955</td><td>-16.750237</td><td>-31.056181</td><td>-27.544612</td><td>6.087093</td><td>19</td></tr><tr><td>25</td><td>0.000577</td><td>0.000009</td><td>0.000238</td><td>0.000001</td><td>50</td><td>0.3</td><td>{&#x27;alpha&#x27;: 50, &#x27;l1_ratio&#x27;: 0.3}</td><td>-29.868949</td><td>-34.423737</td><td>-25.623955</td><td>-16.750237</td><td>-31.056181</td><td>-27.544612</td><td>6.087093</td><td>19</td></tr><tr><td>26</td><td>0.000607</td><td>0.000046</td><td>0.000245</td><td>0.000010</td><td>50</td><td>0.5</td><td>{&#x27;alpha&#x27;: 50, &#x27;l1_ratio&#x27;: 0.5}</td><td>-29.868949</td><td>-34.423737</td><td>-25.623955</td><td>-16.750237</td><td>-31.056181</td><td>-27.544612</td><td>6.087093</td><td>19</td></tr><tr><td>27</td><td>0.000569</td><td>0.000004</td><td>0.000259</td><td>0.000012</td><td>50</td><td>0.7</td><td>{&#x27;alpha&#x27;: 50, &#x27;l1_ratio&#x27;: 0.7}</td><td>-29.868949</td><td>-34.423737</td><td>-25.623955</td><td>-16.750237</td><td>-31.056181</td><td>-27.544612</td><td>6.087093</td><td>19</td></tr><tr><td>28</td><td>0.000582</td><td>0.000022</td><td>0.000244</td><td>0.000011</td><td>50</td><td>0.9</td><td>{&#x27;alpha&#x27;: 50, &#x27;l1_ratio&#x27;: 0.9}</td><td>-29.868949</td><td>-34.423737</td><td>-25.623955</td><td>-16.750237</td><td>-31.056181</td><td>-27.544612</td><td>6.087093</td><td>19</td></tr><tr><td>29</td><td>0.000603</td><td>0.000041</td><td>0.000251</td><td>0.000015</td><td>50</td><td>1.0</td><td>{&#x27;alpha&#x27;: 50, &#x27;l1_ratio&#x27;: 1.0}</td><td>-29.868949</td><td>-34.423737</td><td>-25.623955</td><td>-16.750237</td><td>-31.056181</td><td>-27.544612</td><td>6.087093</td><td>19</td></tr><tr><td>30</td><td>0.000670</td><td>0.000106</td><td>0.000251</td><td>0.000013</td><td>100</td><td>0.1</td><td>{&#x27;alpha&#x27;: 100, &#x27;l1_ratio&#x27;: 0.1}</td><td>-29.868949</td><td>-34.423737</td><td>-25.623955</td><td>-16.750237</td><td>-31.056181</td><td>-27.544612</td><td>6.087093</td><td>19</td></tr><tr><td>31</td><td>0.000764</td><td>0.000179</td><td>0.000343</td><td>0.000054</td><td>100</td><td>0.3</td><td>{&#x27;alpha&#x27;: 100, &#x27;l1_ratio&#x27;: 0.3}</td><td>-29.868949</td><td>-34.423737</td><td>-25.623955</td><td>-16.750237</td><td>-31.056181</td><td>-27.544612</td><td>6.087093</td><td>19</td></tr><tr><td>32</td><td>0.000623</td><td>0.000077</td><td>0.000244</td><td>0.000007</td><td>100</td><td>0.5</td><td>{&#x27;alpha&#x27;: 100, &#x27;l1_ratio&#x27;: 0.5}</td><td>-29.868949</td><td>-34.423737</td><td>-25.623955</td><td>-16.750237</td><td>-31.056181</td><td>-27.544612</td><td>6.087093</td><td>19</td></tr><tr><td>33</td><td>0.000817</td><td>0.000156</td><td>0.000329</td><td>0.000076</td><td>100</td><td>0.7</td><td>{&#x27;alpha&#x27;: 100, &#x27;l1_ratio&#x27;: 0.7}</td><td>-29.868949</td><td>-34.423737</td><td>-25.623955</td><td>-16.750237</td><td>-31.056181</td><td>-27.544612</td><td>6.087093</td><td>19</td></tr><tr><td>34</td><td>0.000590</td><td>0.000017</td><td>0.000242</td><td>0.000004</td><td>100</td><td>0.9</td><td>{&#x27;alpha&#x27;: 100, &#x27;l1_ratio&#x27;: 0.9}</td><td>-29.868949</td><td>-34.423737</td><td>-25.623955</td><td>-16.750237</td><td>-31.056181</td><td>-27.544612</td><td>6.087093</td><td>19</td></tr><tr><td>35</td><td>0.000595</td><td>0.000027</td><td>0.000242</td><td>0.000007</td><td>100</td><td>1.0</td><td>{&#x27;alpha&#x27;: 100, &#x27;l1_ratio&#x27;: 1.0}</td><td>-29.868949</td><td>-34.423737</td><td>-25.623955</td><td>-16.750237</td><td>-31.056181</td><td>-27.544612</td><td>6.087093</td><td>19</td></tr></tbody></table>
<pre><code class="language-python">gridcv_results[
    [
        &#x27;param_alpha&#x27;,
        &#x27;param_l1_ratio&#x27;
    ]
].plot(title=&#x27;Grid Search Hyperparameter :: Parameter&#x27;, figsize=(12,8))
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_08-6701687aa2b8ebccbe700c23f8dd3b0d.webp" width="986" height="682"></p>
<pre><code class="language-python">gridcv_results[
    [
        &#x27;mean_fit_time&#x27;,
        &#x27;std_fit_time&#x27;,
        &#x27;mean_score_time&#x27;
    ]
].plot(title=&#x27;Grid Search Hyperparameter :: Timing&#x27;, figsize=(12,8))
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_09-e4bb60965d11d8f26e1366684b2367bc.webp" width="1007" height="682"></p>
<pre><code class="language-python">gridcv_results[
    [
        &#x27;split0_test_score&#x27;,
        &#x27;split1_test_score&#x27;,
        &#x27;split2_test_score&#x27;,
        &#x27;split3_test_score&#x27;,
        &#x27;split4_test_score&#x27;,
        &#x27;mean_test_score&#x27;,
        &#x27;std_test_score&#x27;,
       &#x27;rank_test_score&#x27;
    ]
].plot(title=&#x27;Grid Search Hyperparameter :: Parameter&#x27;, figsize=(12,8))
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_10-0927e95e6bbb88f4bc3c66715088cad5.webp" width="988" height="682"></p>
<h5 id="model-evaluation-4">Model Evaluation</h5>
<pre><code class="language-python">y_grid_pred = grid_model.predict(X_adv_test)

mean_squared_error(y_adv_test, y_grid_pred)
# 2.380865536033581
</code></pre>
<h2 id="supervised-learning---knn-algorithm">Supervised Learning - KNN Algorithm</h2>
<h3 id="dataset-2">Dataset</h3>
<pre><code class="language-python">wine = load_wine()
print(wine.data.shape)
print(wine.feature_names)
print(wine.data[:1])

# (178, 13)
# [&#x27;alcohol&#x27;, &#x27;malic_acid&#x27;, &#x27;ash&#x27;, &#x27;alcalinity_of_ash&#x27;, &#x27;magnesium&#x27;, &#x27;total_phenols&#x27;, &#x27;flavanoids&#x27;, &#x27;nonflavanoid_phenols&#x27;, &#x27;proanthocyanins&#x27;, &#x27;color_intensity&#x27;, &#x27;hue&#x27;, &#x27;od280/od315_of_diluted_wines&#x27;, &#x27;proline&#x27;]
# [[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00
#   2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]]
</code></pre>
<pre><code class="language-python">wine_df = pd.DataFrame(data=wine.data, columns=wine.feature_names)
wine_df.head(2).T
</code></pre>
<table><thead><tr><th></th><th>0</th><th>1</th></tr></thead><tbody><tr><td>alcohol</td><td>14.23</td><td>13.20</td></tr><tr><td>malic_acid</td><td>1.71</td><td>1.78</td></tr><tr><td>ash</td><td>2.43</td><td>2.14</td></tr><tr><td>alcalinity_of_ash</td><td>15.60</td><td>11.20</td></tr><tr><td>magnesium</td><td>127.00</td><td>100.00</td></tr><tr><td>total_phenols</td><td>2.80</td><td>2.65</td></tr><tr><td>flavanoids</td><td>3.06</td><td>2.76</td></tr><tr><td>nonflavanoid_phenols</td><td>0.28</td><td>0.26</td></tr><tr><td>proanthocyanins</td><td>2.29</td><td>1.28</td></tr><tr><td>color_intensity</td><td>5.64</td><td>4.38</td></tr><tr><td>hue</td><td>1.04</td><td>1.05</td></tr><tr><td>od280/od315_of_diluted_wines</td><td>3.92</td><td>3.40</td></tr><tr><td>proline</td><td>1065.00</td><td>1050.00</td></tr></tbody></table>
<h3 id="data-pre-processing-1">Data Pre-processing</h3>
<pre><code class="language-python"># normalization
scaler = MinMaxScaler()
scaler.fit(wine.data)
wine_norm = scaler.fit_transform(wine.data)
</code></pre>
<pre><code class="language-python"># train/test split
X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(
    wine_norm,
    wine.target,
    test_size=0.3
)

print(X_train_wine.shape, X_test_wine.shape)
# (124, 13) (54, 13)
</code></pre>
<h3 id="model-fitting-2">Model Fitting</h3>
<pre><code class="language-python"># model for k=3
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train_wine, y_train_wine)

y_pred_wine_knn3 = knn.predict(X_test_wine)
print(&#x27;Accuracy Score: &#x27;, (accuracy_score(y_test_wine, y_pred_wine_knn3)*100).round(2), &#x27;%&#x27;)
# Accuracy Score:  98.15 %
</code></pre>
<pre><code class="language-python"># model for k=5
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_wine, y_train_wine)

y_pred_wine_knn5 = knn.predict(X_test_wine)
print(&#x27;Accuracy Score: &#x27;, (accuracy_score(y_test_wine, y_pred_wine_knn5)*100).round(2), &#x27;%&#x27;)
# Accuracy Score:  98.15 %
</code></pre>
<pre><code class="language-python"># model for k=7
knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train_wine, y_train_wine)

y_pred_wine_knn7 = knn.predict(X_test_wine)
print(&#x27;Accuracy Score: &#x27;, (accuracy_score(y_test_wine, y_pred_wine_knn7)*100).round(2), &#x27;%&#x27;)
# Accuracy Score:  96.3 %
</code></pre>
<pre><code class="language-python"># model for k=9
knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train_wine, y_train_wine)

y_pred_wine_knn7 = knn.predict(X_test_wine)
print(&#x27;Accuracy Score: &#x27;, (accuracy_score(y_test_wine, y_pred_wine_knn7)*100).round(2), &#x27;%&#x27;)
# Accuracy Score:  96.3 %
</code></pre>
<h2 id="supervised-learning---decision-tree-classifier">Supervised Learning - Decision Tree Classifier</h2>
<ul>
<li>Does not require normalization</li>
<li>Is not sensitive to missing values</li>
</ul>
<h3 id="dataset-3">Dataset</h3>
<pre><code class="language-python">!wget https://gist.githubusercontent.com/Dviejopomata/ea5869ba4dcff84f8c294dc7402cd4a9/raw/4671f90b8b04ba4db9d67acafaa4c0827cd233c2/bill_authentication.csv -P datasets
</code></pre>
<pre><code class="language-python">bill_auth_df = pd.read_csv(&#x27;datasets/bill_authentication.csv&#x27;)
bill_auth_df.head(3)
</code></pre>
<table><thead><tr><th></th><th>Variance</th><th>Skewness</th><th>Curtosis</th><th>Entropy</th><th>Class</th></tr></thead><tbody><tr><td>0</td><td>3.6216</td><td>8.6661</td><td>-2.8073</td><td>-0.44699</td><td>0</td></tr><tr><td>1</td><td>4.5459</td><td>8.1674</td><td>-2.4586</td><td>-1.46210</td><td>0</td></tr><tr><td>2</td><td>3.8660</td><td>-2.6383</td><td>1.9242</td><td>0.10645</td><td>0</td></tr></tbody></table>
<h3 id="preprocessing-1">Preprocessing</h3>
<pre><code class="language-python"># remove target feature from training set
X_bill = bill_auth_df.drop(&#x27;Class&#x27;, axis=1)
y_bill = bill_auth_df[&#x27;Class&#x27;]
</code></pre>
<pre><code class="language-python">X_train_bill, X_test_bill, y_train_bill, y_test_bill = train_test_split(X_bill, y_bill, test_size=0.2)
</code></pre>
<h3 id="model-fitting-3">Model Fitting</h3>
<pre><code class="language-python">tree_classifier = DecisionTreeClassifier()

tree_classifier.fit(X_train_bill, y_train_bill)
</code></pre>
<h3 id="evaluation">Evaluation</h3>
<pre><code class="language-python">y_pred_bill = tree_classifier.predict(X_test_bill)
</code></pre>
<pre><code class="language-python">conf_mtx_bill = confusion_matrix(y_test_bill, y_pred_bill)
conf_mtx_bill

# array([[150,   2],
#        [  4, 119]])
</code></pre>
<pre><code class="language-python">conf_mtx_bill_plot = ConfusionMatrixDisplay(
    confusion_matrix=conf_mtx_bill,
    display_labels=[False,True]
)

conf_mtx_bill_plot.plot()
plt.show()
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_05-d4b7b3e851232ed2f519f3f2ec6648db.webp" width="533" height="432"></p>
<pre><code class="language-python">report_bill = classification_report(
    y_test_bill, y_pred_bill
)
print(report_bill)
</code></pre>
<table><thead><tr><th></th><th>precision</th><th>recall</th><th>f1-score</th><th>support</th></tr></thead><tbody><tr><td>0</td><td>0.97</td><td>0.99</td><td>0.98</td><td>152</td></tr><tr><td>1</td><td>0.98</td><td>0.97</td><td>0.98</td><td>123</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.98</td><td>275</td></tr><tr><td>macro avg</td><td>0.98</td><td>0.98</td><td>0.98</td><td>275</td></tr><tr><td>weighted avg</td><td>0.98</td><td>0.98</td><td>0.98</td><td>275</td></tr></tbody></table>
<h2 id="supervised-learning---random-forest-classifier">Supervised Learning - Random Forest Classifier</h2>
<ul>
<li>Does not require normalization</li>
<li>Is not sensitive to missing values</li>
<li>Low risk of overfitting</li>
<li>Efficient with large datasets</li>
<li>High accuracy</li>
</ul>
<h3 id="dataset-4">Dataset</h3>
<pre><code class="language-python">!wget https://raw.githubusercontent.com/xjcjiacheng/data-analysis/master/heart%20disease%20UCI/heart.csv -P datasets
</code></pre>
<pre><code class="language-python">heart_df = pd.read_csv(&#x27;datasets/heart.csv&#x27;)
heart_df.head(5)
</code></pre>
<table><thead><tr><th></th><th>age</th><th>sex</th><th>cp</th><th>trestbps</th><th>chol</th><th>fbs</th><th>restecg</th><th>thalach</th><th>exang</th><th>oldpeak</th><th>slope</th><th>ca</th><th>thal</th><th>target</th></tr></thead><tbody><tr><td>0</td><td>63</td><td>1</td><td>3</td><td>145</td><td>233</td><td>1</td><td>0</td><td>150</td><td>0</td><td>2.3</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td>1</td><td>37</td><td>1</td><td>2</td><td>130</td><td>250</td><td>0</td><td>1</td><td>187</td><td>0</td><td>3.5</td><td>0</td><td>0</td><td>2</td><td>1</td></tr><tr><td>2</td><td>41</td><td>0</td><td>1</td><td>130</td><td>204</td><td>0</td><td>0</td><td>172</td><td>0</td><td>1.4</td><td>2</td><td>0</td><td>2</td><td>1</td></tr><tr><td>3</td><td>56</td><td>1</td><td>1</td><td>120</td><td>236</td><td>0</td><td>1</td><td>178</td><td>0</td><td>0.8</td><td>2</td><td>0</td><td>2</td><td>1</td></tr><tr><td>4</td><td>57</td><td>0</td><td>0</td><td>120</td><td>354</td><td>0</td><td>1</td><td>163</td><td>1</td><td>0.6</td><td>2</td><td>0</td><td>2</td><td>1</td></tr></tbody></table>
<h3 id="preprocessing-2">Preprocessing</h3>
<pre><code class="language-python"># remove target feature from training set
X_heart = heart_df.drop(&#x27;target&#x27;, axis=1)
y_heart = heart_df[&#x27;target&#x27;]
</code></pre>
<pre><code class="language-python">X_train_heart, X_test_heart, y_train_heart, y_test_heart = train_test_split(
    X_heart,
    y_heart,
    test_size=0.2,
    random_state=0
)
</code></pre>
<h3 id="model-fitting-4">Model Fitting</h3>
<pre><code class="language-python">forest_classifier = RandomForestClassifier(n_estimators=10, criterion=&#x27;entropy&#x27;)

forest_classifier.fit(X_train_heart, y_train_heart)
</code></pre>
<h3 id="evaluation-1">Evaluation</h3>
<pre><code class="language-python">y_pred_heart = forest_classifier.predict(X_test_heart)
</code></pre>
<pre><code class="language-python">conf_mtx_heart = confusion_matrix(y_test_heart, y_pred_heart)
conf_mtx_heart

# array([[24,  3],
#        [ 5, 29]])
</code></pre>
<pre><code class="language-python">conf_mtx_heart_plot = ConfusionMatrixDisplay(
    confusion_matrix=conf_mtx_heart,
    display_labels=[False,True]
)

conf_mtx_heart_plot.plot()
plt.show()
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_06-568a4915f1265f5a29b6afdc8d4056dd.webp" width="524" height="432"></p>
<pre><code class="language-python">report_heart = classification_report(
    y_test_heart, y_pred_heart
)
print(report_heart)
</code></pre>
<table><thead><tr><th></th><th>precision</th><th>recall</th><th>f1-score</th><th>support</th></tr></thead><tbody><tr><td>0</td><td>0.83</td><td>0.89</td><td>0.86</td><td>27</td></tr><tr><td>1</td><td>0.91</td><td>0.85</td><td>0.88</td><td>34</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.87</td><td>61</td></tr><tr><td>macro avg</td><td>0.87</td><td>0.87</td><td>0.87</td><td>61</td></tr><tr><td>weighted avg</td><td>0.87</td><td>0.87</td><td>0.87</td><td>61</td></tr></tbody></table>
<h3 id="random-forest-hyperparameter-tuning">Random Forest Hyperparameter Tuning</h3>
<h4 id="testing-hyperparameters">Testing Hyperparameters</h4>
<pre><code class="language-python">rdnfor_classifier = RandomForestClassifier(
    n_estimators=2,
    min_samples_split=2,
    min_samples_leaf=1,
    criterion=&#x27;entropy&#x27;
)
rdnfor_classifier.fit(X_train_heart, y_train_heart)
</code></pre>
<pre><code class="language-python">rdnfor_pred = rdnfor_classifier.predict(X_test_heart)
print(&#x27;Accuracy Score: &#x27;, accuracy_score(y_test_heart, rdnfor_pred).round(4)*100, &#x27;%&#x27;)

# Accuracy Score:  73.77 %
</code></pre>
<h4 id="grid-search-cross-validation">Grid-Search Cross-Validation</h4>
<p>Try a set of values for selected Hyperparameter to find the optimal configuration.</p>
<pre><code class="language-python">param_grid = \{
    &#x27;n_estimators&#x27;: [5, 25, 50, 75,100, 125],
    &#x27;min_samples_split&#x27;: [1,2,3],
    &#x27;min_samples_leaf&#x27;: [1,2,3],
    &#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;, &#x27;log_loss&#x27;],
    &#x27;max_features&#x27; : [&#x27;sqrt&#x27;, &#x27;log2&#x27;]
\}

grid_search = GridSearchCV(
    estimator = rdnfor_classifier,
    param_grid = param_grid
)

grid_search.fit(X_train_heart, y_train_heart)
</code></pre>
<pre><code class="language-python">print(&#x27;Best Parameter: &#x27;, grid_search.best_params_)
# Best Parameter:  \{
# &#x27;criterion&#x27;: &#x27;entropy&#x27;,
# &#x27;max_features&#x27;: &#x27;sqrt&#x27;,
# &#x27;min_samples_leaf&#x27;: 2,
# &#x27;min_samples_split&#x27;: 1,
# &#x27;n_estimators&#x27;: 25
# \}
</code></pre>
<pre><code class="language-python">rdnfor_classifier_optimized = RandomForestClassifier(
    n_estimators=25,
    min_samples_split=1,
    min_samples_leaf=2,
    criterion=&#x27;entropy&#x27;,
    max_features=&#x27;sqrt&#x27;
)

rdnfor_classifier_optimized.fit(X_train_heart, y_train_heart)
</code></pre>
<pre><code class="language-python">rdnfor_pred_optimized = rdnfor_classifier_optimized.predict(X_test_heart)
print(&#x27;Accuracy Score: &#x27;, accuracy_score(y_test_heart, rdnfor_pred_optimized).round(4)*100, &#x27;%&#x27;)

# Accuracy Score:  85.25 %
</code></pre>
<h3 id="random-forest-classifier-1---penguins">Random Forest Classifier 1 - Penguins</h3>
<pre><code class="language-python">!wget https://github.com/remijul/dataset/raw/master/penguins_size.csv -P datasets
</code></pre>
<pre><code class="language-python">peng_df = pd.read_csv(&#x27;datasets/penguins_size.csv&#x27;)
peng_df = peng_df.dropna()
peng_df.head(5)
</code></pre>
<table><thead><tr><th></th><th>species</th><th>island</th><th>culmen_length_mm</th><th>culmen_depth_mm</th><th>flipper_length_mm</th><th>body_mass_g</th><th>sex</th></tr></thead><tbody><tr><td>0</td><td>Adelie</td><td>Torgersen</td><td>39.1</td><td>18.7</td><td>181.0</td><td>3750.0</td><td>MALE</td></tr><tr><td>1</td><td>Adelie</td><td>Torgersen</td><td>39.5</td><td>17.4</td><td>186.0</td><td>3800.0</td><td>FEMALE</td></tr><tr><td>2</td><td>Adelie</td><td>Torgersen</td><td>40.3</td><td>18.0</td><td>195.0</td><td>3250.0</td><td>FEMALE</td></tr><tr><td>4</td><td>Adelie</td><td>Torgersen</td><td>36.7</td><td>19.3</td><td>193.0</td><td>3450.0</td><td>FEMALE</td></tr><tr><td>5</td><td>Adelie</td><td>Torgersen</td><td>39.3</td><td>20.6</td><td>190.0</td><td>3650.0</td><td>MALE</td></tr></tbody></table>
<pre><code class="language-python"># drop labels and encode string values
X_peng = pd.get_dummies(peng_df.drop(&#x27;species&#x27;, axis=1),drop_first=True)
y_peng = peng_df[&#x27;species&#x27;]
</code></pre>
<pre><code class="language-python"># train/test split
X_peng_train, X_peng_test, y_peng_train, y_peng_test = train_test_split(
    X_peng,
    y_peng,
    test_size=0.3,
    random_state=42
)
</code></pre>
<pre><code class="language-python"># creating the model
rfc_peng = RandomForestClassifier(
    n_estimators=10,
    max_features=&#x27;sqrt&#x27;,
    random_state=42
)
</code></pre>
<pre><code class="language-python"># model training and running predictions
rfc_peng.fit(X_peng_train, y_peng_train)
peng_pred = rfc_peng.predict(X_peng_test)
print(&#x27;Accuracy Score: &#x27;,accuracy_score(y_peng_test, peng_pred, normalize=True).round(4)*100, &#x27;%&#x27;)
# Accuracy Score:  98.02 %
</code></pre>
<h4 id="feature-importance">Feature Importance</h4>
<pre><code class="language-python"># feature importance for classification
peng_index = [&#x27;importance&#x27;]
peng_data_columns = pd.Series(X_peng.columns)
peng_importance_array = rfc_peng.feature_importances_
peng_importance_df = pd.DataFrame(peng_importance_array, peng_data_columns, peng_index)
peng_importance_df
</code></pre>
<table><thead><tr><th></th><th>importance</th></tr></thead><tbody><tr><td>culmen_length_mm</td><td>0.288928</td></tr><tr><td>culmen_depth_mm</td><td>0.111021</td></tr><tr><td>flipper_length_mm</td><td>0.357994</td></tr><tr><td>body_mass_g</td><td>0.025477</td></tr><tr><td>island_Dream</td><td>0.178498</td></tr><tr><td>island_Torgersen</td><td>0.031042</td></tr><tr><td>sex_FEMALE</td><td>0.004716</td></tr><tr><td>sex_MALE</td><td>0.002324</td></tr></tbody></table>
<pre><code class="language-python">peng_importance_df.sort_values(
    by=&#x27;importance&#x27;,
    ascending=False
).plot(
    kind=&#x27;barh&#x27;,
    title=&#x27;Feature Importance for Species Classification&#x27;,
    figsize=(12,4)
)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_28-99cab7f718ef1ca3135d6fd2b1babe56.webp" width="1095" height="374"></p>
<h4 id="model-evaluation-5">Model Evaluation</h4>
<pre><code class="language-python">report_peng = classification_report(y_peng_test, peng_pred)
print(report_peng)
</code></pre>
<table><thead><tr><th></th><th>precision</th><th>recall</th><th>f1-score</th><th>support</th></tr></thead><tbody><tr><td>Adelie</td><td>0.98</td><td>0.98</td><td>0.98</td><td>49</td></tr><tr><td>Chinstrap</td><td>0.94</td><td>0.94</td><td>0.94</td><td>18</td></tr><tr><td>Gentoo</td><td>1.00</td><td>1.00</td><td>1.00</td><td>34</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.98</td><td>101</td></tr><tr><td>macro avg</td><td>0.97</td><td>0.97</td><td>0.97</td><td>101</td></tr><tr><td>weighted avg</td><td>0.98</td><td>0.98</td><td>0.98</td><td>101</td></tr></tbody></table>
<pre><code class="language-python">conf_mtx_peng = confusion_matrix(y_peng_test, peng_pred)

conf_mtx_peng_plot = ConfusionMatrixDisplay(
    confusion_matrix=conf_mtx_peng
)

conf_mtx_peng_plot.plot(cmap=&#x27;plasma&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_27-dd3f8b897d55e263ea039ae50445d9b1.webp" width="498" height="432"></p>
<h3 id="random-forest-classifier---banknote-authentication">Random Forest Classifier - Banknote Authentication</h3>
<pre><code class="language-python">!wget https://github.com/jbrownlee/Datasets/raw/master/banknote_authentication.csv -P datasets
</code></pre>
<pre><code class="language-python">money_df = pd.read_csv(&#x27;datasets/data-banknote-authentication.csv&#x27;)
money_df.head(5)
</code></pre>
<table><thead><tr><th></th><th>Variance_Wavelet</th><th>Skewness_Wavelet</th><th>Curtosis_Wavelet</th><th>Image_Entropy</th><th>Class</th></tr></thead><tbody><tr><td>0</td><td>3.62160</td><td>8.6661</td><td>-2.8073</td><td>-0.44699</td><td>0</td></tr><tr><td>1</td><td>4.54590</td><td>8.1674</td><td>-2.4586</td><td>-1.46210</td><td>0</td></tr><tr><td>2</td><td>3.86600</td><td>-2.6383</td><td>1.9242</td><td>0.10645</td><td>0</td></tr><tr><td>3</td><td>3.45660</td><td>9.5228</td><td>-4.0112</td><td>-3.59440</td><td>0</td></tr><tr><td>4</td><td>0.32924</td><td>-4.4552</td><td>4.5718</td><td>-0.98880</td><td>0</td></tr></tbody></table>
<pre><code class="language-python">sns.pairplot(money_df, hue=&#x27;Class&#x27;, palette=&#x27;winter&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_29-efb9ced68ed88761a5847abc2a97985a.webp" width="1054" height="986"></p>
<pre><code class="language-python"># drop label for training
X_money = money_df.drop(&#x27;Class&#x27;, axis=1)
y_money = money_df[&#x27;Class&#x27;]
print(X_money.shape, y_money.shape)
</code></pre>
<pre><code class="language-python">X_money_train, X_money_test, y_money_train, y_money_test = train_test_split(
    X_money,
    y_money,
    test_size=0.15,
    random_state=42
)
</code></pre>
<h4 id="grid-search-for-hyperparameters-1">Grid Search for Hyperparameters</h4>
<pre><code class="language-python">rfc_money_base = RandomForestClassifier(oob_score=True)
</code></pre>
<pre><code class="language-python">param_grid = \{
    &#x27;n_estimators&#x27;: [64, 96, 128, 160, 192],
    &#x27;max_features&#x27;: [2,3,4],
    &#x27;bootstrap&#x27;: [True, False]
\}
</code></pre>
<pre><code class="language-python">grid_money = GridSearchCV(rfc_money_base, param_grid) 
grid_money.fit(X_money_train, y_money_train)
grid_money.best_params_
# \{&#x27;bootstrap&#x27;: True, &#x27;max_features&#x27;: 2, &#x27;n_estimators&#x27;: 96\}
</code></pre>
<h4 id="model-training-and-evaluation">Model Training and Evaluation</h4>
<pre><code class="language-python">rfc_money = RandomForestClassifier(
    bootstrap=True,
    max_features=2,
    n_estimators=96,
    oob_score=True
)
rfc_money.fit(X_money_train, y_money_train)
print(&#x27;Out-of-Bag Score: &#x27;, rfc_money.oob_score_.round(4)*100, &#x27;%&#x27;)
# Out-of-Bag Score:  99.14 %
</code></pre>
<pre><code class="language-python">money_pred = rfc_money.predict(X_money_test)
money_report = classification_report(y_money_test, money_pred)
print(money_report)
</code></pre>
<table><thead><tr><th></th><th>precision</th><th>recall</th><th>f1-score</th><th>support</th></tr></thead><tbody><tr><td>0</td><td>0.99</td><td>1.00</td><td>1.00</td><td>111</td></tr><tr><td>1</td><td>1.00</td><td>0.99</td><td>0.99</td><td>95</td></tr><tr><td>accuracy</td><td></td><td></td><td>1.00</td><td>206</td></tr><tr><td>macro avg</td><td>1.00</td><td>0.99</td><td>1.00</td><td>206</td></tr><tr><td>weighted avg</td><td>1.00</td><td>1.00</td><td>1.00</td><td>206</td></tr></tbody></table>
<pre><code class="language-python">conf_mtx_money = confusion_matrix(y_money_test, money_pred)

conf_mtx_money_plot = ConfusionMatrixDisplay(
    confusion_matrix=conf_mtx_money
)

conf_mtx_money_plot.plot(cmap=&#x27;plasma&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_30-93b204ed7aa8dfabb352981184b43330.webp" width="507" height="432"></p>
<h4 id="optimizations">Optimizations</h4>
<pre><code class="language-python"># verify number of estimators found by grid search
errors = []
missclassifications = []

for n in range(1,200):
    rfc = RandomForestClassifier(n_estimators=n, max_features=2)
    rfc.fit(X_money_train, y_money_train)
    preds = rfc.predict(X_money_test)
    
    err = 1 - accuracy_score(y_money_test, preds)
    errors.append(err)
    
    n_missed = np.sum(preds != y_money_test)
    missclassifications.append(n_missed)
</code></pre>
<pre><code class="language-python">plt.figure(figsize=(12,4))
plt.title(&#x27;Errors as a Function of n_estimators&#x27;)
plt.xlabel(&#x27;Estimators&#x27;)
plt.ylabel(&#x27;Error Score&#x27;)
plt.plot(range(1,200), errors)
# there is no noteable improvement above ~10 estimators
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_31-17a160ec82d7a85729bc3a14b2d1621b.webp" width="1018" height="393"></p>
<pre><code class="language-python">plt.figure(figsize=(12,4))
plt.title(&#x27;Misclassifications as a Function of n_estimators&#x27;)
plt.xlabel(&#x27;Estimators&#x27;)
plt.ylabel(&#x27;Misclassifications&#x27;)
plt.plot(range(1,200), missclassifications)
# and the same for misclassifications
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_32-f4e68271802173c1f5669149ad1d4fe8.webp" width="988" height="393"></p>
<h3 id="random-forest-regressor">Random Forest Regressor</h3>
<p>Comparing different regression models to a random forrest regression model.</p>
<pre><code class="language-python"># dataset
!wget https://github.com/vineetsingh028/Rock_Density_Prediction/raw/master/rock_density_xray.csv -P datasets
</code></pre>
<pre><code class="language-python">rock_df = pd.read_csv(&#x27;datasets/rock_density_xray.csv&#x27;)
rock_df.columns = [&#x27;Signal&#x27;, &#x27;Density&#x27;]
rock_df.head(5)
</code></pre>
<table><thead><tr><th></th><th>Signal</th><th>Density</th></tr></thead><tbody><tr><td>0</td><td>72.945124</td><td>2.456548</td></tr><tr><td>1</td><td>14.229877</td><td>2.601719</td></tr><tr><td>2</td><td>36.597334</td><td>1.967004</td></tr><tr><td>3</td><td>9.578899</td><td>2.300439</td></tr><tr><td>4</td><td>21.765897</td><td>2.452374</td></tr></tbody></table>
<pre><code class="language-python">plt.figure(figsize=(12,5))
plt.title(&#x27;X-Ray Bounce Signal Strength vs Rock Density&#x27;)
sns.scatterplot(data=rock_df, x=&#x27;Signal&#x27;, y=&#x27;Density&#x27;)
# the signal vs density plot follows a sine wave - spoiler alert: simpler algorithm
# will fail trying to fit this dataset...
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_33-3935ab1397f3de41aaeecbe7651928f6.webp" width="1001" height="470"></p>
<pre><code class="language-python"># train-test split
X_rock = rock_df[&#x27;Signal&#x27;].values.reshape(-1,1)
y_rock = rock_df[&#x27;Density&#x27;]

X_rock_train, X_rock_test, y_rock_train, y_rock_test = train_test_split(
    X_rock,
    y_rock,
    test_size=0.1,
    random_state=42
)
</code></pre>
<pre><code class="language-python"># normalization
scaler = StandardScaler()
X_rock_train_scaled = scaler.fit_transform(X_rock_train)
X_rock_test_scaled = scaler.transform(X_rock_test)
</code></pre>
<h4 id="vs-linear-regression">vs Linear Regression</h4>
<pre><code class="language-python">lr_rock = LinearRegression()
lr_rock.fit(X_rock_train_scaled, y_rock_train)
</code></pre>
<pre><code class="language-python">lr_rock_preds = lr_rock.predict(X_rock_test_scaled)

mae = mean_absolute_error(y_rock_test, lr_rock_preds)
rmse = np.sqrt(mean_squared_error(y_rock_test, lr_rock_preds))
mean_abs = y_rock_test.mean()
avg_error = mae * 100 / mean_abs

print(&#x27;MAE: &#x27;, mae.round(2), &#x27;RMSE: &#x27;, rmse.round(2), &#x27;Relative Avg. Error: &#x27;, avg_error.round(2), &#x27;%&#x27;)
# MAE:  0.24 RMSE:  0.3 Relative Avg. Error:  10.93 %
</code></pre>
<pre><code class="language-python"># visualize predictions
plt.figure(figsize=(12,5))
plt.plot(X_rock_test, lr_rock_preds, c=&#x27;mediumspringgreen&#x27;)
sns.scatterplot(data=rock_df, x=&#x27;Signal&#x27;, y=&#x27;Density&#x27;, c=&#x27;dodgerblue&#x27;)
plt.title(&#x27;Linear Regression Predictions&#x27;)
plt.show()
# the returned error appears small because the linear regression returns an average
# but it cannot fit a linear line to the contours of the underlying sine wave function
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_34-b335083e05a60960f6cc5fc5503505a2.webp" width="1001" height="470"></p>
<h4 id="vs-polynomial-regression">vs Polynomial Regression</h4>
<pre><code class="language-python"># helper function
def run_model(model, X_train, y_train, X_test, y_test, df):
    
    # FIT MODEL
    model.fit(X_train, y_train)
    
    # EVALUATE
    y_preds = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_preds)
    rmse = np.sqrt(mean_squared_error(y_test, y_preds))
    mean_abs = y_test.mean()
    avg_error = mae * 100 / mean_abs
    print(&#x27;MAE: &#x27;, mae.round(2), &#x27;RMSE: &#x27;, rmse.round(2), &#x27;Relative Avg. Error: &#x27;, avg_error.round(2), &#x27;%&#x27;)
    
    # PLOT RESULTS
    signal_range = np.arange(0,100)
    output = model.predict(signal_range.reshape(-1,1))
    
    
    plt.figure(figsize=(12,5))
    sns.scatterplot(data=df, x=&#x27;Signal&#x27;, y=&#x27;Density&#x27;, c=&#x27;dodgerblue&#x27;)
    plt.plot(signal_range,output, c=&#x27;mediumspringgreen&#x27;)
    plt.title(&#x27;Regression Predictions&#x27;)
    plt.show()
</code></pre>
<pre><code class="language-python"># test helper on previous linear regression
run_model(
    model=lr_rock,
    X_train=X_rock_train,
    y_train=y_rock_train,
    X_test=X_rock_test,
    y_test=y_rock_test,
    df=rock_df
)
</code></pre>
<blockquote>
<p>MAE:  0.24 RMSE:  0.3 Relative Avg. Error:  10.93 %</p>
</blockquote>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_35-f865707b7739c2b6bb080846ebbedd96.webp" width="1001" height="470"></p>
<pre><code class="language-python"># build polynomial model
pipe_poly = make_pipeline(
    PolynomialFeatures(degree=6),
    LinearRegression()
)
</code></pre>
<pre><code class="language-python"># run model
run_model(
    model=pipe_poly,
    X_train=X_rock_train,
    y_train=y_rock_train,
    X_test=X_rock_test,
    y_test=y_rock_test,
    df=rock_df
)
# with a HARD LIMIT of 0-100 for the xray signal a 6th degree polinomial is a good fit
</code></pre>
<blockquote>
<p>MAE:  0.13 RMSE:  0.14 Relative Avg. Error:  5.7 %</p>
</blockquote>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_36-5294021d8c033754d2d79686f6e6dd9f.webp" width="1001" height="470"></p>
<h4 id="vs-kneighbors-regression">vs KNeighbors Regression</h4>
<pre><code class="language-python"># build polynomial model
k_values=[1,5,10,25]

for k in k_values:
    model = KNeighborsRegressor(n_neighbors=k)
    print(model)
    
    # run model
    run_model(
        model,
        X_train=X_rock_train,
        y_train=y_rock_train,
        X_test=X_rock_test,
        y_test=y_rock_test,
        df=rock_df
    )
</code></pre>
<blockquote>
<p>KNeighborsRegressor(n_neighbors=1)</p>
<p>MAE:  0.12 RMSE:  0.17 Relative Avg. Error:  5.47 %</p>
</blockquote>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_37-b1f63d5e3f49fe61106f29118218adc8.webp" width="1001" height="470"></p>
<blockquote>
<p>KNeighborsRegressor()</p>
<p>MAE:  0.13 RMSE:  0.15 Relative Avg. Error:  5.9 %</p>
</blockquote>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_38-0dc089b1d81ae4de299eb86575de3072.webp" width="1001" height="470"></p>
<blockquote>
<p>KNeighborsRegressor(n_neighbors=10)</p>
<p>MAE:  0.12 RMSE:  0.14 Relative Avg. Error:  5.44 %</p>
</blockquote>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_39-18ee99bf280cbcb4c893037802d2166a.webp" width="1001" height="470"></p>
<blockquote>
<p>KNeighborsRegressor(n_neighbors=25)</p>
<p>MAE:  0.14 RMSE:  0.16 Relative Avg. Error:  6.18 %</p>
</blockquote>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_40-88e12161aee75b889bcb73cd05021844.webp" width="1001" height="470"></p>
<h4 id="vs-decision-tree-regression">vs Decision Tree Regression</h4>
<pre><code class="language-python">tree_model = DecisionTreeRegressor()

# run model
run_model(
    model=tree_model,
    X_train=X_rock_train,
    y_train=y_rock_train,
    X_test=X_rock_test,
    y_test=y_rock_test,
    df=rock_df
)
</code></pre>
<blockquote>
<p>MAE:  0.12 RMSE:  0.17 Relative Avg. Error:  5.47 %</p>
</blockquote>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_41-b1f63d5e3f49fe61106f29118218adc8.webp" width="1001" height="470"></p>
<h4 id="vs-support-vector-regression">vs Support Vector Regression</h4>
<pre><code class="language-python">svr_rock = svm.SVR()

param_grid = \{
    &#x27;C&#x27;: [0.01,0.1,1,5,10,100, 1000],
    &#x27;gamma&#x27;: [&#x27;auto&#x27;, &#x27;scale&#x27;]
\}

rock_grid = GridSearchCV(svr_rock, param_grid)
</code></pre>
<pre><code class="language-python"># run model
run_model(
    model=rock_grid,
    X_train=X_rock_train,
    y_train=y_rock_train,
    X_test=X_rock_test,
    y_test=y_rock_test,
    df=rock_df
)
</code></pre>
<blockquote>
<p>MAE:  0.13 RMSE:  0.14 Relative Avg. Error:  5.75 %</p>
</blockquote>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_42-ecac8eb09a77200519f9550acb4d296d.webp" width="1001" height="470"></p>
<h4 id="vs-gradient-boosting-regression">vs Gradient Boosting Regression</h4>
<pre><code class="language-python">gbr_rock = GradientBoostingRegressor()

# run model
run_model(
    model=gbr_rock,
    X_train=X_rock_train,
    y_train=y_rock_train,
    X_test=X_rock_test,
    y_test=y_rock_test,
    df=rock_df
)
</code></pre>
<blockquote>
<p>MAE:  0.13 RMSE:  0.15 Relative Avg. Error:  5.76 %</p>
</blockquote>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_44-9aa58ef98313199d84aff5fe2f0de92a.webp" width="1001" height="470"></p>
<h4 id="vs-ada-boosting-regression">vs Ada Boosting Regression</h4>
<pre><code class="language-python">abr_rock = AdaBoostRegressor()

# run model
run_model(
    model=abr_rock,
    X_train=X_rock_train,
    y_train=y_rock_train,
    X_test=X_rock_test,
    y_test=y_rock_test,
    df=rock_df
)
</code></pre>
<blockquote>
<p>MAE:  0.13 RMSE:  0.14 Relative Avg. Error:  5.67 %</p>
</blockquote>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_45-ee7ba509ebab6f34db7e49c1a138c566.webp" width="1001" height="470"></p>
<h4 id="finally-random-forrest-regression">Finally, Random Forrest Regression</h4>
<pre><code class="language-python">rfr_rock = RandomForestRegressor(n_estimators=10)

# run model
run_model(
    model=rfr_rock,
    X_train=X_rock_train,
    y_train=y_rock_train,
    X_test=X_rock_test,
    y_test=y_rock_test,
    df=rock_df
)
</code></pre>
<blockquote>
<p>MAE:  0.11 RMSE:  0.14 Relative Avg. Error:  5.1 %</p>
</blockquote>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_43-4e24b3adc3f5cc7e68cfe0feec5fe895.webp" width="1001" height="470"></p>
<h2 id="supervised-learning---svc-model">Supervised Learning - SVC Model</h2>
<p><strong>Support Vector Machines</strong> (<code>SVM</code>s) are a set of supervised learning methods used for classification, regression and outliers detection.</p>
<ul>
<li>Effective in high dimensional spaces.</li>
<li>Still effective in cases where number of dimensions is greater than the number of samples.</li>
</ul>
<h3 id="dataset-5">Dataset</h3>
<ul>
<li><a href="https://www.kaggle.com/datasets/dongeorge/seed-from-uci">Three different varieties of the wheat - Kaggle.com</a></li>
</ul>
<p>Measurements of geometrical properties of kernels belonging to three different varieties of wheat:</p>
<ul>
<li><strong>A</strong>: Area,</li>
<li><strong>P</strong>: Perimeter,</li>
<li><strong>C</strong> = 4piA/P^2: Compactness,</li>
<li><strong>LK</strong>: Length of kernel,</li>
<li><strong>WK</strong>: Width of kernel,</li>
<li><strong>A_Coef</strong>: Asymmetry coefficient</li>
<li><strong>LKG</strong>: Length of kernel groove.</li>
</ul>
<pre><code class="language-python">!wget https://raw.githubusercontent.com/prasertcbs/basic-dataset/master/Seed_Data.csv -P datasets
</code></pre>
<pre><code class="language-python">wheat_df = pd.read_csv(&#x27;datasets/Seed_Data.csv&#x27;)
wheat_df.head(5)
</code></pre>
<table><thead><tr><th></th><th>A</th><th>P</th><th>C</th><th>LK</th><th>WK</th><th>A_Coef</th><th>LKG</th><th>target</th></tr></thead><tbody><tr><td>0</td><td>15.26</td><td>14.84</td><td>0.8710</td><td>5.763</td><td>3.312</td><td>2.221</td><td>5.220</td><td>0</td></tr><tr><td>1</td><td>14.88</td><td>14.57</td><td>0.8811</td><td>5.554</td><td>3.333</td><td>1.018</td><td>4.956</td><td>0</td></tr><tr><td>2</td><td>14.29</td><td>14.09</td><td>0.9050</td><td>5.291</td><td>3.337</td><td>2.699</td><td>4.825</td><td>0</td></tr><tr><td>3</td><td>13.84</td><td>13.94</td><td>0.8955</td><td>5.324</td><td>3.379</td><td>2.259</td><td>4.805</td><td>0</td></tr><tr><td>4</td><td>16.14</td><td>14.99</td><td>0.9034</td><td>5.658</td><td>3.562</td><td>1.355</td><td>5.175</td><td>0</td></tr></tbody></table>
<pre><code class="language-python">wheat_df.info()

# &lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;
# RangeIndex: 210 entries, 0 to 209
# Data columns (total 8 columns):
#  #   Column  Non-Null Count  Dtype  
# ---  ------  --------------  -----  
#  0   A       210 non-null    float64
#  1   P       210 non-null    float64
#  2   C       210 non-null    float64
#  3   LK      210 non-null    float64
#  4   WK      210 non-null    float64
#  5   A_Coef  210 non-null    float64
#  6   LKG     210 non-null    float64
#  7   target  210 non-null    int64  
# dtypes: float64(7), int64(1)
# memory usage: 13.2 KB
</code></pre>
<h4 id="preprocessing-3">Preprocessing</h4>
<pre><code class="language-python"># remove target feature from training set
X_wheat = wheat_df.drop(&#x27;target&#x27;, axis=1)
y_wheat = wheat_df[&#x27;target&#x27;]

print(X_wheat.shape, y_wheat.shape)
# (210, 7) (210,)
</code></pre>
<pre><code class="language-python"># train/test split
X_train_wheat, X_test_wheat, y_train_wheat, y_test_wheat = train_test_split(
    X_wheat,
    y_wheat,
    test_size=0.2,
    random_state=42
)
</code></pre>
<pre><code class="language-python"># normalization
sc_wheat = StandardScaler()
X_train_wheat=sc_wheat.fit_transform(X_train_wheat)
X_test_wheat=sc_wheat.fit_transform(X_test_wheat)
</code></pre>
<h4 id="model-training-1">Model Training</h4>
<pre><code class="language-python"># SVM classifier fitting
clf_wheat = svm.SVC()
clf_wheat.fit(X_train_wheat, y_train_wheat)
</code></pre>
<h4 id="model-evaluation-6">Model Evaluation</h4>
<pre><code class="language-python"># Predictions
y_wheat_pred = clf_wheat.predict(X_test_wheat)
</code></pre>
<pre><code class="language-python">print(
    &#x27;Accuracy Score: &#x27;,
    accuracy_score(y_test_wheat, y_wheat_pred, normalize=True).round(4)*100, &#x27;%&#x27;
)
# Accuracy Score:  90.48 %
</code></pre>
<pre><code class="language-python">report_wheat = classification_report(
    y_test_wheat, y_wheat_pred
)
print(report_wheat)
</code></pre>
<table><thead><tr><th></th><th>precision</th><th>recall</th><th>f1-score</th><th>support</th></tr></thead><tbody><tr><td>0</td><td>0.82</td><td>0.82</td><td>0.82</td><td>11</td></tr><tr><td>1</td><td>1.00</td><td>0.93</td><td>0.96</td><td>14</td></tr><tr><td>2</td><td>0.89</td><td>0.94</td><td>0.91</td><td>17</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.90</td><td>42</td></tr><tr><td>macro avg</td><td>0.90</td><td>0.90</td><td>0.90</td><td>42</td></tr><tr><td>weighted avg</td><td>0.91</td><td>0.90</td><td>0.91</td><td>42</td></tr></tbody></table>
<pre><code class="language-python">conf_mtx_wheat = confusion_matrix(y_test_wheat, y_wheat_pred)
conf_mtx_wheat

# array([[ 9,  0,  2],
#        [ 1, 13,  0],
#        [ 1,  0, 16]])
</code></pre>
<pre><code class="language-python">conf_mtx_wheat_plot = ConfusionMatrixDisplay(
    confusion_matrix=conf_mtx_wheat
)

conf_mtx_wheat_plot.plot()
plt.show()
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="data:image/webp;base64,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" width="498" height="438"></p>
<h3 id="margin-plots-for-support-vector-classifier">Margin Plots for Support Vector Classifier</h3>
<pre><code class="language-python"># get dataset
!wget https://github.com/alpeshraj/mouse_viral_study/raw/main/mouse_viral_study.csv -P datasets
</code></pre>
<pre><code class="language-python">mice_df = pd.read_csv(&#x27;datasets/mouse_viral_study.csv&#x27;)
mice_df.head(5)
</code></pre>
<table><thead><tr><th></th><th>Med_1_mL</th><th>Med_2_mL</th><th>Virus Present</th></tr></thead><tbody><tr><td>0</td><td>6.508231</td><td>8.582531</td><td>0</td></tr><tr><td>1</td><td>4.126116</td><td>3.073459</td><td>1</td></tr><tr><td>2</td><td>6.427870</td><td>6.369758</td><td>0</td></tr><tr><td>3</td><td>3.672953</td><td>4.905215</td><td>1</td></tr><tr><td>4</td><td>1.580321</td><td>2.440562</td><td>1</td></tr></tbody></table>
<pre><code class="language-python">sns.scatterplot(data=mice_df, x=&#x27;Med_1_mL&#x27;,y=&#x27;Med_2_mL&#x27;,hue=&#x27;Virus Present&#x27;, palette=&#x27;winter&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_12-36d45aa2f3bba2e0c9efcd4f29d8d789.webp" width="563" height="433"></p>
<pre><code class="language-python"># visualizing a hyperplane to separate the two features
sns.scatterplot(data=mice_df, x=&#x27;Med_1_mL&#x27;,y=&#x27;Med_2_mL&#x27;,hue=&#x27;Virus Present&#x27;, palette=&#x27;winter&#x27;)

x = np.linspace(0,10,100)
m = -1
b = 11
y = m*x + b

plt.plot(x,y,c=&#x27;fuchsia&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_13-508c9e879f8d29cbe6d2c9d9cc6ec40b.webp" width="563" height="433"></p>
<h4 id="svc-with-a-linear-kernel">SVC with a Linear Kernel</h4>
<pre><code class="language-python"># using a support vector classifier to calculate maximize the margin between both classes

y_vir = mice_df[&#x27;Virus Present&#x27;]
X_vir = mice_df.drop(&#x27;Virus Present&#x27;,axis=1)

# kernel : \{&#x27;linear&#x27;, &#x27;poly&#x27;, &#x27;rbf&#x27;, &#x27;sigmoid&#x27;, &#x27;precomputed&#x27;\}
# the smaller the C value the more feature vectors will be inside the margin
model_vir = svm.SVC(kernel=&#x27;linear&#x27;, C=1000)

model_vir.fit(X_vir, y_vir)
</code></pre>
<pre><code class="language-python"># import helper function
from helper.svm_margin_plot import plot_svm_boundary
</code></pre>
<pre><code class="language-python">plot_svm_boundary(model_vir, X_vir, y_vir)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_14-2b4f2c28d7d8d94ffb99bd5c91fcdad2.webp" width="543" height="413"></p>
<pre><code class="language-python"># the smaller the C value the more feature vectors will be inside the margin
model_vir_low_reg = svm.SVC(kernel=&#x27;linear&#x27;, C=0.005)
model_vir_low_reg.fit(X_vir, y_vir)
plot_svm_boundary(model_vir_low_reg, X_vir, y_vir)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_15-58bde7caf25afd45d2d980ea584d1272.webp" width="543" height="413"></p>
<h4 id="svc-with-a-radial-basis-function-kernel">SVC with a Radial Basis Function Kernel</h4>
<pre><code class="language-python">model_vir_rbf = svm.SVC(kernel=&#x27;rbf&#x27;, C=1)
model_vir_rbf.fit(X_vir, y_vir)
plot_svm_boundary(model_vir_rbf, X_vir, y_vir)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_16-44e9153451b3946990893a36c908dae9.webp" width="543" height="413"></p>
<pre><code class="language-python"># # gamma : \{&#x27;scale&#x27;, &#x27;auto&#x27;\} or float, default=&#x27;scale&#x27;
# - if ``gamma=&#x27;scale&#x27;`` (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma,
# - if &#x27;auto&#x27;, uses 1 / n_features
# - if float, must be non-negative.
model_vir_rbf_auto_gamma = svm.SVC(kernel=&#x27;rbf&#x27;, C=1, gamma=&#x27;auto&#x27;)
model_vir_rbf_auto_gamma.fit(X_vir, y_vir)
plot_svm_boundary(model_vir_rbf_auto_gamma, X_vir, y_vir)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_17-811280643c8021edb094006ad1d829f8.webp" width="543" height="413"></p>
<h4 id="svc-with-a-sigmoid-kernel">SVC with a Sigmoid Kernel</h4>
<pre><code class="language-python">model_vir_sigmoid = svm.SVC(kernel=&#x27;sigmoid&#x27;, gamma=&#x27;scale&#x27;)
model_vir_sigmoid.fit(X_vir, y_vir)
plot_svm_boundary(model_vir_sigmoid, X_vir, y_vir)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_18-517c8ddf480c51fac08d0157e538c8b2.webp" width="543" height="413"></p>
<h4 id="svc-with-a-polynomial-kernel">SVC with a Polynomial Kernel</h4>
<pre><code class="language-python">model_vir_poly = svm.SVC(kernel=&#x27;poly&#x27;, C=1, degree=2)
model_vir_poly.fit(X_vir, y_vir)
plot_svm_boundary(model_vir_poly, X_vir, y_vir)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_19-636ca895885b25823af99e2ecf7ea846.webp" width="543" height="413"></p>
<h3 id="grid-search-for-support-vector-classifier">Grid Search for Support Vector Classifier</h3>
<pre><code class="language-python">svm_base_model = svm.SVC()

param_grid = \{
    &#x27;C&#x27;:[0.01, 0.1, 1],
    &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;]
\}
</code></pre>
<pre><code class="language-python">grid = GridSearchCV(svm_base_model, param_grid) 
grid.fit(X_vir, y_vir)
</code></pre>
<pre><code class="language-python">grid.best_params_
# \{&#x27;C&#x27;: 0.01, &#x27;kernel&#x27;: &#x27;linear&#x27;\}
</code></pre>
<h3 id="support-vector-regression">Support Vector Regression</h3>
<pre><code class="language-python"># dataset
!wget https://github.com/fsdhakan/ML/raw/main/cement_slump.csv -P datasets
</code></pre>
<pre><code class="language-python">cement_df = pd.read_csv(&#x27;datasets/cement_slump.csv&#x27;)
cement_df.head(5)
</code></pre>
<table><thead><tr><th></th><th>Cement</th><th>Slag</th><th>Fly ash</th><th>Water</th><th>SP</th><th>Coarse Aggr.</th><th>Fine Aggr.</th><th>SLUMP(cm)</th><th>FLOW(cm)</th><th>Compressive Strength (28-day)(Mpa)</th></tr></thead><tbody><tr><td>0</td><td>273.0</td><td>82.0</td><td>105.0</td><td>210.0</td><td>9.0</td><td>904.0</td><td>680.0</td><td>23.0</td><td>62.0</td><td>34.99</td></tr><tr><td>1</td><td>163.0</td><td>149.0</td><td>191.0</td><td>180.0</td><td>12.0</td><td>843.0</td><td>746.0</td><td>0.0</td><td>20.0</td><td>41.14</td></tr><tr><td>2</td><td>162.0</td><td>148.0</td><td>191.0</td><td>179.0</td><td>16.0</td><td>840.0</td><td>743.0</td><td>1.0</td><td>20.0</td><td>41.81</td></tr><tr><td>3</td><td>162.0</td><td>148.0</td><td>190.0</td><td>179.0</td><td>19.0</td><td>838.0</td><td>741.0</td><td>3.0</td><td>21.5</td><td>42.08</td></tr><tr><td>4</td><td>154.0</td><td>112.0</td><td>144.0</td><td>220.0</td><td>10.0</td><td>923.0</td><td>658.0</td><td>20.0</td><td>64.0</td><td>26.82</td></tr></tbody></table>
<pre><code class="language-python">plt.figure(figsize=(8,8))
sns.heatmap(cement_df.corr(), annot=True, cmap=&#x27;viridis&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_20-a29757c094eaf3845f7ee00d647b51f6.webp" width="888" height="909"></p>
<pre><code class="language-python"># drop labels
X_cement = cement_df.drop(&#x27;Compressive Strength (28-day)(Mpa)&#x27;, axis=1)
y_cement = cement_df[&#x27;Compressive Strength (28-day)(Mpa)&#x27;]
</code></pre>
<pre><code class="language-python"> # train/test split
    X_train_cement, X_test_cement, y_train_cement, y_test_cement = train_test_split(
     X_cement,
     y_cement,
     test_size=0.3,
     random_state=42
 )
</code></pre>
<pre><code class="language-python"># normalize
scaler = StandardScaler()
X_train_cement_scaled = scaler.fit_transform(X_train_cement)
X_test_cement_scaled = scaler.transform(X_test_cement)
</code></pre>
<h4 id="base-model-run">Base Model Run</h4>
<pre><code class="language-python">base_model_cement = svm.SVR()
</code></pre>
<pre><code class="language-python">base_model_cement.fit(X_train_cement_scaled, y_train_cement)

base_model_predictions = base_model_cement.predict(X_test_cement_scaled)
</code></pre>
<pre><code class="language-python">mae = mean_absolute_error(y_test_cement, base_model_predictions)
rmse = mean_squared_error(y_test_cement, base_model_predictions)
mean_abs = y_test_cement.mean()
avg_error = mae * 100 / mean_abs

print(&#x27;MAE: &#x27;, mae.round(2), &#x27;RMSE: &#x27;, rmse.round(2), &#x27;Relative Avg. Error: &#x27;, avg_error.round(2), &#x27;%&#x27;)
</code></pre>
<table><thead><tr><th>MAE</th><th>RMSE</th><th>Relative Avg. Error</th></tr></thead><tbody><tr><td>4.68</td><td>36.95</td><td>12.75 %</td></tr></tbody></table>
<h4 id="grid-search-for-better-hyperparameter">Grid Search for better Hyperparameter</h4>
<pre><code class="language-python">param_grid = \{
    &#x27;C&#x27;: [0.001,0.01,0.1,0.5,1],
    &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;, &#x27;poly&#x27;],
    &#x27;gamma&#x27;: [&#x27;scale&#x27;, &#x27;auto&#x27;],
    &#x27;degree&#x27;: [2,3,4],
    &#x27;epsilon&#x27;: [0,0.01,0.1,0.5,1,2]
\}
</code></pre>
<pre><code class="language-python">cement_grid = GridSearchCV(base_model_cement, param_grid)
cement_grid.fit(X_train_cement_scaled, y_train_cement)
</code></pre>
<pre><code class="language-python">cement_grid.best_params_
# \{&#x27;C&#x27;: 1, &#x27;degree&#x27;: 2, &#x27;epsilon&#x27;: 2, &#x27;gamma&#x27;: &#x27;scale&#x27;, &#x27;kernel&#x27;: &#x27;linear&#x27;\}
</code></pre>
<pre><code class="language-python">cement_grid_predictions = cement_grid.predict(X_test_cement_scaled)
</code></pre>
<pre><code class="language-python">mae_grid = mean_absolute_error(y_test_cement, cement_grid_predictions)
rmse_grid = mean_squared_error(y_test_cement, cement_grid_predictions)
mean_abs = y_test_cement.mean()
avg_error_grid = mae_grid * 100 / mean_abs

print(&#x27;MAE: &#x27;, mae_grid.round(2), &#x27;RMSE: &#x27;, rmse_grid.round(2), &#x27;Relative Avg. Error: &#x27;, avg_error_grid.round(2), &#x27;%&#x27;)
</code></pre>
<table><thead><tr><th>MAE</th><th>RMSE</th><th>Relative Avg. Error</th></tr></thead><tbody><tr><td>1.85</td><td>5.2</td><td>5.05 %</td></tr></tbody></table>
<h3 id="example-task---wine-fraud">Example Task - Wine Fraud</h3>
<h4 id="data-exploration">Data Exploration</h4>
<pre><code class="language-python"># dataset
!wget https://github.com/CAPGAGA/Fraud-in-Wine/raw/main/wine_fraud.csv -P datasets
</code></pre>
<pre><code class="language-python">wine_df = pd.read_csv(&#x27;datasets/wine_fraud.csv&#x27;)
wine_df.head(5)
</code></pre>
<table><thead><tr><th></th><th>fixed acidity</th><th>volatile acidity</th><th>citric acid</th><th>residual sugar</th><th>chlorides</th><th>free sulfur dioxide</th><th>total sulfur dioxide</th><th>density</th><th>pH</th><th>sulphates</th><th>alcohol</th><th>quality</th><th>type</th></tr></thead><tbody><tr><td>0</td><td>7.4</td><td>0.70</td><td>0.00</td><td>1.9</td><td>0.076</td><td>11.0</td><td>34.0</td><td>0.9978</td><td>3.51</td><td>0.56</td><td>9.4</td><td>Legit</td><td>red</td></tr><tr><td>1</td><td>7.8</td><td>0.88</td><td>0.00</td><td>2.6</td><td>0.098</td><td>25.0</td><td>67.0</td><td>0.9968</td><td>3.20</td><td>0.68</td><td>9.8</td><td>Legit</td><td>red</td></tr><tr><td>2</td><td>7.8</td><td>0.76</td><td>0.04</td><td>2.3</td><td>0.092</td><td>15.0</td><td>54.0</td><td>0.9970</td><td>3.26</td><td>0.65</td><td>9.8</td><td>Legit</td><td>red</td></tr><tr><td>3</td><td>11.2</td><td>0.28</td><td>0.56</td><td>1.9</td><td>0.075</td><td>17.0</td><td>60.0</td><td>0.9980</td><td>3.16</td><td>0.58</td><td>9.8</td><td>Legit</td><td>red</td></tr><tr><td>4</td><td>7.4</td><td>0.70</td><td>0.00</td><td>1.9</td><td>0.076</td><td>11.0</td><td>34.0</td><td>0.9978</td><td>3.51</td><td>0.56</td><td>9.4</td><td>Legit</td><td>red</td></tr></tbody></table>
<pre><code class="language-python">wine_df.value_counts(&#x27;quality&#x27;)
</code></pre>
<table><thead><tr><th>quality</th><th></th></tr></thead><tbody><tr><td>Legit</td><td>6251</td></tr><tr><td>Fraud</td><td>246</td></tr><tr><td><em>dtype: int64</em></td><td></td></tr></tbody></table>
<pre><code class="language-python">wine_df[&#x27;quality&#x27;].value_counts().plot(
    kind=&#x27;bar&#x27;,
    figsize=(10,5), 
    title=&#x27;Wine - Quality distribution&#x27;)

</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_21-cd9a41bd66b3651cd255b032a3b21528.webp" width="839" height="476"></p>
<pre><code class="language-python">plt.figure(figsize=(10, 5))
plt.title(&#x27;Wine - Quality distribution by Type&#x27;)

sns.countplot(
    data=wine_df,
    x=&#x27;quality&#x27;,
    hue=&#x27;type&#x27;,
    palette=&#x27;winter&#x27;
)

plt.savefig(&#x27;assets/Scikit_Learn_22.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="data:image/webp;base64,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" width="859" height="470"></p>
<pre><code class="language-python">wine_df_white = wine_df[wine_df[&#x27;type&#x27;] == &#x27;white&#x27;]
wine_df_red = wine_df[wine_df[&#x27;type&#x27;] == &#x27;red&#x27;]
</code></pre>
<pre><code class="language-python"># fraud percentage by wine type
legit_white_wines = wine_df_white.value_counts(&#x27;quality&#x27;)[0]
fraud_white_wines = wine_df_white.value_counts(&#x27;quality&#x27;)[1]
white_fraud_percentage = fraud_white_wines * 100 / (legit_white_wines + fraud_white_wines)

legit_red_wines = wine_df_red.value_counts(&#x27;quality&#x27;)[0]
fraud_red_wines = wine_df_red.value_counts(&#x27;quality&#x27;)[1]
red_fraud_percentage = fraud_red_wines * 100 / (legit_red_wines + fraud_red_wines)

print(
    &#x27;Fraud Percentage: \nWhite Wines: &#x27;,
    white_fraud_percentage.round(2),
    &#x27;% \nRed Wines: &#x27;,
    red_fraud_percentage.round(2),
    &#x27;%&#x27;
)
</code></pre>
<table><thead><tr><th>Fraud Percentage:</th><th></th></tr></thead><tbody><tr><td>White Wines:</td><td>3.74 %</td></tr><tr><td>Red Wines:</td><td>3.94 %</td></tr></tbody></table>
<pre><code class="language-python"># make features numeric
feature_map = \{
    &#x27;Legit&#x27;: 0,
    &#x27;Fraud&#x27;: 1,
    &#x27;red&#x27;: 0,
    &#x27;white&#x27;: 1
\}

wine_df[&#x27;quality_enc&#x27;] = wine_df[&#x27;quality&#x27;].map(feature_map)
wine_df[&#x27;type_enc&#x27;] = wine_df[&#x27;type&#x27;].map(feature_map)
wine_df[[&#x27;quality&#x27;, &#x27;quality_enc&#x27;, &#x27;type&#x27;, &#x27;type_enc&#x27;]]
</code></pre>
<table><thead><tr><th></th><th>quality</th><th>quality_enc</th><th>type</th><th>type_enc</th></tr></thead><tbody><tr><td>0</td><td>Legit</td><td>0</td><td>red</td><td>0</td></tr><tr><td>1</td><td>Legit</td><td>0</td><td>red</td><td>0</td></tr><tr><td>2</td><td>Legit</td><td>0</td><td>red</td><td>0</td></tr><tr><td>3</td><td>Legit</td><td>0</td><td>red</td><td>0</td></tr><tr><td>4</td><td>Legit</td><td>0</td><td>red</td><td>0</td></tr><tr><td>...</td><td></td><td></td><td></td><td></td></tr><tr><td>6492</td><td>Legit</td><td>0</td><td>white</td><td>1</td></tr><tr><td>6493</td><td>Legit</td><td>0</td><td>white</td><td>1</td></tr><tr><td>6494</td><td>Legit</td><td>0</td><td>white</td><td>1</td></tr><tr><td>6495</td><td>Legit</td><td>0</td><td>white</td><td>1</td></tr><tr><td>6496</td><td>Legit</td><td>0</td><td>white</td><td>1</td></tr><tr><td><em>6497 rows × 4 columns</em></td><td></td><td></td><td></td><td></td></tr></tbody></table>
<pre><code class="language-python"># find correlations
wine_df.corr(numeric_only=True)
</code></pre>
<table><thead><tr><th></th><th>fixed acidity</th><th>volatile acidity</th><th>citric acid</th><th>residual sugar</th><th>chlorides</th><th>free sulfur dioxide</th><th>total sulfur dioxide</th><th>density</th><th>pH</th><th>sulphates</th><th>alcohol</th><th>quality_enc</th><th>type_enc</th></tr></thead><tbody><tr><td>fixed acidity</td><td>1.000000</td><td>0.219008</td><td>0.324436</td><td>-0.111981</td><td>0.298195</td><td>-0.282735</td><td>-0.329054</td><td>0.458910</td><td>-0.252700</td><td>0.299568</td><td>-0.095452</td><td>0.021794</td><td>-0.486740</td></tr><tr><td>volatile acidity</td><td>0.219008</td><td>1.000000</td><td>-0.377981</td><td>-0.196011</td><td>0.377124</td><td>-0.352557</td><td>-0.414476</td><td>0.271296</td><td>0.261454</td><td>0.225984</td><td>-0.037640</td><td>0.151228</td><td>-0.653036</td></tr><tr><td>citric acid</td><td>0.324436</td><td>-0.377981</td><td>1.000000</td><td>0.142451</td><td>0.038998</td><td>0.133126</td><td>0.195242</td><td>0.096154</td><td>-0.329808</td><td>0.056197</td><td>-0.010493</td><td>-0.061789</td><td>0.187397</td></tr><tr><td>residual sugar</td><td>-0.111981</td><td>-0.196011</td><td>0.142451</td><td>1.000000</td><td>-0.128940</td><td>0.402871</td><td>0.495482</td><td>0.552517</td><td>-0.267320</td><td>-0.185927</td><td>-0.359415</td><td>-0.048756</td><td>0.348821</td></tr><tr><td>chlorides</td><td>0.298195</td><td>0.377124</td><td>0.038998</td><td>-0.128940</td><td>1.000000</td><td>-0.195045</td><td>-0.279630</td><td>0.362615</td><td>0.044708</td><td>0.395593</td><td>-0.256916</td><td>0.034499</td><td>-0.512678</td></tr><tr><td>free sulfur dioxide</td><td>-0.282735</td><td>-0.352557</td><td>0.133126</td><td>0.402871</td><td>-0.195045</td><td>1.000000</td><td>0.720934</td><td>0.025717</td><td>-0.145854</td><td>-0.188457</td><td>-0.179838</td><td>-0.085204</td><td>0.471644</td></tr><tr><td>total sulfur dioxide</td><td>-0.329054</td><td>-0.414476</td><td>0.195242</td><td>0.495482</td><td>-0.279630</td><td>0.720934</td><td>1.000000</td><td>0.032395</td><td>-0.238413</td><td>-0.275727</td><td>-0.265740</td><td>-0.035252</td><td>0.700357</td></tr><tr><td>density</td><td>0.458910</td><td>0.271296</td><td>0.096154</td><td>0.552517</td><td>0.362615</td><td>0.025717</td><td>0.032395</td><td>1.000000</td><td>0.011686</td><td>0.259478</td><td>-0.686745</td><td>0.016351</td><td>-0.390645</td></tr><tr><td>pH</td><td>-0.252700</td><td>0.261454</td><td>-0.329808</td><td>-0.267320</td><td>0.044708</td><td>-0.145854</td><td>-0.238413</td><td>0.011686</td><td>1.000000</td><td>0.192123</td><td>0.121248</td><td>0.020107</td><td>-0.329129</td></tr><tr><td>sulphates</td><td>0.299568</td><td>0.225984</td><td>0.056197</td><td>-0.185927</td><td>0.395593</td><td>-0.188457</td><td>-0.275727</td><td>0.259478</td><td>0.192123</td><td>1.000000</td><td>-0.003029</td><td>-0.034046</td><td>-0.487218</td></tr><tr><td>alcohol</td><td>-0.095452</td><td>-0.037640</td><td>-0.010493</td><td>-0.359415</td><td>-0.256916</td><td>-0.179838</td><td>-0.265740</td><td>-0.686745</td><td>0.121248</td><td>-0.003029</td><td>1.000000</td><td>-0.051141</td><td>0.032970</td></tr><tr><td>quality_enc</td><td>0.021794</td><td>0.151228</td><td>-0.061789</td><td>-0.048756</td><td>0.034499</td><td>-0.085204</td><td>-0.035252</td><td>0.016351</td><td>0.020107</td><td>-0.034046</td><td>-0.051141</td><td>1.000000</td><td>-0.004598</td></tr><tr><td>type_enc</td><td>-0.486740</td><td>-0.653036</td><td>0.187397</td><td>0.348821</td><td>-0.512678</td><td>0.471644</td><td>0.700357</td><td>-0.390645</td><td>-0.329129</td><td>-0.487218</td><td>0.032970</td><td>-0.004598</td><td>1.000000</td></tr></tbody></table>
<pre><code class="language-python">plt.figure(figsize=(12,8))
sns.heatmap(wine_df.corr(numeric_only=True), annot=True, cmap=&#x27;viridis&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_23-ab6de122374c746f2994c9de26046068.webp" width="1024" height="781"></p>
<pre><code class="language-python"># how does the quality correlate to measurements
wine_df.corr(numeric_only=True)[&#x27;quality_enc&#x27;]
</code></pre>
<table><thead><tr><th>Quality Correlstion</th><th></th></tr></thead><tbody><tr><td>fixed acidity</td><td>0.021794</td></tr><tr><td>volatile acidity</td><td>0.151228</td></tr><tr><td>citric acid</td><td>-0.061789</td></tr><tr><td>residual sugar</td><td>-0.048756</td></tr><tr><td>chlorides</td><td>0.034499</td></tr><tr><td>free sulfur dioxide</td><td>-0.085204</td></tr><tr><td>total sulfur dioxide</td><td>-0.035252</td></tr><tr><td>density</td><td>0.016351</td></tr><tr><td>pH</td><td>0.020107</td></tr><tr><td>sulphates</td><td>-0.034046</td></tr><tr><td>alcohol</td><td>-0.051141</td></tr><tr><td>quality_enc</td><td>1.000000</td></tr><tr><td>type_enc</td><td>-0.004598</td></tr><tr><td><em>Name: quality_enc, dtype: float64</em></td><td></td></tr></tbody></table>
<pre><code class="language-python">wine_df.corr(numeric_only=True)[&#x27;quality_enc&#x27;][:-2].sort_values().plot(
    figsize=(12,5),
    kind=&#x27;bar&#x27;,
    title=&#x27;Correlation of Measurements to Quality&#x27;
)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_24-748a3c6db6dfb6a7ecaa366eb584240a.webp" width="1001" height="567"></p>
<h4 id="regression-model">Regression Model</h4>
<pre><code class="language-python"># separate target + remove string values
X_wine = wine_df.drop([&#x27;quality_enc&#x27;, &#x27;quality&#x27;, &#x27;type&#x27;], axis=1)
y_wine = wine_df[&#x27;quality&#x27;]

print(X_wine.shape, y_wine.shape)
</code></pre>
<pre><code class="language-python"># train-test split
X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(
    X_wine,
    y_wine,
    test_size=0.1,
    random_state=42
)
</code></pre>
<pre><code class="language-python"># normalization
scaler = StandardScaler()
X_wine_train_scaled = scaler.fit_transform(X_wine_train)
X_wine_test_scaled = scaler.transform(X_wine_test)
</code></pre>
<pre><code class="language-python"># create the SVC model using class_weight to balance out the
# dataset that heavily leaning towards non-frauds
svc_wine_base = svm.SVC(
    kernel=&#x27;rbf&#x27;,
    class_weight=&#x27;balanced&#x27;
)
</code></pre>
<pre><code class="language-python"># grid search
param_grid = \{
    &#x27;C&#x27;: [0.5, 1, 1.5, 2, 2.5],
    &#x27;gamma&#x27; : [&#x27;scale&#x27;, &#x27;auto&#x27;]
\}

wine_grid = GridSearchCV(svc_wine_base, param_grid)
wine_grid.fit(X_wine_train_scaled, y_wine_train)
print(&#x27;Best Params: &#x27;, wine_grid.best_params_)
# Best Params:  \{&#x27;C&#x27;: 2.5, &#x27;gamma&#x27;: &#x27;auto&#x27;\}
</code></pre>
<pre><code class="language-python">y_wine_pred = wine_grid.predict(X_wine_test_scaled)
</code></pre>
<pre><code class="language-python">print(
    &#x27;Accuracy Score: &#x27;,
    accuracy_score(y_wine_test, y_wine_pred, normalize=True).round(4)*100, &#x27;%&#x27;
)
# Accuracy Score:  84.77 %
</code></pre>
<pre><code class="language-python">report_wine = classification_report(
    y_wine_test, y_wine_pred
)
print(report_wine)
</code></pre>
<table><thead><tr><th></th><th>precision</th><th>recall</th><th>f1-score</th><th>support</th></tr></thead><tbody><tr><td>Fraud</td><td>0.16</td><td>0.68</td><td>0.26</td><td>25</td></tr><tr><td>Legit</td><td>0.99</td><td>0.85</td><td>0.92</td><td>625</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.85</td><td>650</td></tr><tr><td>macro avg</td><td>0.57</td><td>0.77</td><td>0.59</td><td>650</td></tr><tr><td>weighted avg</td><td>0.95</td><td>0.85</td><td>0.89</td><td>650</td></tr></tbody></table>
<pre><code class="language-python">conf_mtx_wine = confusion_matrix(y_wine_test, y_wine_pred)
conf_mtx_wine

# array([[ 17,   8],
#        [ 91, 534]])
</code></pre>
<pre><code class="language-python">conf_mtx_wine_plot = ConfusionMatrixDisplay(
    confusion_matrix=conf_mtx_wine
)

conf_mtx_wine_plot.plot(cmap=&#x27;plasma&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_25-01775b691ab30f8f66bfd36fff5fbb2a.webp" width="507" height="432"></p>
<pre><code class="language-python"># expand grid search
param_grid = \{
    &#x27;C&#x27;: [1000, 1050, 1100, 1050, 1200],
    &#x27;gamma&#x27; : [&#x27;scale&#x27;, &#x27;auto&#x27;]
\}

wine_grid = GridSearchCV(svc_wine_base, param_grid)
wine_grid.fit(X_wine_train_scaled, y_wine_train)
print(&#x27;Best Params: &#x27;, wine_grid.best_params_)
# Best Params:  \{&#x27;C&#x27;: 1100, &#x27;gamma&#x27;: &#x27;scale&#x27;\}
</code></pre>
<pre><code class="language-python">y_wine_pred = wine_grid.predict(X_wine_test_scaled)
print(&#x27;Accuracy Score: &#x27;,accuracy_score(y_wine_test, y_wine_pred, normalize=True).round(4)*100, &#x27;%&#x27;)
# Accuracy Score:  94.31 %
report_wine = classification_report(y_wine_test, y_wine_pred)
print(report_wine)
conf_mtx_wine = confusion_matrix(y_wine_test, y_wine_pred)

conf_mtx_wine_plot = ConfusionMatrixDisplay(
    confusion_matrix=conf_mtx_wine
)

conf_mtx_wine_plot.plot(cmap=&#x27;plasma&#x27;)
</code></pre>
<table><thead><tr><th></th><th>precision</th><th>recall</th><th>f1-score</th><th>support</th></tr></thead><tbody><tr><td>Fraud</td><td>0.29</td><td>0.32</td><td>0.30</td><td>25</td></tr><tr><td>Legit</td><td>0.97</td><td>0.97</td><td>0.97</td><td>625</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.85</td><td>650</td></tr><tr><td>macro avg</td><td>0.63</td><td>0.64</td><td>0.64</td><td>650</td></tr><tr><td>weighted avg</td><td>0.95</td><td>0.94</td><td>0.94</td><td>650</td></tr></tbody></table>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_26-9f6a60340c79469c8d83fe5d75a58417.webp" width="507" height="435"></p>
<h2 id="supervised-learning---boosting-methods">Supervised Learning - Boosting Methods</h2>
<pre><code class="language-python"># dataset - label mushrooms as poisonous or eatable
!wget https://github.com/semnan-university-ai/Mushroom/raw/main/Mushroom.csv -P datasets
</code></pre>
<h3 id="dataset-exploration">Dataset Exploration</h3>
<pre><code class="language-python">shroom_df = pd.read_csv(&#x27;datasets/mushrooms.csv&#x27;)
shroom_df.head(5).transpose()
</code></pre>
<p><a href="https://archive.ics.uci.edu/ml/datasets/mushroom">Mushroom Data Set</a></p>
<ol>
<li><strong>cap-shape</strong>: bell = <code>b</code>, conical = <code>c</code>, convex = <code>x</code>, flat = <code>f</code>,  knobbed = <code>k</code>, sunken = <code>s</code></li>
<li><strong>cap-surface</strong>: fibrous = <code>f</code>, grooves = <code>g</code>, scaly = <code>y</code>, smooth = <code>s</code></li>
<li><strong>cap-color</strong>: brown = <code>n</code>, buff = <code>b</code>, cinnamon = <code>c</code>, gray = <code>g</code>, green = <code>r</code>,  pink = <code>p</code>, purple = <code>u</code>, red = <code>e</code>, white = <code>w</code>, yellow = <code>y</code></li>
<li><strong>bruises?</strong>: bruises = <code>t</code>, no = <code>f</code></li>
<li><strong>odor</strong>: almond = <code>a</code>, anise = <code>l</code>, creosote = <code>c</code>, fishy = <code>y</code>, foul = <code>f</code>,  musty = <code>m</code>, none = <code>n</code>, pungent = <code>p</code>, spicy = <code>s</code></li>
<li><strong>gill-attachment</strong>: attached = <code>a</code>, descending = <code>d</code>, free = <code>f</code>, notched = <code>n</code></li>
<li><strong>gill-spacing</strong>: close = <code>c</code>, crowded = <code>w</code>, distant = <code>d</code></li>
<li><strong>gill-size</strong>: broad = <code>b</code>, narrow = <code>n</code></li>
<li><strong>gill-color</strong>: black = <code>k</code>, brown = <code>n</code>, buff = <code>b</code>, chocolate = <code>h</code>, gray = <code>g</code>,  green = <code>r</code>, orange = <code>o</code>, pink = <code>p</code>, purple = <code>u</code>, red = <code>e</code>,  white = <code>w</code>, yellow = <code>y</code></li>
<li><strong>stalk-shape</strong>: enlarging = <code>e</code>, tapering = <code>t</code></li>
<li><strong>stalk-root</strong>: bulbous = <code>b</code>, club = <code>c</code>, cup = <code>u</code>, equal = <code>e</code>,  rhizomorphs = <code>z</code>, rooted = <code>r</code>, missing = <code>?</code></li>
<li><strong>stalk-surface-above-ring</strong>: fibrous = <code>f</code>, scaly = <code>y</code>, silky = <code>k</code>, smooth = <code>s</code></li>
<li><strong>stalk-surface-below-ring</strong>: fibrous = <code>f</code>, scaly = <code>y</code>, silky = <code>k</code>, smooth = <code>s</code></li>
<li><strong>stalk-color-above-ring</strong>: brown = <code>n</code>, buff = <code>b</code>, cinnamon = <code>c</code>, gray = <code>g</code>, orange = <code>o</code>,  pink = <code>p</code>, red = <code>e</code>, white = <code>w</code>, yellow = <code>y</code></li>
<li><strong>stalk-color-below-ring</strong>: brown = <code>n</code>, buff = <code>b</code>, cinnamon = <code>c</code>, gray = <code>g</code>, orange = <code>o</code>,  pink = <code>p</code>, red = <code>e</code>, white = <code>w</code>, yellow = <code>y</code></li>
<li><strong>veil-type</strong>: partial = <code>p</code>, universal = <code>u</code></li>
<li><strong>veil-color</strong>: brown = <code>n</code>, orange = <code>o</code>, white = <code>w</code>, yellow = <code>y</code></li>
<li><strong>ring-number</strong>: none = <code>n</code>, one = <code>o</code>, two = <code>t</code></li>
<li><strong>ring-type</strong>: cobwebby = <code>c</code>, evanescent = <code>e</code>, flaring = <code>f</code>, large = <code>l</code>,  none = <code>n</code>, pendant = <code>p</code>, sheathing = <code>s</code>, zone = <code>z</code></li>
<li><strong>spore-print-color</strong>: black = <code>k</code>, brown = <code>n</code>, buff = <code>b</code>, chocolate = <code>h</code>, green = <code>r</code>,  orange = <code>o</code>, purple = <code>u</code>, white = <code>w</code>, yellow = <code>y</code></li>
<li><strong>population</strong>: abundant = <code>a</code>, clustered = <code>c</code>, numerous = <code>n</code>,  scattered = <code>s</code>, several = <code>v</code>, solitary = `y</li>
<li><strong>habitat</strong>: grasses = <code>g</code>, leaves = <code>l</code>, meadows = <code>m</code>, paths = <code>p</code>,  urban = <code>u</code>, waste = <code>w</code>, woods = <code>d</code></li>
</ol>
<table><thead><tr><th></th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>class</td><td>p</td><td>e</td><td>e</td><td>p</td><td>e</td></tr><tr><td>cap-shape</td><td>x</td><td>x</td><td>b</td><td>x</td><td>x</td></tr><tr><td>cap-surface</td><td>s</td><td>s</td><td>s</td><td>y</td><td>s</td></tr><tr><td>cap-color</td><td>n</td><td>y</td><td>w</td><td>w</td><td>g</td></tr><tr><td>bruises</td><td>t</td><td>t</td><td>t</td><td>t</td><td>f</td></tr><tr><td>odor</td><td>p</td><td>a</td><td>l</td><td>p</td><td>n</td></tr><tr><td>gill-attachment</td><td>f</td><td>f</td><td>f</td><td>f</td><td>f</td></tr><tr><td>gill-spacing</td><td>c</td><td>c</td><td>c</td><td>c</td><td>w</td></tr><tr><td>gill-size</td><td>n</td><td>b</td><td>b</td><td>n</td><td>b</td></tr><tr><td>gill-color</td><td>k</td><td>k</td><td>n</td><td>n</td><td>k</td></tr><tr><td>stalk-shape</td><td>e</td><td>e</td><td>e</td><td>e</td><td>t</td></tr><tr><td>stalk-root</td><td>e</td><td>c</td><td>c</td><td>e</td><td>e</td></tr><tr><td>stalk-surface-above-ring</td><td>s</td><td>s</td><td>s</td><td>s</td><td>s</td></tr><tr><td>stalk-surface-below-ring</td><td>s</td><td>s</td><td>s</td><td>s</td><td>s</td></tr><tr><td>stalk-color-above-ring</td><td>w</td><td>w</td><td>w</td><td>w</td><td>w</td></tr><tr><td>stalk-color-below-ring</td><td>w</td><td>w</td><td>w</td><td>w</td><td>w</td></tr><tr><td>veil-type</td><td>p</td><td>p</td><td>p</td><td>p</td><td>p</td></tr><tr><td>veil-color</td><td>w</td><td>w</td><td>w</td><td>w</td><td>w</td></tr><tr><td>ring-number</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td></tr><tr><td>ring-type</td><td>p</td><td>p</td><td>p</td><td>p</td><td>e</td></tr><tr><td>spore-print-color</td><td>k</td><td>n</td><td>n</td><td>k</td><td>n</td></tr><tr><td>population</td><td>s</td><td>n</td><td>n</td><td>s</td><td>a</td></tr><tr><td>habitat</td><td>u</td><td>g</td><td>m</td><td>u</td><td>g</td></tr></tbody></table>
<pre><code class="language-python">shroom_df.isnull().sum()
</code></pre>
<table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>class</td><td>0</td></tr><tr><td>cap-shape</td><td>0</td></tr><tr><td>cap-surface</td><td>0</td></tr><tr><td>cap-color</td><td>0</td></tr><tr><td>bruises</td><td>0</td></tr><tr><td>odor</td><td>0</td></tr><tr><td>gill-attachment</td><td>0</td></tr><tr><td>gill-spacing</td><td>0</td></tr><tr><td>gill-size</td><td>0</td></tr><tr><td>gill-color</td><td>0</td></tr><tr><td>stalk-shape</td><td>0</td></tr><tr><td>stalk-root</td><td>0</td></tr><tr><td>stalk-surface-above-ring</td><td>0</td></tr><tr><td>stalk-surface-below-ring</td><td>0</td></tr><tr><td>stalk-color-above-ring</td><td>0</td></tr><tr><td>stalk-color-below-ring</td><td>0</td></tr><tr><td>veil-type</td><td>0</td></tr><tr><td>veil-color</td><td>0</td></tr><tr><td>ring-number</td><td>0</td></tr><tr><td>ring-type</td><td>0</td></tr><tr><td>spore-print-color</td><td>0</td></tr><tr><td>population</td><td>0</td></tr><tr><td>habitat</td><td>0</td></tr><tr><td><em>dtype: int64</em></td><td></td></tr></tbody></table>
<pre><code class="language-python">feature_df = shroom_df.describe().transpose().reset_index(
    names=[&#x27;feature&#x27;]
).sort_values(
    &#x27;unique&#x27;, ascending=False
)
</code></pre>
<table><thead><tr><th></th><th>feature</th><th>count</th><th>unique</th><th>top</th><th>freq</th></tr></thead><tbody><tr><td>9</td><td>gill-color</td><td>8124</td><td>12</td><td>b</td><td>1728</td></tr><tr><td>3</td><td>cap-color</td><td>8124</td><td>10</td><td>n</td><td>2284</td></tr><tr><td>20</td><td>spore-print-color</td><td>8124</td><td>9</td><td>w</td><td>2388</td></tr><tr><td>5</td><td>odor</td><td>8124</td><td>9</td><td>n</td><td>3528</td></tr><tr><td>15</td><td>stalk-color-below-ring</td><td>8124</td><td>9</td><td>w</td><td>4384</td></tr><tr><td>14</td><td>stalk-color-above-ring</td><td>8124</td><td>9</td><td>w</td><td>4464</td></tr><tr><td>22</td><td>habitat</td><td>8124</td><td>7</td><td>d</td><td>3148</td></tr><tr><td>1</td><td>cap-shape</td><td>8124</td><td>6</td><td>x</td><td>3656</td></tr><tr><td>21</td><td>population</td><td>8124</td><td>6</td><td>v</td><td>4040</td></tr><tr><td>19</td><td>ring-type</td><td>8124</td><td>5</td><td>p</td><td>3968</td></tr><tr><td>11</td><td>stalk-root</td><td>8124</td><td>5</td><td>b</td><td>3776</td></tr><tr><td>12</td><td>stalk-surface-above-ring</td><td>8124</td><td>4</td><td>s</td><td>5176</td></tr><tr><td>13</td><td>stalk-surface-below-ring</td><td>8124</td><td>4</td><td>s</td><td>4936</td></tr><tr><td>17</td><td>veil-color</td><td>8124</td><td>4</td><td>w</td><td>7924</td></tr><tr><td>2</td><td>cap-surface</td><td>8124</td><td>4</td><td>y</td><td>3244</td></tr><tr><td>18</td><td>ring-number</td><td>8124</td><td>3</td><td>o</td><td>7488</td></tr><tr><td>10</td><td>stalk-shape</td><td>8124</td><td>2</td><td>t</td><td>4608</td></tr><tr><td>8</td><td>gill-size</td><td>8124</td><td>2</td><td>b</td><td>5612</td></tr><tr><td>7</td><td>gill-spacing</td><td>8124</td><td>2</td><td>c</td><td>6812</td></tr><tr><td>6</td><td>gill-attachment</td><td>8124</td><td>2</td><td>f</td><td>7914</td></tr><tr><td>4</td><td>bruises</td><td>8124</td><td>2</td><td>f</td><td>4748</td></tr><tr><td>0</td><td>class</td><td>8124</td><td>2</td><td>e</td><td>4208</td></tr><tr><td>16</td><td>veil-type</td><td>8124</td><td>1</td><td>p</td><td>8124</td></tr></tbody></table>
<pre><code class="language-python">plt.figure(figsize=(12,8))
plt.title(&#x27;Mushroom Features :: Number of unique Features&#x27;)
sns.barplot(data=feature_df, y=&#x27;feature&#x27;, x=&#x27;unique&#x27;, orient=&#x27;h&#x27;, palette=&#x27;summer_r&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_47-65f4ce2d245af4b300399da9f0198515.webp" width="1148" height="701"></p>
<pre><code class="language-python">plt.figure(figsize=(10,4))
plt.title(&#x27;Mushroom Count :: Editable vs Poisonous&#x27;)
sns.countplot(data=shroom_df, x=&#x27;class&#x27;, palette=&#x27;seismic_r&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_46-fffd386b94125a19350c6688fb2ab206.webp" width="859" height="393"></p>
<h3 id="adaptive-boosting">Adaptive Boosting</h3>
<pre><code class="language-python"># remove lable class
X_shroom = shroom_df.drop(&#x27;class&#x27;, axis=1)
# make all values numeric
X_shroom = pd.get_dummies(X_shroom, drop_first=True)

y_shroom = shroom_df[&#x27;class&#x27;]
</code></pre>
<pre><code class="language-python"># train/test split
X_shroom_train, X_shroom_test, y_shroom_train, y_shroom_test = train_test_split(
    X_shroom,
    y_shroom,
    test_size=0.15,
    random_state=42
)
</code></pre>
<h4 id="feature-exploration">Feature Exploration</h4>
<pre><code class="language-python"># don&#x27;t try fit a perfect model but only return
# the most important feature for classification
abc_shroom = AdaBoostClassifier(estimator=None, n_estimators=1)
abc_shroom.fit(X_shroom_train,y_shroom_train)
</code></pre>
<pre><code class="language-python">shroom_preds = abc_shroom.predict(X_shroom_test)

print(&#x27;Accuracy Score: &#x27;,accuracy_score(y_shroom_test, shroom_preds, normalize=True).round(4)*100, &#x27;%&#x27;)
# Accuracy Score:  88.35 %
</code></pre>
<pre><code class="language-python">report_shroom = classification_report(y_shroom_test, shroom_preds)
print(report_shroom)
</code></pre>
<table><thead><tr><th></th><th>precision</th><th>recall</th><th>f1-score</th><th>support</th></tr></thead><tbody><tr><td>e</td><td>0.97</td><td>0.80</td><td>0.88</td><td>637</td></tr><tr><td>p</td><td>0.82</td><td>0.97</td><td>0.89</td><td>582</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.88</td><td>1219</td></tr><tr><td>macro avg</td><td>0.89</td><td>0.89</td><td>0.88</td><td>1219</td></tr><tr><td>weighted avg</td><td>0.90</td><td>0.88</td><td>0.88</td><td>1219</td></tr></tbody></table>
<pre><code class="language-python">conf_mtx_shroom = confusion_matrix(y_shroom_test, shroom_preds)

conf_mtx_shroom_plot = ConfusionMatrixDisplay(
    confusion_matrix=conf_mtx_shroom
)

conf_mtx_shroom_plot.plot(cmap=&#x27;winter_r&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_48-efd1100eeedc49aea09c4e5ee9ed19c8.webp" width="507" height="432"></p>
<pre><code class="language-python"># the model was fit on a single feature and still resulted in a pretty good performance.
# Let&#x27;s find out what feature was chosen for the classification.

shroom_index = [&#x27;importance&#x27;]
shroom_data_columns = pd.Series(X_shroom.columns)
shroom_importance_array = abc_shroom.feature_importances_
shroom_importance_df = pd.DataFrame(shroom_importance_array, shroom_data_columns, shroom_index)
shroom_importance_df.value_counts()
</code></pre>
<table><thead><tr><th>importance</th><th>count</th></tr></thead><tbody><tr><td>0.0</td><td>94</td></tr><tr><td>1.0</td><td>1</td></tr><tr><td><em>dtype: int64</em></td><td></td></tr></tbody></table>
<pre><code class="language-python"># plot a slice of the dataframe to find the feature
shroom_importance_df_sorted = shroom_importance_df.sort_values(
    by=&#x27;importance&#x27;,
    ascending=True
)

shroom_importance_df_sorted[-5:].plot(
    kind=&#x27;barh&#x27;,
    title=&#x27;Feature Importance for Mushroom Classification&#x27;,
    figsize=(8,4)
)
</code></pre>
<p>The most important feature (as determined by the model) is the odor - in this case a odor of <code>none</code> is the best indicator to classify a poisonous mushroom:</p>
<blockquote>
<p>odor: almond = a, anise = l, creosote = c, fishy = y, foul = f, musty = m, none = n, pungent = p, spicy = s</p>
</blockquote>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_49-c0b06ce13dcaced3e514507b65ea4047.webp" width="767" height="374"></p>
<pre><code class="language-python"># the mojority of poisonous mushrooms do have an odor
# naking the lack of it a good indicator for an eatable variety
plt.figure(figsize=(12,4))
plt.title(&#x27;Mushroom Odor vs Class&#x27;)
sns.countplot(data=shroom_df, x=&#x27;odor&#x27;, hue=&#x27;class&#x27;, palette=&#x27;summer&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_50-a369f27cfbe114d65f930bca9d40afb2.webp" width="1014" height="393"></p>
<h4 id="optimizing-hyperparameters">Optimizing Hyperparameters</h4>
<pre><code class="language-python"># find out how many of the 95 features you have
# to add to your model to get a better fit

error_rates = []

for estimators in range(1,96):
    model = AdaBoostClassifier(n_estimators=estimators)
    model.fit(X_shroom_train,y_shroom_train)
    preds = model.predict(X_shroom_test)
    
    err = 1 - accuracy_score(y_shroom_test, preds)
    error_rates.append(err)
</code></pre>
<pre><code class="language-python">x_range=range(1,96)
plt.figure(figsize=(10,4))
plt.title(&#x27;Adaboost Error Rate vs n_estimators&#x27;)
plt.xlabel(&#x27;n_estimators&#x27;)
plt.ylabel(&#x27;Error Rate&#x27;)
plt.xticks(np.arange(min(x_range), max(x_range)+1, 3.0))
plt.plot(x_range, error_rates)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_51-510b226aeb2816e3fec84f7aca4778c0.webp" width="855" height="393"></p>
<pre><code class="language-python"># already after 16 estimators there is no
# visible improvment for the error rate
abc_shroom2 = AdaBoostClassifier(estimator=None, n_estimators=16)
abc_shroom2.fit(X_shroom_train,y_shroom_train)

shroom_preds2 = abc_shroom2.predict(X_shroom_test)

print(&#x27;Accuracy Score: &#x27;,accuracy_score(y_shroom_test, shroom_preds2, normalize=True).round(4)*100, &#x27;%&#x27;)
# Accuracy Score:  99.92 %

report_shroom2 = classification_report(y_shroom_test, shroom_preds2)
print(report_shroom2)
</code></pre>
<table><thead><tr><th></th><th>precision</th><th>recall</th><th>f1-score</th><th>support</th></tr></thead><tbody><tr><td>e</td><td>1.00</td><td>1.00</td><td>1.00</td><td>637</td></tr><tr><td>p</td><td>1.00</td><td>1.00</td><td>1.00</td><td>582</td></tr><tr><td>accuracy</td><td></td><td></td><td>1.00</td><td>1219</td></tr><tr><td>macro avg</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1219</td></tr><tr><td>weighted avg</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1219</td></tr></tbody></table>
<pre><code class="language-python">conf_mtx_shroom2 = confusion_matrix(y_shroom_test, shroom_preds2)

conf_mtx_shroom_plot2 = ConfusionMatrixDisplay(
    confusion_matrix=conf_mtx_shroom2
)

conf_mtx_shroom_plot2.plot(cmap=&#x27;winter_r&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_52-1a7ea88479a83630b8938dea0b470168.webp" width="507" height="432"></p>
<pre><code class="language-python">shroom_index = [&#x27;importance&#x27;]
shroom_data_columns = pd.Series(X_shroom.columns)
shroom_importance_array = abc_shroom2.feature_importances_
shroom_importance_df = pd.DataFrame(shroom_importance_array, shroom_data_columns, shroom_index)
shroom_importance_df.value_counts()

# there are 12 features now that are deemed important
</code></pre>
<table><thead><tr><th>importance</th><th>count</th></tr></thead><tbody><tr><td>0.0000</td><td>83</td></tr><tr><td>0.0625</td><td>9</td></tr><tr><td>0.1250</td><td>2</td></tr><tr><td>0.1875</td><td>1</td></tr><tr><td><em>dtype: int64</em></td><td></td></tr></tbody></table>
<pre><code class="language-python">shroom_importance_df_sorted = shroom_importance_df.sort_values(
    by=&#x27;importance&#x27;,
    ascending=True
).tail(13)
</code></pre>
<table><thead><tr><th></th><th>importance</th></tr></thead><tbody><tr><td>gill-size_n</td><td>0.1875</td></tr><tr><td>population_v</td><td>0.1250</td></tr><tr><td>odor_n</td><td>0.1250</td></tr><tr><td>odor_c</td><td>0.0625</td></tr><tr><td>stalk-shape_t</td><td>0.0625</td></tr><tr><td>spore-print-color_w</td><td>0.0625</td></tr><tr><td>population_c</td><td>0.0625</td></tr><tr><td>ring-type_p</td><td>0.0625</td></tr><tr><td>spore-print-color_r</td><td>0.0625</td></tr><tr><td>stalk-surface-above-ring_k</td><td>0.0625</td></tr><tr><td>gill-spacing_w</td><td>0.0625</td></tr><tr><td>odor_f</td><td>0.0625</td></tr><tr><td>stalk-color-below-ring_w</td><td>0.0000</td></tr></tbody></table>
<pre><code class="language-python">plt.figure(figsize=(10,6))
plt.title(&#x27;Features important to classify poisonous Mushrooms&#x27;)

sns.barplot(
    data=shroom_importance_df_sorted.tail(13),
    y=shroom_importance_df_sorted.tail(13).index,
    x=&#x27;importance&#x27;,
    orient=&#x27;h&#x27;,
    palette=&#x27;summer&#x27;
)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_53-9b46d91c3fbb53fe812adc1e2647770f.webp" width="989" height="547"></p>
<h3 id="gradient-boosting">Gradient Boosting</h3>
<h4 id="gridsearch-for-best-hyperparameter">Gridsearch for best Hyperparameter</h4>
<pre><code class="language-python">gb_shroom = GradientBoostingClassifier()
</code></pre>
<pre><code class="language-python">param_grid = \{
    &#x27;n_estimators&#x27;: [50, 100, 150],
    &#x27;learning_rate&#x27;: [0.05,0.1,0.2],
    &#x27;max_depth&#x27;: [2,3,4,5]
\}
</code></pre>
<pre><code class="language-python">shroom_grid = GridSearchCV(gb_shroom, param_grid)
shroom_grid.fit(X_shroom_train, y_shroom_train)
shroom_grid.best_params_
# \{&#x27;learning_rate&#x27;: 0.05, &#x27;max_depth&#x27;: 4, &#x27;n_estimators&#x27;: 150\}
</code></pre>
<pre><code class="language-python">shroom_grid_preds = shroom_grid.predict(X_shroom_test)

print(&#x27;Accuracy Score: &#x27;,accuracy_score(y_shroom_test, shroom_grid_preds, normalize=True).round(4)*100, &#x27;%&#x27;)
# Accuracy Score:  100.0 %

report_shroom_grid_preds = classification_report(y_shroom_test, shroom_grid_preds)
print(report_shroom_grid_preds)
</code></pre>
<table><thead><tr><th></th><th>precision</th><th>recall</th><th>f1-score</th><th>support</th></tr></thead><tbody><tr><td>e</td><td>1.00</td><td>1.00</td><td>1.00</td><td>637</td></tr><tr><td>p</td><td>1.00</td><td>1.00</td><td>1.00</td><td>582</td></tr><tr><td>accuracy</td><td></td><td></td><td>1.00</td><td>1219</td></tr><tr><td>macro avg</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1219</td></tr><tr><td>weighted avg</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1219</td></tr></tbody></table>
<pre><code class="language-python">conf_mtx_shroom_grid = confusion_matrix(y_shroom_test, shroom_grid_preds)

conf_mtx_shroom_grid_plot = ConfusionMatrixDisplay(
    confusion_matrix=conf_mtx_shroom_grid
)

conf_mtx_shroom_grid_plot.plot(cmap=&#x27;winter_r&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_54-d383b40378efc1e400898f649cbadcd7.webp" width="507" height="432"></p>
<h4 id="feature-importance-1">Feature Importance</h4>
<pre><code class="language-python">shroom_feature_importance = shroom_grid.best_estimator_.feature_importances_
feature_importance_df = pd.DataFrame(
    index = X_shroom.columns,
    data = shroom_feature_importance,
    columns = [&#x27;importance&#x27;]
)

# kick all features that have zero importance and sort by importance
feature_importance_df = feature_importance_df[
    feature_importance_df[&#x27;importance&#x27;] &gt; 3e-03
].sort_values(
    by=&#x27;importance&#x27;,
    ascending=False
)
</code></pre>
<pre><code class="language-python">plt.figure(figsize=(10,6))
plt.title(&#x27;Features important to classify poisonous Mushrooms&#x27;)

sns.barplot(
    data=feature_importance_df,
    y=feature_importance_df.index,
    x=&#x27;importance&#x27;,
    orient=&#x27;h&#x27;,
    palette=&#x27;summer&#x27;
)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_55-85cdf15bdbeb42c5f7d1965f83360ee3.webp" width="1014" height="470"></p>
<h2 id="supervised-learning---naive-bayes-nlp">Supervised Learning - Naive Bayes NLP</h2>
<h3 id="feature-extraction">Feature Extraction</h3>
<pre><code class="language-python">text = [
    &#x27;This is a dataset for binary sentiment classification&#x27;,
    &#x27;containing substantially more data than previous benchmark datasets&#x27;,
    &#x27;We provide a set of 25,000 highly polar movie reviews for training&#x27;,
    &#x27;And 25,000 for testing&#x27;,
    &#x27;There is additional unlabeled data for use as well&#x27;,
    &#x27;Raw text and already processed bag of words formats are provided&#x27;
]
</code></pre>
<h4 id="countvectorizer--tfidftransformer">CountVectorizer &amp; TfidfTransformer</h4>
<pre><code class="language-python">cv = CountVectorizer(stop_words=&#x27;english&#x27;)
cv_sparse_matrix = cv.fit_transform(text)
# &lt;6x30 sparse matrix of type &#x27;&lt;class &#x27;numpy.int64&#x27;&gt;&#x27;
# 	with 33 stored elements in Compressed Sparse Row format&gt;
</code></pre>
<pre><code class="language-python">print(cv_sparse_matrix.todense())
# [[0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]
#  [0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0]
#  [1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0]
#  [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]
#  [0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]
#  [0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1]]
</code></pre>
<pre><code class="language-python">print(cv.vocabulary_)
# \{&#x27;dataset&#x27;: 9, &#x27;binary&#x27;: 5, &#x27;sentiment&#x27;: 21, &#x27;classification&#x27;: 6, &#x27;containing&#x27;: 7, &#x27;substantially&#x27;: 23, &#x27;data&#x27;: 8, &#x27;previous&#x27;: 15, &#x27;benchmark&#x27;: 4, &#x27;datasets&#x27;: 10, &#x27;provide&#x27;: 17, &#x27;set&#x27;: 22, &#x27;25&#x27;: 1, &#x27;000&#x27;: 0, &#x27;highly&#x27;: 12, &#x27;polar&#x27;: 14, &#x27;movie&#x27;: 13, &#x27;reviews&#x27;: 20, &#x27;training&#x27;: 26, &#x27;testing&#x27;: 24, &#x27;additional&#x27;: 2, &#x27;unlabeled&#x27;: 27, &#x27;use&#x27;: 28, &#x27;raw&#x27;: 19, &#x27;text&#x27;: 25, &#x27;processed&#x27;: 16, &#x27;bag&#x27;: 3, &#x27;words&#x27;: 29, &#x27;formats&#x27;: 11, &#x27;provided&#x27;: 18\}
</code></pre>
<pre><code class="language-python">tfidf_trans = TfidfTransformer()
tfidf_trans_results = tfidf_trans.fit_transform(cv_sparse_matrix)
</code></pre>
<pre><code class="language-python">print(tfidf_trans_results.todense())
# [[0.         0.         0.         0.         0.         0.5
#   0.5        0.         0.         0.5        0.         0.
#   0.         0.         0.         0.         0.         0.
#   0.         0.         0.         0.5        0.         0.
#   0.         0.         0.         0.         0.         0.        ]
#  [0.         0.         0.         0.         0.4198708  0.
#   0.         0.4198708  0.34430007 0.         0.4198708  0.
#   0.         0.         0.         0.4198708  0.         0.
#   0.         0.         0.         0.         0.         0.4198708
#   0.         0.         0.         0.         0.         0.        ]
#  [0.28386526 0.28386526 0.         0.         0.         0.
#   0.         0.         0.         0.         0.         0.
#   0.3461711  0.3461711  0.3461711  0.         0.         0.3461711
#   0.         0.         0.3461711  0.         0.3461711  0.
#   0.         0.         0.3461711  0.         0.         0.        ]
#  [0.5355058  0.5355058  0.         0.         0.         0.
#   0.         0.         0.         0.         0.         0.
#   0.         0.         0.         0.         0.         0.
#   0.         0.         0.         0.         0.         0.
#   0.65304446 0.         0.         0.         0.         0.        ]
#  [0.         0.         0.52182349 0.         0.         0.
#   0.         0.         0.42790272 0.         0.         0.
#   0.         0.         0.         0.         0.         0.
#   0.         0.         0.         0.         0.         0.
#   0.         0.         0.         0.52182349 0.52182349 0.        ]
#  [0.         0.         0.         0.37796447 0.         0.
#   0.         0.         0.         0.         0.         0.37796447
#   0.         0.         0.         0.         0.37796447 0.
#   0.37796447 0.37796447 0.         0.         0.         0.
#   0.         0.37796447 0.         0.         0.         0.37796447]]
</code></pre>
<h4 id="tfidfvectorizer">TfidfVectorizer</h4>
<pre><code class="language-python">tfidf_vec = TfidfVectorizer(
    lowercase=True,
    analyzer=&#x27;word&#x27;,
    stop_words=&#x27;english&#x27;
)

tfidf_vec_results = tfidf_vec.fit_transform(text)
# &lt;6x30 sparse matrix of type &#x27;&lt;class &#x27;numpy.float64&#x27;&gt;&#x27;
# 	with 33 stored elements in Compressed Sparse Row format&gt;
</code></pre>
<pre><code class="language-python">print(tfidf_trans_results == tfidf_vec_results)
# True
</code></pre>
<h4 id="dataset-exploration-1">Dataset Exploration</h4>
<pre><code class="language-python">!wget https://raw.githubusercontent.com/kunal-lalwani/Twitter-US-Airlines-Sentiment-Analysis/master/Tweets.csv -P datasets
</code></pre>
<pre><code class="language-python">tweet_df = pd.read_csv(&#x27;datasets/Tweets.csv&#x27;)
tweet_df.head(3).transpose()
</code></pre>
<table><thead><tr><th></th><th>0</th><th>1</th><th>2</th></tr></thead><tbody><tr><td>tweet_id</td><td>570306133677760513</td><td>570301130888122368</td><td>570301083672813571</td></tr><tr><td>airline_sentiment</td><td>neutral</td><td>positive</td><td>neutral</td></tr><tr><td>airline_sentiment_confidence</td><td>1.0</td><td>0.3486</td><td>0.6837</td></tr><tr><td>negativereason</td><td>NaN</td><td>NaN</td><td>NaN</td></tr><tr><td>negativereason_confidence</td><td>NaN</td><td>0.0</td><td>NaN</td></tr><tr><td>airline</td><td>Virgin America</td><td>Virgin America</td><td>Virgin America</td></tr><tr><td>airline_sentiment_gold</td><td>NaN</td><td>NaN</td><td>NaN</td></tr><tr><td>name</td><td>cairdin</td><td>jnardino</td><td>yvonnalynn</td></tr><tr><td>negativereason_gold</td><td>NaN</td><td>NaN</td><td>NaN</td></tr><tr><td>retweet_count</td><td>0</td><td>0</td><td>0</td></tr><tr><td>text</td><td>@VirginAmerica What @dhepburn said.</td><td>@VirginAmerica plus you&#x27;ve added commercials t...</td><td>@VirginAmerica I didn&#x27;t today... Must mean I n...</td></tr><tr><td>tweet_coord</td><td>NaN</td><td>NaN</td><td>NaN</td></tr><tr><td>tweet_created</td><td>2015-02-24 11:35:52 -0800</td><td>2015-02-24 11:15:59 -0800</td><td>2015-02-24 11:15:48 -0800</td></tr><tr><td>tweet_location</td><td>NaN</td><td>NaN</td><td>Lets Play</td></tr><tr><td>user_timezone</td><td>Eastern Time (US &amp; Canada)</td><td>Pacific Time (US &amp; Canada)</td><td>Central Time (US &amp; Canada)</td></tr></tbody></table>
<pre><code class="language-python">plt.figure(figsize=(12,5))
plt.title(&#x27;Tweet Sentiment Classification by Airline&#x27;)
sns.countplot(
    data=tweet_df,
    x=&#x27;airline&#x27;,
    hue=&#x27;airline_sentiment&#x27;,
    palette=&#x27;cool&#x27;
)

plt.savefig(&#x27;assets/Scikit_Learn_56.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_56-85cdf15bdbeb42c5f7d1965f83360ee3.webp" width="1014" height="470"></p>
<pre><code class="language-python">plt.figure(figsize=(12,6))
plt.title(&#x27;Tweet Sentiment Classification with negative Reason&#x27;)
sns.countplot(
    data=tweet_df,
    x=&#x27;airline&#x27;,
    hue=&#x27;negativereason&#x27;,
    palette=&#x27;cool&#x27;
)

plt.savefig(&#x27;assets/Scikit_Learn_57.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_57-4c69d11e39f9324fe2f37dbffb938d49.webp" width="1005" height="547"></p>
<h4 id="data-preprocessing">Data Preprocessing</h4>
<pre><code class="language-python">tweet_data = tweet_df[[&#x27;airline_sentiment&#x27;, &#x27;text&#x27;]]
</code></pre>
<pre><code class="language-python">X_tweet = tweet_data[&#x27;text&#x27;]
y_tweet = tweet_data[&#x27;airline_sentiment&#x27;]
</code></pre>
<pre><code class="language-python"># train/ test split
X_tweet_train, X_tweet_test, y_tweet_train, y_tweet_test = train_test_split(
    X_tweet,
    y_tweet,
    test_size=0.2,
    random_state=42
)
</code></pre>
<h4 id="tfidf-vectorizer">TFIDF Vectorizer</h4>
<pre><code class="language-python">tfidf_tweet_vec = TfidfVectorizer(
    lowercase=True,
    analyzer=&#x27;word&#x27;,
    stop_words=&#x27;english&#x27;
)

X_tweet_tfidf_train = tfidf_tweet_vec.fit_transform(X_tweet_train)
# &lt;11712x12987 sparse matrix of type &#x27;&lt;class &#x27;numpy.float64&#x27;&gt;&#x27;
# 	with 106745 stored elements in Compressed Sparse Row format&gt;
X_tweet_tfidf_test = tfidf_tweet_vec.transform(X_tweet_test)
</code></pre>
<h4 id="model-comparison">Model Comparison</h4>
<pre><code class="language-python"># report helper function
def report(model):
    preds = model.predict(X_tweet_tfidf_test)
    
    print(classification_report(y_tweet_test, preds))
    
    conf_mtx = confusion_matrix(y_tweet_test, preds)
    conf_mtx_plot = ConfusionMatrixDisplay(
        confusion_matrix=conf_mtx
    )
    conf_mtx_plot.plot(cmap=&#x27;plasma&#x27;)
</code></pre>
<pre><code class="language-python">logreg_tweet = LogisticRegression(max_iter=1000)
logreg_tweet.fit(X_tweet_tfidf_train, y_tweet_train)
</code></pre>
<pre><code class="language-python">report(logreg_tweet)
</code></pre>
<table><thead><tr><th></th><th>precision</th><th>recall</th><th>f1-score</th><th>support</th></tr></thead><tbody><tr><td>negative</td><td>0.82</td><td>0.93</td><td>0.88</td><td>1889</td></tr><tr><td>neutral</td><td>0.66</td><td>0.48</td><td>0.56</td><td>580</td></tr><tr><td>positive</td><td>0.79</td><td>0.63</td><td>0.70</td><td>459</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.80</td><td>2928</td></tr><tr><td>macro avg</td><td>0.76</td><td>0.68</td><td>0.71</td><td>2928</td></tr><tr><td>weighted avg</td><td>0.79</td><td>0.80</td><td>0.78</td><td>2928</td></tr></tbody></table>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_58-a2c14d3e9faf77376e121b9709730b34.webp" width="516" height="432"></p>
<pre><code class="language-python">rbf_svc_tweet = svm.SVC()
rbf_svc_tweet.fit(X_tweet_tfidf_train, y_tweet_train)
</code></pre>
<pre><code class="language-python">report(rbf_svc_tweet)
</code></pre>
<table><thead><tr><th></th><th>precision</th><th>recall</th><th>f1-score</th><th>support</th></tr></thead><tbody><tr><td>negative</td><td>0.81</td><td>0.95</td><td>0.87</td><td>1889</td></tr><tr><td>neutral</td><td>0.68</td><td>0.42</td><td>0.52</td><td>580</td></tr><tr><td>positive</td><td>0.80</td><td>0.61</td><td>0.69</td><td>459</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.79</td><td>2928</td></tr><tr><td>macro avg</td><td>0.76</td><td>0.66</td><td>0.69</td><td>2928</td></tr><tr><td>weighted avg</td><td>0.78</td><td>0.79</td><td>0.77</td><td>2928</td></tr></tbody></table>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_59-0e02c3bc40cbbe8983f9ae0593ee657c.webp" width="516" height="432"></p>
<pre><code class="language-python">linear_svc_tweet = svm.LinearSVC()
linear_svc_tweet.fit(X_tweet_tfidf_train, y_tweet_train)
</code></pre>
<pre><code class="language-python">report(linear_svc_tweet)
</code></pre>
<table><thead><tr><th></th><th>precision</th><th>recall</th><th>f1-score</th><th>support</th></tr></thead><tbody><tr><td>negative</td><td>0.85</td><td>0.91</td><td>0.88</td><td>1889</td></tr><tr><td>neutral</td><td>0.64</td><td>0.54</td><td>0.58</td><td>580</td></tr><tr><td>positive</td><td>0.76</td><td>0.67</td><td>0.71</td><td>459</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.80</td><td>2928</td></tr><tr><td>macro avg</td><td>0.75</td><td>0.71</td><td>0.72</td><td>2928</td></tr><tr><td>weighted avg</td><td>0.79</td><td>0.80</td><td>0.79</td><td>2928</td></tr></tbody></table>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_60-42255e99e92daea659acb00c21dab757.webp" width="516" height="432"></p>
<pre><code class="language-python">nb_tweets = MultinomialNB()
nb_tweets.fit(X_tweet_tfidf_train, y_tweet_train)
</code></pre>
<pre><code class="language-python">report(nb_tweets)
# The Naive Bayes classifies almost all tweets as negative
# which means it does well with searching neg tweets
# but ends up classifying a lot neutral and pos tweets as neg
</code></pre>
<table><thead><tr><th></th><th>precision</th><th>recall</th><th>f1-score</th><th>support</th></tr></thead><tbody><tr><td>negative</td><td>0.69</td><td>0.99</td><td>0.81</td><td>1889</td></tr><tr><td>neutral</td><td>0.75</td><td>0.15</td><td>0.25</td><td>580</td></tr><tr><td>positive</td><td>0.94</td><td>0.18</td><td>0.31</td><td>459</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.70</td><td>2928</td></tr><tr><td>macro avg</td><td>0.79</td><td>0.44</td><td>0.46</td><td>2928</td></tr><tr><td>weighted avg</td><td>0.74</td><td>0.70</td><td>0.62</td><td>2928</td></tr></tbody></table>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_61-be2d40f3643bcf8248569b5ff82e1bfb.webp" width="994" height="510"></p>
<h4 id="model-deployment">Model Deployment</h4>
<pre><code class="language-python"># building a pipeline to ingest new tweets with the best performing model
pipe = Pipeline(
    [
        (&#x27;tfidf&#x27;, TfidfVectorizer()),
        (&#x27;svc&#x27;, svm.SVC())
    ]
)
</code></pre>
<pre><code class="language-python"># before deployment retrain on entire dataset
pipe.fit(X_tweet, y_tweet)
</code></pre>
<pre><code class="language-python"># test prediction
print(pipe.predict([
    &#x27;good flight&#x27;,
    &#x27;terrible service&#x27;,
    &#x27;too late&#x27;,
    &#x27;ok flight&#x27;,
    &#x27;Thank you&#x27;
]))
# [&#x27;positive&#x27; &#x27;negative&#x27; &#x27;negative&#x27; &#x27;neutral&#x27; &#x27;positive&#x27;]
</code></pre>
<h3 id="text-classification">Text Classification</h3>
<p>IMDB Dataset of 50K Movie Reviews
<a href="https://ai.stanford.edu/~amaas/data/sentiment/">https://ai.stanford.edu/~amaas/data/sentiment/</a></p>
<h4 id="data-exploration-1">Data Exploration</h4>
<pre><code class="language-python">imdb_df = pd.read_csv(&#x27;datasets/moviereviews.csv&#x27;)
imdb_df.head()
</code></pre>
<table><thead><tr><th></th><th>label</th><th>review</th></tr></thead><tbody><tr><td>0</td><td>neg</td><td>how do films like mouse hunt get into theatres...</td></tr><tr><td>1</td><td>neg</td><td>some talented actresses are blessed with a dem...</td></tr><tr><td>2</td><td>pos</td><td>this has been an extraordinary year for austra...</td></tr><tr><td>3</td><td>pos</td><td>according to hollywood movies made in last few...</td></tr><tr><td>4</td><td>neg</td><td>my first press screening of 1998 and already i...</td></tr></tbody></table>
<pre><code class="language-python">imdb_df.info()

# &lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;
# RangeIndex: 2000 entries, 0 to 1999
# Data columns (total 2 columns):
#  #   Column  Non-Null Count  Dtype 
# ---  ------  --------------  ----- 
#  0   label   2000 non-null   object
#  1   review  1965 non-null   object
# dtypes: object(2)
# memory usage: 31.4+ KB
</code></pre>
<pre><code class="language-python"># find missing
imdb_df.isnull().sum()
# label      0
# review    35
# dtype: int64
</code></pre>
<pre><code class="language-python"># drop missing
imdb_df = imdb_df.dropna(axis=0)
imdb_df.isnull().sum()
# label     0
# review    0
# dtype: int64
</code></pre>
<pre><code class="language-python"># make sure there a no empty string reviews
# (imdb_df[&#x27;review&#x27;] == &#x27;  &#x27;).sum()
imdb_df[&#x27;review&#x27;].str.isspace().sum()
# 27
</code></pre>
<pre><code class="language-python"># remove empty string reviews
imdb_df = imdb_df[~imdb_df[&#x27;review&#x27;].str.isspace()]
imdb_df = imdb_df[imdb_df[&#x27;review&#x27;] != &#x27;&#x27;]
</code></pre>
<pre><code class="language-python">imdb_df[&#x27;review&#x27;].str.isspace().sum()
# 0
</code></pre>
<pre><code class="language-python"># is the dataset balanced
imdb_df[&#x27;label&#x27;].value_counts()
# neg    969
# pos    969
# Name: label, dtype: int64
</code></pre>
<h4 id="top-30-features-by-label">Top 30 Features by Label</h4>
<pre><code class="language-python"># find top 20 words in negative reviews
imdb_neg_df = imdb_df[imdb_df[&#x27;label&#x27;] == &#x27;neg&#x27;]

count_vectorizer = CountVectorizer(analyzer=&#x27;word&#x27;, stop_words=&#x27;english&#x27;)
bag_of_words = count_vectorizer.fit_transform(imdb_neg_df[&#x27;review&#x27;])
sum_words = bag_of_words.sum(axis=0)
</code></pre>
<pre><code class="language-python">words_freq = [
    (word, sum_words[0, idx]) for word, idx in count_vectorizer.vocabulary_.items()
]

words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
</code></pre>
<pre><code class="language-python">x, y = zip(*words_freq[:30])

plt.figure(figsize=(12,5))
plt.bar(x,y)
plt.xticks(rotation=90)
plt.title(&#x27;Top30 Words used in Negative Reviews&#x27;)

plt.savefig(&#x27;assets/Scikit_Learn_62.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_62-740f0dd632547e8a2502c2b987e92ab2.webp" width="994" height="510"></p>
<pre><code class="language-python"># find top 20 words in positive reviews
imdb_pos_df = imdb_df[imdb_df[&#x27;label&#x27;] != &#x27;neg&#x27;]

count_vectorizer = CountVectorizer(analyzer=&#x27;word&#x27;, stop_words=&#x27;english&#x27;)
bag_of_words = count_vectorizer.fit_transform(imdb_pos_df[&#x27;review&#x27;])
sum_words = bag_of_words.sum(axis=0)
</code></pre>
<pre><code class="language-python">words_freq = [
    (word, sum_words[0, idx]) for word, idx in count_vectorizer.vocabulary_.items()
]

words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
</code></pre>
<pre><code class="language-python">x, y = zip(*words_freq[:30])

plt.figure(figsize=(12,5))
plt.bar(x,y)
plt.xticks(rotation=90)
plt.title(&#x27;Top30 Words used in Positive Reviews&#x27;)

plt.savefig(&#x27;assets/Scikit_Learn_63.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_63-366cc23b240fcfc2aed773307e23c54a.webp" width="994" height="510"></p>
<h4 id="data-preprocessing-1">Data Preprocessing</h4>
<pre><code class="language-python">X_rev = imdb_df[&#x27;review&#x27;]
y_rev = imdb_df[&#x27;label&#x27;]
</code></pre>
<pre><code class="language-python"># train/ test split
X_rev_train, X_rev_test, y_rev_train, y_rev_test = train_test_split(
    X_rev,
    y_rev,
    test_size=0.2,
    random_state=42
)
</code></pre>
<pre><code class="language-python">tfidf_rev_vec = TfidfVectorizer(
    lowercase=True,
    analyzer=&#x27;word&#x27;,
    stop_words=&#x27;english&#x27;
)

X_rev_tfidf_train = tfidf_rev_vec.fit_transform(X_rev_train)
X_rev_tfidf_test = tfidf_rev_vec.transform(X_rev_test)
</code></pre>
<h4 id="model-training-2">Model Training</h4>
<pre><code class="language-python">nb_rev = MultinomialNB()
nb_rev.fit(X_rev_tfidf_train, y_rev_train)
</code></pre>
<pre><code class="language-python">preds = nb_rev.predict(X_rev_tfidf_test)
print(classification_report(y_rev_test, preds))
</code></pre>
<table><thead><tr><th></th><th>precision</th><th>recall</th><th>f1-score</th><th>support</th></tr></thead><tbody><tr><td>neg</td><td>0.79</td><td>0.88</td><td>0.83</td><td>188</td></tr><tr><td>pos</td><td>0.87</td><td>0.78</td><td>0.82</td><td>200</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.82</td><td>388</td></tr><tr><td>macro avg</td><td>0.83</td><td>0.83</td><td>0.82</td><td>388</td></tr><tr><td>weighted avg</td><td>0.83</td><td>0.82</td><td>0.82</td><td>388</td></tr></tbody></table>
<pre><code class="language-python">conf_mtx = confusion_matrix(y_rev_test, preds)
conf_mtx_plot = ConfusionMatrixDisplay(
     confusion_matrix=conf_mtx
)
conf_mtx_plot.plot(cmap=&#x27;plasma&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_64-8c1403f47d6ca52d1b66f63252a9a72d.webp" width="507" height="432"></p>
<h2 id="unsupervised-learning---kmeans-clustering">Unsupervised Learning - KMeans Clustering</h2>
<h3 id="dataset-exploration-2">Dataset Exploration</h3>
<pre><code class="language-python">!wget https://github.com/selva86/datasets/raw/master/bank-full.csv -P datasets
</code></pre>
<pre><code class="language-python">bank_df = pd.read_csv(&#x27;datasets/bank-full.csv&#x27;, sep=&#x27;;&#x27;)
bank_df.head(5).transpose()
</code></pre>
<table><thead><tr><th></th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>age</td><td>56</td><td>57</td><td>37</td><td>40</td><td>56</td></tr><tr><td>job</td><td>housemaid</td><td>services</td><td>services</td><td>admin.</td><td>services</td></tr><tr><td>marital</td><td>married</td><td>married</td><td>married</td><td>married</td><td>married</td></tr><tr><td>education</td><td>basic.4y</td><td>high.school</td><td>high.school</td><td>basic.6y</td><td>high.school</td></tr><tr><td>default</td><td>no</td><td>unknown</td><td>no</td><td>no</td><td>no</td></tr><tr><td>housing</td><td>no</td><td>no</td><td>yes</td><td>no</td><td>no</td></tr><tr><td>loan</td><td>no</td><td>no</td><td>no</td><td>no</td><td>yes</td></tr><tr><td>contact</td><td>telephone</td><td>telephone</td><td>telephone</td><td>telephone</td><td>telephone</td></tr><tr><td>month</td><td>may</td><td>may</td><td>may</td><td>may</td><td>may</td></tr><tr><td>day_of_week</td><td>mon</td><td>mon</td><td>mon</td><td>mon</td><td>mon</td></tr><tr><td>duration</td><td>261</td><td>149</td><td>226</td><td>151</td><td>307</td></tr><tr><td>campaign</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td></tr><tr><td>pdays</td><td>999</td><td>999</td><td>999</td><td>999</td><td>999</td></tr><tr><td>previous</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>poutcome</td><td>nonexistent</td><td>nonexistent</td><td>nonexistent</td><td>nonexistent</td><td>nonexistent</td></tr><tr><td>emp.var.rate</td><td>1.1</td><td>1.1</td><td>1.1</td><td>1.1</td><td>1.1</td></tr><tr><td>cons.price.idx</td><td>93.994</td><td>93.994</td><td>93.994</td><td>93.994</td><td>93.994</td></tr><tr><td>cons.conf.idx</td><td>-36.4</td><td>-36.4</td><td>-36.4</td><td>-36.4</td><td>-36.4</td></tr><tr><td>euribor3m</td><td>4.857</td><td>4.857</td><td>4.857</td><td>4.857</td><td>4.857</td></tr><tr><td>nr.employed</td><td>5191.0</td><td>5191.0</td><td>5191.0</td><td>5191.0</td><td>5191.0</td></tr><tr><td>y</td><td>no</td><td>no</td><td>no</td><td>no</td><td>no</td></tr></tbody></table>
<pre><code class="language-python">bank_df.describe()
</code></pre>
<table><thead><tr><th></th><th>age</th><th>duration</th><th>campaign</th><th>pdays</th><th>previous</th><th>emp.var.rate</th><th>cons.price.idx</th><th>cons.conf.idx</th><th>euribor3m</th><th>nr.employed</th></tr></thead><tbody><tr><td>count</td><td>41188.00000</td><td>41188.000000</td><td>41188.000000</td><td>41188.000000</td><td>41188.000000</td><td>41188.000000</td><td>41188.000000</td><td>41188.000000</td><td>41188.000000</td><td>41188.000000</td></tr><tr><td>mean</td><td>40.02406</td><td>258.285010</td><td>2.567593</td><td>962.475454</td><td>0.172963</td><td>0.081886</td><td>93.575664</td><td>-40.502600</td><td>3.621291</td><td>5167.035911</td></tr><tr><td>std</td><td>10.42125</td><td>259.279249</td><td>2.770014</td><td>186.910907</td><td>0.494901</td><td>1.570960</td><td>0.578840</td><td>4.628198</td><td>1.734447</td><td>72.251528</td></tr><tr><td>min</td><td>17.00000</td><td>0.000000</td><td>1.000000</td><td>0.000000</td><td>0.000000</td><td>-3.400000</td><td>92.201000</td><td>-50.800000</td><td>0.634000</td><td>4963.600000</td></tr><tr><td>25%</td><td>32.00000</td><td>102.000000</td><td>1.000000</td><td>999.000000</td><td>0.000000</td><td>-1.800000</td><td>93.075000</td><td>-42.700000</td><td>1.344000</td><td>5099.100000</td></tr><tr><td>50%</td><td>38.00000</td><td>180.000000</td><td>2.000000</td><td>999.000000</td><td>0.000000</td><td>1.100000</td><td>93.749000</td><td>-41.800000</td><td>4.857000</td><td>5191.000000</td></tr><tr><td>75%</td><td>47.00000</td><td>319.000000</td><td>3.000000</td><td>999.000000</td><td>0.000000</td><td>1.400000</td><td>93.994000</td><td>-36.400000</td><td>4.961000</td><td>5228.100000</td></tr><tr><td>max</td><td>98.00000</td><td>4918.000000</td><td>56.000000</td><td>999.000000</td><td>7.000000</td><td>1.400000</td><td>94.767000</td><td>-26.900000</td><td>5.045000</td><td>5228.100000</td></tr></tbody></table>
<pre><code class="language-python">plt.figure(figsize=(12, 5))
plt.title(&#x27;Age Distribution by Marital Status&#x27;)

sns.histplot(
    data=bank_df,
    x=&#x27;age&#x27;,
    bins=50,
    hue=&#x27;marital&#x27;,
    palette=&#x27;winter&#x27;,
    kde=True
)

plt.savefig(&#x27;assets/Scikit_Learn_65.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_65-d22615d34bb83a8cf84bd55b5d8ba887.webp" width="1014" height="470"></p>
<pre><code class="language-python">plt.figure(figsize=(12, 5))
plt.title(&#x27;Age Distribution by Loan Status&#x27;)

sns.histplot(
    data=bank_df,
    x=&#x27;age&#x27;,
    bins=50,
    hue=&#x27;loan&#x27;,
    palette=&#x27;winter&#x27;,
    kde=True
)

plt.savefig(&#x27;assets/Scikit_Learn_66.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_66-9529748991a708186c441c74ee843b8e.webp" width="1014" height="470"></p>
<pre><code class="language-python"># remove columns with `pday`s = 999 (placeholder for never)
plt.figure(figsize=(12, 5))
plt.title(&#x27;Distribution of Days Since Last Contacted by Loan Status&#x27;)

sns.histplot(
    data=bank_df[bank_df[&#x27;pdays&#x27;] != 999],
    x=&#x27;pdays&#x27;,
    hue=&#x27;loan&#x27;,
    palette=&#x27;winter&#x27;,
    kde=True
)

plt.savefig(&#x27;assets/Scikit_Learn_67.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_67-696d36ed3637b7220f604f0d09ca7751.webp" width="1005" height="470"></p>
<pre><code class="language-python"># Create call duration in minutes column
bank_df[&#x27;duration_minutes&#x27;] = bank_df[&#x27;duration&#x27;].apply(lambda x: x/60).round(1)

plt.figure(figsize=(12, 5))
plt.title(&#x27;Distribution Contact Duration by Contact Type&#x27;)
plt.xlim(0,20)
sns.histplot(
    data=bank_df,
    x=&#x27;duration_minutes&#x27;,
    hue=&#x27;contact&#x27;,
    palette=&#x27;winter&#x27;,
    kde=True
)

plt.savefig(&#x27;assets/Scikit_Learn_68.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_68-34b57bce4f0248413512fceba6489188.webp" width="1029" height="470"></p>
<pre><code class="language-python">plt.figure(figsize=(16, 5))
plt.title(&#x27;Customer Jobs Countplot by Loan Defaults&#x27;)
sns.countplot(
    data=bank_df,
    x=&#x27;job&#x27;,
    order=bank_df[&#x27;job&#x27;].value_counts().index,
    palette=&#x27;winter&#x27;,
    hue=&#x27;default&#x27;
)

plt.savefig(&#x27;assets/Scikit_Learn_69.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_69-94229f2d311de30c68ec513b31528ff2.webp" width="1324" height="470"></p>
<pre><code class="language-python">plt.figure(figsize=(16, 5))
plt.title(&#x27;Customer Education Countplot by Loan Defaults&#x27;)
sns.countplot(
    data=bank_df,
    x=&#x27;education&#x27;,
    order=bank_df[&#x27;education&#x27;].value_counts().index,
    palette=&#x27;winter&#x27;,
    hue=&#x27;default&#x27;
)

plt.savefig(&#x27;assets/Scikit_Learn_70.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_70-b1480b4dae6e1c474ee7e75f6a07cc0e.webp" width="1333" height="470"></p>
<pre><code class="language-python">sns.pairplot(
    data=bank_df,
    hue=&#x27;marital&#x27;,
    palette=&#x27;winter&#x27;
)

plt.savefig(&#x27;assets/Scikit_Learn_71.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_71-ef93f4d5be8b58fea5fb8f060bd5e2d3.webp" width="3087" height="2945"></p>
<h3 id="dataset-preprocessing-1">Dataset Preprocessing</h3>
<pre><code class="language-python"># encode categorical features
X_bank = pd.get_dummies(bank_df)
</code></pre>
<pre><code class="language-python"># normalize data
bank_scaler = StandardScaler()

X_bank_scaled = bank_scaler.fit_transform(X_bank)
</code></pre>
<h3 id="model-training-3">Model Training</h3>
<pre><code class="language-python">bank_model = KMeans(
    n_clusters=2,
    n_init=&#x27;auto&#x27;,
    random_state=42
)
# fit to find cluster centers and predict what center every feature belongs to
bank_cluster_labels = bank_model.fit_predict(X_bank_scaled)
</code></pre>
<pre><code class="language-python"># add predicted label to source dataframe
X_bank[&#x27;Cluster&#x27;] = bank_cluster_labels
</code></pre>
<pre><code class="language-python">X_bank[&#x27;Cluster&#x27;].value_counts()
# 0    26871
# 1    14317
# Name: Cluster, dtype: int64
</code></pre>
<pre><code class="language-python"># How do the feature correlate with the predicted labels
label_corr = X_bank.corr()[&#x27;Cluster&#x27;]
print(label_corr.iloc[:-1].sort_values())
</code></pre>
<pre><code class="language-python">plt.figure(figsize=(10,14))
label_corr.iloc[:-1].sort_values().plot(kind=&#x27;barh&#x27;)
plt.title(&#x27;Feature Importance&#x27;)

plt.savefig(&#x27;assets/Scikit_Learn_72.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_72-250bdb43fb5be74a8d5f728f96cd9fbf.webp" width="1015" height="1144"></p>
<h3 id="choosing-a-k-value">Choosing a K Value</h3>
<pre><code class="language-python"># visualize the sum distance of your datapoints to the
# predicted cluster centers as a function of number of clusters
sum_squared_distance = []

for k in range(2,20):
    model = KMeans(n_clusters=k, n_init=&#x27;auto&#x27;)
    model.fit(X_bank_scaled)
    
    sum_squared_distance.append(model.inertia_)
</code></pre>
<pre><code class="language-python">plt.figure(figsize=(10,5))
plt.title(&#x27;SSD as a Function of Number of Cluster&#x27;)
plt.plot(range(2,20), sum_squared_distance, &#x27;o--&#x27;)

plt.savefig(&#x27;assets/Scikit_Learn_73.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_73-d572f1c35dfb9e8ab020d74ca3436c93.webp" width="826" height="451"></p>
<pre><code class="language-python">plt.figure(figsize=(10,5))
plt.title(&#x27;Difference in SSD as a Function of Number of Clusters&#x27;)
pd.Series(sum_squared_distance).diff().plot(kind=&#x27;bar&#x27;)

plt.savefig(&#x27;assets/Scikit_Learn_74.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<blockquote>
<p>There are two &#x27;elbows&#x27; - one between k=5-6 (behold the 0-index in Pandas!) and the second one between k=14-15. Both of them are potential good values for the number of cluster <code>k</code>.</p>
</blockquote>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_74-21c4edab99cdd0f135ee74c5db93588e.webp" width="868" height="454"></p>
<h4 id="re-fitting-the-model">Re-fitting the Model</h4>
<pre><code class="language-python">bank_model = KMeans(
    n_clusters=6,
    n_init=&#x27;auto&#x27;,
    random_state=42
)
# fit to find cluster centers and predict what center every feature belongs to
bank_cluster_labels = bank_model.fit_predict(X_bank_scaled)
</code></pre>
<pre><code class="language-python"># add predicted label to source dataframe
X_bank[&#x27;Cluster&#x27;] = bank_cluster_labels
X_bank[&#x27;Cluster&#x27;].value_counts()
# 5    10713
# 0    10663
# 1     8164
# 3     5566
# 4     3322
# 2     2760
# Name: Cluster, dtype: int64
</code></pre>
<h3 id="example-1--color-quantization">Example 1 : Color Quantization</h3>
<pre><code class="language-python">img_array = mpimg.imread(&#x27;assets/gz.jpg&#x27;)
img_array.shape
# (325, 640, 3)
</code></pre>
<pre><code class="language-python">plt.imshow(img_array)
plt.title(&#x27;Original Image&#x27;)
plt.savefig(&#x27;assets/Scikit_Learn_75.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_75-bb76627a3159fa58e6cbd0edbc8eec51.webp" width="552" height="317"></p>
<pre><code class="language-python"># flatten the image from 3 to 2 dimensions
(height, width, colour) = img_array.shape
img_array2d = img_array.reshape(height*width,colour)
img_array2d.shape
# (208000, 3)
</code></pre>
<pre><code class="language-python"># reduce colour space to 6 clusters
colour_model = KMeans(n_clusters=6, n_init=&#x27;auto&#x27;)
colour_labels = colour_model.fit_predict(img_array2d)
</code></pre>
<pre><code class="language-python"># get rgb value for each of the 6 cluster centers
rgb_colours = colour_model.cluster_centers_.round(0).astype(int)
rgb_colours
# array([[186, 111,  58],
#        [ 31,  11,  16],
#        [135,  72,  46],
#        [236, 157,  73],
#        [ 81,  40,  34],
#        [252, 199, 125]])
</code></pre>
<pre><code class="language-python"># assign these rgb values to each pixel within the cluster
# and reshape to original 3d array
quantized_image = np.reshape(rgb_colours[colour_labels],(height,width,colour))
</code></pre>
<pre><code class="language-python">plt.imshow(quantized_image)
plt.title(&#x27;Quantized Image&#x27;)
plt.savefig(&#x27;assets/Scikit_Learn_76.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_76-0d690dd39317a64d833427f664b325e6.webp" width="552" height="317"></p>
<h3 id="example-2--country-clustering">Example 2 : Country Clustering</h3>
<h4 id="dataset-exploration-3">Dataset Exploration</h4>
<pre><code class="language-python">!wget https://github.com/priyansh21112002/CIA-Country-Description/raw/main/CIA_Country_Facts.csv -P datasets
</code></pre>
<pre><code class="language-python">country_df = pd.read_csv(&#x27;datasets/CIA_Country_Facts.csv&#x27;)
country_df.head(5).transpose()
</code></pre>
<table><thead><tr><th></th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>Country</td><td>Afghanistan</td><td>Albania</td><td>Algeria</td><td>American Samoa</td><td>Andorra</td></tr><tr><td>Region</td><td>ASIA (EX. NEAR EAST)</td><td>EASTERN EUROPE</td><td>NORTHERN AFRICA</td><td>OCEANIA</td><td>WESTERN EUROPE</td></tr><tr><td>Population</td><td>31056997</td><td>3581655</td><td>32930091</td><td>57794</td><td>71201</td></tr><tr><td>Area (sq. mi.)</td><td>647500</td><td>28748</td><td>2381740</td><td>199</td><td>468</td></tr><tr><td>Pop. Density (per sq. mi.)</td><td>48.0</td><td>124.6</td><td>13.8</td><td>290.4</td><td>152.1</td></tr><tr><td>Coastline (coast/area ratio)</td><td>0.0</td><td>1.26</td><td>0.04</td><td>58.29</td><td>0.0</td></tr><tr><td>Net migration</td><td>23.06</td><td>-4.93</td><td>-0.39</td><td>-20.71</td><td>6.6</td></tr><tr><td>Infant mortality (per 1000 births)</td><td>163.07</td><td>21.52</td><td>31.0</td><td>9.27</td><td>4.05</td></tr><tr><td>GDP ($ per capita)</td><td>700.0</td><td>4500.0</td><td>6000.0</td><td>8000.0</td><td>19000.0</td></tr><tr><td>Literacy (%)</td><td>36.0</td><td>86.5</td><td>70.0</td><td>97.0</td><td>100.0</td></tr><tr><td>Phones (per 1000)</td><td>3.2</td><td>71.2</td><td>78.1</td><td>259.5</td><td>497.2</td></tr><tr><td>Arable (%)</td><td>12.13</td><td>21.09</td><td>3.22</td><td>10.0</td><td>2.22</td></tr><tr><td>Crops (%)</td><td>0.22</td><td>4.42</td><td>0.25</td><td>15.0</td><td>0.0</td></tr><tr><td>Other (%)</td><td>87.65</td><td>74.49</td><td>96.53</td><td>75.0</td><td>97.78</td></tr><tr><td>Climate</td><td>1.0</td><td>3.0</td><td>1.0</td><td>2.0</td><td>3.0</td></tr><tr><td>Birthrate</td><td>46.6</td><td>15.11</td><td>17.14</td><td>22.46</td><td>8.71</td></tr><tr><td>Deathrate</td><td>20.34</td><td>5.22</td><td>4.61</td><td>3.27</td><td>6.25</td></tr><tr><td>Agriculture</td><td>0.38</td><td>0.232</td><td>0.101</td><td>NaN</td><td>NaN</td></tr><tr><td>Industry</td><td>0.24</td><td>0.188</td><td>0.6</td><td>NaN</td><td>NaN</td></tr><tr><td>Service</td><td>0.38</td><td>0.579</td><td>0.298</td><td>NaN</td><td>NaN</td></tr></tbody></table>
<pre><code class="language-python">fig, axes = plt.subplots(figsize=(10,5), nrows=1, ncols=2)
plt.suptitle(&#x27;Country Population Histogram&#x27;)

axes[0].set_xlabel(&#x27;Population&#x27;)
axes[0].set_ylabel(&#x27;Frequency&#x27;)

axes[0].hist(
    x=country_df[&#x27;Population&#x27;],
    range=None,
    density=True,
    histtype=&#x27;bar&#x27;,
    orientation=&#x27;vertical&#x27;,
    color=&#x27;dodgerblue&#x27;
)

axes[1].set_xlabel(&#x27;Population (&lt;100Mio)&#x27;)
axes[1].set_ylabel(&#x27;Frequency&#x27;)

axes[1].hist(
    x=country_df[&#x27;Population&#x27;],
    range=[0, 1e8],
    density=True,
    histtype=&#x27;bar&#x27;,
    orientation=&#x27;vertical&#x27;,
    color=&#x27;fuchsia&#x27;
)

plt.savefig(&#x27;assets/Scikit_Learn_77.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_77-44efec5ef2eb2c5e06083af26ac15d35.webp" width="833" height="498"></p>
<pre><code class="language-python">plt.figure(figsize=(12, 5))
plt.title(&#x27;GDP ($ per capita) by Region&#x27;)

sns.barplot(
    data=country_df,
    y=&#x27;Region&#x27;,
    x=&#x27;GDP ($ per capita)&#x27;,
    estimator=np.mean,
    errorbar=&#x27;sd&#x27;,
    orient=&#x27;h&#x27;,
    palette=&#x27;cool&#x27;
)

plt.savefig(&#x27;assets/Scikit_Learn_78.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_78-a06a20b037322ab15fcb77301aa1c8d0.webp" width="1212" height="470"></p>
<pre><code class="language-python">plt.figure(figsize=(10, 6))

sns.scatterplot(
    y=&#x27;Phones (per 1000)&#x27;,
    x=&#x27;GDP ($ per capita)&#x27;,
    data=country_df,
    hue=&#x27;Region&#x27;,
    palette=&#x27;cool&#x27;,
).set_title(&#x27;GDP ($ per capita) vs. Phones (per 1000)&#x27;)

plt.savefig(&#x27;assets/Scikit_Learn_79.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_79-a78f28cc6deb6e72924a86238ed2ff9a.webp" width="859" height="547"></p>
<pre><code class="language-python">plt.figure(figsize=(10, 6))

sns.scatterplot(
    y=&#x27;Literacy (%)&#x27;,
    x=&#x27;GDP ($ per capita)&#x27;,
    data=country_df,
    hue=&#x27;Region&#x27;,
    palette=&#x27;cool&#x27;,
).set_title(&#x27;GDP ($ per capita) vs. Literacy (%)&#x27;)

plt.savefig(&#x27;assets/Scikit_Learn_80.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_80-3f1b7ce7e47f63ada2ea6b9e187b3d3b.webp" width="850" height="547"></p>
<pre><code class="language-python">plt.figure(figsize=(20, 12), dpi=200)
plt.title(&#x27;Correlation Heatmap CIA Country Dataset&#x27;)

sns.heatmap(
    country_df.corr(numeric_only=True),
    linewidth=0.5,
    cmap=&#x27;seismic&#x27;,
    annot=True
)

plt.savefig(&#x27;assets/Scikit_Learn_81.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_81-f87342f6a41510e66166f6bf82a7c8ba.webp" width="3352" height="2411"></p>
<pre><code class="language-python">plt.figure(figsize=(20, 12), dpi=200)
sns.clustermap(
    country_df.corr(numeric_only=True),
    linewidth=0.5,
    cmap=&#x27;seismic&#x27;,
    annot=False,
    col_cluster=False
)

plt.savefig(&#x27;assets/Scikit_Learn_82.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_82-f613916ff0cf1a37dec1b775cad1afd1.webp" width="989" height="990"></p>
<h4 id="dataset-preprocessing-2">Dataset Preprocessing</h4>
<pre><code class="language-python"># find columns with missing values
country_df.isnull().sum()
</code></pre>
<table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Country</td><td>0</td></tr><tr><td>Region</td><td>0</td></tr><tr><td>Population</td><td>0</td></tr><tr><td>Area (sq. mi.)</td><td>0</td></tr><tr><td>Pop. Density (per sq. mi.)</td><td>0</td></tr><tr><td>Coastline (coast/area ratio)</td><td>0</td></tr><tr><td>Net migration</td><td>3</td></tr><tr><td>Infant mortality (per 1000 births)</td><td>3</td></tr><tr><td>GDP ($ per capita)</td><td>1</td></tr><tr><td>Literacy (%)</td><td>18</td></tr><tr><td>Phones (per 1000)</td><td>4</td></tr><tr><td>Arable (%)</td><td>2</td></tr><tr><td>Crops (%)</td><td>2</td></tr><tr><td>Other (%)</td><td>2</td></tr><tr><td>Climate</td><td>22</td></tr><tr><td>Birthrate</td><td>3</td></tr><tr><td>Deathrate</td><td>4</td></tr><tr><td>Agriculture</td><td>15</td></tr><tr><td>Industry</td><td>16</td></tr><tr><td>Service</td><td>15</td></tr><tr><td><em>dtype: int64</em></td><td></td></tr></tbody></table>
<pre><code class="language-python"># what countries don&#x27;t have an agriculture value
country_df[pd.isnull(country_df[&#x27;Agriculture&#x27;])][&#x27;Country&#x27;]
# all countries without agriculture data will not have a
# whole lot of agriculture output. The same is true for &#x27;Industry&#x27;
# and &#x27;Service&#x27; These values can be set to zero:
</code></pre>
<table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>3</td><td>American Samoa</td></tr><tr><td>4</td><td>Andorra</td></tr><tr><td>78</td><td>Gibraltar</td></tr><tr><td>80</td><td>Greenland</td></tr><tr><td>83</td><td>Guam</td></tr><tr><td>134</td><td>Mayotte</td></tr><tr><td>140</td><td>Montserrat</td></tr><tr><td>144</td><td>Nauru</td></tr><tr><td>153</td><td>N. Mariana Islands</td></tr><tr><td>171</td><td>Saint Helena</td></tr><tr><td>174</td><td>St Pierre &amp; Miquelon</td></tr><tr><td>177</td><td>San Marino</td></tr><tr><td>208</td><td>Turks &amp; Caicos Is</td></tr><tr><td>221</td><td>Wallis and Futuna</td></tr><tr><td>223</td><td>Western Sahara</td></tr><tr><td><em>Name: Country, dtype: object</em></td><td></td></tr></tbody></table>
<pre><code class="language-python"># set missing values to zero for Agriculture, Industry and Service
# define what default values you want to fill
values = \{
    &quot;Agriculture&quot;: 0,
    &quot;Industry&quot;: 0,
    &quot;Service&quot;: 0,
\}
# and replace missing with values
country_df = country_df.fillna(value=values)
</code></pre>
<pre><code class="language-python"># another datapoint that is often missing is climate
# the climate can be estimated by countries in the same Region
country_df[pd.isnull(country_df[&#x27;Climate&#x27;])][[&#x27;Country&#x27;, &#x27;Region&#x27;, &#x27;Climate&#x27;]]
</code></pre>
<table><thead><tr><th></th><th>Country</th><th>Region</th><th>Climate</th></tr></thead><tbody><tr><td>5</td><td>Angola</td><td>SUB-SAHARAN AFRICA</td><td>NaN</td></tr><tr><td>36</td><td>Canada</td><td>NORTHERN AMERICA</td><td>NaN</td></tr><tr><td>50</td><td>Croatia</td><td>EASTERN EUROPE</td><td>NaN</td></tr><tr><td>66</td><td>Faroe Islands</td><td>WESTERN EUROPE</td><td>NaN</td></tr><tr><td>78</td><td>Gibraltar</td><td>WESTERN EUROPE</td><td>NaN</td></tr><tr><td>101</td><td>Italy</td><td>WESTERN EUROPE</td><td>NaN</td></tr><tr><td>115</td><td>Lebanon</td><td>NEAR EAST</td><td>NaN</td></tr><tr><td>118</td><td>Libya</td><td>NORTHERN AFRICA</td><td>NaN</td></tr><tr><td>120</td><td>Lithuania</td><td>BALTICS</td><td>NaN</td></tr><tr><td>121</td><td>Luxembourg</td><td>WESTERN EUROPE</td><td>NaN</td></tr><tr><td>129</td><td>Malta</td><td>WESTERN EUROPE</td><td>NaN</td></tr><tr><td>137</td><td>Moldova</td><td>C.W. OF IND. STATES</td><td>NaN</td></tr><tr><td>138</td><td>Monaco</td><td>WESTERN EUROPE</td><td>NaN</td></tr><tr><td>141</td><td>Morocco</td><td>NORTHERN AFRICA</td><td>NaN</td></tr><tr><td>145</td><td>Nepal</td><td>ASIA (EX. NEAR EAST)</td><td>NaN</td></tr><tr><td>169</td><td>Russia</td><td>C.W. OF IND. STATES</td><td>NaN</td></tr><tr><td>171</td><td>Saint Helena</td><td>SUB-SAHARAN AFRICA</td><td>NaN</td></tr><tr><td>174</td><td>St Pierre &amp; Miquelon</td><td>NORTHERN AMERICA</td><td>NaN</td></tr><tr><td>177</td><td>San Marino</td><td>WESTERN EUROPE</td><td>NaN</td></tr><tr><td>181</td><td>Serbia</td><td>EASTERN EUROPE</td><td>NaN</td></tr><tr><td>186</td><td>Slovenia</td><td>EASTERN EUROPE</td><td>NaN</td></tr><tr><td>200</td><td>Tanzania</td><td>SUB-SAHARAN AFRICA</td><td>NaN</td></tr></tbody></table>
<pre><code class="language-python">country_df[pd.isnull(country_df[&#x27;Climate&#x27;])][&#x27;Region&#x27;].value_counts()
</code></pre>
<table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>WESTERN EUROPE</td><td>7</td></tr><tr><td>SUB-SAHARAN AFRICA</td><td>3</td></tr><tr><td>EASTERN EUROPE</td><td>3</td></tr><tr><td>NORTHERN AMERICA</td><td>2</td></tr><tr><td>NORTHERN AFRICA</td><td>2</td></tr><tr><td>C.W. OF IND. STATES</td><td>2</td></tr><tr><td>NEAR EAST</td><td>1</td></tr><tr><td>BALTICS</td><td>1</td></tr><tr><td>ASIA (EX. NEAR EAST)</td><td>1</td></tr><tr><td><em>Name: Region, dtype: int64</em></td><td></td></tr></tbody></table>
<pre><code class="language-python"># the Region value has annoying whitespaces that need to be stripped
country_df[&#x27;Region&#x27;] = country_df[&#x27;Region&#x27;].apply(lambda x: x.strip())
</code></pre>
<pre><code class="language-python"># climate zones in western europe
country_df[country_df[&#x27;Region&#x27;] == &#x27;WESTERN EUROPE&#x27;][&#x27;Climate&#x27;].value_counts()
</code></pre>
<pre><code class="language-python"># climate zones in SUB-SAHARAN AFRICA
country_df[country_df[&#x27;Region&#x27;] == &#x27;SUB-SAHARAN AFRICA&#x27;][&#x27;Climate&#x27;].value_counts()
</code></pre>
<pre><code class="language-python"># climate zones in EASTERN EUROPE
country_df[country_df[&#x27;Region&#x27;] == &#x27;EASTERN EUROPE&#x27;][&#x27;Climate&#x27;].value_counts()
</code></pre>
<pre><code class="language-python"># climate zones in NORTHERN AMERICA
country_df[country_df[&#x27;Region&#x27;] == &#x27;NORTHERN AMERICA&#x27;][&#x27;Climate&#x27;].value_counts()
</code></pre>
<pre><code class="language-python"># climate zones in NORTHERN AFRICA
country_df[country_df[&#x27;Region&#x27;] == &#x27;NORTHERN AFRICA&#x27;][&#x27;Climate&#x27;].value_counts()
</code></pre>
<pre><code class="language-python"># climate zones in C.W. OF IND. STATES
country_df[country_df[&#x27;Region&#x27;] == &#x27;C.W. OF IND. STATES&#x27;][&#x27;Climate&#x27;].value_counts()
</code></pre>
<pre><code class="language-python"># climate zones in NEAR EAST
country_df[country_df[&#x27;Region&#x27;] == &#x27;NEAR EAST&#x27;][&#x27;Climate&#x27;].value_counts()
</code></pre>
<pre><code class="language-python"># climate zones in BALTICS
country_df[country_df[&#x27;Region&#x27;] == &#x27;BALTICS&#x27;][&#x27;Climate&#x27;].value_counts()
</code></pre>
<pre><code class="language-python"># climate zones in ASIA (EX. NEAR EAST)
country_df[country_df[&#x27;Region&#x27;] == &#x27;ASIA (EX. NEAR EAST)&#x27;][&#x27;Climate&#x27;].value_counts()
</code></pre>
<pre><code class="language-python"># we can either use the top value to fill missing climate data points
# or use a mean value:
country_df[&#x27;Climate&#x27;] = country_df[&#x27;Climate&#x27;].fillna(country_df.groupby(&#x27;Region&#x27;)[&#x27;Climate&#x27;].transform(&#x27;mean&#x27;))
</code></pre>
<pre><code class="language-python"># there are more missing values, e.g. literacy:
country_df[pd.isnull(country_df[&#x27;Literacy (%)&#x27;])][[&#x27;Country&#x27;, &#x27;Region&#x27;, &#x27;Literacy (%)&#x27;]]
</code></pre>
<table><thead><tr><th></th><th>Country</th><th>Region</th><th>Literacy (%)</th></tr></thead><tbody><tr><td>25</td><td>Bosnia &amp; Herzegovina</td><td>EASTERN EUROPE</td><td>NaN</td></tr><tr><td>66</td><td>Faroe Islands</td><td>WESTERN EUROPE</td><td>NaN</td></tr><tr><td>74</td><td>Gaza Strip</td><td>NEAR EAST</td><td>NaN</td></tr><tr><td>78</td><td>Gibraltar</td><td>WESTERN EUROPE</td><td>NaN</td></tr><tr><td>80</td><td>Greenland</td><td>NORTHERN AMERICA</td><td>NaN</td></tr><tr><td>85</td><td>Guernsey</td><td>WESTERN EUROPE</td><td>NaN</td></tr><tr><td>99</td><td>Isle of Man</td><td>WESTERN EUROPE</td><td>NaN</td></tr><tr><td>104</td><td>Jersey</td><td>WESTERN EUROPE</td><td>NaN</td></tr><tr><td>108</td><td>Kiribati</td><td>OCEANIA</td><td>NaN</td></tr><tr><td>123</td><td>Macedonia</td><td>EASTERN EUROPE</td><td>NaN</td></tr><tr><td>134</td><td>Mayotte</td><td>SUB-SAHARAN AFRICA</td><td>NaN</td></tr><tr><td>144</td><td>Nauru</td><td>OCEANIA</td><td>NaN</td></tr><tr><td>185</td><td>Slovakia</td><td>EASTERN EUROPE</td><td>NaN</td></tr><tr><td>187</td><td>Solomon Islands</td><td>OCEANIA</td><td>NaN</td></tr><tr><td>209</td><td>Tuvalu</td><td>OCEANIA</td><td>NaN</td></tr><tr><td>220</td><td>Virgin Islands</td><td>LATIN AMER. &amp; CARIB</td><td>NaN</td></tr><tr><td>222</td><td>West Bank</td><td>NEAR EAST</td><td>NaN</td></tr><tr><td>223</td><td>Western Sahara</td><td>NORTHERN AFRICA</td><td>NaN</td></tr></tbody></table>
<pre><code class="language-python"># here we can also fill with mean values:
country_df[&#x27;Literacy (%)&#x27;] = country_df[&#x27;Literacy (%)&#x27;].fillna(country_df.groupby(&#x27;Region&#x27;)[&#x27;Literacy (%)&#x27;].transform(&#x27;mean&#x27;))
</code></pre>
<pre><code class="language-python"># the remaining rows with missing values can be dropped for now
country_df = country_df.dropna(axis=0)
country_df.isnull().sum()
</code></pre>
<table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Country</td><td>0</td></tr><tr><td>Region</td><td>0</td></tr><tr><td>Population</td><td>0</td></tr><tr><td>Area (sq. mi.)</td><td>0</td></tr><tr><td>Pop. Density (per sq. mi.)</td><td>0</td></tr><tr><td>Coastline (coast/area ratio)</td><td>0</td></tr><tr><td>Net migration</td><td>0</td></tr><tr><td>Infant mortality (per 1000 births)</td><td>0</td></tr><tr><td>GDP ($ per capita)</td><td>0</td></tr><tr><td>Literacy (%)</td><td>0</td></tr><tr><td>Phones (per 1000)</td><td>0</td></tr><tr><td>Arable (%)</td><td>0</td></tr><tr><td>Crops (%)</td><td>0</td></tr><tr><td>Other (%)</td><td>0</td></tr><tr><td>Climate</td><td>0</td></tr><tr><td>Birthrate</td><td>0</td></tr><tr><td>Deathrate</td><td>0</td></tr><tr><td>Agriculture</td><td>0</td></tr><tr><td>Industry</td><td>0</td></tr><tr><td>Service</td><td>0</td></tr><tr><td><em>dtype: int64</em></td><td></td></tr></tbody></table>
<pre><code class="language-python"># drop the country column as it is a unique
# classifier that will not help with clustering
country_df_dropped = country_df.drop([&#x27;Country&#x27;], axis=1)
</code></pre>
<pre><code class="language-python"># the region column is useful but needs to be encoded
country_df_dropped = pd.get_dummies(country_df_dropped)
</code></pre>
<pre><code class="language-python">country_df_dropped.head(5).transpose()
</code></pre>
<table><thead><tr><th></th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>Population</td><td>31056997.00</td><td>3581655.000</td><td>3.293009e+07</td><td>57794.00</td><td>71201.00</td></tr><tr><td>Area (sq. mi.)</td><td>647500.00</td><td>28748.000</td><td>2.381740e+06</td><td>199.00</td><td>468.00</td></tr><tr><td>Pop. Density (per sq. mi.)</td><td>48.00</td><td>124.600</td><td>1.380000e+01</td><td>290.40</td><td>152.10</td></tr><tr><td>Coastline (coast/area ratio)</td><td>0.00</td><td>1.260</td><td>4.000000e-02</td><td>58.29</td><td>0.00</td></tr><tr><td>Net migration</td><td>23.06</td><td>-4.930</td><td>-3.900000e-01</td><td>-20.71</td><td>6.60</td></tr><tr><td>Infant mortality (per 1000 births)</td><td>163.07</td><td>21.520</td><td>3.100000e+01</td><td>9.27</td><td>4.05</td></tr><tr><td>GDP ($ per capita)</td><td>700.00</td><td>4500.000</td><td>6.000000e+03</td><td>8000.00</td><td>19000.00</td></tr><tr><td>Literacy (%)</td><td>36.00</td><td>86.500</td><td>7.000000e+01</td><td>97.00</td><td>100.00</td></tr><tr><td>Phones (per 1000)</td><td>3.20</td><td>71.200</td><td>7.810000e+01</td><td>259.50</td><td>497.20</td></tr><tr><td>Arable (%)</td><td>12.13</td><td>21.090</td><td>3.220000e+00</td><td>10.00</td><td>2.22</td></tr><tr><td>Crops (%)</td><td>0.22</td><td>4.420</td><td>2.500000e-01</td><td>15.00</td><td>0.00</td></tr><tr><td>Other (%)</td><td>87.65</td><td>74.490</td><td>9.653000e+01</td><td>75.00</td><td>97.78</td></tr><tr><td>Climate</td><td>1.00</td><td>3.000</td><td>1.000000e+00</td><td>2.00</td><td>3.00</td></tr><tr><td>Birthrate</td><td>46.60</td><td>15.110</td><td>1.714000e+01</td><td>22.46</td><td>8.71</td></tr><tr><td>Deathrate</td><td>20.34</td><td>5.220</td><td>4.610000e+00</td><td>3.27</td><td>6.25</td></tr><tr><td>Agriculture</td><td>0.38</td><td>0.232</td><td>1.010000e-01</td><td>0.00</td><td>0.00</td></tr><tr><td>Industry</td><td>0.24</td><td>0.188</td><td>6.000000e-01</td><td>0.00</td><td>0.00</td></tr><tr><td>Service</td><td>0.38</td><td>0.579</td><td>2.980000e-01</td><td>0.00</td><td>0.00</td></tr><tr><td>Region_ASIA (EX. NEAR EAST)</td><td>1.00</td><td>0.000</td><td>0.000000e+00</td><td>0.00</td><td>0.00</td></tr><tr><td>Region_BALTICS</td><td>0.00</td><td>0.000</td><td>0.000000e+00</td><td>0.00</td><td>0.00</td></tr><tr><td>Region_C.W. OF IND. STATES</td><td>0.00</td><td>0.000</td><td>0.000000e+00</td><td>0.00</td><td>0.00</td></tr><tr><td>Region_EASTERN EUROPE</td><td>0.00</td><td>1.000</td><td>0.000000e+00</td><td>0.00</td><td>0.00</td></tr><tr><td>Region_LATIN AMER. &amp; CARIB</td><td>0.00</td><td>0.000</td><td>0.000000e+00</td><td>0.00</td><td>0.00</td></tr><tr><td>Region_NEAR EAST</td><td>0.00</td><td>0.000</td><td>0.000000e+00</td><td>0.00</td><td>0.00</td></tr><tr><td>Region_NORTHERN AFRICA</td><td>0.00</td><td>0.000</td><td>1.000000e+00</td><td>0.00</td><td>0.00</td></tr><tr><td>Region_NORTHERN AMERICA</td><td>0.00</td><td>0.000</td><td>0.000000e+00</td><td>0.00</td><td>0.00</td></tr><tr><td>Region_OCEANIA</td><td>0.00</td><td>0.000</td><td>0.000000e+00</td><td>1.00</td><td>0.00</td></tr><tr><td>Region_SUB-SAHARAN AFRICA</td><td>0.00</td><td>0.000</td><td>0.000000e+00</td><td>0.00</td><td>0.00</td></tr><tr><td>Region_WESTERN EUROPE</td><td>0.00</td><td>0.000</td><td>0.000000e+00</td><td>0.00</td><td>1.00</td></tr></tbody></table>
<pre><code class="language-python"># to be able to compare all datapoints they need to be normalized
country_scaler = StandardScaler()
country_df_scaled = country_scaler.fit_transform(country_df_dropped)
</code></pre>
<h4 id="model-training-4">Model Training</h4>
<pre><code class="language-python"># finding a good k-value for number of cluster
ssd_country = []

for k in range(2,30):
    model = KMeans(n_clusters=k, n_init=&#x27;auto&#x27;)
    model.fit(country_df_scaled)
    
    ssd_country.append(model.inertia_)
</code></pre>
<pre><code class="language-python">plt.figure(figsize=(10,5))
plt.title(&#x27;SSD as a Function of Number of Cluster&#x27;)
plt.plot(range(2,30), ssd_country, &#x27;o--&#x27;)

plt.savefig(&#x27;assets/Scikit_Learn_83.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="data:image/webp;base64,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" width="839" height="451"></p>
<pre><code class="language-python">plt.figure(figsize=(10,5))
plt.title(&#x27;Difference in SSD as a Function of Number of Clusters&#x27;)
pd.Series(ssd_country).diff().plot(kind=&#x27;bar&#x27;)

plt.savefig(&#x27;assets/Scikit_Learn_84.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_84-589150d506920a7c10094e681a41d3d0.webp" width="842" height="454"></p>
<pre><code class="language-python">country_model = KMeans(
    n_clusters=14,
    n_init=&#x27;auto&#x27;,
    random_state=42
)
# fit to find cluster centers and predict what center every feature belongs to
country_cluster_labels = country_model.fit_predict(country_df_scaled)
</code></pre>
<h4 id="model-evaluation-7">Model Evaluation</h4>
<pre><code class="language-python"># add predicted label to source dataframe
country_df[&#x27;Cluster14&#x27;] = country_cluster_labels
country_df[&#x27;Cluster14&#x27;].value_counts()
</code></pre>
<pre><code class="language-python">plt.figure(figsize=(10, 7))
sns.set(style=&#x27;darkgrid&#x27;)

# hue/style by categorical column
sns.scatterplot(
    x=&#x27;GDP ($ per capita)&#x27;,
    y=&#x27;Literacy (%)&#x27;,
    data=country_df,
    s=40,
    alpha=0.6,
    hue=&#x27;Cluster14&#x27;,
    palette=&#x27;cool&#x27;,
    style=&#x27;Region&#x27;
).set_title(&#x27;Country Clusters with k=14&#x27;)

plt.savefig(&#x27;assets/Scikit_Learn_85.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_85-ee55b60d195e255bc16643ee6de0c5a8.webp" width="860" height="633"></p>
<pre><code class="language-python"># repeat but only with 3 cluster
country_model2 = KMeans(
    n_clusters=3,
    n_init=&#x27;auto&#x27;,
    random_state=42
)
# fit to find cluster centers and predict what center every feature belongs to
country_cluster_labels2 = country_model2.fit_predict(country_df_scaled)

# add predicted label to source dataframe
country_df[&#x27;Cluster3&#x27;] = country_cluster_labels2

plt.figure(figsize=(10, 7))
sns.set(style=&#x27;darkgrid&#x27;)

# hue/style by categorical column
sns.scatterplot(
    x=&#x27;GDP ($ per capita)&#x27;,
    y=&#x27;Literacy (%)&#x27;,
    data=country_df,
    s=40,
    alpha=0.6,
    hue=&#x27;Cluster3&#x27;,
    palette=&#x27;cool&#x27;,
    style=&#x27;Region&#x27;
).set_title(&#x27;Country Clusters with k=3&#x27;)

plt.savefig(&#x27;assets/Scikit_Learn_86.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_86-6ba3753619b9a4feb2aa7ffdf306db32.webp" width="860" height="633"></p>
<pre><code class="language-python"># How do the feature correlate with the predicted labels
country_label_corr = country_df.corr()[&#x27;Cluster3&#x27;]
print(country_label_corr.iloc[:-1].sort_values())
</code></pre>
<p><strong>Feature Correlation</strong></p>
<table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Literacy (%)</td><td>-0.413704</td></tr><tr><td>Crops (%)</td><td>-0.152936</td></tr><tr><td>Coastline (coast/area ratio)</td><td>-0.132610</td></tr><tr><td>Service</td><td>-0.070495</td></tr><tr><td>Area (sq. mi.)</td><td>-0.062183</td></tr><tr><td>Phones (per 1000)</td><td>-0.037538</td></tr><tr><td>Population</td><td>-0.024969</td></tr><tr><td>Industry</td><td>0.008487</td></tr><tr><td>Arable (%)</td><td>0.034891</td></tr><tr><td>Climate</td><td>0.049659</td></tr><tr><td>Other (%)</td><td>0.050444</td></tr><tr><td>Pop. Density (per sq. mi.)</td><td>0.101062</td></tr><tr><td>GDP ($ per capita)</td><td>0.122206</td></tr><tr><td>Agriculture</td><td>0.250750</td></tr><tr><td>Net migration</td><td>0.316226</td></tr><tr><td>Birthrate</td><td>0.369940</td></tr><tr><td>Infant mortality (per 1000 births)</td><td>0.412365</td></tr><tr><td>Deathrate</td><td>0.575814</td></tr><tr><td><em>Name: Cluster, dtype: float64</em></td><td></td></tr></tbody></table>
<pre><code class="language-python">plt.figure(figsize=(10,6))
country_label_corr.iloc[:-1].sort_values().plot(kind=&#x27;barh&#x27;)
plt.title(&#x27;Feature Importance&#x27;)

plt.savefig(&#x27;assets/Scikit_Learn_87.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_87-896d467b240f6fa5e432cdc2ad87c71f.webp" width="1061" height="532"></p>
<h4 id="plotly-choropleth-map">Plotly Choropleth Map</h4>
<pre><code class="language-python">iso_codes = pd.read_csv(&#x27;datasets/country-iso-codes.csv&#x27;)
iso_map = iso_codes.set_index(&#x27;Country&#x27;)[&#x27;ISO Code&#x27;].to_dict()
</code></pre>
<pre><code class="language-python">country_df[&#x27;ISO Code&#x27;] = country_df[&#x27;Country&#x27;].map(iso_map)
country_df[[&#x27;Country&#x27;,&#x27;ISO Code&#x27;]].head(5)
</code></pre>
<table><thead><tr><th></th><th>Country</th><th>ISO Code</th></tr></thead><tbody><tr><td>0</td><td>Afghanistan</td><td>AFG</td></tr><tr><td>1</td><td>Albania</td><td>ALB</td></tr><tr><td>2</td><td>Algeria</td><td>DZA</td></tr><tr><td>3</td><td>American Samoa</td><td>ASM</td></tr><tr><td>4</td><td>Andorra</td><td>AND</td></tr></tbody></table>
<pre><code class="language-python">fig = px.choropleth(
    country_df,
    locations=&#x27;ISO Code&#x27;,
    color=&#x27;Cluster3&#x27;,
    hover_name=&#x27;Country&#x27;,
    color_continuous_scale=px.colors.sequential.Plasma
)

fig.show()
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_88-b28bf6243610951239209b43e5fe0e60.webp" width="859" height="525"></p>
<pre><code class="language-python">fig = px.choropleth(
    country_df,
    locations=&#x27;ISO Code&#x27;,
    color=&#x27;Cluster14&#x27;,
    hover_name=&#x27;Country&#x27;,
    color_continuous_scale=px.colors.sequential.Plasma
)

fig.show()
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_89-50296fe8f839a3fbb9865e21b26a5fcc.webp" width="859" height="525"></p>
<h2 id="unsupervised-learning---agglomerative-clustering">Unsupervised Learning - Agglomerative Clustering</h2>
<h3 id="dataset-preprocessing-3">Dataset Preprocessing</h3>
<blockquote>
<p><a href="https://archive.ics.uci.edu/dataset/9/auto+mpg">autompg_data: The Auto-MPG dataset for regression</a>
Revised from CMU StatLib library, data concerns city-cycle fuel consumption</p>
</blockquote>
<pre><code class="language-python">autoMPG_df = pd.read_csv(&#x27;datasets/auto-mpg.csv&#x27;)
autoMPG_df.head(5)
</code></pre>
<table><thead><tr><th></th><th>mpg</th><th>cylinders</th><th>displacement</th><th>horsepower</th><th>weight</th><th>acceleration</th><th>model_year</th><th>origin</th><th>name</th></tr></thead><tbody><tr><td>0</td><td>18.0</td><td>8</td><td>307.0</td><td>130.0</td><td>3504</td><td>12.0</td><td>70</td><td>usa</td><td>chevrolet chevelle malibu</td></tr><tr><td>1</td><td>15.0</td><td>8</td><td>350.0</td><td>165.0</td><td>3693</td><td>11.5</td><td>70</td><td>usa</td><td>buick skylark 320</td></tr><tr><td>2</td><td>18.0</td><td>8</td><td>318.0</td><td>150.0</td><td>3436</td><td>11.0</td><td>70</td><td>usa</td><td>plymouth satellite</td></tr><tr><td>3</td><td>16.0</td><td>8</td><td>304.0</td><td>150.0</td><td>3433</td><td>12.0</td><td>70</td><td>usa</td><td>amc rebel sst</td></tr><tr><td>4</td><td>17.0</td><td>8</td><td>302.0</td><td>140.0</td><td>3449</td><td>10.5</td><td>70</td><td>usa</td><td>ford torino</td></tr></tbody></table>
<pre><code class="language-python">autoMPG_df[&#x27;origin&#x27;].value_counts()
# there are only 3 countries of origin - can be turned into a dummy variable
</code></pre>
<pre><code class="language-python">autoMPG_dummy_df = pd.get_dummies(autoMPG_df.drop(&#x27;name&#x27;, axis=1))
autoMPG_dummy_df.head(5)
</code></pre>
<table><thead><tr><th></th><th>mpg</th><th>cylinders</th><th>displacement</th><th>horsepower</th><th>weight</th><th>acceleration</th><th>model_year</th><th>origin_europe</th><th>origin_japan</th><th>origin_usa</th></tr></thead><tbody><tr><td>0</td><td>18.0</td><td>8</td><td>307.0</td><td>130.0</td><td>3504</td><td>12.0</td><td>70</td><td>False</td><td>False</td><td>True</td></tr><tr><td>1</td><td>15.0</td><td>8</td><td>350.0</td><td>165.0</td><td>3693</td><td>11.5</td><td>70</td><td>False</td><td>False</td><td>True</td></tr><tr><td>2</td><td>18.0</td><td>8</td><td>318.0</td><td>150.0</td><td>3436</td><td>11.0</td><td>70</td><td>False</td><td>False</td><td>True</td></tr><tr><td>3</td><td>16.0</td><td>8</td><td>304.0</td><td>150.0</td><td>3433</td><td>12.0</td><td>70</td><td>False</td><td>False</td><td>True</td></tr><tr><td>4</td><td>17.0</td><td>8</td><td>302.0</td><td>140.0</td><td>3449</td><td>10.5</td><td>70</td><td>False</td><td>False</td><td>True</td></tr></tbody></table>
<pre><code class="language-python"># normalize dataset
scaler = MinMaxScaler()
autoMPG_scaled = pd.DataFrame(
    scaler.fit_transform(autoMPG_dummy_df), columns=autoMPG_dummy_df.columns
)
autoMPG_scaled.describe()
</code></pre>
<table><thead><tr><th></th><th>mpg</th><th>cylinders</th><th>displacement</th><th>horsepower</th><th>weight</th><th>acceleration</th><th>model_year</th><th>origin_europe</th><th>origin_japan</th><th>origin_usa</th></tr></thead><tbody><tr><td>count</td><td>392.000000</td><td>392.000000</td><td>392.000000</td><td>392.000000</td><td>392.000000</td><td>392.000000</td><td>392.000000</td><td>392.000000</td><td>392.000000</td><td>392.000000</td></tr><tr><td>mean</td><td>0.384200</td><td>0.494388</td><td>0.326646</td><td>0.317768</td><td>0.386897</td><td>0.448888</td><td>0.498299</td><td>0.173469</td><td>0.201531</td><td>0.625000</td></tr><tr><td>std</td><td>0.207580</td><td>0.341157</td><td>0.270398</td><td>0.209191</td><td>0.240829</td><td>0.164218</td><td>0.306978</td><td>0.379136</td><td>0.401656</td><td>0.484742</td></tr><tr><td>min</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.000000</td></tr><tr><td>25%</td><td>0.212766</td><td>0.200000</td><td>0.095607</td><td>0.157609</td><td>0.173589</td><td>0.343750</td><td>0.250000</td><td>0.000000</td><td>0.000000</td><td>0.000000</td></tr><tr><td>50%</td><td>0.365691</td><td>0.200000</td><td>0.214470</td><td>0.258152</td><td>0.337539</td><td>0.446429</td><td>0.500000</td><td>0.000000</td><td>0.000000</td><td>1.000000</td></tr><tr><td>75%</td><td>0.531915</td><td>1.000000</td><td>0.536822</td><td>0.434783</td><td>0.567550</td><td>0.537202</td><td>0.750000</td><td>0.000000</td><td>0.000000</td><td>1.000000</td></tr><tr><td>max</td><td>1.000000</td><td>1.000000</td><td>1.000000</td><td>1.000000</td><td>1.000000</td><td>1.000000</td><td>1.000000</td><td>1.000000</td><td>1.000000</td><td>1.000000</td></tr></tbody></table>
<pre><code class="language-python">plt.figure(figsize=(12,10))

sns.heatmap(autoMPG_scaled, annot=False, cmap=&#x27;viridis&#x27;)

plt.savefig(&#x27;assets/Scikit_Learn_90.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_90-f5eb2588dbf57474a76a9e3a0158c22c.webp" width="917" height="900"></p>
<pre><code class="language-python">sns.clustermap(
    autoMPG_scaled.corr(numeric_only=True),
    linewidth=0.5,
    cmap=&#x27;seismic&#x27;,
    annot=True,
    col_cluster=False
)

plt.savefig(&#x27;assets/Scikit_Learn_91.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_91-e56345f0e5ef67a9c45114af4e34069f.webp" width="990" height="990"></p>
<h3 id="assigning-cluster-labels">Assigning Cluster Labels</h3>
<h4 id="known-number-of-clusters">Known Number of Clusters</h4>
<pre><code class="language-python"># there are ~ 4 clusters visible - let&#x27;s try to agglomerate them
autoMPG_model = AgglomerativeClustering(n_clusters=4)
cluster_labels = autoMPG_model.fit_predict(autoMPG_scaled)
autoMPG_df[&#x27;label&#x27;] = cluster_labels
autoMPG_df.head(5)
</code></pre>
<table><thead><tr><th></th><th>mpg</th><th>cylinders</th><th>displacement</th><th>horsepower</th><th>weight</th><th>acceleration</th><th>model_year</th><th>origin</th><th>name</th><th>label</th></tr></thead><tbody><tr><td>0</td><td>18.0</td><td>8</td><td>307.0</td><td>130.0</td><td>3504</td><td>12.0</td><td>70</td><td>usa</td><td>chevrolet chevelle malibu</td><td>2</td></tr><tr><td>1</td><td>15.0</td><td>8</td><td>350.0</td><td>165.0</td><td>3693</td><td>11.5</td><td>70</td><td>usa</td><td>buick skylark 320</td><td>2</td></tr><tr><td>2</td><td>18.0</td><td>8</td><td>318.0</td><td>150.0</td><td>3436</td><td>11.0</td><td>70</td><td>usa</td><td>plymouth satellite</td><td>2</td></tr><tr><td>3</td><td>16.0</td><td>8</td><td>304.0</td><td>150.0</td><td>3433</td><td>12.0</td><td>70</td><td>usa</td><td>amc rebel sst</td><td>2</td></tr><tr><td>4</td><td>17.0</td><td>8</td><td>302.0</td><td>140.0</td><td>3449</td><td>10.5</td><td>70</td><td>usa</td><td>ford torino</td><td>2</td></tr></tbody></table>
<pre><code class="language-python">plt.figure(figsize=(12,5))
sns.scatterplot(
    x=&#x27;mpg&#x27;,
    y=&#x27;horsepower&#x27;,
    data=autoMPG_df,
    hue=&#x27;label&#x27;,
    palette=&#x27;cool_r&#x27;,
    style=&#x27;origin&#x27;
).set_title(&#x27;Horsepower as a function of Miles-per-gallon&#x27;)

plt.savefig(&#x27;assets/Scikit_Learn_92.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_92-ee9a572afbd30613bf13f37a31dbaab9.webp" width="1005" height="470"></p>
<pre><code class="language-python">plt.figure(figsize=(12,5))
sns.scatterplot(
    x=&#x27;model_year&#x27;,
    y=&#x27;mpg&#x27;,
    data=autoMPG_df,
    hue=&#x27;label&#x27;,
    palette=&#x27;cool_r&#x27;,
    style=&#x27;origin&#x27;
).set_title(&#x27;Model Year as a function of Miles-per-gallon&#x27;)
plt.legend(bbox_to_anchor=(1.01,1.01))

plt.savefig(&#x27;assets/Scikit_Learn_93.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_93-561ce9ae5b0e86466e7bfd8a92023186.webp" width="1111" height="470"></p>
<pre><code class="language-python">figure, axes = plt.subplots(1, 3, sharex=True,figsize=(15, 5))
figure.suptitle(&#x27;Country of Origin&#x27;)

axes[0].set_title(&#x27;second chart with no data&#x27;)

sns.scatterplot(
    x=&#x27;horsepower&#x27;,
    y=&#x27;mpg&#x27;,
    data=autoMPG_df[autoMPG_df[&#x27;origin&#x27;] == &#x27;europe&#x27;],
    hue=&#x27;label&#x27;,
    palette=&#x27;cool_r&#x27;,
    style=&#x27;model_year&#x27;,
    ax=axes[0]
).set_title(&#x27;Europe&#x27;)

axes[1].set_title(&#x27;Europe&#x27;)

sns.scatterplot(
    x=&#x27;horsepower&#x27;,
    y=&#x27;mpg&#x27;,
    data=autoMPG_df[autoMPG_df[&#x27;origin&#x27;] == &#x27;japan&#x27;],
    hue=&#x27;label&#x27;,
    palette=&#x27;cool_r&#x27;,
    style=&#x27;model_year&#x27;,
    ax=axes[1]
).set_title(&#x27;Japan&#x27;)

axes[2].set_title(&#x27;second chart with no data&#x27;)

sns.scatterplot(
    x=&#x27;horsepower&#x27;,
    y=&#x27;mpg&#x27;,
    data=autoMPG_df[autoMPG_df[&#x27;origin&#x27;] == &#x27;usa&#x27;],
    hue=&#x27;label&#x27;,
    palette=&#x27;cool_r&#x27;,
    style=&#x27;model_year&#x27;,
    ax=axes[2]
).set_title(&#x27;USA&#x27;) 
plt.legend(bbox_to_anchor=(1.01,1.01))

plt.savefig(&#x27;assets/Scikit_Learn_94.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
# nice... perfect separation by country!
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_94-e696b53c8be4d4aa67f93222b508add0.webp" width="1371" height="498"></p>
<h4 id="unknown-number-of-clusters">Unknown Number of Clusters</h4>
<p>The Clustermap created above allowed us to estimate the amount of clusters needed to accuratly label the dataset based on the <strong>Dendrogram</strong> displayed on the left side. If we do not know how many clusters are present in our dataset we can define a maximum distance threshold a cluster can have before being merged with surrounding clusters. Setting this threshold to zero results in a number of clusters == number of datapoints.</p>
<pre><code class="language-python">autoMPG_model_auto = AgglomerativeClustering(
    n_clusters=None,
    metric=&#x27;euclidean&#x27;,
    distance_threshold=0
)
cluster_labels_auto = autoMPG_model_auto.fit_predict(autoMPG_scaled)
len(np.unique(cluster_labels_auto))
# threshold of zero leads to 392 clusters == number of rows in our dataset
</code></pre>
<pre><code class="language-python"># find out a good distance threshold
linkage_matrix = hierarchy.linkage(autoMPG_model_auto.children_)
linkage_matrix
# [`cluster[i]`, `cluster[j]`, `distance between`, `number of members`]
</code></pre>
<pre><code class="language-python"># to display this matrix we can use the above mentioned dendrogram
plt.figure(figsize=(20,10))
plt.title(&#x27;Hierarchy Dendrogram for 8 Classes&#x27;)
dendro = hierarchy.dendrogram(linkage_matrix, truncate_mode=&#x27;lastp&#x27;, p=9)

plt.savefig(&#x27;assets/Scikit_Learn_95.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
# The higher the y-value the larger the distance between the connected clusters
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_95-3a76c3f9742e734d4f2bcbe35b3f1b07.webp" width="1597" height="840"></p>
<pre><code class="language-python"># since the miles-per-gallons are a good indicator for the label
# what is the max distance between two points here:
car_max_mpg = autoMPG_scaled.iloc[autoMPG_scaled[&#x27;mpg&#x27;].idxmax()]
car_min_mpg = autoMPG_scaled.iloc[autoMPG_scaled[&#x27;mpg&#x27;].idxmin()]

np.linalg.norm(car_max_mpg - car_min_mpg)
# 3.1128158766165406
# if the max distance is ~3 the threshold should be &lt; 3
</code></pre>
<pre><code class="language-python">autoMPG_model_auto = AgglomerativeClustering(
    n_clusters=None,
    metric=&#x27;euclidean&#x27;,
    distance_threshold=2
)
cluster_labels_auto = autoMPG_model_auto.fit_predict(autoMPG_scaled)
len(np.unique(cluster_labels_auto))
# threshold of two leads to 11 clusters
</code></pre>
<pre><code class="language-python">autoMPG_model_auto = AgglomerativeClustering(
    n_clusters=None,
    metric=&#x27;euclidean&#x27;,
    distance_threshold=3
)
cluster_labels_auto = autoMPG_model_auto.fit_predict(autoMPG_scaled)
len(np.unique(cluster_labels_auto))
# threshold of three leads to 9 clusters
</code></pre>
<pre><code class="language-python">autoMPG_df[&#x27;label_auto&#x27;] = cluster_labels_auto
</code></pre>
<pre><code class="language-python">figure, axes = plt.subplots(1, 3, sharex=True,figsize=(15, 6))
figure.suptitle(&#x27;Country of Origin&#x27;)

axes[0].set_title(&#x27;second chart with no data&#x27;)

sns.scatterplot(
    x=&#x27;horsepower&#x27;,
    y=&#x27;mpg&#x27;,
    data=autoMPG_df[autoMPG_df[&#x27;origin&#x27;] == &#x27;europe&#x27;],
    hue=&#x27;label_auto&#x27;,
    palette=&#x27;cool_r&#x27;,
    style=&#x27;model_year&#x27;,
    ax=axes[0]
).set_title(&#x27;Europe&#x27;)

axes[1].set_title(&#x27;Europe&#x27;)

sns.scatterplot(
    x=&#x27;horsepower&#x27;,
    y=&#x27;mpg&#x27;,
    data=autoMPG_df[autoMPG_df[&#x27;origin&#x27;] == &#x27;japan&#x27;],
    hue=&#x27;label_auto&#x27;,
    palette=&#x27;cool_r&#x27;,
    style=&#x27;model_year&#x27;,
    ax=axes[1]
).set_title(&#x27;Japan&#x27;)

axes[2].set_title(&#x27;second chart with no data&#x27;)

sns.scatterplot(
    x=&#x27;horsepower&#x27;,
    y=&#x27;mpg&#x27;,
    data=autoMPG_df[autoMPG_df[&#x27;origin&#x27;] == &#x27;usa&#x27;],
    hue=&#x27;label_auto&#x27;,
    palette=&#x27;cool_r&#x27;,
    style=&#x27;model_year&#x27;,
    ax=axes[2]
).set_title(&#x27;USA&#x27;) 
plt.legend(bbox_to_anchor=(1.01,1.01))

plt.savefig(&#x27;assets/Scikit_Learn_96.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
# the division by countries is still there. but we are now getting
# sub-classes within each country - which might be important depending on your set goal
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_96-91670f18a4074a5e1648be38608bb260.webp" width="1371" height="585"></p>
<h2 id="unsupervised-learning---density-based-spatial-clustering-dbscan">Unsupervised Learning - Density-based Spatial Clustering (DBSCAN)</h2>
<h3 id="dbscan-vs-kmeans">DBSCAN vs KMeans</h3>
<pre><code class="language-python">blobs_df = pd.read_csv(&#x27;datasets/blobs.csv&#x27;)
blobs_df.tail(2)
</code></pre>
<table><thead><tr><th></th><th>X1</th><th>X2</th></tr></thead><tbody><tr><td>1498</td><td>5.454552</td><td>6.461246</td></tr><tr><td>1499</td><td>-7.769230</td><td>7.014384</td></tr></tbody></table>
<pre><code class="language-python">plt.figure(figsize=(12,5))
plt.title(&#x27;Blobs Dataset&#x27;)
sns.scatterplot(data=blobs_df, x=&#x27;X1&#x27;, y=&#x27;X2&#x27;)

plt.savefig(&#x27;assets/Scikit_Learn_97.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_97-7fce7abf7b09dd615ee0c8153b3d3e51.webp" width="1021" height="470"></p>
<pre><code class="language-python">moons_df = pd.read_csv(&#x27;datasets/moons.csv&#x27;)
moons_df.tail(2)
</code></pre>
<table><thead><tr><th></th><th>X1</th><th>X2</th></tr></thead><tbody><tr><td>1498</td><td>1.803858</td><td>-0.154705</td></tr><tr><td>1499</td><td>0.203305</td><td>0.079049</td></tr></tbody></table>
<pre><code class="language-python">plt.figure(figsize=(12,5))
plt.title(&#x27;Moons Dataset&#x27;)
sns.scatterplot(data=moons_df, x=&#x27;X1&#x27;, y=&#x27;X2&#x27;)

plt.savefig(&#x27;assets/Scikit_Learn_98.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_98-62cea7ed95a919da2037899d286d9dd5.webp" width="1021" height="470"></p>
<pre><code class="language-python">circles_df = pd.read_csv(&#x27;datasets/circles.csv&#x27;)
circles_df.tail(2)
</code></pre>
<table><thead><tr><th></th><th>X1</th><th>X2</th></tr></thead><tbody><tr><td>1498</td><td>0.027432</td><td>-0.264891</td></tr><tr><td>1499</td><td>-0.216732</td><td>0.183006</td></tr></tbody></table>
<pre><code class="language-python">plt.figure(figsize=(12,5))
plt.title(&#x27;Circles Dataset&#x27;)
sns.scatterplot(data=circles_df, x=&#x27;X1&#x27;, y=&#x27;X2&#x27;)

plt.savefig(&#x27;assets/Scikit_Learn_99.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_99-fa700742ddcde74019cb4ab7fcbceb2d.webp" width="1012" height="470"></p>
<pre><code class="language-python">def display_categories(model, data, axis):
    labels = model.fit_predict(data)
    sns.scatterplot(data=data, x=&#x27;X1&#x27;, y=&#x27;X2&#x27;, hue=labels, palette=&#x27;cool&#x27; , ax=axis)
</code></pre>
<pre><code class="language-python">km_model_blobs = KMeans(n_clusters=3, init=&#x27;random&#x27;, n_init=&#x27;auto&#x27;)
db_model_blobs = DBSCAN(eps=0.5, min_samples=5)

figure, axes = plt.subplots(1, 2, sharex=True,figsize=(12, 6))
figure.suptitle(&#x27;3 Blobs Dataset&#x27;)

axes[0].set_title(&#x27;KMeans Clustering&#x27;)
display_categories(km_model_blobs, blobs_df, axes[0])

axes[1].set_title(&#x27;DBSCAN Clustering&#x27;)
display_categories(db_model_blobs, blobs_df, axes[1])

plt.savefig(&#x27;assets/Scikit_Learn_100.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_100-ef457d03d48d8019fcf567fc7c4c3143.webp" width="1029" height="585"></p>
<pre><code class="language-python">km_model_moons = KMeans(n_clusters=2, init=&#x27;random&#x27;, n_init=&#x27;auto&#x27;)
db_model_moons = DBSCAN(eps=0.2, min_samples=5)

figure, axes = plt.subplots(1, 2, sharex=True,figsize=(12, 6))
figure.suptitle(&#x27;2 Moons Dataset&#x27;)

axes[0].set_title(&#x27;KMeans Clustering&#x27;)
display_categories(km_model_moons, moons_df, axes[0])

axes[1].set_title(&#x27;DBSCAN Clustering&#x27;)
display_categories(db_model_moons, moons_df, axes[1])

plt.savefig(&#x27;assets/Scikit_Learn_101.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_101-5417d0fb4e8b6c13776e0c8682e833c1.webp" width="1021" height="585"></p>
<pre><code class="language-python">km_model_circles = KMeans(n_clusters=2, init=&#x27;random&#x27;, n_init=&#x27;auto&#x27;)
db_model_circles = DBSCAN(eps=0.2, min_samples=5)

figure, axes = plt.subplots(1, 2, sharex=True,figsize=(12, 6))
figure.suptitle(&#x27;2 Circles Dataset&#x27;)

axes[0].set_title(&#x27;KMeans Clustering&#x27;)
display_categories(km_model_circles, circles_df, axes[0])

axes[1].set_title(&#x27;DBSCAN Clustering&#x27;)
display_categories(db_model_circles, circles_df, axes[1])

plt.savefig(&#x27;assets/Scikit_Learn_102.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_102-3085611afb39b86b5582867975756685.webp" width="1012" height="585"></p>
<h3 id="dbscan-hyperparameter-tuning">DBSCAN Hyperparameter Tuning</h3>
<pre><code class="language-python">two_blobs_df = pd.read_csv(&#x27;datasets/two-blobs.csv&#x27;)
two_blobs_otl_df = pd.read_csv(&#x27;datasets/two-blobs-outliers.csv&#x27;)
</code></pre>
<pre><code class="language-python"># default hyperparameter
db_model_base = DBSCAN(eps=0.5, min_samples=5)

figure, axes = plt.subplots(1, 2, sharex=True,figsize=(12, 6))
figure.suptitle(&#x27;2 Blobs Dataset - Default Hyperparameter&#x27;)

axes[0].set_title(&#x27;DBSCAN Clustering w/o Outliers&#x27;)
display_categories(db_model_base, two_blobs_df, axes[0])

axes[1].set_title(&#x27;DBSCAN Clustering with Outliers&#x27;)
display_categories(db_model_base, two_blobs_otl_df, axes[1])

plt.savefig(&#x27;assets/Scikit_Learn_103.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
# points around cluster 1 are assigned to be outliers
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_103-7716202fcaaff3dffd2066a3c99a50b5.webp" width="999" height="585"></p>
<pre><code class="language-python"># reducing epsilon reduces the max distance (epsilon)
# points are allowed to have and still be assigned to a cluster
db_model_dec = DBSCAN(eps=0.001, min_samples=5)

figure, axes = plt.subplots(1, 2, sharex=True,figsize=(12, 6))
figure.suptitle(&#x27;2 Blobs Dataset - Reduced Epsilon&#x27;)

axes[0].set_title(&#x27;DBSCAN Clustering w/o Outliers&#x27;)
display_categories(db_model_dec, two_blobs_df, axes[0])

axes[1].set_title(&#x27;DBSCAN Clustering with Outliers&#x27;)
display_categories(db_model_dec, two_blobs_otl_df, axes[1])

plt.savefig(&#x27;assets/Scikit_Learn_104.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
# distance is too small - every point becomes it&#x27;s own cluster and is assigned as an outlier
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_104-070cca82c6a19830b5ef6fed11d7c6d3.webp" width="999" height="585"></p>
<pre><code class="language-python"># increasing epsilon increases the max distance (epsilon)
# points are allowed to have and still be assigned to a cluster
db_model_inc = DBSCAN(eps=10, min_samples=5)

figure, axes = plt.subplots(1, 2, sharex=True,figsize=(12, 6))
figure.suptitle(&#x27;2 Blobs Dataset - Increased Epsilon&#x27;)

axes[0].set_title(&#x27;DBSCAN Clustering w/o Outliers&#x27;)
display_categories(db_model_inc, two_blobs_df, axes[0])

axes[1].set_title(&#x27;DBSCAN Clustering with Outliers&#x27;)
display_categories(db_model_inc, two_blobs_otl_df, axes[1])

plt.savefig(&#x27;assets/Scikit_Learn_105.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
# distance is too big - every point becomes becomes part of the same cluster
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_105-87ebb38622e159a3f81403f10f4f1862.webp" width="999" height="585"></p>
<h4 id="elbow-plot">Elbow Plot</h4>
<pre><code class="language-python">epsilon_value_range = np.linspace(0.0001, 1, 100)

n_outliers = []
perc_outlier = []
n_clusters = []

for epsilon in epsilon_value_range:
    dbscan_model = DBSCAN(eps=epsilon)
    dbscan_model.fit(two_blobs_otl_df)
    
    # total number of outliers
    n_outliers.append(np.sum(dbscan_model.labels_ == -1))
    # percentage of outliers
    perc_outlier.append(
        100 * np.sum(dbscan_model.labels_ == -1) / len(dbscan_model.labels_)
    )
    # number of clusters
    n_clusters.append(len(np.unique(dbscan_model.labels_)))
</code></pre>
<pre><code class="language-python">plt.figure(figsize=(12,5))
plt.title(&#x27;Elbow Plot - DBSCAN Hyperparameter&#x27;)
plt.xlabel(&#x27;Epsilon (Max Distance between Points)&#x27;)
plt.ylabel(&#x27;Number of Outliers&#x27;)
plt.ylim(0,10)
# we expect 3 outliers
plt.hlines(y=3, xmin=0, xmax=0.7, color=&#x27;fuchsia&#x27;)
# 3 outliers are reached somewhere around eps=0.7
plt.vlines(x=0.7, ymin=0, ymax=3, color=&#x27;fuchsia&#x27;)
sns.lineplot(x=epsilon_value_range, y=n_outliers)

plt.savefig(&#x27;assets/Scikit_Learn_107.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="data:image/webp;base64,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" width="996" height="470"></p>
<pre><code class="language-python">plt.figure(figsize=(12,5))
plt.title(&#x27;Number of Clusters by Epsilon Range&#x27;)
plt.xlabel(&#x27;Epsilon (Max Distance between Points)&#x27;)
plt.ylabel(&#x27;Number of Clusters&#x27;)
# we expect 2 clusters + outliers
plt.hlines(y=3, xmin=0, xmax=1, color=&#x27;fuchsia&#x27;)
plt.ylim(0,50)
plt.xlim(0,1)
sns.lineplot(x=epsilon_value_range, y=n_clusters)

plt.savefig(&#x27;assets/Scikit_Learn_108.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
# we already reach 3 cluster with an epsilon of 0.2
# but as seen above we need an epsilon of 0.7 to reduce
# the number of outliers to 3
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_108-c7787c5ae26603572bb4689598c825c8.webp" width="1007" height="470"></p>
<pre><code class="language-python"># find the optimum
# rule of thumb for min_samples = 2*n_dim
n_dim = two_blobs_otl_df.shape[1]
db_model_opt = DBSCAN(eps=0.7, min_samples=2*n_dim)

figure, axes = plt.subplots(1, 2, sharex=True,figsize=(12, 6))
figure.suptitle(&#x27;2 Blobs Dataset - Optimal Epsilon&#x27;)

axes[0].set_title(&#x27;DBSCAN Clustering w/o Outliers&#x27;)
display_categories(db_model_opt, two_blobs_df, axes[0])

axes[1].set_title(&#x27;DBSCAN Clustering with Outliers&#x27;)
display_categories(db_model_opt, two_blobs_otl_df, axes[1])

plt.savefig(&#x27;assets/Scikit_Learn_106.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
# the 3 outliers are labled as such and every other point is assigned to one of the two clusters
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_106-4331cd584a8abfdd43d7f97f0b7a4dfd.webp" width="999" height="585"></p>
<pre><code class="language-python"># find number of outliers
print(&#x27;Number of Outliers&#x27;, np.sum(db_model_opt.labels_ == -1))
# Number of Outliers 3
# get outlier percentage
print(&#x27;Percentage of Outliers&#x27;, (100 * np.sum(db_model_opt.labels_ == -1) / len(db_model_opt.labels_)).round(2),&#x27;%&#x27;)
# Percentage of Outliers 0.3 %
</code></pre>
<h3 id="realworld-dataset">Realworld Dataset</h3>
<blockquote>
<p><a href="https://archive.ics.uci.edu/dataset/292/wholesale+customers">Wholesale customers</a>
The data set refers to clients of a wholesale distributor. It includes the annual spending in monetary units (m.u.) on diverse product categories</p>
</blockquote>
<p><strong>Additional Information</strong></p>
<ol>
<li><strong>FRESH</strong>: annual spending (m.u.) on fresh products (Continuous)</li>
<li><strong>MILK</strong>: annual spending (m.u.) on milk products (Continuous)</li>
<li><strong>GROCERY</strong>: annual spending (m.u.) on grocery products (Continuous)</li>
<li><strong>FROZEN</strong>: annual spending (m.u.) on frozen products (Continuous)</li>
<li><strong>DETERGENTS_PAPER</strong>: annual spending (m.u.) on detergents and paper products (Continuous)</li>
<li><strong>DELICATESSEN</strong>: annual spending (m.u.)on and delicatessen products (Continuous)</li>
<li><strong>CHANNEL</strong>: customers Channel - Horeca (Hotel/Restaurant/Cafe) or Retail channel (Nominal)</li>
<li><strong>REGION</strong>: customers Region - Lisnon, Oporto or Other (Nominal)</li>
</ol>
<h4 id="dataset-exploration-4">Dataset Exploration</h4>
<pre><code class="language-python">wholesale_df = pd.read_csv(&#x27;datasets/wholesome-customers-data.csv&#x27;)
wholesale_df.head(5)
</code></pre>
<table><thead><tr><th></th><th>Channel</th><th>Region</th><th>Fresh</th><th>Milk</th><th>Grocery</th><th>Frozen</th><th>Detergents_Paper</th><th>Delicassen</th></tr></thead><tbody><tr><td>0</td><td>2</td><td>3</td><td>12669</td><td>9656</td><td>7561</td><td>214</td><td>2674</td><td>1338</td></tr><tr><td>1</td><td>2</td><td>3</td><td>7057</td><td>9810</td><td>9568</td><td>1762</td><td>3293</td><td>1776</td></tr><tr><td>2</td><td>2</td><td>3</td><td>6353</td><td>8808</td><td>7684</td><td>2405</td><td>3516</td><td>7844</td></tr><tr><td>3</td><td>1</td><td>3</td><td>13265</td><td>1196</td><td>4221</td><td>6404</td><td>507</td><td>1788</td></tr><tr><td>4</td><td>2</td><td>3</td><td>22615</td><td>5410</td><td>7198</td><td>3915</td><td>1777</td><td>5185</td></tr></tbody></table>
<pre><code class="language-python">
wholesale_df.info()
</code></pre>
<pre><code class="language-python">plt.figure(figsize=(12,5))
plt.title(&#x27;Whole Sale: Milk Products vs Groceries&#x27;)
sns.scatterplot(
    data=wholesale_df,
    x=&#x27;Milk&#x27;, y=&#x27;Grocery&#x27;,
    hue=&#x27;Channel&#x27;, style=&#x27;Region&#x27;,
    palette=&#x27;winter&#x27;
)

plt.savefig(&#x27;assets/Scikit_Learn_109.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_109-110772ad19c3727b10308da6fec226bb.webp" width="1023" height="470"></p>
<pre><code class="language-python">plt.figure(figsize=(10, 5))
plt.title(&#x27;Whole Sale: Milk Products by Distribution Channel&#x27;)

sns.histplot(
    data=wholesale_df,
    x=&#x27;Milk&#x27;,
    bins=50,
    hue=&#x27;Channel&#x27;,
    palette=&#x27;winter&#x27;,
    kde=True
)

plt.savefig(&#x27;assets/Scikit_Learn_110.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_110-bf999150c6fc56aa191e555df7be5f0c.webp" width="850" height="470"></p>
<pre><code class="language-python">sns.clustermap(
    wholesale_df.corr(),
    linewidth=0.5,
    cmap=&#x27;winter&#x27;,
    annot=True,
    col_cluster=False
)

plt.savefig(&#x27;assets/Scikit_Learn_111.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_111-6a9645b8a4d05990a7f77b8dd754e4d1.webp" width="989" height="990"></p>
<pre><code class="language-python">sns.pairplot(
    data=wholesale_df,
    hue=&#x27;Region&#x27;,
    palette=&#x27;winter&#x27;
)

plt.savefig(&#x27;assets/Scikit_Learn_112.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_112-36ab928c98b53e95de75016d7ff4c84a.webp" width="1796" height="1721"></p>
<h4 id="data-preprocessing-2">Data Preprocessing</h4>
<pre><code class="language-python"># normalize feature set
scaler = StandardScaler()
wholesale_scaled = pd.DataFrame(
    scaler.fit_transform(wholesale_df), columns=wholesale_df.columns
)
</code></pre>
<pre><code class="language-python">wholesale_scaled.describe()
</code></pre>
<table><thead><tr><th></th><th>Channel</th><th>Region</th><th>Fresh</th><th>Milk</th><th>Grocery</th><th>Frozen</th><th>Detergents_Paper</th><th>Delicassen</th></tr></thead><tbody><tr><td>count</td><td>4.400000e+02</td><td>4.400000e+02</td><td>4.400000e+02</td><td>440.000000</td><td>4.400000e+02</td><td>4.400000e+02</td><td>4.400000e+02</td><td>4.400000e+02</td></tr><tr><td>mean</td><td>1.614870e-17</td><td>3.552714e-16</td><td>-3.431598e-17</td><td>0.000000</td><td>-4.037175e-17</td><td>3.633457e-17</td><td>2.422305e-17</td><td>-8.074349e-18</td></tr><tr><td>std</td><td>1.001138e+00</td><td>1.001138e+00</td><td>1.001138e+00</td><td>1.001138</td><td>1.001138e+00</td><td>1.001138e+00</td><td>1.001138e+00</td><td>1.001138e+00</td></tr><tr><td>min</td><td>-6.902971e-01</td><td>-1.995342e+00</td><td>-9.496831e-01</td><td>-0.778795</td><td>-8.373344e-01</td><td>-6.283430e-01</td><td>-6.044165e-01</td><td>-5.402644e-01</td></tr><tr><td>25%</td><td>-6.902971e-01</td><td>-7.023369e-01</td><td>-7.023339e-01</td><td>-0.578306</td><td>-6.108364e-01</td><td>-4.804306e-01</td><td>-5.511349e-01</td><td>-3.964005e-01</td></tr><tr><td>50%</td><td>-6.902971e-01</td><td>5.906683e-01</td><td>-2.767602e-01</td><td>-0.294258</td><td>-3.366684e-01</td><td>-3.188045e-01</td><td>-4.336004e-01</td><td>-1.985766e-01</td></tr><tr><td>75%</td><td>1.448652e+00</td><td>5.906683e-01</td><td>3.905226e-01</td><td>0.189092</td><td>2.849105e-01</td><td>9.946441e-02</td><td>2.184822e-01</td><td>1.048598e-01</td></tr><tr><td>max</td><td>1.448652e+00</td><td>5.906683e-01</td><td>7.927738e+00</td><td>9.183650</td><td>8.936528e+00</td><td>1.191900e+01</td><td>7.967672e+00</td><td>1.647845e+01</td></tr></tbody></table>
<h4 id="model-hyperparameter-tuning">Model Hyperparameter Tuning</h4>
<pre><code class="language-python">epsilon_value_range = np.linspace(0.001, 3, 100)
n_dim = wholesale_scaled.shape[1]

n_outliers = []
perc_outlier = []
n_clusters = []

for epsilon in epsilon_value_range:
    dbscan_model = DBSCAN(eps=epsilon, min_samples=2*n_dim)
    dbscan_model.fit(wholesale_scaled)
    
    # total number of outliers
    n_outliers.append(np.sum(dbscan_model.labels_ == -1))
    # percentage of outliers
    perc_outlier.append(
        100 * np.sum(dbscan_model.labels_ == -1) / len(dbscan_model.labels_)
    )
    # number of clusters
    n_clusters.append(len(np.unique(dbscan_model.labels_)))
</code></pre>
<pre><code class="language-python">plt.figure(figsize=(12,5))
plt.title(&#x27;Elbow Plot - DBSCAN Hyperparameter&#x27;)
plt.xlabel(&#x27;Epsilon (Max Distance between Points)&#x27;)
plt.ylabel(&#x27;Number of Outliers&#x27;)
plt.hlines(y=25, xmin=0, xmax=2, color=&#x27;fuchsia&#x27;)
plt.vlines(x=2, ymin=0, ymax=25, color=&#x27;fuchsia&#x27;)
sns.lineplot(x=epsilon_value_range, y=n_outliers)

plt.savefig(&#x27;assets/Scikit_Learn_113.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_113-c393adf84cc5623d2ef501567a4ecd28.webp" width="1005" height="470"></p>
<pre><code class="language-python">plt.figure(figsize=(12,5))
plt.title(&#x27;Number of Clusters by Epsilon Range&#x27;)
plt.xlabel(&#x27;Epsilon (Max Distance between Points)&#x27;)
plt.ylabel(&#x27;Number of Clusters&#x27;)
plt.hlines(y=3, xmin=0, xmax=2, color=&#x27;fuchsia&#x27;)
plt.vlines(x=2, ymin=0, ymax=3, color=&#x27;fuchsia&#x27;)
sns.lineplot(x=epsilon_value_range, y=n_clusters)

plt.savefig(&#x27;assets/Scikit_Learn_114.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="data:image/webp;base64,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" width="988" height="470"></p>
<pre><code class="language-python">def wholesale_categories(model, data, x, y, axis):
    labels = model.fit_predict(data)
    sns.scatterplot(data=data, x=x, y=y, hue=labels, palette=&#x27;cool&#x27; , ax=axis)
</code></pre>
<pre><code class="language-python">db_model_opt = DBSCAN(eps=2.0, min_samples=2*n_dim)

figure, axes = plt.subplots(1, 2, sharex=True,figsize=(12, 6))
figure.suptitle(&#x27;Whole Sale Dataset - DBSCAN Cluster (Normalized)&#x27;)

axes[0].set_title(&#x27;DBSCAN Clustering Milk Products vs Groceries&#x27;)
wholesale_categories(
    model=db_model_opt,
    data=wholesale_scaled,
    x=&#x27;Milk&#x27;, y=&#x27;Grocery&#x27;,
    axis=axes[0]
)

axes[1].set_title(&#x27;DBSCAN Clustering Milk Products vs Delicassen&#x27;)
wholesale_categories(
    model=db_model_opt,
    data=wholesale_scaled,
    x=&#x27;Milk&#x27;, y=&#x27;Delicassen&#x27;,
    axis=axes[1]
)

plt.savefig(&#x27;assets/Scikit_Learn_115a.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_115a-6523e34afc435d2a0f239bc84ab420db.webp" width="988" height="585"></p>
<pre><code class="language-python"># add labels to original dataframe
wholesale_df[&#x27;Label&#x27;] = db_model_opt.fit_predict(wholesale_scaled)
wholesale_df[&#x27;Label&#x27;].head(5)
</code></pre>
<pre><code class="language-python"># remove outliers
wholesale_df_wo_otl = wholesale_df[wholesale_df[&#x27;Label&#x27;] != -1]
</code></pre>
<pre><code class="language-python">db_model_opt = DBSCAN(eps=3.0, min_samples=2*n_dim)

figure, axes = plt.subplots(1, 2, sharex=True,figsize=(12, 6))
figure.suptitle(&#x27;Whole Sale Dataset - DBSCAN Cluster (w/o Outliers)&#x27;)

axes[0].set_title(&#x27;DBSCAN Clustering Milk Products vs Groceries&#x27;)
sns.scatterplot(
    data=wholesale_df_wo_otl,
    x=&#x27;Milk&#x27;, y=&#x27;Grocery&#x27;,
    hue=&#x27;Label&#x27;,
    palette=&#x27;cool&#x27;,
    ax=axes[0]
)

axes[1].set_title(&#x27;DBSCAN Clustering Milk Products vs Delicassen&#x27;)
sns.scatterplot(
    data=wholesale_df_wo_otl,
    x=&#x27;Milk&#x27;, y=&#x27;Delicassen&#x27;,
    hue=&#x27;Label&#x27;,
    palette=&#x27;cool&#x27;,
    ax=axes[1]
)

plt.savefig(&#x27;assets/Scikit_Learn_115b.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_115b-9b9d9080a71666982c51f0d363726f85.webp" width="1024" height="585"></p>
<pre><code class="language-python"># see if the mean values of each cluster differ from each other
grouped_df = wholesale_df.groupby(&#x27;Label&#x27;).mean()
</code></pre>
<table><thead><tr><th>Label</th><th>Channel</th><th>Region</th><th>Fresh</th><th>Milk</th><th>Grocery</th><th>Frozen</th><th>Detergents_Paper</th><th>Delicassen</th></tr></thead><tbody><tr><td>-1</td><td>1.52</td><td>2.480000</td><td>27729.920000</td><td>22966.960000</td><td>26609.600000</td><td>11289.640000</td><td>11173.560000</td><td>6707.160000</td></tr><tr><td>0</td><td>2.00</td><td>2.620155</td><td>8227.666667</td><td>8615.852713</td><td>13859.674419</td><td>1447.759690</td><td>5969.581395</td><td>1498.457364</td></tr><tr><td>1</td><td>1.00</td><td>2.513986</td><td>12326.972028</td><td>3023.559441</td><td>3655.328671</td><td>3086.181818</td><td>763.783217</td><td>1083.786713</td></tr></tbody></table>
<pre><code class="language-python">scaler = MinMaxScaler()
grouped_scaler = pd.DataFrame(
    scaler.fit_transform(grouped_df), columns=grouped_df.columns, index=[&#x27;Outlier&#x27;, &#x27;Cluster 1&#x27;, &#x27;Cluster 2&#x27;]
)
grouped_scaler.head()
</code></pre>
<table><thead><tr><th></th><th>Channel</th><th>Region</th><th>Fresh</th><th>Milk</th><th>Grocery</th><th>Frozen</th><th>Detergents_Paper</th><th>Delicassen</th></tr></thead><tbody><tr><td>Outlier</td><td>0.52</td><td>0.000000</td><td>1.000000</td><td>1.000000</td><td>1.000000</td><td>1.000000</td><td>1.000000</td><td>1.000000</td></tr><tr><td>Cluster 1</td><td>1.00</td><td>1.000000</td><td>0.000000</td><td>0.280408</td><td>0.444551</td><td>0.000000</td><td>0.500087</td><td>0.073741</td></tr><tr><td>Cluster 2</td><td>0.00</td><td>0.242489</td><td>0.210196</td><td>0.000000</td><td>0.000000</td><td>0.166475</td><td>0.000000</td><td>0.000000</td></tr></tbody></table>
<pre><code class="language-python">plt.figure(figsize=(12, 3))
plt.title(&#x27;Scaled Cluster / Outliers Comparison (Normalized)&#x27;)

sns.heatmap(
    grouped_scaler,
    linewidth=0.5,
    cmap=&#x27;coolwarm&#x27;,
    annot=True
)

plt.savefig(&#x27;assets/Scikit_Learn_116.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_116-562ce0908b16fe40092e80f49b4b79f6.webp" width="877" height="407"></p>
<pre><code class="language-python">grouped_df = grouped_df.drop([&#x27;Labels&#x27;], axis=1)
</code></pre>
<pre><code class="language-python"># remove outlier
wholesale_clusters = grouped_df.drop(-1, axis=0)
wholesale_clusters.head()
</code></pre>
<table><thead><tr><th>Label</th><th>Channel</th><th>Region</th><th>Fresh</th><th>Milk</th><th>Grocery</th><th>Frozen</th><th>Detergents_Paper</th><th>Delicassen</th></tr></thead><tbody><tr><td>0</td><td>2.0</td><td>2.620155</td><td>8227.666667</td><td>8615.852713</td><td>13859.674419</td><td>1447.759690</td><td>5969.581395</td><td>1498.457364</td></tr><tr><td>1</td><td>1.0</td><td>2.513986</td><td>12326.972028</td><td>3023.559441</td><td>3655.328671</td><td>3086.181818</td><td>763.783217</td><td>1083.786713</td></tr></tbody></table>
<pre><code class="language-python">plt.figure(figsize=(12, 3))
plt.title(&#x27;Mean Spending Values for Cluster 1 and 2&#x27;)

sns.heatmap(
    wholesale_clusters,
    linewidth=0.5,
    cmap=&#x27;coolwarm&#x27;,
    annot=True
)

plt.savefig(&#x27;assets/Scikit_Learn_117.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_117-cf42ea0fc6ede7053f0fd807ae691151.webp" width="918" height="407"></p>
<h2 id="dimensionality-reduction---principal-component-analysis-pca">Dimensionality Reduction - Principal Component Analysis (PCA)</h2>
<h3 id="dataset-preprocessing-4">Dataset Preprocessing</h3>
<p>Breast cancer wisconsin (diagnostic) dataset.</p>
<ul>
<li>Attribute Information:<!-- -->
<ul>
<li>radius (mean of distances from center to points on the perimeter)</li>
<li>texture (standard deviation of gray-scale values)</li>
<li>perimeter</li>
<li>area</li>
<li>smoothness (local variation in radius lengths)</li>
<li>compactness (perimeter^2 / area - 1.0)</li>
<li>concavity (severity of concave portions of the contour)</li>
<li>concave points (number of concave portions of the contour)</li>
<li>symmetry</li>
<li>fractal dimension (&quot;coastline approximation&quot; - 1)</li>
</ul>
</li>
</ul>
<p>The mean, standard error, and &quot;worst&quot; or largest (mean of the three worst/largest values) of these features were computed for each image, resulting in 30 features.  For instance, field 0 is Mean Radius, field 10 is Radius SE, field 20 is Worst Radius.</p>
<ul>
<li>class:<!-- -->
<ul>
<li>WDBC-Malignant</li>
<li>WDBC-Benign</li>
</ul>
</li>
</ul>
<pre><code class="language-python">tumor_df = pd.read_csv(&#x27;datasets/cancer-tumor-data-features.csv&#x27;)
tumor_df.head(5).transpose()
</code></pre>
<table><thead><tr><th></th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>mean radius</td><td>17.990000</td><td>20.570000</td><td>19.690000</td><td>11.420000</td><td>20.290000</td></tr><tr><td>mean texture</td><td>10.380000</td><td>17.770000</td><td>21.250000</td><td>20.380000</td><td>14.340000</td></tr><tr><td>mean perimeter</td><td>122.800000</td><td>132.900000</td><td>130.000000</td><td>77.580000</td><td>135.100000</td></tr><tr><td>mean area</td><td>1001.000000</td><td>1326.000000</td><td>1203.000000</td><td>386.100000</td><td>1297.000000</td></tr><tr><td>mean smoothness</td><td>0.118400</td><td>0.084740</td><td>0.109600</td><td>0.142500</td><td>0.100300</td></tr><tr><td>mean compactness</td><td>0.277600</td><td>0.078640</td><td>0.159900</td><td>0.283900</td><td>0.132800</td></tr><tr><td>mean concavity</td><td>0.300100</td><td>0.086900</td><td>0.197400</td><td>0.241400</td><td>0.198000</td></tr><tr><td>mean concave points</td><td>0.147100</td><td>0.070170</td><td>0.127900</td><td>0.105200</td><td>0.104300</td></tr><tr><td>mean symmetry</td><td>0.241900</td><td>0.181200</td><td>0.206900</td><td>0.259700</td><td>0.180900</td></tr><tr><td>mean fractal dimension</td><td>0.078710</td><td>0.056670</td><td>0.059990</td><td>0.097440</td><td>0.058830</td></tr><tr><td>radius error</td><td>1.095000</td><td>0.543500</td><td>0.745600</td><td>0.495600</td><td>0.757200</td></tr><tr><td>texture error</td><td>0.905300</td><td>0.733900</td><td>0.786900</td><td>1.156000</td><td>0.781300</td></tr><tr><td>perimeter error</td><td>8.589000</td><td>3.398000</td><td>4.585000</td><td>3.445000</td><td>5.438000</td></tr><tr><td>area error</td><td>153.400000</td><td>74.080000</td><td>94.030000</td><td>27.230000</td><td>94.440000</td></tr><tr><td>smoothness error</td><td>0.006399</td><td>0.005225</td><td>0.006150</td><td>0.009110</td><td>0.011490</td></tr><tr><td>compactness error</td><td>0.049040</td><td>0.013080</td><td>0.040060</td><td>0.074580</td><td>0.024610</td></tr><tr><td>concavity error</td><td>0.053730</td><td>0.018600</td><td>0.038320</td><td>0.056610</td><td>0.056880</td></tr><tr><td>concave points error</td><td>0.015870</td><td>0.013400</td><td>0.020580</td><td>0.018670</td><td>0.018850</td></tr><tr><td>symmetry error</td><td>0.030030</td><td>0.013890</td><td>0.022500</td><td>0.059630</td><td>0.017560</td></tr><tr><td>fractal dimension error</td><td>0.006193</td><td>0.003532</td><td>0.004571</td><td>0.009208</td><td>0.005115</td></tr><tr><td>worst radius</td><td>25.380000</td><td>24.990000</td><td>23.570000</td><td>14.910000</td><td>22.540000</td></tr><tr><td>worst texture</td><td>17.330000</td><td>23.410000</td><td>25.530000</td><td>26.500000</td><td>16.670000</td></tr><tr><td>worst perimeter</td><td>184.600000</td><td>158.800000</td><td>152.500000</td><td>98.870000</td><td>152.200000</td></tr><tr><td>worst area</td><td>2019.000000</td><td>1956.000000</td><td>1709.000000</td><td>567.700000</td><td>1575.000000</td></tr><tr><td>worst smoothness</td><td>0.162200</td><td>0.123800</td><td>0.144400</td><td>0.209800</td><td>0.137400</td></tr><tr><td>worst compactness</td><td>0.665600</td><td>0.186600</td><td>0.424500</td><td>0.866300</td><td>0.205000</td></tr><tr><td>worst concavity</td><td>0.711900</td><td>0.241600</td><td>0.450400</td><td>0.686900</td><td>0.400000</td></tr><tr><td>worst concave points</td><td>0.265400</td><td>0.186000</td><td>0.243000</td><td>0.257500</td><td>0.162500</td></tr><tr><td>worst symmetry</td><td>0.460100</td><td>0.275000</td><td>0.361300</td><td>0.663800</td><td>0.236400</td></tr><tr><td>worst fractal dimension</td><td>0.118900</td><td>0.089020</td><td>0.087580</td><td>0.173000</td><td>0.076780</td></tr></tbody></table>
<pre><code class="language-python"># normalizing data
scaler = StandardScaler()
tumor_scaled_arr = scaler.fit_transform(tumor_df)
</code></pre>
<pre><code class="language-python">tumor_scaled_df = pd.DataFrame(
    tumor_scaled_arr, columns=tumor_df.columns
)
tumor_scaled_df.head(5).transpose()
</code></pre>
<table><thead><tr><th></th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>mean radius</td><td>1.097064</td><td>1.829821</td><td>1.579888</td><td>-0.768909</td><td>1.750297</td></tr><tr><td>mean texture</td><td>-2.073335</td><td>-0.353632</td><td>0.456187</td><td>0.253732</td><td>-1.151816</td></tr><tr><td>mean perimeter</td><td>1.269934</td><td>1.685955</td><td>1.566503</td><td>-0.592687</td><td>1.776573</td></tr><tr><td>mean area</td><td>0.984375</td><td>1.908708</td><td>1.558884</td><td>-0.764464</td><td>1.826229</td></tr><tr><td>mean smoothness</td><td>1.568466</td><td>-0.826962</td><td>0.942210</td><td>3.283553</td><td>0.280372</td></tr><tr><td>mean compactness</td><td>3.283515</td><td>-0.487072</td><td>1.052926</td><td>3.402909</td><td>0.539340</td></tr><tr><td>mean concavity</td><td>2.652874</td><td>-0.023846</td><td>1.363478</td><td>1.915897</td><td>1.371011</td></tr><tr><td>mean concave points</td><td>2.532475</td><td>0.548144</td><td>2.037231</td><td>1.451707</td><td>1.428493</td></tr><tr><td>mean symmetry</td><td>2.217515</td><td>0.001392</td><td>0.939685</td><td>2.867383</td><td>-0.009560</td></tr><tr><td>mean fractal dimension</td><td>2.255747</td><td>-0.868652</td><td>-0.398008</td><td>4.910919</td><td>-0.562450</td></tr><tr><td>radius error</td><td>2.489734</td><td>0.499255</td><td>1.228676</td><td>0.326373</td><td>1.270543</td></tr><tr><td>texture error</td><td>-0.565265</td><td>-0.876244</td><td>-0.780083</td><td>-0.110409</td><td>-0.790244</td></tr><tr><td>perimeter error</td><td>2.833031</td><td>0.263327</td><td>0.850928</td><td>0.286593</td><td>1.273189</td></tr><tr><td>area error</td><td>2.487578</td><td>0.742402</td><td>1.181336</td><td>-0.288378</td><td>1.190357</td></tr><tr><td>smoothness error</td><td>-0.214002</td><td>-0.605351</td><td>-0.297005</td><td>0.689702</td><td>1.483067</td></tr><tr><td>compactness error</td><td>1.316862</td><td>-0.692926</td><td>0.814974</td><td>2.744280</td><td>-0.048520</td></tr><tr><td>concavity error</td><td>0.724026</td><td>-0.440780</td><td>0.213076</td><td>0.819518</td><td>0.828471</td></tr><tr><td>concave points error</td><td>0.660820</td><td>0.260162</td><td>1.424827</td><td>1.115007</td><td>1.144205</td></tr><tr><td>symmetry error</td><td>1.148757</td><td>-0.805450</td><td>0.237036</td><td>4.732680</td><td>-0.361092</td></tr><tr><td>fractal dimension error</td><td>0.907083</td><td>-0.099444</td><td>0.293559</td><td>2.047511</td><td>0.499328</td></tr><tr><td>worst radius</td><td>1.886690</td><td>1.805927</td><td>1.511870</td><td>-0.281464</td><td>1.298575</td></tr><tr><td>worst texture</td><td>-1.359293</td><td>-0.369203</td><td>-0.023974</td><td>0.133984</td><td>-1.466770</td></tr><tr><td>worst perimeter</td><td>2.303601</td><td>1.535126</td><td>1.347475</td><td>-0.249939</td><td>1.338539</td></tr><tr><td>worst area</td><td>2.001237</td><td>1.890489</td><td>1.456285</td><td>-0.550021</td><td>1.220724</td></tr><tr><td>worst smoothness</td><td>1.307686</td><td>-0.375612</td><td>0.527407</td><td>3.394275</td><td>0.220556</td></tr><tr><td>worst compactness</td><td>2.616665</td><td>-0.430444</td><td>1.082932</td><td>3.893397</td><td>-0.313395</td></tr><tr><td>worst concavity</td><td>2.109526</td><td>-0.146749</td><td>0.854974</td><td>1.989588</td><td>0.613179</td></tr><tr><td>worst concave points</td><td>2.296076</td><td>1.087084</td><td>1.955000</td><td>2.175786</td><td>0.729259</td></tr><tr><td>worst symmetry</td><td>2.750622</td><td>-0.243890</td><td>1.152255</td><td>6.046041</td><td>-0.868353</td></tr><tr><td>worst fractal dimension</td><td>1.937015</td><td>0.281190</td><td>0.201391</td><td>4.935010</td><td>-0.397100</td></tr></tbody></table>
<h3 id="model-fitting-5">Model Fitting</h3>
<pre><code class="language-python">pca_model = PCA(n_components=2)
pca_results = pca_model.fit_transform(tumor_scaled_df)
</code></pre>
<pre><code class="language-python">print(pca_model.explained_variance_ratio_)
print(np.sum(pca_model.explained_variance_ratio_))
# the two principal components are able to describe
# 63% of the variance in the dataset
# [0.44272026 0.18971182]
# 0.6324320765155945
</code></pre>
<pre><code class="language-python"># adding components to original dataframe
tumor_df[[&#x27;PC1&#x27;,&#x27;PC2&#x27;]] = pca_results
tumor_df[[&#x27;PC1&#x27;,&#x27;PC2&#x27;]].head(5).transpose()
</code></pre>
<table><thead><tr><th></th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>PC1</td><td>9.192837</td><td>2.387802</td><td>5.733896</td><td>7.122953</td><td>3.935302</td></tr><tr><td>PC2</td><td>1.948583</td><td>-3.768172</td><td>-1.075174</td><td>10.275589</td><td>-1.948072</td></tr></tbody></table>
<pre><code class="language-python">plt.figure(figsize=(12,5))
plt.title(&#x27;Principal Component Analysis - Cancer Tumor Dataset&#x27;)
sns.scatterplot(
    data=tumor_df,
    x=&#x27;PC1&#x27;, y=&#x27;PC2&#x27;
)

plt.savefig(&#x27;assets/Scikit_Learn_118.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_118-46f6be28b6a70b40c0789a6c6c16aae4.webp" width="1012" height="470"></p>
<pre><code class="language-python"># get label data from dataset to confirm that we still have
# separably clusters after reducing the dimensions to 2
from sklearn.datasets import load_breast_cancer
</code></pre>
<pre><code class="language-python">tumor_dataset = load_breast_cancer()
tumor_dataset.keys()
# dict_keys([&#x27;data&#x27;, &#x27;target&#x27;, &#x27;frame&#x27;, &#x27;target_names&#x27;, &#x27;DESCR&#x27;, &#x27;feature_names&#x27;, &#x27;filename&#x27;, &#x27;data_module&#x27;])
</code></pre>
<pre><code class="language-python">tumor_dataset[&#x27;target&#x27;]
</code></pre>
<pre><code class="language-python">plt.figure(figsize=(12,5))
plt.title(&#x27;PCA Cancer Tumor Dataset - Coloured by Labels&#x27;)
sns.scatterplot(
    data=tumor_df,
    x=&#x27;PC1&#x27;, y=&#x27;PC2&#x27;,
    hue=tumor_dataset[&#x27;target&#x27;],
    palette=&#x27;winter&#x27;
)

plt.savefig(&#x27;assets/Scikit_Learn_119.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_119-b303474d4ec947d906d35872fe6151ae.webp" width="1012" height="470"></p>
<pre><code class="language-python"># as shown above we get around 63% of the variance explained by using 2 principal components
# since the dataset has 30 features 30 principal components will explain 100% of the variance

explained_variance = []

for n in range(1,31):
    pca = PCA(n_components=n)
    pca.fit(tumor_scaled_df)
    
    explained_variance.append(np.sum(pca.explained_variance_ratio_))
</code></pre>
<pre><code class="language-python">plt.figure(figsize=(10, 5))
plt.title(&#x27;Explained Variance by Number of Principal Components&#x27;)
plt.xlabel(&#x27;Principal Components&#x27;)
sns.set(style=&#x27;darkgrid&#x27;)
sns.barplot(
    data=pd.DataFrame(explained_variance, columns=[&#x27;Explained Variance&#x27;]),
    x=np.arange(1,31),
    y=&#x27;Explained Variance&#x27;
)

plt.savefig(&#x27;assets/Scikit_Learn_120.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_120-471756cb98288f7867a63aafc954c0a6.webp" width="855" height="479"></p>
<h3 id="dataset-2-1">Dataset 2</h3>
<ul>
<li><a href="https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/digits.rst">Digits Dataset</a></li>
</ul>
<p>What handwritten numbers are the hardest to tell apart for a ML Model?</p>
<pre><code class="language-python">digits_df = pd.read_csv(&#x27;datasets/digits.csv&#x27;)
digits_df.head(5).transpose()
</code></pre>
<table><thead><tr><th></th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>pixel_0_0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>pixel_0_1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>pixel_0_2</td><td>5.0</td><td>0.0</td><td>0.0</td><td>7.0</td><td>0.0</td></tr><tr><td>pixel_0_3</td><td>13.0</td><td>12.0</td><td>4.0</td><td>15.0</td><td>1.0</td></tr><tr><td>pixel_0_4</td><td>9.0</td><td>13.0</td><td>15.0</td><td>13.0</td><td>11.0</td></tr><tr><td>...</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>pixel_7_4</td><td>10.0</td><td>16.0</td><td>11.0</td><td>13.0</td><td>16.0</td></tr><tr><td>pixel_7_5</td><td>0.0</td><td>10.0</td><td>16.0</td><td>9.0</td><td>4.0</td></tr><tr><td>pixel_7_6</td><td>0.0</td><td>0.0</td><td>9.0</td><td>0.0</td><td>0.0</td></tr><tr><td>pixel_7_7</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>number_label</td><td>0.0</td><td>1.0</td><td>2.0</td><td>3.0</td><td>4.0</td></tr></tbody></table>
<pre><code class="language-python"># drop label column
X_digits = digits_df.drop(&#x27;number_label&#x27;, axis=1)
digits_labels = digits_df[&#x27;number_label&#x27;]
</code></pre>
<pre><code class="language-python"># select a single images
img_idx = 333
Single_Digit = np.array(X_digits.iloc[img_idx])
Single_Digit.shape
# the images inside the dataset are flattened
# (64,)
</code></pre>
<pre><code class="language-python"># need to be turned back into their 8x8 pixel format
Single_Digit = Single_Digit.reshape((8, 8))
Single_Digit.shape
# (8, 8)
</code></pre>
<pre><code class="language-python"># Display the Image
plt.figure(figsize=(4,4))
plt.imshow(Single_Digit, interpolation=&#x27;nearest&#x27;, cmap=&#x27;plasma&#x27;)
plt.title(&#x27;Digit Label: %d&#x27; % digits_labels[img_idx])
plt.show()
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="data:image/webp;base64,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" width="350" height="378"></p>
<pre><code class="language-python">plt.figure(figsize=(8,6))
plt.title(&#x27;Digit Label: %d&#x27; % digits_labels[0])

sns.heatmap(
    Single_Digit,
    linewidth=0.5,
    cmap=&#x27;plasma_r&#x27;,
    annot=True
)

plt.savefig(&#x27;assets/Scikit_Learn_122.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_122-8b4098cadb5ed13572954e3b64702006.webp" width="630" height="532"></p>
<h4 id="dataset-2-preprocessing">Dataset 2 Preprocessing</h4>
<pre><code class="language-python"># normalize data
scaler = StandardScaler()
digits_scaled = pd.DataFrame(
    scaler.fit_transform(X_digits), columns=X_digits.columns
)
digits_scaled.head(5).transpose()
</code></pre>
<table><thead><tr><th></th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>pixel_0_0</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.000000</td></tr><tr><td>pixel_0_1</td><td>-0.335016</td><td>-0.335016</td><td>-0.335016</td><td>-0.335016</td><td>-0.335016</td></tr><tr><td>pixel_0_2</td><td>-0.043081</td><td>-1.094937</td><td>-1.094937</td><td>0.377661</td><td>-1.094937</td></tr><tr><td>pixel_0_3</td><td>0.274072</td><td>0.038648</td><td>-1.844742</td><td>0.744919</td><td>-2.551014</td></tr><tr><td>pixel_0_4</td><td>-0.664478</td><td>0.268751</td><td>0.735366</td><td>0.268751</td><td>-0.197863</td></tr><tr><td>...</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>pixel_7_3</td><td>0.208293</td><td>-0.249010</td><td>-2.078218</td><td>0.208293</td><td>-2.306869</td></tr><tr><td>pixel_7_4</td><td>-0.366771</td><td>0.849632</td><td>-0.164037</td><td>0.241430</td><td>0.849632</td></tr><tr><td>pixel_7_5</td><td>-1.146647</td><td>0.548561</td><td>1.565686</td><td>0.379040</td><td>-0.468564</td></tr><tr><td>pixel_7_6</td><td>-0.505670</td><td>-0.505670</td><td>1.695137</td><td>-0.505670</td><td>-0.505670</td></tr><tr><td>pixel_7_7</td><td>-0.196008</td><td>-0.196008</td><td>-0.196008</td><td>-0.196008</td><td>-0.196008</td></tr></tbody></table>
<h4 id="model-fitting-6">Model Fitting</h4>
<pre><code class="language-python">pca_model2 = PCA(n_components=2)
pca_results2 = pca_model2.fit_transform(digits_scaled)
print(np.sum(pca_model2.explained_variance_ratio_))
# reducing the number of dimensions from 64 -&gt; 2 leads to 22% explained variance
</code></pre>
<pre><code class="language-python">X_digits[[&#x27;PC1&#x27;,&#x27;PC2&#x27;]] = pca_results2
X_digits[[&#x27;PC1&#x27;,&#x27;PC2&#x27;]].head(5).transpose()
</code></pre>
<table><thead><tr><th></th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>PC1</td><td>1.914264</td><td>0.588997</td><td>1.302144</td><td>-3.020847</td><td>4.528854</td></tr><tr><td>PC2</td><td>-0.954564</td><td>0.924622</td><td>-0.317291</td><td>-0.868696</td><td>-1.093369</td></tr></tbody></table>
<pre><code class="language-python">plt.figure(figsize=(12,5))
plt.title(&#x27;PCA Digits Dataset - Coloured by Labels&#x27;)
sns.scatterplot(
    data=X_digits,
    x=&#x27;PC1&#x27;, y=&#x27;PC2&#x27;,
    hue=digits_labels,
    palette=&#x27;tab20&#x27;
)
plt.legend(bbox_to_anchor=(1.01,1.01))

plt.savefig(&#x27;assets/Scikit_Learn_123.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
# numbers 4 and 7 are very distinct. There is some overlap between 6 and 0 and between 2 and 3
# but you can still get some separation. All the numbers in the middle are &#x27;problematic&#x27; and 
# probably need a larger amount training data.
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_123-4c3ea9cf2802d99c3a1fd05501b6df8c.webp" width="1105" height="479"></p>
<pre><code class="language-python"># how many components would we have to add to reach 80% explained variance
explained_variance = []

for n in range(1,65):
    pca = PCA(n_components=n)
    pca.fit(digits_scaled)
    
    explained_variance.append(np.sum(pca.explained_variance_ratio_))
</code></pre>
<pre><code class="language-python">plt.figure(figsize=(16, 5))
plt.title(&#x27;Explained Variance by Number of Principal Components&#x27;)
plt.xlabel(&#x27;Principal Components&#x27;)
sns.set(style=&#x27;darkgrid&#x27;)
sns.barplot(
    data=pd.DataFrame(explained_variance, columns=[&#x27;Explained Variance&#x27;]),
    x=np.arange(1,65),
    y=&#x27;Explained Variance&#x27;
)

plt.savefig(&#x27;assets/Scikit_Learn_124.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
# we need more than 20 principal components out of 64 to reach 80% expainable variance:
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_124-d07839bd05554dc069fbc3dca48905ae.webp" width="1321" height="479"></p>
<pre><code class="language-python"># rerun the training for 3 components for ~30% explained variance
pca_model3 = PCA(n_components=3)
pca_results3 = pca_model3.fit_transform(digits_scaled)
print(np.sum(pca_model3.explained_variance_ratio_))
# reducing the number of dimensions from 64 -&gt; 3 leads to 30% explained variance
</code></pre>
<pre><code class="language-python">X_digits[[&#x27;PC1&#x27;,&#x27;PC2&#x27;,&#x27;PC3&#x27;]] = pca_results3
X_digits[[&#x27;PC1&#x27;,&#x27;PC2&#x27;,&#x27;PC3&#x27;]].head(5).transpose()
</code></pre>
<table><thead><tr><th></th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>PC1</td><td>1.914213</td><td>0.588981</td><td>1.302030</td><td>-3.020765</td><td>4.528946</td></tr><tr><td>PC2</td><td>-0.954510</td><td>0.924646</td><td>-0.317199</td><td>-0.868788</td><td>-1.093498</td></tr><tr><td>PC3</td><td>-3.945982</td><td>3.924713</td><td>3.023435</td><td>-0.801779</td><td>0.973213</td></tr></tbody></table>
<pre><code class="language-python">%matplotlib notebook
fig = plt.figure(figsize=(8,8))
ax = plt.axes(projection=&#x27;3d&#x27;)
ax.scatter3D(
    xs=X_digits[&#x27;PC1&#x27;],
    ys=X_digits[&#x27;PC2&#x27;],
    zs=X_digits[&#x27;PC3&#x27;],
    c=digits_labels,
    cmap=&#x27;tab20&#x27;
)
ax.set_title(&#x27;PCA Digits Dataset - Coloured by Labels&#x27;)
ax.set(
    xticklabels=[],
    yticklabels=[],
    zticklabels=[],
    xlabel=&#x27;PC1&#x27;,
    ylabel=&#x27;PC2&#x27;,
    zlabel=&#x27;PC3&#x27;,
)

# plt.savefig(&#x27;assets/Scikit_Learn_125.webp&#x27;, bbox_inches=&#x27;tight&#x27;)
</code></pre>
<p><img alt="scikit-learn - Machine Learning in Python" src="/assets/images/Scikit_Learn_125-8bbafee6fc26e79f40a8f8391d83a99e.webp" width="660" height="652"></p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/python">Python</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/s-klearn">SKlearn</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/cheating">Cheating</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Development/Python/2023-05-20-python-sklearn-cheat-sheet/index.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Development/Python/2023-05-28-telco-churn-cohort-study/2023-05-28"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Supervised Learning with Scikit-Learn</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Development/Python/2023-05-18-python-asserts/2023-05-18"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Python Asserts in Data Science Cheat Sheet</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#working-with-missing-values" class="table-of-contents__link toc-highlight">Working with Missing Values</a><ul><li><a href="#missing-indicator" class="table-of-contents__link toc-highlight">Missing Indicator</a></li><li><a href="#simple-imputer" class="table-of-contents__link toc-highlight">Simple Imputer</a></li><li><a href="#drop-missing-data" class="table-of-contents__link toc-highlight">Drop Missing Data</a></li></ul></li><li><a href="#categorical-data-preprocessing" class="table-of-contents__link toc-highlight">Categorical Data Preprocessing</a><ul><li><a href="#ordinal-encoder" class="table-of-contents__link toc-highlight">Ordinal Encoder</a></li><li><a href="#label-encoder" class="table-of-contents__link toc-highlight">Label Encoder</a></li><li><a href="#onehot--encoder" class="table-of-contents__link toc-highlight">OneHot  Encoder</a></li></ul></li><li><a href="#loading-sk-datasets" class="table-of-contents__link toc-highlight">Loading SK Datasets</a><ul><li><a href="#toy-datasets" class="table-of-contents__link toc-highlight">Toy Datasets</a></li><li><a href="#real-world-datasets" class="table-of-contents__link toc-highlight">Real World Datasets</a></li><li><a href="#openml-datasets" class="table-of-contents__link toc-highlight">OpenML Datasets</a></li></ul></li><li><a href="#supervised-learning---regression-models" class="table-of-contents__link toc-highlight">Supervised Learning - Regression Models</a><ul><li><a href="#simple-linear-regression" class="table-of-contents__link toc-highlight">Simple Linear Regression</a></li><li><a href="#elasticnet-regression" class="table-of-contents__link toc-highlight">ElasticNet Regression</a></li><li><a href="#multiple-linear-regression" class="table-of-contents__link toc-highlight">Multiple Linear Regression</a></li></ul></li><li><a href="#supervised-learning---logistic-regression-model" class="table-of-contents__link toc-highlight">Supervised Learning - Logistic Regression Model</a><ul><li><a href="#binary-logistic-regression" class="table-of-contents__link toc-highlight">Binary Logistic Regression</a></li><li><a href="#logistic-regression-pipelines" class="table-of-contents__link toc-highlight">Logistic Regression Pipelines</a></li><li><a href="#pipeline" class="table-of-contents__link toc-highlight">Pipeline</a></li></ul></li><li><a href="#supervised-learning---knn-algorithm" class="table-of-contents__link toc-highlight">Supervised Learning - KNN Algorithm</a><ul><li><a href="#dataset-2" class="table-of-contents__link toc-highlight">Dataset</a></li><li><a href="#data-pre-processing-1" class="table-of-contents__link toc-highlight">Data Pre-processing</a></li><li><a href="#model-fitting-2" class="table-of-contents__link toc-highlight">Model Fitting</a></li></ul></li><li><a href="#supervised-learning---decision-tree-classifier" class="table-of-contents__link toc-highlight">Supervised Learning - Decision Tree Classifier</a><ul><li><a href="#dataset-3" class="table-of-contents__link toc-highlight">Dataset</a></li><li><a href="#preprocessing-1" class="table-of-contents__link toc-highlight">Preprocessing</a></li><li><a href="#model-fitting-3" class="table-of-contents__link toc-highlight">Model Fitting</a></li><li><a href="#evaluation" class="table-of-contents__link toc-highlight">Evaluation</a></li></ul></li><li><a href="#supervised-learning---random-forest-classifier" class="table-of-contents__link toc-highlight">Supervised Learning - Random Forest Classifier</a><ul><li><a href="#dataset-4" class="table-of-contents__link toc-highlight">Dataset</a></li><li><a href="#preprocessing-2" class="table-of-contents__link toc-highlight">Preprocessing</a></li><li><a href="#model-fitting-4" class="table-of-contents__link toc-highlight">Model Fitting</a></li><li><a href="#evaluation-1" class="table-of-contents__link toc-highlight">Evaluation</a></li><li><a href="#random-forest-hyperparameter-tuning" class="table-of-contents__link toc-highlight">Random Forest Hyperparameter Tuning</a></li><li><a href="#random-forest-classifier-1---penguins" class="table-of-contents__link toc-highlight">Random Forest Classifier 1 - Penguins</a></li><li><a href="#random-forest-classifier---banknote-authentication" class="table-of-contents__link toc-highlight">Random Forest Classifier - Banknote Authentication</a></li><li><a href="#random-forest-regressor" class="table-of-contents__link toc-highlight">Random Forest Regressor</a></li></ul></li><li><a href="#supervised-learning---svc-model" class="table-of-contents__link toc-highlight">Supervised Learning - SVC Model</a><ul><li><a href="#dataset-5" class="table-of-contents__link toc-highlight">Dataset</a></li><li><a href="#margin-plots-for-support-vector-classifier" class="table-of-contents__link toc-highlight">Margin Plots for Support Vector Classifier</a></li><li><a href="#grid-search-for-support-vector-classifier" class="table-of-contents__link toc-highlight">Grid Search for Support Vector Classifier</a></li><li><a href="#support-vector-regression" class="table-of-contents__link toc-highlight">Support Vector Regression</a></li><li><a href="#example-task---wine-fraud" class="table-of-contents__link toc-highlight">Example Task - Wine Fraud</a></li></ul></li><li><a href="#supervised-learning---boosting-methods" class="table-of-contents__link toc-highlight">Supervised Learning - Boosting Methods</a><ul><li><a href="#dataset-exploration" class="table-of-contents__link toc-highlight">Dataset Exploration</a></li><li><a href="#adaptive-boosting" class="table-of-contents__link toc-highlight">Adaptive Boosting</a></li><li><a href="#gradient-boosting" class="table-of-contents__link toc-highlight">Gradient Boosting</a></li></ul></li><li><a href="#supervised-learning---naive-bayes-nlp" class="table-of-contents__link toc-highlight">Supervised Learning - Naive Bayes NLP</a><ul><li><a href="#feature-extraction" class="table-of-contents__link toc-highlight">Feature Extraction</a></li><li><a href="#text-classification" class="table-of-contents__link toc-highlight">Text Classification</a></li></ul></li><li><a href="#unsupervised-learning---kmeans-clustering" class="table-of-contents__link toc-highlight">Unsupervised Learning - KMeans Clustering</a><ul><li><a href="#dataset-exploration-2" class="table-of-contents__link toc-highlight">Dataset Exploration</a></li><li><a href="#dataset-preprocessing-1" class="table-of-contents__link toc-highlight">Dataset Preprocessing</a></li><li><a href="#model-training-3" class="table-of-contents__link toc-highlight">Model Training</a></li><li><a href="#choosing-a-k-value" class="table-of-contents__link toc-highlight">Choosing a K Value</a></li><li><a href="#example-1--color-quantization" class="table-of-contents__link toc-highlight">Example 1 : Color Quantization</a></li><li><a href="#example-2--country-clustering" class="table-of-contents__link toc-highlight">Example 2 : Country Clustering</a></li></ul></li><li><a href="#unsupervised-learning---agglomerative-clustering" class="table-of-contents__link toc-highlight">Unsupervised Learning - Agglomerative Clustering</a><ul><li><a href="#dataset-preprocessing-3" class="table-of-contents__link toc-highlight">Dataset Preprocessing</a></li><li><a href="#assigning-cluster-labels" class="table-of-contents__link toc-highlight">Assigning Cluster Labels</a></li></ul></li><li><a href="#unsupervised-learning---density-based-spatial-clustering-dbscan" class="table-of-contents__link toc-highlight">Unsupervised Learning - Density-based Spatial Clustering (DBSCAN)</a><ul><li><a href="#dbscan-vs-kmeans" class="table-of-contents__link toc-highlight">DBSCAN vs KMeans</a></li><li><a href="#dbscan-hyperparameter-tuning" class="table-of-contents__link toc-highlight">DBSCAN Hyperparameter Tuning</a></li><li><a href="#realworld-dataset" class="table-of-contents__link toc-highlight">Realworld Dataset</a></li></ul></li><li><a href="#dimensionality-reduction---principal-component-analysis-pca" class="table-of-contents__link toc-highlight">Dimensionality Reduction - Principal Component Analysis (PCA)</a><ul><li><a href="#dataset-preprocessing-4" class="table-of-contents__link toc-highlight">Dataset Preprocessing</a></li><li><a href="#model-fitting-5" class="table-of-contents__link toc-highlight">Model Fitting</a></li><li><a href="#dataset-2-1" class="table-of-contents__link toc-highlight">Dataset 2</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Research</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Notebook</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/tags">Tags</a></li><li class="footer__item"><a class="footer__link-item" href="/Curriculum-Vitae">CV</a></li></ul></div><div class="col footer__col"><div class="footer__title">Social</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.linkedin.com/in/mike-polinowski-6396ba121/" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/MikePolinowski" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.flickr.com/people/149680084@N06/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Flickr<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/mpolinowski" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Mike Polinowski, INSTAR Deutschland GmbH, Shenzhen - China.</div></div></div></footer></div>
</body>
</html>