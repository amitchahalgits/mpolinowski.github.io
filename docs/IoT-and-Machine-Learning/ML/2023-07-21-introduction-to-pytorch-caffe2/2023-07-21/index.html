<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-IoT-and-Machine-Learning/ML/2023-07-21-introduction-to-pytorch-caffe2/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.0">
<title data-rh="true">Introduction to Caffe2 | Mike Polinowski</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://mpolinowski.github.io/docs/IoT-and-Machine-Learning/ML/2023-07-21-introduction-to-pytorch-caffe2/2023-07-21"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Introduction to Caffe2 | Mike Polinowski"><meta data-rh="true" name="description" content="Deep Learning Framework with Python for flexibility and C++ for speed."><meta data-rh="true" property="og:description" content="Deep Learning Framework with Python for flexibility and C++ for speed."><link data-rh="true" rel="icon" href="/img/icons/favicon-32x32.png"><link data-rh="true" rel="canonical" href="https://mpolinowski.github.io/docs/IoT-and-Machine-Learning/ML/2023-07-21-introduction-to-pytorch-caffe2/2023-07-21"><link data-rh="true" rel="alternate" href="https://mpolinowski.github.io/docs/IoT-and-Machine-Learning/ML/2023-07-21-introduction-to-pytorch-caffe2/2023-07-21" hreflang="en"><link data-rh="true" rel="alternate" href="https://mpolinowski.github.io/docs/IoT-and-Machine-Learning/ML/2023-07-21-introduction-to-pytorch-caffe2/2023-07-21" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Mike Polinowski RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Mike Polinowski Atom Feed">




<link rel="icon" href="/img/angular_momentum.png">
<link rel="manifest" href="/manifest.json">
<meta name="theme-color" content="rgb(37,194,160)">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#000">
<link rel="apple-touch-icon" href="/img/angular_momentum.png">
<link rel="mask-icon" href="/img/angular_momentum.png" color="rgb(33,33,33)">
<meta name="msapplication-TileImage" content="/img/angular_momentum.png">
<meta name="msapplication-TileColor" content="#000">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-P74BDWF0C6"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-P74BDWF0C6",{})</script><link rel="stylesheet" href="/assets/css/styles.08c8c484.css">
<script src="/assets/js/runtime~main.839e810d.js" defer="defer"></script>
<script src="/assets/js/main.9aa3fe7c.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"dark")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/angular_momentum.png" alt="Mike Polinowski :: Dev Notebook" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/angular_momentum.png" alt="Mike Polinowski :: Dev Notebook" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Mike Polinowski</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/docs/tags">Tags</a><a class="navbar__item navbar__link" href="/Search">Search</a><a href="https://mpolinowski.github.io/Personal" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">About<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/mpolinowski" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently dark mode)" aria-label="Switch between dark and light mode (currently dark mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/development">Development</a><button aria-label="Expand sidebar category &#x27;Development&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/devops">DevOps</a><button aria-label="Expand sidebar category &#x27;DevOps&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/docs/category/machine-learning-ai-and-computer-vision">Machine Learning, AI and Computer Vision</a><button aria-label="Collapse sidebar category &#x27;Machine Learning, AI and Computer Vision&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" tabindex="0" href="/docs/category/machine-learning">Machine Learning</a><button aria-label="Collapse sidebar category &#x27;Machine Learning&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-10-01--delib-face-detection/2023-10-01">DLIB Face Recognition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-09-23--yolo8-listen/2023-09-23">Audio Classification with Computer Vision</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-09-21--cvat-automatic-annotation/2023-09-21">CVAT Semi-automatic and Automatic Annotation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-09-19--cvat-computer-vision-annotation-tool/2023-09-19">Computer Vision Annotation Tool (CVAT) Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-09-17--yolo8-nightshift/2023-09-17">YOLOv8 Nightshift</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-09-15--yolo8-tracking-and-ocr/2023-09-15">YOLOv8 License Plate Detection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-09-10--model-explainability-shap/2023-09-11">Scikit-Learn ML Model Explainability</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-09-05--semantic-segmentation-in-opencv/2023-09-05">Using Tensorflow Models in OpenCV</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-09-01--yolo-i-know-flowers/2023-09-01">YOLOv8 Image Classifier</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-31--semantic_segmentation_detectron2_openimages_dataset/2023-08-31">Detectron Object Detection with OpenImages Dataset (WIP)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-30--instance_segmentation_detectron2_model_zoo_mask_rcnn/2023-08-30">Instance Segmentation with PyTorch (Mask RCNN)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-29--semantic-segmentation-detectron2-model-zoo-faster-rcnn/2023-08-29">Image Segmentation with PyTorch (Faster RCNN)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-28--semantic-segmentation-detectron2-model-zoo/2023-08-28">Image Segmentation with PyTorch (RCNN)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-27--image-segmentation-with-pytorch/2023-08-27">Image Segmentation with PyTorch</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-21--pytorch-development-in-docker/2023-08-21">Containerized PyTorch Dev Workflow</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-13-tensorflow-i-know-flowers-model-eval/2023-08-13">Tensorflow Image Classifier - Model Evaluation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-12-tensorflow-i-know-flowers-xception/2023-08-12">Tensorflow Image Classifier - Xception</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-11-tensorflow-i-know-flowers-vit/2023-08-11">Tensorflow Image Classifier - ViT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-10-tensorflow-i-know-flowers-nasnetmobile/2023-08-10">Tensorflow Image Classifier - NASNetMobile</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-09-tensorflow-i-know-flowers-mobilenetv3small/2023-08-09">Tensorflow Image Classifier - MobileNetV3Small</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-08-tensorflow-i-know-flowers-mobilenetv3large/2023-08-08">Tensorflow Image Classifier - MobileNetV3Large</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-07-tensorflow-i-know-flowers-mobilenetv2/2023-08-07">Tensorflow Image Classifier - MobileNetV2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-06-tensorflow-i-know-flowers-inceptionv3/2023-08-06">Tensorflow Image Classifier - InceptionV3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-05-tensorflow-i-know-flowers-efficientnetv2s/2023-08-05">Tensorflow Image Classifier - EfficientNetV2S</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-04-tensorflow-i-know-flowers-efficientnetv2b0/2023-08-04">Tensorflow Image Classifier - EfficientNetV2B0</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-03-tensorflow-i-know-flowers-deit/2023-08-03">Tensorflow Image Classifier - Data-efficient Image Transformers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-02-tensorflow-i-know-flowers-preprocessing/2023-08-02">Tensorflow Image Classifier - Data Pre-processing</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-01-tensorflow-i-know-flowers-intro/2023-08-01">Tensorflow Image Classifier - Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-07-27-tensorflow-vision-transformer/2023-07-27">Tensorflow VITs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-07-26-tensorflow-human-emotion-detector/2023-07-26">Human Emotion Detection with Tensorflow</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-07-25-onnx-models/2023-07-25">Working with ONNX Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-07-21-introduction-to-pytorch-caffe2/2023-07-21">Introduction to Caffe2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-07-02-sql-in-data-science-ml/2023-07-02">SQL in Data Science - Machine Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-06-30-sql-in-data-science-advanced/2023-06-30">SQL in Data Science - Slightly more Advanced Queries</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-06-27-sql-in-data-science-basics/2023-06-27">SQL in Data Science - The Basics using Python</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-06-26-autogluon-transit-photometry-dataset/2023-06-26">Detection of Exoplanets using Transit Photometry</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-04-19-tensorflow-natural-language-processing/2023-04-19">(Re) Introduction to Tensorflow Natural Language Processing</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-04-16-deep-3d-image-segmentation/2023-04-16">3D Image Classification</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-04-14-manifold-learning-for-image-segmentation/2023-04-14">Dimensionality Reduction for Image Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-04-13-fisher-discriminant-analysis/2023-04-13">Fisher Linear Discriminant Analysis (LDA)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-04-13-isometric-mapping/2023-04-13">Isometric Mapping (ISOMAP)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-04-13-multi-dimensional-scaling/2023-04-13">Multidimensional Scaling (MDS)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-04-12-tstochastic-neighbor-embedding/2023-04-12">tStochastic Neighbor Embedding (t-SNE)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-04-11-locally-linear-embedding/2023-04-11">Locally Linear Embedding (LLE)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-04-09-principal-component-analysis/2023-04-09">Principal Component Analysis (PCA)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-03-26-tensorflow-unsupervised-learning-generative-adversial-networks/2023-03-26">Tensorflow 2 - Unsupervised Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-03-26-tensorflow-unsupervised-learning-autoencoders-super-resolution/2023-03-26">Tensorflow 2 - Unsupervised Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-03-24-tensorflow-unsupervised-learning-autoencoders/2023-03-24">Tensorflow 2 - Unsupervised Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-03-16-tensorflow-transfer-learning-scaling/2023-03-16">Tensorflow 2 - Transfer Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-03-11-tensorflow-transfer-learning-fine-tuning/2023-03-11">Tensorflow 2 - Transfer Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-03-06-tensorflow-transfer-learning-feature-extraction/2023-03-06">Tensorflow 2 - Transfer Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-03-05-tensorflow-convolutional-neural-network-multiclass-classifications/2023-03-05">Tensorflow 2 - Convolutional Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-03-03-tensorflow-convolutional-neural-network-binary-classifications/2023-03-03">Tensorflow 2 - Convolutional Neural Networks</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-03-02-tensorflow-neural-network-multi-classification/2023-03-02">Tensorflow 2 - Neural Network Classifications</a><button aria-label="Expand sidebar category &#x27;Tensorflow 2 - Neural Network Classifications&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-28-tensorflow-neural-network-classification-model-evaluation/2023-02-28">Tensorflow 2 - Neural Network Classification</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-27-tensorflow-neural-network-classification/2023-02-27">Tensorflow 2 - Neural Network Classification</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-26-tensorflow-neural-network-regression-data-preprocessing/2023-02-26">Tensorflow 2 - Neural Network Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-26-tensorflow-neural-network-regression-real-dataset/2023-02-26">Tensorflow 2 - Neural Network Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-25-tensorflow-neural-network-regression-experiments/2023-02-25">Tensorflow 2 - Neural Network Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-24-tensorflow-neural-network-regression-evaluation/2023-02-24">Tensorflow 2 - Neural Network Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-23-tensorflow-neural-network-regression/2023-02-23">Tensorflow 2 - Neural Network Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-22-tensorflow-tensors-3/2023-02-22">Tensorflow 2 - An (Re)Introduction 2023 (3)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-21-tensorflow-tensors-2/2023-02-21">Tensorflow 2 - An (Re)Introduction 2023 (2)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-19-tensorflow-introduction/2023-02-19">Tensorflow 2 - An (Re)Introduction 2023</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-vgg16/2023-02-18">Keras for Tensorflow - VGG16 Network Architecture</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-rnn/2023-02-18">Keras for Tensorflow - Recurrent Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-17-keras-introduction-cnn/2023-02-17">Keras for Tensorflow - Convolutional Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-16-keras-introduction-ann/2023-02-16">Keras for Tensorflow - Artificial Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-15-as-one-yolo-object-tracking/2023-02-15">YOLOv8 with AS-One</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-14-keras-introduction/2023-02-14">Keras for Tensorflow - An (Re)Introduction 2023</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-01-30-predicting-wine-quality/2023-01-30">SciKit Wine Quality</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-01-28-opencv-coin-counter/2023-01-28">OpenCV Count My Money</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-01-14-yolov7_to_tensorflow/2023-01-14">YOLOv7 to Tensorflow</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-01-13-yolov7_data_conversion/2023-01-13">YOLOv7 Label Conversion</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-01-10-yolov7_custom_data/2023-01-10">YOLOv7 Training with Custom Data</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-01-08-depth-vision-midas/2023-01-08">MiDaS Depth Vision</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-01-05-yolov7/2023-01-05">YOLOv7 Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-31-tf-rnn-text-generation/2022-12-31">Recurrent Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-28-tf-gan-image-generator/2022-12-28">Deep Convolutional Generative Adversarial Network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-21-tf-downsampling/2022-12-21">Tensorflow Downsampling</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-21-tf-deepdream/2022-12-21">Tensorflow Deep Dream</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-20-tf-representation/2022-12-19">Tensorflow Representation Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-19-tf-hub/2022-12-19">Tensorflow Hub</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-18-tf-transfer-learning/2022-12-18">Tensorflow Transfer Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-16-tf-cifar/2022-12-16">Tensorflow Image Classification</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6/2022-12-12">Breast Histopathology Image Segmentation Part 6</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part5/2022-12-12">Breast Histopathology Image Segmentation Part 5</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part4/2022-12-11">Breast Histopathology Image Segmentation Part 4</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part3/2022-12-11">Breast Histopathology Image Segmentation Part 3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part2/2022-12-11">Breast Histopathology Image Segmentation Part 2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-10-tf-breast-cancer-classification-part1/2022-12-10">Breast Histopathology Image Segmentation Part 1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-11-27-containerized-deep-learning/2022-11-27">Deep Docker on Arch</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-04-04-pytorch-face-restoration/2022-04-04">Face Restoration with GFPGAN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-04-03-pytorch-real-super-resolution/2022-04-03">Super Resolution with Real-ESRGAN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-04-02-pytorch-super-resolution/2022-04-02">Super Resolution with ESRGAN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-04-01-tensorflow-audio-classifier/2022-04-01">Deep Audio</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-02-20--yolo-app-yolov5-data-prep/2022-02-20">Yolo App - YOLOv5 Data Preparation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-02-19--yolo-app-flask/2022-02-19">Yolo App - Flask Web Application</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-02-18--yolo-app-ocr/2022-02-18">Yolo App - Tesseract Optical Character Recognition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-02-17--yolo-app-prediction-pipeline/2022-02-17">Yolo App - Pipeline Predictions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-02-16--yolo-app-tensorflow-model/2022-02-16">Yolo App - Train a Model with Tensorflow</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-02-15--yolo-app-get-data/2022-02-15">Yolo App - Data Collection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-12-10--opencv-optical-flow-tracking/2021-12-10">OpenCV Optical Flow Algorithm for Object Tracking</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-12-09--opencv-camshift-tracking/2021-12-09">OpenCV CAMshift Algorithm for Object Tracking</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-12-08--opencv-meanshift-tracking/2021-12-08">OpenCV Meanshift Algorithm for Object Tracking</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-12-07--opencv-detection-and-tracking/2021-12-07">OpenCV Object Detection and Tracking</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-12-06--opencv-object-tracking/2021-12-06">OpenCV Object Tracking</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-12-05--opencv-face-detection/2021-12-05">OpenCV Face Detection and Privacy</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-12-04--opencv-image-objects/2021-12-04">OpenCV Image Objects</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-12-03--opencv-image-operations/2021-12-03">OpenCV Image Operations</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-12-02--opencv-with-videos/2021-12-02">OpenCV, Streams and Video Files</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-12-01--opencv-with-images/2021-12-01">OpenCV and Images</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-15--facebook-prophet-introduction/2021-11-15">Introduction into FB Prophet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-14--tensorflow-model-for-tfjs/2021-11-14">Tensorflow.js React App</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-13--tensorflow-model-zoo/2021-11-13">Tensorflow2 Model Zoo</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-12--tensorflow-crash-course-part-v/2021-11-12">Tensorflow2 Crash Course - Part V</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-11--tensorflow-crash-course-part-iv/2021-11-11">Tensorflow2 Crash Course - Part IV</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-10--tensorflow-crash-course-part-iii/2021-11-10">Tensorflow2 Crash Course - Part III</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-09--tensorflow-crash-course-part-ii/2021-11-09">Tensorflow2 Crash Course - Part II</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-08--tensorflow-crash-course-part-i/2021-11-08">Tensorflow Crash Course - Part I</a><button aria-label="Expand sidebar category &#x27;Tensorflow Crash Course - Part I&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-07--opencv-crash-course-part-ii/2021-11-07">OpenCV Crash Course Part II</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-06--opencv-crash-course-part-i/2021-11-06">OpenCV Crash Course Part I</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-05--license-plates-yolov4-opencv-tesseract/2021-11-05">License Plate Recognition with YOLOv4, OpenCV and Tesseract</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-04--installing-yolov4/2021-11-04">Installing YOLOv4 with Anaconda</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-03--streamlit-opencv-mediapipe/2021-11-03">Streamlit user interface for openCV/Mediapipe face mesh app</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-02--spacy_ner_predictions/2021-11-02">spaCy NER Predictions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-01--spacy_natural_language_processing/2021-11-01">spaCy NER on Arch Linux</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2019-04-01--introduction-to-keras/2019-04-01">Introduction to Keras</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-10-31--tesseract_ocr_arch_linux/2021-10-31">Tesseract OCR on Arch Linux</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2019-03-31--introduction-to-tensorflow-2-beta/2019-03-31">Introduction to TensorFlow 2 Beta</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2018-01-02--machine-learning-with-python/2018-01-02">Machine Learning with SciKit Learn</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/category/aiops">AIOps</a><button aria-label="Expand sidebar category &#x27;AIOps&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/automation-deep-vision-and-robotics">Automation, Deep Vision and Robotics</a><button aria-label="Expand sidebar category &#x27;Automation, Deep Vision and Robotics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/category/machine-learning-ai-and-computer-vision"><span itemprop="name">Machine Learning, AI and Computer Vision</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/category/machine-learning"><span itemprop="name">Machine Learning</span></a><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Introduction to Caffe2</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><p><img alt="Guangzhou, China" src="/assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-5a33ed1aeac871d5b7a7594cc7d702c8.jpg" width="1500" height="581"></p>
<h1 id="introduction-to-caffe2">Introduction to Caffe2</h1>
<ul>
<li><a href="#introduction-to-caffe2">Introduction to Caffe2</a>
<ul>
<li><a href="#setup-with-docker">Setup with Docker</a>
<ul>
<li><a href="#testing-installation">Testing Installation</a></li>
</ul>
</li>
<li><a href="#caffe-tutorial">Caffe Tutorial</a>
<ul>
<li><a href="#caffe2-basic-concepts---operators--nets">Caffe2 Basic Concepts - Operators &amp; Nets</a>
<ul>
<li><a href="#workspaces">Workspaces</a></li>
<li><a href="#operators">Operators</a></li>
<li><a href="#nets">Nets</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#loading-pre-trained-models">Loading Pre-Trained Models</a>
<ul>
<li><a href="#description">Description</a></li>
<li><a href="#code">Code</a>
<ul>
<li><a href="#inputs">Inputs</a></li>
<li><a href="#setup-paths">Setup paths</a></li>
<li><a href="#image-preprocessing">Image Preprocessing</a></li>
</ul>
</li>
<li><a href="#prepare-the-cnn-and-run-the-net">Prepare the CNN and run the net!</a>
<ul>
<li><a href="#process-results">Process Results</a></li>
<li><a href="#feeding-larger-batches">Feeding Larger Batches</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#loading-datasets">Loading Datasets</a></li>
<li><a href="#image-loading-and-preprocessing">Image Loading and Preprocessing</a>
<ul>
<li><a href="#caffe-uses-bgr-order">Caffe Uses BGR Order</a></li>
<li><a href="#caffe-prefers-chw-order">Caffe Prefers CHW Order</a></li>
<li><a href="#rotation-and-mirroring">Rotation and Mirroring</a></li>
<li><a href="#sizing">Sizing</a></li>
<li><a href="#rescaling">Rescaling</a></li>
<li><a href="#cropping">Cropping</a></li>
<li><a href="#upscaling">Upscaling</a></li>
<li><a href="#batch-processing">Batch Processing</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><a href="https://github.com/mpolinowski/morning-caffe2">Github Repository</a></p>
<h2 id="setup-with-docker">Setup with Docker</h2>
<p>There <a href="https://hub.docker.com/r/caffe2ai/caffe2">several images available</a> with and without GPU support - the image tagged <code>latest</code> comes with everything included:</p>
<pre><code class="language-bash">docker pull caffe2ai/caffe2:latest

docker run -it -v /opt/caffe:/home -p 8888:8888 caffe2ai/caffe2:latest jupyter notebook --no-browser --ip=0.0.0.0 --port=8888 --allow-root /home
</code></pre>
<p>Make sure that <code>/opt/caffe</code> exists and can be written into by the Docker user.</p>
<pre><code class="language-bash">Copy/paste this URL into your browser when you connect for the first time,
to login with a token: http://0.0.0.0:8888/?token=9346cde0b9a37cda784d193e4e03a18c760847ace645f6cb
</code></pre>
<p>You can now access the <strong>Jupyter Notebook</strong> on your servers IP address on port <code>8888</code> with the generated token above.</p>
<h3 id="testing-installation">Testing Installation</h3>
<p>Create a new notebook and verify that that Caffe2 is up and running:</p>
<pre><code class="language-py">from caffe2.python import workspace
import numpy as np
print (&quot;Creating random data&quot;)
data = np.random.rand(3, 2)
print(data)
print (&quot;Adding data to workspace ...&quot;)
workspace.FeedBlob(&quot;mydata&quot;, data)
print (&quot;Retrieving data from workspace&quot;)
mydata = workspace.FetchBlob(&quot;mydata&quot;)
print(mydata) 
</code></pre>
<p><img alt="Introduction to Caffe2" src="/assets/images/introduction-to-pytorch-caffe_01-52365d331443f5232285f1dd260c1cd1.png" width="1057" height="702"></p>
<p>It works!</p>
<h2 id="caffe-tutorial">Caffe Tutorial</h2>
<pre><code class="language-bash">cd /opt/caffe
git clone --recursive https://github.com/caffe2/tutorials caffe2_tutorials
</code></pre>
<p><img alt="Introduction to Caffe2" src="/assets/images/introduction-to-pytorch-caffe_02-a60006288066565368d6e220f7df93a4.png" width="999" height="367"></p>
<h3 id="caffe2-basic-concepts---operators--nets">Caffe2 Basic Concepts - Operators &amp; Nets</h3>
<p>In this tutorial we will go through a set of Caffe2 basics: the basic concepts including how operators and nets are being written.</p>
<p>First, let&#x27;s import Caffe2. <code>core</code> and <code>workspace</code> are usually the two that you need most. If you want to manipulate protocol buffers generated by Caffe2, you probably also want to import <code>caffe2_pb2</code> from <code>caffe2.proto</code>.</p>
<pre><code class="language-python">from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

# We&#x27;ll also import a few standard python libraries
from matplotlib import pyplot
import numpy as np
import time

# These are the droids you are looking for.
from caffe2.python import core, workspace
from caffe2.proto import caffe2_pb2

# Let&#x27;s show all plots inline.
%matplotlib inline
</code></pre>
<p>You might see a warning saying that caffe2 does not have GPU support. That means you are running a CPU-only build. Don&#x27;t be alarmed - anything CPU is still runnable without a problem.</p>
<h4 id="workspaces">Workspaces</h4>
<p>Let&#x27;s cover workspaces first, where all the data resides.</p>
<p>Similar to Matlab, the Caffe2 workspace consists of blobs you create and store in memory. For now, consider a blob to be a N-dimensional Tensor similar to numpy&#x27;s ndarray, but contiguous. Down the road, we will show you that a blob is actually a typed pointer that can store any type of C++ objects, but Tensor is the most common type stored in a blob. Let&#x27;s show what the interface looks like.</p>
<p><code>Blobs()</code> prints out all existing blobs in the workspace.
<code>HasBlob()</code> queries if a blob exists in the workspace. As of now, we don&#x27;t have any.</p>
<pre><code class="language-python">print(&quot;Current blobs in the workspace: {}&quot;.format(workspace.Blobs()))
print(&quot;Workspace has blob &#x27;X&#x27;? {}&quot;.format(workspace.HasBlob(&quot;X&quot;)))
</code></pre>
<p>We can feed blobs into the workspace using <code>FeedBlob()</code>.</p>
<pre><code class="language-python">X = np.random.randn(2, 3).astype(np.float32)
print(&quot;Generated X from numpy:\n{}&quot;.format(X))
workspace.FeedBlob(&quot;X&quot;, X)
</code></pre>
<p>Now, let&#x27;s take a look at what blobs are in the workspace.</p>
<pre><code class="language-python">print(&quot;Current blobs in the workspace: {}&quot;.format(workspace.Blobs()))
print(&quot;Workspace has blob &#x27;X&#x27;? {}&quot;.format(workspace.HasBlob(&quot;X&quot;)))
print(&quot;Fetched X:\n{}&quot;.format(workspace.FetchBlob(&quot;X&quot;)))
</code></pre>
<p>Let&#x27;s verify that the arrays are equal.</p>
<pre><code class="language-python">np.testing.assert_array_equal(X, workspace.FetchBlob(&quot;X&quot;))
</code></pre>
<p>Note that if you try to access a blob that does not exist, an error will be thrown:</p>
<pre><code class="language-python">try:
    workspace.FetchBlob(&quot;invincible_pink_unicorn&quot;)
except RuntimeError as err:
    print(err)
</code></pre>
<p>One thing that you might not use immediately: you can have multiple workspaces in Python using different names, and switch between them. Blobs in different workspaces are separate from each other. You can query the current workspace using <code>CurrentWorkspace</code>. Let&#x27;s try switching the workspace by name (gutentag) and creating a new one if it doesn&#x27;t exist.</p>
<pre><code class="language-python">print(&quot;Current workspace: {}&quot;.format(workspace.CurrentWorkspace()))
print(&quot;Current blobs in the workspace: {}&quot;.format(workspace.Blobs()))

# Switch the workspace. The second argument &quot;True&quot; means creating 
# the workspace if it is missing.
workspace.SwitchWorkspace(&quot;gutentag&quot;, True)

# Let&#x27;s print the current workspace. Note that there is nothing in the
# workspace yet.
print(&quot;Current workspace: {}&quot;.format(workspace.CurrentWorkspace()))
print(&quot;Current blobs in the workspace: {}&quot;.format(workspace.Blobs()))
</code></pre>
<p>Let&#x27;s switch back to the default workspace.</p>
<pre><code class="language-python">workspace.SwitchWorkspace(&quot;default&quot;)
print(&quot;Current workspace: {}&quot;.format(workspace.CurrentWorkspace()))
print(&quot;Current blobs in the workspace: {}&quot;.format(workspace.Blobs()))
</code></pre>
<p>Finally, <code>ResetWorkspace()</code> clears anything that is in the current workspace.</p>
<pre><code class="language-python">workspace.ResetWorkspace()
print(&quot;Current blobs in the workspace after reset: {}&quot;.format(workspace.Blobs()))
</code></pre>
<h4 id="operators">Operators</h4>
<p>Operators in Caffe2 are kind of like functions. From the C++ side, they all derive from a common interface, and are registered by type, so that we can call different operators during runtime. The interface of operators is defined in <code>caffe2/proto/caffe2.proto</code>. Basically, it takes in a bunch of inputs, and produces a bunch of outputs.</p>
<p>Remember, when we say &quot;create an operator&quot; in Caffe2 Python, nothing gets run yet. All it does is create the protocol buffer that specifies what the operator should be. At a later time it will be sent to the C++ backend for execution. If you are not familiar with protobuf, it is a json-like serialization tool for structured data. Find more about protocol buffers <a href="https://developers.google.com/protocol-buffers/">here</a>.</p>
<p>Let&#x27;s see an actual example.</p>
<pre><code class="language-python"># Create an operator.
op = core.CreateOperator(
    &quot;Relu&quot;, # The type of operator that we want to run
    [&quot;X&quot;], # A list of input blobs by their names
    [&quot;Y&quot;], # A list of output blobs by their names
)
# and we are done!
</code></pre>
<p>As we mentioned, the created op is actually a protobuf object. Let&#x27;s show the content.</p>
<pre><code class="language-python">print(&quot;Type of the created op is: {}&quot;.format(type(op)))
print(&quot;Content:\n&quot;)
print(str(op))
</code></pre>
<p>Ok, let&#x27;s run the operator. We first feed the input X to the workspace.
Then the simplest way to run an operator is to do <code>workspace.RunOperatorOnce(operator)</code></p>
<pre><code class="language-python">workspace.FeedBlob(&quot;X&quot;, np.random.randn(2, 3).astype(np.float32))
workspace.RunOperatorOnce(op)
</code></pre>
<p>After execution, let&#x27;s see if the operator is doing the right thing.</p>
<p>In this case, the operator is a common activation function used in neural networks, called [ReLU](<a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">https://en.wikipedia.org/wiki/Rectifier_(neural_networks)</a>, or Rectified Linear Unit activation. ReLU activation helps to add necessary non-linear characteristics to the neural network classifier, and is defined as:</p>
<p>$$ReLU(x) = max(0, x)$$</p>
<pre><code class="language-python">print(&quot;Current blobs in the workspace: {}\n&quot;.format(workspace.Blobs()))
print(&quot;X:\n{}\n&quot;.format(workspace.FetchBlob(&quot;X&quot;)))
print(&quot;Y:\n{}\n&quot;.format(workspace.FetchBlob(&quot;Y&quot;)))
print(&quot;Expected:\n{}\n&quot;.format(np.maximum(workspace.FetchBlob(&quot;X&quot;), 0)))
</code></pre>
<p>This is working if your Expected output matches your Y output in this example.</p>
<p>Operators also take optional arguments if needed. They are specified as key-value pairs. Let&#x27;s take a look at one simple example, which takes a tensor and fills it with Gaussian random variables.</p>
<pre><code class="language-python">op = core.CreateOperator(
    &quot;GaussianFill&quot;,
    [], # GaussianFill does not need any parameters.
    [&quot;Z&quot;],
    shape=[100, 100], # shape argument as a list of ints.
    mean=1.0,  # mean as a single float
    std=1.0, # std as a single float
)
print(&quot;Content of op:\n&quot;)
print(str(op))
</code></pre>
<p>Let&#x27;s run it and see if things are as intended.</p>
<pre><code class="language-python">workspace.RunOperatorOnce(op)
temp = workspace.FetchBlob(&quot;Z&quot;)
pyplot.hist(temp.flatten(), bins=50)
pyplot.title(&quot;Distribution of Z&quot;)
</code></pre>
<p>If you see a bell shaped curve then it worked!</p>
<h4 id="nets">Nets</h4>
<p>Nets are essentially computation graphs. We keep the name <code>Net</code> for backward consistency (and also to pay tribute to neural nets). A Net is composed of multiple operators just like a program written as a sequence of commands. Let&#x27;s take a look.</p>
<p>When we talk about nets, we will also talk about BlobReference, which is an object that wraps around a string so we can do easy chaining of operators.</p>
<p>Let&#x27;s create a network that is essentially the equivalent of the following python math:</p>
<pre><code>X = np.random.randn(2, 3)
W = np.random.randn(5, 3)
b = np.ones(5)
Y = X * W^T + b
</code></pre>
<p>We&#x27;ll show the progress step by step. Caffe2&#x27;s <code>core.Net</code> is a wrapper class around a NetDef protocol buffer.</p>
<p>When creating a network, its underlying protocol buffer is essentially empty other than the network name. Let&#x27;s create the net and then show the proto content.</p>
<pre><code class="language-python">net = core.Net(&quot;my_first_net&quot;)
print(&quot;Current network proto:\n\n{}&quot;.format(net.Proto()))
</code></pre>
<p>Let&#x27;s create a blob called X, and use GaussianFill to fill it with some random data.</p>
<pre><code class="language-python">X = net.GaussianFill([], [&quot;X&quot;], mean=0.0, std=1.0, shape=[2, 3], run_once=0)
print(&quot;New network proto:\n\n{}&quot;.format(net.Proto()))
</code></pre>
<p>You might have observed a few differences from the earlier <code>core.CreateOperator</code> call. Basically, when using a net, you can directly create an operator <em>and</em> add it to the net at the same time by calling <code>net.SomeOp</code> where SomeOp is a registered type string of an operator. This gets translated to:</p>
<pre><code class="language-python">op = core.CreateOperator(&quot;SomeOp&quot;, ...)
net.Proto().op.append(op)
</code></pre>
<p>Also, you might be wondering what X is. X is a <code>BlobReference</code> which records two things:</p>
<ul>
<li>
<p>The blob&#x27;s name, which is accessed with <code>str(X)</code></p>
</li>
<li>
<p>The net it got created from, which is recorded by the internal variable <code>_from_net</code></p>
</li>
</ul>
<p>Let&#x27;s verify it. Also, remember, we are not actually running anything yet, so X contains nothing but a symbol. Don&#x27;t expect to get any numerical values out of it right now :)</p>
<pre><code class="language-python">print(&quot;Type of X is: {}&quot;.format(type(X)))
print(&quot;The blob name is: {}&quot;.format(str(X)))
</code></pre>
<p>Let&#x27;s continue to create W and b.</p>
<pre><code class="language-python">W = net.GaussianFill([], [&quot;W&quot;], mean=0.0, std=1.0, shape=[5, 3], run_once=0)
b = net.ConstantFill([], [&quot;b&quot;], shape=[5,], value=1.0, run_once=0)
</code></pre>
<p>Now, one simple code sugar: since the BlobReference objects know what net it is generated from, in addition to creating operators from net, you can also create operators from BlobReferences. Let&#x27;s create the FC operator in this way.</p>
<pre><code class="language-python">Y = X.FC([W, b], [&quot;Y&quot;])
</code></pre>
<p>Under the hood, <code>X.FC(...)</code> simply delegates to <code>net.FC</code> by inserting <code>X</code> as the first input of the corresponding operator, so what we did above is equivalent to</p>
<pre><code class="language-python">Y = net.FC([X, W, b], [&quot;Y&quot;])
</code></pre>
<p>Let&#x27;s take a look at the current network.</p>
<pre><code class="language-python">print(&quot;Current network proto:\n\n{}&quot;.format(net.Proto()))
</code></pre>
<p>Too verbose huh? Let&#x27;s try to visualize it as a graph. Caffe2 ships with a very minimal graph visualization tool for this purpose.</p>
<pre><code class="language-python">from caffe2.python import net_drawer
from IPython import display
graph = net_drawer.GetPydotGraph(net, rankdir=&quot;LR&quot;)
display.Image(graph.create_png(), width=800)
</code></pre>
<p>So we have defined a Net, but nothing has been executed yet. Remember that the net above is essentially a protobuf that holds the definition of the network. When we actually run the network, what happens under the hood is:</p>
<ul>
<li>A C++ net object is instantiated from the protobuf</li>
<li>The instantiated net&#x27;s Run() function is called</li>
</ul>
<p>Before we do anything, we should clear any earlier workspace variables with <code>ResetWorkspace()</code>.</p>
<p>Then there are two ways to run a net from Python. We will do the first option in the example below.</p>
<ol>
<li>Call <code>workspace.RunNetOnce()</code>, which instantiates, runs and immediately destructs the network</li>
<li>Call <code>workspace.CreateNet()</code> to create the C++ net object owned by the workspace, then call <code>workspace.RunNet()</code>, passing the name of the network to it</li>
</ol>
<pre><code class="language-python">workspace.ResetWorkspace()
print(&quot;Current blobs in the workspace: {}&quot;.format(workspace.Blobs()))
workspace.RunNetOnce(net)
print(&quot;Blobs in the workspace after execution: {}&quot;.format(workspace.Blobs()))
# Let&#x27;s dump the contents of the blobs
for name in workspace.Blobs():
    print(&quot;{}:\n{}&quot;.format(name, workspace.FetchBlob(name)))
</code></pre>
<p>Now let&#x27;s try the second way to create the net, and run it. First, clear the variables with <code>ResetWorkspace()</code>. Then create the net with the workspace&#x27;s <code>net</code> object that we created earlier using <code>CreateNet(net_object)</code>. Finally, run the net with <code>RunNet(net_name)</code>.</p>
<pre><code class="language-python">workspace.ResetWorkspace()
print(&quot;Current blobs in the workspace: {}&quot;.format(workspace.Blobs()))
workspace.CreateNet(net)
workspace.RunNet(net.Proto().name)
print(&quot;Blobs in the workspace after execution: {}&quot;.format(workspace.Blobs()))
for name in workspace.Blobs():
    print(&quot;{}:\n{}&quot;.format(name, workspace.FetchBlob(name)))
</code></pre>
<p>There are a few differences between <code>RunNetOnce</code> and <code>RunNet</code>, but the main difference is the computational overhead. Since <code>RunNetOnce</code> involves serializing the protobuf to pass between Python and C and instantiating the network, it may take longer to run. Let&#x27;s run a test and see what the time overhead is.</p>
<pre><code class="language-python"># It seems that %timeit magic does not work well with
# C++ extensions so we&#x27;ll basically do for loops
start = time.time()
for i in range(1000):
    workspace.RunNetOnce(net)
end = time.time()
print(&#x27;Run time per RunNetOnce: {}&#x27;.format((end - start) / 1000))

start = time.time()
for i in range(1000):
    workspace.RunNet(net.Proto().name)
end = time.time()
print(&#x27;Run time per RunNet: {}&#x27;.format((end - start) / 1000))
</code></pre>
<p>Congratulations, you now know the many of the key components of the Caffe2 Python API! Ready for more Caffe2? Check out the rest of the tutorials for a variety of interesting use-cases!</p>
<h2 id="loading-pre-trained-models">Loading Pre-Trained Models</h2>
<h3 id="description">Description</h3>
<p>In this tutorial, we will use the pre-trained <code>squeezenet</code> model from the <a href="https://github.com/caffe2/caffe2/wiki/Model-Zoo">ModelZoo</a> to classify our own images. As input, we will provide the path (or URL) to an image we want to classify. It will also be helpful to know the <a href="https://gist.githubusercontent.com/aaronmarkham/cd3a6b6ac071eca6f7b4a6e40e6038aa/raw/9edb4038a37da6b5a44c3b5bc52e448ff09bfe5b/alexnet_codes">ImageNet object code</a> for the image so we can verify our results. The &#x27;object code&#x27; is nothing more than the integer label for the class used during training, for example &quot;985&quot; is the code for the class &quot;daisy&quot;. Note, although we are using squeezenet here, this tutorial serves as a somewhat universal method for running inference on pretrained models.</p>
<p>Note, assuming the last layer of the network is a softmax layer, the results come back as a multidimensional array of probabilities with length equal to the number of classes that the model was trained on. The probabilities may be indexed by the object code (integer type), so if you know the object code you can index the results array at that index to view the network&#x27;s confidence that the input image is of that class.</p>
<p><strong>Model Download Options</strong></p>
<p>Although we will use <code>squeezenet</code> here, you can check out the <a href="https://github.com/caffe2/caffe2/wiki/Model-Zoo">Model Zoo for pre-trained models</a> to browse/download a variety of pretrained models, or you can use Caffe2&#x27;s <code>caffe2.python.models.download</code> module to easily acquire pre-trained models from <a href="http://github.com/caffe2/models">Github caffe2/models</a>.</p>
<p>For our purposes, we will use the <code>models.download</code> module to download <code>squeezenet</code> into the <code>/caffe2/python/models</code> folder of our local Caffe2 installation with the following command:</p>
<pre><code>python -m caffe2.python.models.download -i squeezenet
</code></pre>
<p><strong>Update</strong>: The repository has been archived - I am manually downloading this into the container <code>/caffe2/caffe2/python/models/squeezenet</code>:</p>
<ul>
<li><code>wget https://github.com/facebookarchive/models/raw/master/squeezenet/init_net.pb</code></li>
<li><code>wget https://github.com/facebookarchive/models/raw/master/squeezenet/predict_net.pb</code></li>
<li><code>wget https://github.com/facebookarchive/models/raw/master/squeezenet/value_info.json</code></li>
</ul>
<p>If the above download worked then you should have a directory named squeezenet in your <code>/caffe2/python/models</code> folder that contains <code>init_net.pb</code> and <code>predict_net.pb</code>. Note, if you do not use the <code>-i</code> flag, the model will be downloaded to your CWD, however it will still be a directory named squeezenet containing two protobuf files. Alternatively, if you wish to download all of the models, you can clone the entire repo using:</p>
<pre><code>git clone https://github.com/caffe2/models
</code></pre>
<h3 id="code">Code</h3>
<p>Before we start, lets take care of the required imports.</p>
<pre><code class="language-python">from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals
%matplotlib inline
from caffe2.proto import caffe2_pb2
import numpy as np
import skimage.io
import skimage.transform
from matplotlib import pyplot
import os
from caffe2.python import core, workspace, models
import urllib2
import operator
print(&quot;Required modules imported.&quot;)
</code></pre>
<pre><code class="language-bash">!mkdir -p /caffe2/caffe2/python/models/squeezenet

!wget https://github.com/facebookarchive/models/raw/master/squeezenet/init_net.pb -P /caffe2/caffe2/python/models/squeezenet
!wget https://github.com/facebookarchive/models/raw/master/squeezenet/predict_net.pb -P /caffe2/caffe2/python/models/squeezenet
!wget https://github.com/facebookarchive/models/raw/master/squeezenet/value_info.json -P /caffe2/caffe2/python/models/squeezenet

!ls -la /caffe2/caffe2/python/models/squeezenet
</code></pre>
<pre><code class="language-bash">total 6052
drwxr-xr-x 1 root root      80 Jul 24 06:17 .
drwxr-xr-x 1 root root      20 Jul 24 06:13 ..
-rw-r--r-- 1 root root 6181001 Jul 24 06:15 init_net.pb
-rw-r--r-- 1 root root    6175 Jul 24 06:15 predict_net.pb
-rw-r--r-- 1 root root      32 Jul 24 06:17 value_info.json
</code></pre>
<h4 id="inputs">Inputs</h4>
<p>Here, we will specify the inputs to be used for this run, including the input image, the model location, the mean file (optional), the required size of the image, and the location of the label mapping file.</p>
<p>I downloaded an image <code>flower.jpg</code> and placed it next to the Jupyter Notebook in <code>/opt/caffe/caffe2_tutorials</code>:</p>
<pre><code class="language-python"># Configuration --- Change to your setup and preferences!
# This directory should contain the models downloaded from the model zoo. To run this 
#   tutorial, make sure there is a &#x27;squeezenet&#x27; directory at this location that 
#   contains both the &#x27;init_net.pb&#x27; and &#x27;predict_net.pb&#x27;
CAFFE_MODELS = &quot;/caffe2/caffe2/python/models&quot;

# Some sample images you can try, or use any URL to a regular image.
IMAGE_LOCATION = &quot;flower.jpg&quot;

# What model are we using?
#    Format below is the model&#x27;s: &lt;folder, INIT_NET, predict_net, mean, input image size&gt;
#    You can switch &#x27;squeezenet&#x27; out with &#x27;bvlc_alexnet&#x27;, &#x27;bvlc_googlenet&#x27; or others that you have downloaded
MODEL = &#x27;squeezenet&#x27;, &#x27;init_net.pb&#x27;, &#x27;predict_net.pb&#x27;, &#x27;ilsvrc_2012_mean.npy&#x27;, 227

# labels - these help decypher the output and source from a list from ImageNet&#x27;s object labels 
#    to provide an result like &quot;tabby cat&quot; or &quot;lemon&quot; depending on what&#x27;s in the picture 
#   you submit to the CNN.
labels =  &quot;https://gist.githubusercontent.com/aaronmarkham/cd3a6b6ac071eca6f7b4a6e40e6038aa/raw/9edb4038a37da6b5a44c3b5bc52e448ff09bfe5b/alexnet_codes&quot;
print(&quot;Config set!&quot;)
</code></pre>
<h4 id="setup-paths">Setup paths</h4>
<p>With the configs set, we can now load the mean file (if it exists), as well as the predict net and the init net.</p>
<pre><code class="language-python"># set paths and variables from model choice and prep image
CAFFE_MODELS = os.path.expanduser(CAFFE_MODELS)

# mean can be 128 or custom based on the model
# gives better results to remove the colors found in all of the training images
MEAN_FILE = os.path.join(CAFFE_MODELS, MODEL[0], MODEL[3])
if not os.path.exists(MEAN_FILE):
    print(&quot;No mean file found!&quot;)
    mean = 128
else:
    print (&quot;Mean file found!&quot;)
    mean = np.load(MEAN_FILE).mean(1).mean(1)
    mean = mean[:, np.newaxis, np.newaxis]
print(&quot;mean was set to: &quot;, mean)

# some models were trained with different image sizes, this helps you calibrate your image
INPUT_IMAGE_SIZE = MODEL[4]

# make sure all of the files are around...
INIT_NET = os.path.join(CAFFE_MODELS, MODEL[0], MODEL[1])
PREDICT_NET = os.path.join(CAFFE_MODELS, MODEL[0], MODEL[2])

# Check to see if the files exist
if not os.path.exists(INIT_NET):
    print(&quot;WARNING: &quot; + INIT_NET + &quot; not found!&quot;)
else:
    if not os.path.exists(PREDICT_NET):
        print(&quot;WARNING: &quot; + PREDICT_NET + &quot; not found!&quot;)
    else:
        print(&quot;All needed files found!&quot;)
        
</code></pre>
<h4 id="image-preprocessing">Image Preprocessing</h4>
<p>Now that we have our inputs specified and verified the existance of the input network, we can load the image and pre-processing the image for ingestion into a Caffe2 convolutional neural network! This is a very important step as the trained CNN requires a specifically sized input image whose values are from a particular distribution.</p>
<pre><code class="language-python"># Function to crop the center cropX x cropY pixels from the input image
def crop_center(img,cropx,cropy):
    y,x,c = img.shape
    startx = x//2-(cropx//2)
    starty = y//2-(cropy//2)    
    return img[starty:starty+cropy,startx:startx+cropx]

# Function to rescale the input image to the desired height and/or width. This function will preserve
#   the aspect ratio of the original image while making the image the correct scale so we can retrieve
#   a good center crop. This function is best used with center crop to resize any size input images into
#   specific sized images that our model can use.
def rescale(img, input_height, input_width):
    # Get original aspect ratio
    aspect = img.shape[1]/float(img.shape[0])
    if(aspect&gt;1):
        # landscape orientation - wide image
        res = int(aspect * input_height)
        imgScaled = skimage.transform.resize(img, (input_width, res))
    if(aspect&lt;1):
        # portrait orientation - tall image
        res = int(input_width/aspect)
        imgScaled = skimage.transform.resize(img, (res, input_height))
    if(aspect == 1):
        imgScaled = skimage.transform.resize(img, (input_width, input_height))
    return imgScaled

# Load the image as a 32-bit float
#    Note: skimage.io.imread returns a HWC ordered RGB image of some size
img = skimage.img_as_float(skimage.io.imread(IMAGE_LOCATION)).astype(np.float32)
print(&quot;Original Image Shape: &quot; , img.shape)

# Rescale the image to comply with our desired input size. This will not make the image 227x227
#    but it will make either the height or width 227 so we can get the ideal center crop.
img = rescale(img, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE)
print(&quot;Image Shape after rescaling: &quot; , img.shape)
pyplot.figure()
pyplot.imshow(img)
pyplot.title(&#x27;Rescaled image&#x27;)

# Crop the center 227x227 pixels of the image so we can feed it to our model
img = crop_center(img, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE)
print(&quot;Image Shape after cropping: &quot; , img.shape)
pyplot.figure()
pyplot.imshow(img)
pyplot.title(&#x27;Center Cropped&#x27;)

# switch to CHW (HWC --&gt; CHW)
img = img.swapaxes(1, 2).swapaxes(0, 1)
print(&quot;CHW Image Shape: &quot; , img.shape)

pyplot.figure()
for i in range(3):
    # For some reason, pyplot subplot follows Matlab&#x27;s indexing
    # convention (starting with 1). Well, we&#x27;ll just follow it...
    pyplot.subplot(1, 3, i+1)
    pyplot.imshow(img[i])
    pyplot.axis(&#x27;off&#x27;)
    pyplot.title(&#x27;RGB channel %d&#x27; % (i+1))

# switch to BGR (RGB --&gt; BGR)
img = img[(2, 1, 0), :, :]

# remove mean for better results
img = img * 255 - mean

# add batch size axis which completes the formation of the NCHW shaped input that we want
img = img[np.newaxis, :, :, :].astype(np.float32)

print(&quot;NCHW image (ready to be used as input): &quot;, img.shape)
</code></pre>
<p><img alt="Introduction to Caffe2" src="/assets/images/introduction-to-pytorch-caffe_03-9d8db0f1a0330a8c16d2d9bc0db5f48f.png" width="382" height="672"></p>
<h3 id="prepare-the-cnn-and-run-the-net">Prepare the CNN and run the net!</h3>
<p>Now that the image is ready to be ingested by the CNN, let&#x27;s open the protobufs, load them into the workspace, and run the net.</p>
<pre><code class="language-python"># Read the contents of the input protobufs into local variables
with open(INIT_NET, &quot;rb&quot;) as f:
    init_net = f.read()
with open(PREDICT_NET, &quot;rb&quot;) as f:
    predict_net = f.read()

# Initialize the predictor from the input protobufs
p = workspace.Predictor(init_net, predict_net)

# Run the net and return prediction
NCHW_batch = np.zeros((1,3,227,227))
NCHW_batch[0] = img
results = p.run([NCHW_batch.astype(np.float32)])
# Turn it into something we can play with and examine which is in a multi-dimensional array
results = np.asarray(results)
print(&quot;results shape: &quot;, results.shape)

# Quick way to get the top-1 prediction result
# Squeeze out the unnecessary axis. This returns a 1-D array of length 1000
preds = np.squeeze(results)
# Get the prediction and the confidence by finding the maximum value and index of maximum value in preds array
curr_pred, curr_conf = max(enumerate(preds), key=operator.itemgetter(1))
print(&quot;Prediction: &quot;, curr_pred)
print(&quot;Confidence: &quot;, curr_conf)
</code></pre>
<ul>
<li><strong>Prediction</strong>:  <code>723</code></li>
<li><strong>Confidence</strong>:  <code>0.910284</code></li>
</ul>
<h4 id="process-results">Process Results</h4>
<p>Recall ImageNet is a 1000 class dataset and observe that it is no coincidence that the third axis of results is length 1000. This axis is holding the probability for each category in the pre-trained model. So when you look at the results array at a specific index, the number can be interpreted as the probability that the input belongs to the class corresponding to that index. Now that we have run the predictor and collected the results, we can interpret them by matching them to their corresponding english labels.</p>
<pre><code class="language-python"># the rest of this is digging through the results 
results = np.delete(results, 1)
index = 0
highest = 0
arr = np.empty((0,2), dtype=object)
arr[:,0] = int(10)
arr[:,1:] = float(10)
for i, r in enumerate(results):
    # imagenet index begins with 1!
    i=i+1
    arr = np.append(arr, np.array([[i,r]]), axis=0)
    if (r &gt; highest):
        highest = r
        index = i 

# top N results
N = 5
topN = sorted(arr, key=lambda x: x[1], reverse=True)[:N]
print(&quot;Raw top {} results: {}&quot;.format(N,topN))

# Isolate the indexes of the top-N most likely classes
topN_inds = [int(x[0]) for x in topN]
print(&quot;Top {} classes in order: {}&quot;.format(N,topN_inds))

# Now we can grab the code list and create a class Look Up Table
response = urllib2.urlopen(labels)
class_LUT = []
for line in response:
    code, result = line.partition(&quot;:&quot;)[::2]
    code = code.strip()
    result = result.replace(&quot;&#x27;&quot;, &quot;&quot;)
    if code.isdigit():
        class_LUT.append(result.split(&quot;,&quot;)[0][1:])
        
# For each of the top-N results, associate the integer result with an actual class
for n in topN:
    print(&quot;Model predicts &#x27;{}&#x27; with {}% confidence&quot;.format(class_LUT[int(n[0])],float(&quot;{0:.2f}&quot;.format(n[1]*100))))

</code></pre>
<pre><code class="language-bash">Raw top 5 results: [array([723.0, 0.9102839827537537], dtype=object), array([968.0, 0.017375782132148743], dtype=object), array([719.0, 0.010471619665622711], dtype=object), array([985.0, 0.009765725582838058], dtype=object), array([767.0, 0.006287392228841782], dtype=object)]
</code></pre>
<ul>
<li><strong>Top 5</strong> classes in order: <code>[723, 968, 719, 985, 767]</code>
<ul>
<li><strong>Model predicts</strong>: <code>pinwheel</code> with <code>91.03%</code> confidence</li>
<li><strong>Model predicts</strong>: <code>cup</code> with <code>1.74%</code> confidence</li>
<li><strong>Model predicts</strong>: <code>piggy</code>bank&#x27; with <code>1.05%</code> confidence</li>
<li><strong>Model predicts</strong>: <code>daisy</code> with <code>0.98%</code> confidence</li>
<li><strong>Model predicts</strong>: <code>rubber eraser</code> with <code>0.63%</code> confidence</li>
</ul>
</li>
</ul>
<h4 id="feeding-larger-batches">Feeding Larger Batches</h4>
<p>Above is an example of how to feed one image at a time. We can achieve higher throughput if we feed multiple images at a time in a single batch. Recall, the data fed into the classifier is in &#x27;NCHW&#x27; order, so to feed multiple images, we will expand the &#x27;N&#x27; axis.</p>
<pre><code class="language-python"># List of input images to be fed
images = [&quot;images/cowboy-hat.jpg&quot;,
            &quot;images/cell-tower.jpg&quot;,
            &quot;images/Ducreux.jpg&quot;,
            &quot;images/pretzel.jpg&quot;,
            &quot;images/orangutan.jpg&quot;,
            &quot;images/aircraft-carrier.jpg&quot;,
            &quot;images/cat.jpg&quot;]

# Allocate space for the batch of formatted images
NCHW_batch = np.zeros((len(images),3,227,227))
print (&quot;Batch Shape: &quot;,NCHW_batch.shape)

# For each of the images in the list, format it and place it in the batch
for i,curr_img in enumerate(images):
    img = skimage.img_as_float(skimage.io.imread(curr_img)).astype(np.float32)
    img = rescale(img, 227, 227)
    img = crop_center(img, 227, 227)
    img = img.swapaxes(1, 2).swapaxes(0, 1)
    img = img[(2, 1, 0), :, :]
    img = img * 255 - mean
    NCHW_batch[i] = img

print(&quot;NCHW image (ready to be used as input): &quot;, NCHW_batch.shape)

# Run the net on the batch
results = p.run([NCHW_batch.astype(np.float32)])

# Turn it into something we can play with and examine which is in a multi-dimensional array
results = np.asarray(results)

# Squeeze out the unnecessary axis
preds = np.squeeze(results)
print(&quot;Squeezed Predictions Shape, with batch size {}: {}&quot;.format(len(images),preds.shape))

# Describe the results
for i,pred in enumerate(preds):
    print(&quot;Results for: &#x27;{}&#x27;&quot;.format(images[i]))
    # Get the prediction and the confidence by finding the maximum value 
    #   and index of maximum value in preds array
    curr_pred, curr_conf = max(enumerate(pred), key=operator.itemgetter(1))
    print(&quot;\tPrediction: &quot;, curr_pred)
    print(&quot;\tClass Name: &quot;, class_LUT[int(curr_pred)])
    print(&quot;\tConfidence: &quot;, curr_conf)
</code></pre>
<ul>
<li><strong>Batch Shape</strong>:  <code>(8, 3, 227, 227)</code></li>
<li><strong>NCHW image</strong> (ready to be used as input):  (8, 3, 227, 227)</li>
<li><strong>Squeezed Predictions Shape</strong>, with batch size 8: (8, 1000)</li>
<li><strong>Results for</strong>: &#x27;images/cowboy-hat.jpg&#x27;<!-- -->
<ul>
<li>Prediction:  515</li>
<li>Class Name:  cowboy hat</li>
<li>Confidence:  0.850092</li>
</ul>
</li>
<li><strong>Results for</strong>: <code>images/cell-tower.jpg</code>
<ul>
<li>Prediction:  645</li>
<li>Class Name:  maypole</li>
<li>Confidence:  0.185843</li>
</ul>
</li>
<li><strong>Results for</strong>: <code>images/Ducreux.jpg</code>
<ul>
<li>Prediction:  568</li>
<li>Class Name:  fur coat</li>
<li>Confidence:  0.102531</li>
</ul>
</li>
<li><strong>Results for</strong>: <code>images/pretzel.jpg</code>
<ul>
<li>Prediction:  932</li>
<li>Class Name:  pretzel</li>
<li>Confidence:  0.999622</li>
</ul>
</li>
<li><strong>Results for</strong>: <code>images/orangutan.jpg</code>
<ul>
<li>Prediction:  365</li>
<li>Class Name:  orangutan</li>
<li>Confidence:  0.992006</li>
</ul>
</li>
<li><strong>Results for</strong>: <code>images/aircraft-carrier.jpg</code>
<ul>
<li>Prediction:  403</li>
<li>Class Name:  aircraft carrier</li>
<li>Confidence:  0.999878</li>
</ul>
</li>
<li><strong>Results for</strong>: <code>images/cat.jpg</code>
<ul>
<li>Prediction:  281</li>
<li>Class Name:  tabby</li>
<li>Confidence:  0.513315</li>
</ul>
</li>
<li><strong>Results for</strong>: <code>images/flower.jpg</code>
<ul>
<li>Prediction:  985</li>
<li>Class Name:  daisy</li>
<li>Confidence:  0.982227</li>
</ul>
</li>
</ul>
<h2 id="loading-datasets">Loading Datasets</h2>
<p>So Caffe2 uses a binary DB format to store the data that we would like to train models on. A Caffe2 DB is a glorified name of a key-value storage where the keys are usually randomized so that the batches are approximately i.i.d. The values are the real stuff here: they contain the serialized strings of the specific data formats that you would like your training algorithm to ingest. So, the stored DB would look (semantically) like this:</p>
<p><code>key1 value1</code>, <code>key2 value2</code>, <code>key3 value3</code> ...</p>
<p>To a DB, it treats the keys and values as strings, but you probably want structured contents. One way to do this is to use a TensorProtos protocol buffer: it essentially wraps Tensors, aka multi-dimensional arrays, together with the tensor data type and shape information. Then, one can use the TensorProtosDBInput operator to load the data into an SGD training fashion.</p>
<p>Here, we will show you one example of how to create your own dataset. To this end, we will use the UCI Iris dataset - which was a very popular classical dataset for classifying Iris flowers. It contains 4 real-valued features representing the dimensions of the flower, and classifies things into 3 types of Iris flowers. The dataset can be downloaded <a href="https://archive.ics.uci.edu/ml/datasets/Iris">here</a>.</p>
<pre><code class="language-python"># First let&#x27;s import some necessities
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

%matplotlib inline
import urllib2 # for downloading the dataset from the web.
import numpy as np
from matplotlib import pyplot
from StringIO import StringIO
from caffe2.python import core, utils, workspace
from caffe2.proto import caffe2_pb2
</code></pre>
<pre><code class="language-python">f = urllib2.urlopen(&#x27;https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&#x27;)
raw_data = f.read()
print(&#x27;Raw data looks like this:&#x27;)
print(raw_data[:100] + &#x27;...&#x27;)
</code></pre>
<pre><code class="language-bash">Raw data looks like this:
5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,...
</code></pre>
<pre><code class="language-python"># load the features to a feature matrix.
features = np.loadtxt(StringIO(raw_data), dtype=np.float32, delimiter=&#x27;,&#x27;, usecols=(0, 1, 2, 3))
# load the labels to a feature matrix
label_converter = lambda s : {&#x27;Iris-setosa&#x27;:0, &#x27;Iris-versicolor&#x27;:1, &#x27;Iris-virginica&#x27;:2}[s]
labels = np.loadtxt(StringIO(raw_data), dtype=np.int, delimiter=&#x27;,&#x27;, usecols=(4,), converters={4: label_converter})
</code></pre>
<p>Before we do training, one thing that is often beneficial is to separate the dataset into training and testing. In this case, let&#x27;s randomly shuffle the data, use the first 100 data points to do training, and the remaining 50 to do testing. For more sophisticated approaches, you can use e.g. cross validation to separate your dataset into multiple training and testing splits. Read more about cross validation <a href="http://scikit-learn.org/stable/modules/cross_validation.html">here</a>.</p>
<pre><code class="language-python">random_index = np.random.permutation(150)
features = features[random_index]
labels = labels[random_index]

train_features = features[:100]
train_labels = labels[:100]
test_features = features[100:]
test_labels = labels[100:]
</code></pre>
<pre><code class="language-python"># Let&#x27;s plot the first two features together with the label.
# Remember, while we are plotting the testing feature distribution
# here too, you might not be supposed to do so in real research,
# because one should not peek into the testing data.
legend = [&#x27;rx&#x27;, &#x27;b+&#x27;, &#x27;go&#x27;]
pyplot.title(&quot;Training data distribution, feature 0 and 1&quot;)
for i in range(3):
    pyplot.plot(train_features[train_labels==i, 0], train_features[train_labels==i, 1], legend[i])
pyplot.figure()
pyplot.title(&quot;Testing data distribution, feature 0 and 1&quot;)
for i in range(3):
    pyplot.plot(test_features[test_labels==i, 0], test_features[test_labels==i, 1], legend[i])
</code></pre>
<p><img alt="Introduction to Caffe2" src="/assets/images/introduction-to-pytorch-caffe_04-29e0161d91e68b2ef667be9a6ddaac32.png" width="388" height="540"></p>
<p>Now, as promised, let&#x27;s put things into a Caffe2 DB. In this DB, what would happen is that we will use &quot;train_xxx&quot; as the key, and use a TensorProtos object to store two tensors for each data point: one as the feature and one as the label. We will use Caffe2&#x27;s Python DB interface to do so.</p>
<pre><code class="language-python"># First, let&#x27;s see how one can construct a TensorProtos protocol buffer from numpy arrays.
feature_and_label = caffe2_pb2.TensorProtos()
feature_and_label.protos.extend([
    utils.NumpyArrayToCaffe2Tensor(features[0]),
    utils.NumpyArrayToCaffe2Tensor(labels[0])])
print(&#x27;This is what the tensor proto looks like for a feature and its label:&#x27;)
print(str(feature_and_label))
print(&#x27;This is the compact string that gets written into the db:&#x27;)
print(feature_and_label.SerializeToString())
</code></pre>
<pre><code class="language-python"># Now, actually write the db.

def write_db(db_type, db_name, features, labels):
    db = core.C.create_db(db_type, db_name, core.C.Mode.write)
    transaction = db.new_transaction()
    for i in range(features.shape[0]):
        feature_and_label = caffe2_pb2.TensorProtos()
        feature_and_label.protos.extend([
            utils.NumpyArrayToCaffe2Tensor(features[i]),
            utils.NumpyArrayToCaffe2Tensor(labels[i])])
        transaction.put(
            &#x27;train_%03d&#x27;.format(i),
            feature_and_label.SerializeToString())
    # Close the transaction, and then close the db.
    del transaction
    del db

write_db(&quot;minidb&quot;, &quot;iris_train.minidb&quot;, train_features, train_labels)
write_db(&quot;minidb&quot;, &quot;iris_test.minidb&quot;, test_features, test_labels)
</code></pre>
<p>Now, let&#x27;s create a very simple network that only consists of one single TensorProtosDBInput operator, to showcase how we load data from the DB that we created. For training, you might want to do something more complex: creating a network, train it, get the model, and run the prediction service. To this end you can look at the MNIST tutorial for details.</p>
<pre><code class="language-python">net_proto = core.Net(&quot;example_reader&quot;)
dbreader = net_proto.CreateDB([], &quot;dbreader&quot;, db=&quot;iris_train.minidb&quot;, db_type=&quot;minidb&quot;)
net_proto.TensorProtosDBInput([dbreader], [&quot;X&quot;, &quot;Y&quot;], batch_size=16)

print(&quot;The net looks like this:&quot;)
print(str(net_proto.Proto()))
</code></pre>
<pre><code class="language-python">workspace.CreateNet(net_proto)
</code></pre>
<pre><code class="language-python"># Let&#x27;s run it to get batches of features.
workspace.RunNet(net_proto.Proto().name)
print(&quot;The first batch of feature is:&quot;)
print(workspace.FetchBlob(&quot;X&quot;))
print(&quot;The first batch of label is:&quot;)
print(workspace.FetchBlob(&quot;Y&quot;))

# Let&#x27;s run again.
workspace.RunNet(net_proto.Proto().name)
print(&quot;The second batch of feature is:&quot;)
print(workspace.FetchBlob(&quot;X&quot;))
print(&quot;The second batch of label is:&quot;)
print(workspace.FetchBlob(&quot;Y&quot;))
</code></pre>
<h2 id="image-loading-and-preprocessing">Image Loading and Preprocessing</h2>
<p>In this tutorial we&#x27;re going to look at how we can load in images from a local file or a URL which you can then utilize in other tutorials or examples. Also, we&#x27;re going to go in depth on the kinds of preprocessing that is necessary to utilize Caffe2 with images.</p>
<pre><code class="language-python">from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

%matplotlib inline
import skimage
import skimage.io as io
import skimage.transform 
import sys
import numpy as np
import math
from matplotlib import pyplot
import matplotlib.image as mpimg
print(&quot;Required modules imported.&quot;)
</code></pre>
<h3 id="caffe-uses-bgr-order">Caffe Uses BGR Order</h3>
<ul>
<li><strong>Test an Image</strong>
<ul>
<li>In the code block below use IMAGE_LOCATION to load what you would like to test. Just change the comment flags to go through each round of the Tutorial. In this way, you&#x27;ll get to see what happens with a variety of image formats and some tips on how you might preprocess them. If you want to try your own image, drop it in the images folder or use a remote URL. When you pick a remote URL, make it easy on yourself and try to find a URL that points to a common image file type and extension versus some long identifier or query string which might just break this next step.</li>
</ul>
</li>
<li><strong>Color Issues</strong>
<ul>
<li>Keep in mind when you load images from smartphone cameras that you may run into color formatting issues. Below we show an example of how flipping between RGB and BGR can impact an image. This would obviously throw off detection in your model. Due to legacy support of OpenCV in Caffe and how it handles images in Blue-Green-Red (BGR) order instead of the more commonly used Red-Green-Blue (RGB) order, Caffe2 also expects BGR order. In many ways this decision helps in the long run as you use different computer vision utilities and libraries, but it also can be the source of confusion.</li>
</ul>
</li>
</ul>
<pre><code class="language-python"># You can load either local IMAGE_FILE or remote URL
# For Round 1 of this tutorial, try a local image.
IMAGE_LOCATION = &#x27;flower.jpg&#x27;

# For Round 2 of this tutorial, try a URL image with a flower: 
# IMAGE_LOCATION = &quot;https://cdn.pixabay.com/photo/2015/02/10/21/28/flower-631765_1280.jpg&quot;
# IMAGE_LOCATION = &quot;images/flower.jpg&quot;

# For Round 3 of this tutorial, try another URL image with lots of people:
# IMAGE_LOCATION = &quot;https://upload.wikimedia.org/wikipedia/commons/1/18/NASA_Astronaut_Group_15.jpg&quot;
# IMAGE_LOCATION = &quot;images/astronauts.jpg&quot;

# For Round 4 of this tutorial, try a URL image with a portrait!
# IMAGE_LOCATION = &quot;https://upload.wikimedia.org/wikipedia/commons/9/9a/Ducreux1.jpg&quot;
# IMAGE_LOCATION = &quot;images/Ducreux.jpg&quot;

img = skimage.img_as_float(skimage.io.imread(IMAGE_LOCATION)).astype(np.float32)

# test color reading
# show the original image
pyplot.figure()
pyplot.subplot(1,2,1)
pyplot.imshow(img)
pyplot.axis(&#x27;on&#x27;)
pyplot.title(&#x27;Original image = RGB&#x27;)

# show the image in BGR - just doing RGB-&gt;BGR temporarily for display
imgBGR = img[:, :, (2, 1, 0)]
#pyplot.figure()
pyplot.subplot(1,2,2)
pyplot.imshow(imgBGR)
pyplot.axis(&#x27;on&#x27;)
pyplot.title(&#x27;OpenCV, Caffe2 = BGR&#x27;)
</code></pre>
<p><img alt="Introduction to Caffe2" src="/assets/images/introduction-to-pytorch-caffe_05-8408c66472fa2fe8c0c9b9180998fd24.png" width="386" height="151"></p>
<p>As you can see in the example above, the difference in order is very important to keep in mind. In the code block below we&#x27;ll be taking the image and converting to BGR order for Caffe to process it appropriately.</p>
<h3 id="caffe-prefers-chw-order">Caffe Prefers CHW Order</h3>
<ul>
<li><strong>H</strong>: Height</li>
<li><strong>W</strong>: Width</li>
<li><strong>C</strong>: Channel (as in color)</li>
</ul>
<p>Digging even deeper into how image data can be stored is the memory allocation order. You might have noticed when we first loaded the image that we forced it through some interesting transformations. These were data transformations that let us play with the image as if it were a cube. What we see is on top of the cube, and manipulating the layers below can change what we view. We can tinker with it&#x27;s underlying properties and as you saw above, swap colors quite easily.</p>
<p>For GPU processing, which is what Caffe2 excels at, this order needs to be CHW. For CPU processing, this order is generally HWC. Essentially, you&#x27;re going to want to use CHW and make sure that step is included in your image pipeline. Tweak RGB to be BGR, which is encapsulated as this &quot;C&quot; payload, then tweak HWC, the &quot;C&quot; being the very same colors you just switched around.</p>
<p>You may ask why! And the reason points to cuDNN which is what helps accelerate processing on GPUs. It uses only CHW, and we&#x27;ll sum it up by saying it is faster.</p>
<pre><code class="language-python">img_hcw = &quot;flower.jpg&quot;
img_hcw = skimage.img_as_float(skimage.io.imread(img_hcw)).astype(np.float32)
print(&quot;Image shape before HWC --&gt; CHW conversion: &quot;, img_hcw.shape)
# swapping the axes to go from HWC to CHW
# uncomment the next line and run this block!
img_chw = img_hcw.swapaxes(1, 2).swapaxes(0, 1)
print(&quot;Image shape after HWC --&gt; CHW conversion: &quot;, img_chw.shape)
# we know this is going to go wrong, so...
try:
    # Plot original
    pyplot.figure()
    pyplot.subplot(1, 2, 1)
    pyplot.imshow(img_hcw)
    pyplot.axis(&#x27;on&#x27;)
    pyplot.title(&#x27;hcw&#x27;)
    pyplot.subplot(1, 2, 2)
    pyplot.imshow(img_chw)
    pyplot.axis(&#x27;on&#x27;)
    pyplot.title(&#x27;chw&#x27;)
except:
    print(&quot;Here come bad things!&quot;)
    # TypeError: Invalid dimensions for image data
    raise 
</code></pre>
<p><img alt="Introduction to Caffe2" src="/assets/images/introduction-to-pytorch-caffe_11-4361a30172e8ad27d5c4beda1aae38ff.png" width="376" height="189"></p>
<h3 id="rotation-and-mirroring">Rotation and Mirroring</h3>
<p>This topic is usually reserved for images that are coming from a smart phone. Phones, in general, take great pictures, but do a horrible job communicating how the image was taken and what orientation it should be in. Then there&#x27;s the user who does everything under the sun with their phone&#x27;s cameras, making them do things its designer never expected. Cameras - right, because there are often two cameras and these two cameras take different sized pictures in both pixel count and aspect ratio, and not only that, they sometimes take them mirrored, and they sometimes take them in portrait and landscape modes, and sometimes they don&#x27;t bother to tell which mode they were in.</p>
<p>In many ways this is the first thing you need to evaluate in your pipeline, then look at sizing (described below), then figure out the color situation. If you&#x27;re developing for iOS, then you&#x27;re in luck, it&#x27;s going to be relatively easy. If you&#x27;re a super-hacker wizard developer with lead-lined shorts and developing for Android, then at least you have lead-lined shorts.</p>
<pre><code class="language-python"># Image came in sideways - it should be a portait image!
# How you detect this depends on the platform
# Could be a flag from the camera object
# Could be in the EXIF data
# ROTATED_IMAGE = &quot;https://upload.wikimedia.org/wikipedia/commons/8/87/Cell_Phone_Tower_in_Ladakh_India_with_Buddhist_Prayer_Flags.jpg&quot;
ROTATED_IMAGE = &quot;images/cell-tower.jpg&quot;
imgRotated = skimage.img_as_float(skimage.io.imread(ROTATED_IMAGE)).astype(np.float32)
pyplot.figure()
pyplot.imshow(imgRotated)
pyplot.axis(&#x27;on&#x27;)
pyplot.title(&#x27;Rotated image&#x27;)

# Image came in flipped or mirrored - text is backwards!
# Again detection depends on the platform
# This one is intended to be read by drivers in their rear-view mirror
# MIRROR_IMAGE = &quot;https://upload.wikimedia.org/wikipedia/commons/2/27/Mirror_image_sign_to_be_read_by_drivers_who_are_backing_up_-b.JPG&quot;
MIRROR_IMAGE = &quot;images/mirror-image.jpg&quot;
imgMirror = skimage.img_as_float(skimage.io.imread(MIRROR_IMAGE)).astype(np.float32)
pyplot.figure()
pyplot.imshow(imgMirror)
pyplot.axis(&#x27;on&#x27;)
pyplot.title(&#x27;Mirror image&#x27;)
</code></pre>
<p><img alt="Introduction to Caffe2" src="/assets/images/introduction-to-pytorch-caffe_06-bc2a796863243b31aa64346fa4c90aa3.png" width="352" height="538"></p>
<p>Let&#x27;s transform these images into something Caffe2 and the standard detection models we have around can detect. Also, this little trick might save you if, say for example, you really had to detect the cell tower but there&#x27;s no EXIF data to be found: then you&#x27;d cycle through every rotation, and every flip, spawning many derivatives of this photo and run them all through. When the percentage of confidence of detection is high enough, Bam!, you found the orientation you needed and that sneaky cell tower.</p>
<pre><code class="language-python"># Run me to flip the image back and forth
imgMirror = np.fliplr(imgMirror)
pyplot.figure()
pyplot.imshow(imgMirror)
pyplot.axis(&#x27;off&#x27;)
pyplot.title(&#x27;Mirror image&#x27;)

# Run me to rotate the image 90 degrees
imgRotated = np.rot90(imgRotated, 3)
pyplot.figure()
pyplot.imshow(imgRotated)
pyplot.axis(&#x27;off&#x27;)
pyplot.title(&#x27;Rotated image&#x27;)
</code></pre>
<p><img alt="Introduction to Caffe2" src="/assets/images/introduction-to-pytorch-caffe_07-0274f0916356f23ce9425024ada01aef.png" width="310" height="526"></p>
<h3 id="sizing">Sizing</h3>
<p>Part of preprocessing is resizing. For reasons we won&#x27;t get into here, images in the Caffe2 pipeline should be square. Also, to help with performance, they should be resized to a standard height and width which is usually going to be smaller than your original source. In the example below we&#x27;re resizing to <code>256 x 256</code> pixels, however you might notice that the input_height and input_width is set to <code>224 x 224</code> which is then used to specify the crop. This is what several image-based models are expecting. They were trained on images sized to <code>224 x 224</code> and in order for the model to properly identify the suspect images you throw at it, these should also be <code>224 x 224</code>.</p>
<pre><code class="language-python"># Model is expecting 224 x 224, so resize/crop needed.
# First, let&#x27;s resize the image to 256*256
orig_h, orig_w, _ = img.shape
print(&quot;Original image&#x27;s shape is {}x{}&quot;.format(orig_h, orig_w))
input_height, input_width = 224, 224
print(&quot;Model&#x27;s input shape is {}x{}&quot;.format(input_height, input_width))
img256 = skimage.transform.resize(img, (256, 256))

# Plot original and resized images for comparison
f, axarr = pyplot.subplots(1,2)
axarr[0].imshow(img)
axarr[0].set_title(&quot;Original Image (&quot; + str(orig_h) + &quot;x&quot; + str(orig_w) + &quot;)&quot;)
axarr[0].axis(&#x27;on&#x27;)
axarr[1].imshow(img256)
axarr[1].axis(&#x27;on&#x27;)
axarr[1].set_title(&#x27;Resized image to 256x256&#x27;)
pyplot.tight_layout()

print(&quot;New image shape:&quot; + str(img256.shape))
</code></pre>
<ul>
<li>Original image&#x27;s shape is 534x800</li>
<li>Model&#x27;s input shape is 224x224</li>
<li>New image shape:(256, 256, 3)</li>
</ul>
<p><img alt="Introduction to Caffe2" src="/assets/images/introduction-to-pytorch-caffe_08-50e9bb046f144402d8655f4bc8f195e4.png" width="427" height="212"></p>
<h3 id="rescaling">Rescaling</h3>
<p>If you imagine portait images versus landscape images you&#x27;ll know that there are a lot of things that can get messed up by doing a slopping resize. Rescaling is assuming that you&#x27;re locking down the aspect ratio to prevent distortion in the image. In this case, we&#x27;ll scale down the image to the shortest side that matches with the model&#x27;s input size.</p>
<ul>
<li><strong>Landscape</strong>: limit resize by the height</li>
<li><strong>Portrait</strong>: limit resize by the width</li>
</ul>
<pre><code class="language-python">print(&quot;Original image shape:&quot; + str(img.shape) + &quot; and remember it should be in H, W, C!&quot;)
print(&quot;Model&#x27;s input shape is {}x{}&quot;.format(input_height, input_width))
aspect = img.shape[1]/float(img.shape[0])
print(&quot;Orginal aspect ratio: &quot; + str(aspect))
if(aspect&gt;1):
    # landscape orientation - wide image
    res = int(aspect * input_height)
    imgScaled = skimage.transform.resize(img, (input_height, res))
if(aspect&lt;1):
    # portrait orientation - tall image
    res = int(input_width/aspect)
    imgScaled = skimage.transform.resize(img, (res, input_width))
if(aspect == 1):
    imgScaled = skimage.transform.resize(img, (input_height, input_width))
pyplot.figure()
pyplot.imshow(imgScaled)
pyplot.axis(&#x27;on&#x27;)
pyplot.title(&#x27;Rescaled image&#x27;)
print(&quot;New image shape:&quot; + str(imgScaled.shape) + &quot; in HWC&quot;)
</code></pre>
<ul>
<li>Original image shape:(534, 800, 3) and remember it should be in H, W, C!</li>
<li>Model&#x27;s input shape is 224x224</li>
<li>Orginal aspect ratio: 1.49812734082</li>
<li>New image shape:(224, 335, 3) in HWC</li>
</ul>
<p>At this point only one dimension is set to what the model&#x27;s input requires. We still need to crop one side to make a square.</p>
<h3 id="cropping">Cropping</h3>
<p>There are a variety of strategies we could utilize. In fact, we could backpeddle and decide to do a center crop. So instead of scaling down to the smallest we could get on at least one side, we take a chunk out of the middle. If we had done that without scaling we would have ended up with just part of a flower pedal, so we still needed some resizing of the image.</p>
<p>Below we&#x27;ll try a few strategies for cropping:</p>
<ol>
<li>Just grab the exact dimensions you need from the middle!</li>
<li>Resize to a square that&#x27;s pretty close then grab from the middle.</li>
<li>Use the rescaled image and grab the middle.</li>
</ol>
<pre><code class="language-python"># Compare the images and cropping strategies
# Try a center crop on the original for giggles
print(&quot;Original image shape:&quot; + str(img.shape) + &quot; and remember it should be in H, W, C!&quot;)
def crop_center(img,cropx,cropy):
    y,x,c = img.shape
    startx = x//2-(cropx//2)
    starty = y//2-(cropy//2)    
    return img[starty:starty+cropy,startx:startx+cropx]
# yes, the function above should match resize and take a tuple...

pyplot.figure()
# Original image
imgCenter = crop_center(img,224,224)
pyplot.subplot(1,3,1)
pyplot.imshow(imgCenter)
pyplot.axis(&#x27;on&#x27;)
pyplot.title(&#x27;Original&#x27;)

# Now let&#x27;s see what this does on the distorted image
img256Center = crop_center(img256,224,224)
pyplot.subplot(1,3,2)
pyplot.imshow(img256Center)
pyplot.axis(&#x27;on&#x27;)
pyplot.title(&#x27;Squeezed&#x27;)

# Scaled image
imgScaledCenter = crop_center(imgScaled,224,224)
pyplot.subplot(1,3,3)
pyplot.imshow(imgScaledCenter)
pyplot.axis(&#x27;on&#x27;)
pyplot.title(&#x27;Scaled&#x27;)

pyplot.tight_layout()
</code></pre>
<p><img alt="Introduction to Caffe2" src="/assets/images/introduction-to-pytorch-caffe_09-7dc84647ac6bed4679ad460f6e093213.png" width="429" height="145"></p>
<p>As you can see that didn&#x27;t work out so well, except for maybe the last one. The middle one may be just fine too, but you won&#x27;t know until you try on the model and test a lot of candidate images. At this point we can look at the difference we have, split it in half and remove some pixels from each side. This does have a drawback, however, as an off-center subject of interest would get clipped.</p>
<h3 id="upscaling">Upscaling</h3>
<p>What do you do when the images you want to run are &quot;tiny&quot;? In our example we&#x27;ve been prepping for Input Images with the spec of <code>224x224</code>. Consider this <code>128x128</code> image below.</p>
<p>The most basic approach is going from a small square to a bigger square and using the defauls skimage provides for you. This resize method defaults the interpolation order parameter to 1 which happens to be bi-linear if you even cared, but it is worth mentioning because these might be the fine-tuning knobs you need later to fix problems, such as strange visual artifacts, that can be introduced in upscaling images.</p>
<pre><code class="language-python">imgTiny = &quot;images/Cellsx128.png&quot;
imgTiny = skimage.img_as_float(skimage.io.imread(imgTiny)).astype(np.float32)
print(&quot;Original image shape: &quot;, imgTiny.shape)
imgTiny224 = skimage.transform.resize(imgTiny, (224, 224))
print(&quot;Upscaled image shape: &quot;, imgTiny224.shape)
# Plot original
pyplot.figure()
pyplot.subplot(1, 2, 1)
pyplot.imshow(imgTiny)
pyplot.axis(&#x27;on&#x27;)
pyplot.title(&#x27;128x128&#x27;)
# Plot upscaled
pyplot.subplot(1, 2, 2)
pyplot.imshow(imgTiny224)
pyplot.axis(&#x27;on&#x27;)
pyplot.title(&#x27;224x224&#x27;)
</code></pre>
<ul>
<li>Original image shape:  (128, 128, 4)</li>
<li>Upscaled image shape:  (224, 224, 4)</li>
</ul>
<p><img alt="Introduction to Caffe2" src="/assets/images/introduction-to-pytorch-caffe_10-9c35f675b41de2c662336e9747e14c73.png" width="377" height="195"></p>
<h3 id="batch-processing">Batch Processing</h3>
<p>In the last steps below we are going to switch the image&#x27;s data order to BGR, stuff that into the Color column, then reorder the columns for GPU processing (<code>HCW</code> --&gt; <code>CHW</code>) and then add a fourth dimension (N) to the image to track the number of images. In theory, you can just keep adding dimensions to your data, but this one is required for Caffe2 as it relays to Caffe how many images to expect in this batch. We set it to one (1) to indicate there&#x27;s only one image going into Caffe in this batch. Note that in the final output when we check <code>img.shape</code> the order is quite different. We&#x27;ve added N for number of images, and changed the order like so: <code>N</code>, <code>C</code>, <code>H</code>, <code>W</code>.</p>
<pre><code class="language-python"># This next line helps with being able to rerun this section
# if you want to try the outputs of the different crop strategies above
# swap out imgScaled with img (original) or img256 (squeezed)
imgCropped = crop_center(imgScaled,224,224)
print(&quot;Image shape before HWC --&gt; CHW conversion: &quot;, imgCropped.shape)
# (1) Since Caffe expects CHW order and the current image is HWC,
#     we will need to change the order.
imgCropped = imgCropped.swapaxes(1, 2).swapaxes(0, 1)
print(&quot;Image shape after HWC --&gt; CHW conversion: &quot;, imgCropped.shape)

pyplot.figure()
for i in range(3):
    # For some reason, pyplot subplot follows Matlab&#x27;s indexing
    # convention (starting with 1). Well, we&#x27;ll just follow it...
    pyplot.subplot(1, 3, i+1)
    pyplot.imshow(imgCropped[i], cmap=pyplot.cm.gray)
    pyplot.axis(&#x27;off&#x27;)
    pyplot.title(&#x27;RGB channel %d&#x27; % (i+1))

# (2) Caffe uses a BGR order due to legacy OpenCV issues, so we
#     will change RGB to BGR.
imgCropped = imgCropped[(2, 1, 0), :, :]
print(&quot;Image shape after BGR conversion: &quot;, imgCropped.shape)

# for discussion later - not helpful at this point
# (3) (Optional) We will subtract the mean image. Note that skimage loads
#     image in the [0, 1] range so we multiply the pixel values
#     first to get them into [0, 255].
#mean_file = os.path.join(CAFFE_ROOT, &#x27;python/caffe/imagenet/ilsvrc_2012_mean.npy&#x27;)
#mean = np.load(mean_file).mean(1).mean(1)
#img = img * 255 - mean[:, np.newaxis, np.newaxis]

pyplot.figure()
for i in range(3):
    # For some reason, pyplot subplot follows Matlab&#x27;s indexing
    # convention (starting with 1). Well, we&#x27;ll just follow it...
    pyplot.subplot(1, 3, i+1)
    pyplot.imshow(imgCropped[i], cmap=pyplot.cm.gray)
    pyplot.axis(&#x27;off&#x27;)
    pyplot.title(&#x27;BGR channel %d&#x27; % (i+1))
# (4) Finally, since caffe2 expect the input to have a batch term
#     so we can feed in multiple images, we will simply prepend a
#     batch dimension of size 1. Also, we will make sure image is
#     of type np.float32.
imgCropped = imgCropped[np.newaxis, :, :, :].astype(np.float32)
print(&#x27;Final input shape is:&#x27;, imgCropped.shape)
</code></pre>
<ul>
<li>Image shape before HWC --&gt; CHW conversion:  (224, 224, 3)</li>
<li>Image shape after HWC --&gt; CHW conversion:  (3, 224, 224)</li>
<li>Image shape after BGR conversion:  (3, 224, 224)</li>
<li>Final input shape is: (1, 3, 224, 224)</li>
</ul>
<p><img alt="Introduction to Caffe2" src="/assets/images/introduction-to-pytorch-caffe_12-674dbefa22615346cbb0becbda4e64ea.png" width="352" height="289"></p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/python">Python</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/machine-learning">Machine Learning</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-07-21-introduction-to-pytorch-caffe2/index.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/IoT-and-Machine-Learning/ML/2023-07-25-onnx-models/2023-07-25"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Working with ONNX Models</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/IoT-and-Machine-Learning/ML/2023-07-02-sql-in-data-science-ml/2023-07-02"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">SQL in Data Science - Machine Learning</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#setup-with-docker" class="table-of-contents__link toc-highlight">Setup with Docker</a><ul><li><a href="#testing-installation" class="table-of-contents__link toc-highlight">Testing Installation</a></li></ul></li><li><a href="#caffe-tutorial" class="table-of-contents__link toc-highlight">Caffe Tutorial</a><ul><li><a href="#caffe2-basic-concepts---operators--nets" class="table-of-contents__link toc-highlight">Caffe2 Basic Concepts - Operators &amp; Nets</a></li></ul></li><li><a href="#loading-pre-trained-models" class="table-of-contents__link toc-highlight">Loading Pre-Trained Models</a><ul><li><a href="#description" class="table-of-contents__link toc-highlight">Description</a></li><li><a href="#code" class="table-of-contents__link toc-highlight">Code</a></li><li><a href="#prepare-the-cnn-and-run-the-net" class="table-of-contents__link toc-highlight">Prepare the CNN and run the net!</a></li></ul></li><li><a href="#loading-datasets" class="table-of-contents__link toc-highlight">Loading Datasets</a></li><li><a href="#image-loading-and-preprocessing" class="table-of-contents__link toc-highlight">Image Loading and Preprocessing</a><ul><li><a href="#caffe-uses-bgr-order" class="table-of-contents__link toc-highlight">Caffe Uses BGR Order</a></li><li><a href="#caffe-prefers-chw-order" class="table-of-contents__link toc-highlight">Caffe Prefers CHW Order</a></li><li><a href="#rotation-and-mirroring" class="table-of-contents__link toc-highlight">Rotation and Mirroring</a></li><li><a href="#sizing" class="table-of-contents__link toc-highlight">Sizing</a></li><li><a href="#rescaling" class="table-of-contents__link toc-highlight">Rescaling</a></li><li><a href="#cropping" class="table-of-contents__link toc-highlight">Cropping</a></li><li><a href="#upscaling" class="table-of-contents__link toc-highlight">Upscaling</a></li><li><a href="#batch-processing" class="table-of-contents__link toc-highlight">Batch Processing</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Research</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Notebook</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/tags">Tags</a></li><li class="footer__item"><a class="footer__link-item" href="/Curriculum-Vitae">CV</a></li></ul></div><div class="col footer__col"><div class="footer__title">Social</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.linkedin.com/in/mike-polinowski-6396ba121/" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/MikePolinowski" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.flickr.com/people/149680084@N06/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Flickr<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/mpolinowski" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Mike Polinowski, INSTAR Deutschland GmbH, Shenzhen - China.</div></div></div></footer></div>
</body>
</html>